{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92a1bd03-2515-45ed-951c-7ce704da55fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Patrick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "import builtins\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from wordcloud import WordCloud\n",
    "from gensim.models import TfidfModel, Phrases, phrases, CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import gensim.models.ldamodel as LDAModel\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.corpora import MmCorpus\n",
    "pyLDAvis.enable_notebook()\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8962bdfe-d6a7-4df9-8b1f-efdaf9deae74",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scraper\n",
    "I scraped the data off of the Stanford Encyclopedia of Philosophy at random. I saved the body of each document to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76f0edc6-43ef-4496-96ba-fcc41b8b512b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nNUM_DOCS = 3000\\ndirectory = 'pages//'\\ndocs = []\\n\\ndef clean_filename(filename):\\n    filename = unidecode(filename)\\n    bad_chars = ['‘', ''']\\n    for char in bad_chars:\\n        if char in filename:\\n            print(filename)\\n            filename = filename.replace(char, '')\\n    return filename\\n\\nwhile len(os.listdir('pages//')) < NUM_DOCS:\\n    # load a page\\n    url = 'https://plato.stanford.edu/cgi-bin/encyclopedia/random'\\n    page = requests.get(url)\\n    soup = bs(page.content, 'html.parser')\\n    \\n    # Locate the title and clean it up\\n    # Every title contains the name of the site, so that is removed\\n    # Forward slashes are replaced with dashes\\n    title = ' '.join(soup.title.text.strip('\\n').split()[:-4]).replace('/', '-')\\n    title = clean_filename(title)\\n    # Save the body of the document\\n    if title not in docs:\\n        docs.append(title)\\n        body = soup.find('div', id='main-text')\\n        # A minority of pages did not have anything in the body\\n        if body:\\n            paragraphs = '\\n'.join([paragraph.text.strip('\\n') for paragraph in body.find_all('p')])\\n            with open(f'{directory+title}.txt', 'w', encoding='utf8') as f:\\n                for paragraph in paragraphs:\\n                    f.write(paragraph)\\n    count = len(os.listdir('pages//'))\\n    if not count % 500:\\n        print(count)\\n    \\n    \\n    # sleep to be nice\\n    time.sleep(3)\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "NUM_DOCS = 3000\n",
    "directory = 'pages//'\n",
    "docs = []\n",
    "\n",
    "def clean_filename(filename):\n",
    "    filename = unidecode(filename)\n",
    "    bad_chars = ['‘', '\\'']\n",
    "    for char in bad_chars:\n",
    "        if char in filename:\n",
    "            print(filename)\n",
    "            filename = filename.replace(char, '')\n",
    "    return filename\n",
    "\n",
    "while len(os.listdir('pages//')) < NUM_DOCS:\n",
    "    # load a page\n",
    "    url = 'https://plato.stanford.edu/cgi-bin/encyclopedia/random'\n",
    "    page = requests.get(url)\n",
    "    soup = bs(page.content, 'html.parser')\n",
    "    \n",
    "    # Locate the title and clean it up\n",
    "    # Every title contains the name of the site, so that is removed\n",
    "    # Forward slashes are replaced with dashes\n",
    "    title = ' '.join(soup.title.text.strip('\\n').split()[:-4]).replace('/', '-')\n",
    "    title = clean_filename(title)\n",
    "    # Save the body of the document\n",
    "    if title not in docs:\n",
    "        docs.append(title)\n",
    "        body = soup.find('div', id='main-text')\n",
    "        # A minority of pages did not have anything in the body\n",
    "        if body:\n",
    "            paragraphs = '\\n'.join([paragraph.text.strip('\\n') for paragraph in body.find_all('p')])\n",
    "            with open(f'{directory+title}.txt', 'w', encoding='utf8') as f:\n",
    "                for paragraph in paragraphs:\n",
    "                    f.write(paragraph)\n",
    "    count = len(os.listdir('pages//'))\n",
    "    if not count % 500:\n",
    "        print(count)\n",
    "    \n",
    "    \n",
    "    # sleep to be nice\n",
    "    time.sleep(3)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d86a66-92e6-496a-abaf-dfe821fba659",
   "metadata": {},
   "source": [
    "# Read in the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "890267e5-46e4-42db-a071-380fe663b534",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dir = 'pages//'\n",
    "doc_locations = []\n",
    "for file in os.listdir(doc_dir):\n",
    "    if file.split('.')[-1] == 'txt':\n",
    "        doc_locations.append(doc_dir+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e1704d3-eb47-48b6-b963-eec6051bb769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1730 documents\n"
     ]
    }
   ],
   "source": [
    "raw_docs = []\n",
    "for loc in doc_locations:\n",
    "    with open(loc, encoding='utf8') as f:\n",
    "        raw_docs.append('\\n'.join(f.readlines()))\n",
    "print(f'There are {len(raw_docs)} documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde31fdf-7455-4dda-b512-4289659fdec3",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Documents are split into tokens, with each word being a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d9c45e-b72f-4aeb-aad5-813caf6c4017",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1730/1730 [02:19<00:00, 12.39it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.word_tokenize\n",
    "tokenized_docs = [tokenizer(doc) for doc in tqdm(raw_docs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89504d21-5b08-4c50-8247-d0fa50bd84bb",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "A lemma is the \"base\" form of a word given the word's part of speech. For example, the lemma of \"running\" is \"run,\" the lemma of \"cares\" is \"care\" and so on. Lemmatization reduces words to their lemmas. Since lemmatization is part of speech dependent, each word must first be tagged with its part of speech.\n",
    "\n",
    "I use lemmatization here because I am only introduced in the meaning of words regardless of their form. For example, the difference between \"cares,\" \"caring,\" \"cared\" and so on are meaningless for this application. An added bonus is that lemmatization will reduce the number of unique words in the text.\n",
    "\n",
    "Note when tagging the part of speech, there is a catch all category of words that are not lemmatized. This catches tokens that would not be lemmatized anyway like numbers, conjunctions and determiners (\"all,\" \"every,\" etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328f26bb-279a-4829-a330-674a1775da7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 875 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██████████████████▌                                                             | 203/875 [01:47<05:55,  1.89it/s]"
     ]
    }
   ],
   "source": [
    "lemmatized_docs = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Takes an untagged document, a sequence of tokens, tags each token in the\n",
    "# document with a part of speech, and converts those tags into a form that\n",
    "# the lemmatizer can use.\n",
    "# Returns a tagged document, a list of tuples in the form (token, PoS)\n",
    "def tagger(untagged_doc):\n",
    "    tagged_doc = nltk.pos_tag(untagged_doc)\n",
    "    formatted_tags = []\n",
    "    for tok in tagged_doc:\n",
    "        unformatted_tag = tok[1]\n",
    "        # Convert POS tags into format usable by nltk\n",
    "        if unformatted_tag.startswith('N'):\n",
    "            formatted_tag = wordnet.NOUN\n",
    "        elif unformatted_tag.startswith('V'):\n",
    "            formatted_tag = wordnet.VERB\n",
    "        elif unformatted_tag.startswith('R'):\n",
    "            formatted_tag = wordnet.ADV\n",
    "        elif unformatted_tag.startswith('J'):\n",
    "            formatted_tag = wordnet.ADJ\n",
    "        # The\n",
    "        else:\n",
    "            formatted_tag = unformatted_tag\n",
    "        formatted_tags.append((tok[0], formatted_tag))\n",
    "    return formatted_tags\n",
    "        \n",
    "    \n",
    "pos_to_lemmatize = [wordnet.NOUN, wordnet.VERB, wordnet.ADV, wordnet.ADJ]\n",
    "lemmatized_docs = []\n",
    "\n",
    "# Select a subset of documents - helps with memory problems\n",
    "num_docs = 875\n",
    "random.shuffle(tokenized_docs)\n",
    "doc_subset = tokenized_docs[:875]\n",
    "print(f'Selected {len(doc_subset)} documents')\n",
    "\n",
    "# Tag and lemmatize each document\n",
    "for tokenized_doc in tqdm(doc_subset):\n",
    "    # POS tagging\n",
    "    tagged_doc = tagger(tokenized_doc)\n",
    "    # Lemmatization\n",
    "    lemmatized_doc = [lemmatizer.lemmatize(tok[0], tok[1]) for tok in tagged_doc if tok[1] in pos_to_lemmatize]\n",
    "    lemmatized_docs.append(lemmatized_doc)\n",
    "\n",
    "print(f'Lemmatized {len(lemmatized_docs)} documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0831c9a6-7dd3-484c-9432-0a0c661607d6",
   "metadata": {},
   "source": [
    "# Remove Stopwords, Punctuation, LaTeX, and Short Words.\n",
    "The guiding principle has been that I only want to keep meaningful words. As such, I remove the following categories:\n",
    "* Stopwords: words that are commonly used to the point of meaninglessness (ex. \"the,\" \"is\")\n",
    "* Punctuation: Not words and not meaningful\n",
    "* LaTeX: Not words and not meaningful\n",
    "* Short words: Unlikely to be meaningful. Most should be caught when removing stopwords, but there may be, for example, variables used in the text (ex. \"If X is true\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10af916-2d79-4142-a0fc-e11ea11aadf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEP has a lot of equations, etc. This helper function will help remove them.\n",
    "def not_latek(tok):\n",
    "    bad_chars = string.punctuation\n",
    "    # Don't want to catch, for example eighteenth-century\n",
    "    bad_chars = bad_chars.replace('-','')\n",
    "    for char in bad_chars:\n",
    "        if char in tok:\n",
    "            return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc47c316-9db5-4368-8229-d778435ce319",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_docs = []\n",
    "min_tok_length = 3\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords += string.punctuation\n",
    "bad_toks = set()\n",
    "for doc in lemmatized_docs:\n",
    "    clean_doc = []\n",
    "    for tok in doc:\n",
    "        if len(tok) > min_tok_length and tok not in stopwords and not_latek(tok):\n",
    "            clean_doc.append(tok.lower())\n",
    "    clean_docs.append(clean_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763fcb6f-a99d-4bb9-9b36-77326ba5be81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f8dd94-2265-4ecf-90b7-8f91ea54ac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for doc in tqdm(clean_docs):\n",
    "    corpus += doc\n",
    "print(f'Combined the {len(clean_docs)} docs into a single list of {len(corpus)} words.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a7838-28f5-4b72-a8d1-9f6d38d648af",
   "metadata": {},
   "source": [
    "## Document Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9321b532-b71a-4cf7-bc2f-30117224df96",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(doc) for doc in clean_docs]\n",
    "sns.histplot(lengths)\n",
    "\n",
    "print(f'The mean article length is {int(np.mean(lengths))} words.')\n",
    "print(f'The shortest article is {np.min(lengths)} and the longest is {np.max(lengths)} words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97ee88d-72bc-4a98-b8ec-8ff548d898b9",
   "metadata": {},
   "source": [
    " ## Word Frequencies\n",
    " Here, I visualize the 20 most used words across the corpus. This acts as a nice sanity check after processing the data, making sure that most of the top words are meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d761436-1039-408b-9a07-9e11d67cfd94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "counter = Counter(corpus)\n",
    "most_common = counter.most_common()\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "\n",
    "for word, count in most_common[:20]:\n",
    "    x.append(word)\n",
    "    y.append(count)\n",
    "sns.barplot(x=y, y=x, orient='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6546ca5-a50e-4cf5-a916-addd2748e05a",
   "metadata": {},
   "source": [
    "Most of the top words are the sort of meaningful words one would expect in a philosophy website. Some are fairly generic (ex. \"also\" and \"take\"), but they are the minority. I am not concerned with them unless they are over-represented as relevant terms in the LDA model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9842c298-8f5e-4336-8c1b-032e9b1d58dc",
   "metadata": {},
   "source": [
    "# Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366e82d0-a242-442c-b22e-553f0f2bfa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud()\n",
    "wc.generate(' '.join(corpus))\n",
    "plt.imshow(wc)\n",
    "plt.grid(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2acbb4-31b2-490d-8dbc-849ff2aa0578",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tf-Idf Models\n",
    "Term-Frequency-Inverse Document Frequency gives a score that quantifies the  importance of certain words. The score for a word increases for the number of times the word is used in the document, but decreases as the word is used more across different documents in the corpus. As such, terms will have higher scores if they are used frequently in relatively few documents, and those terms will presumably be important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b56b29-34a8-42b3-8dc4-77c78678f748",
   "metadata": {},
   "source": [
    "## Learning Bigrams\n",
    "An n-gram is a sequence of n consecutive tokens. For the sake of this analysis I consider the most common unigrams (single words, above) and bigrams. I will lose information about word order by using a bag of word model, so grouping together consecutive tokens will prove useful.\n",
    "\n",
    "As a rule of thumb, a bigram has a more specific meaning than a unigram. For example the word \"ninth\" tells us little to nothing - this could be in a context like \"ninth place\" or \"ninth person.\" \"Ninth century\" will be found in fewer contexts.\n",
    "\n",
    "There is no inherent reason to stop at bigrams; however, even a trigram analysis is fruitless here.To generate bigrams, I use gensim's Phrases module, which attempts to learn useful bigrams, rather than finding every bigram in a text. I also attempt to find trigrams using the library, but am unable to do so, even with the model tuned liberally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e089b76f-08d4-40df-89e1-29b5303942cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bigram_phrases = Phrases(clean_docs, min_count=1, threshold=10)\n",
    "\n",
    "bigram = phrases.Phraser(bigram_phrases)\n",
    "\n",
    "trigram = phrases.Phrases(bigram[clean_docs], min_count=1, threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064fceb8-dd67-4667-b926-caa5af8a2e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = []\n",
    "bigrams_trigrams_docs = []\n",
    "trigrams = []\n",
    "\n",
    "# Find bigrams and trigrams in each document\n",
    "for doc in clean_docs:\n",
    "    bigrams_trigrams_doc = []\n",
    "    for i in trigram[doc]:\n",
    "        if len(i.split(\"_\")) == 2:\n",
    "            bigrams.append(i)\n",
    "            bigrams_trigrams_doc.append(i)\n",
    "        elif len(i.split(\"_\")) == 3:\n",
    "            trigrams.append(i)\n",
    "            bigrams_trigrams_doc.append(i)\n",
    "        else:\n",
    "            bigrams_trigrams_doc.append(i)\n",
    "    bigrams_trigrams_docs.append(bigrams_trigrams_doc)\n",
    "print(f'Learned {len(set(bigrams))} bigrams and {len(set(trigrams))} trigrams.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fc2b06-a316-4e61-9b2c-e60b620874fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb0f8c6-4a3a-493f-86a2-542226653a9a",
   "metadata": {},
   "source": [
    "An immediate concern here is that, even with parameters set to be very accepting of phrases, we learn no trigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd99002-cba0-4048-babc-9903b7210222",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_X = 20\n",
    "\n",
    "counter = Counter(bigrams)\n",
    "most_common = counter.most_common()[:20]\n",
    "\n",
    "x = [tup[0] for tup in most_common]\n",
    "y = [tup[1] for tup in most_common]\n",
    "\n",
    "sns.barplot(x=y, y=x, orientation='horizontal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ebbc4b-0cd5-4374-9e07-cc14740a0bf1",
   "metadata": {},
   "source": [
    "The bigrams are somewhat promising with respect to their potential to discriminate between topics. Some of the topics have obviously topic-specific menaings like philosophy_science (philosophy of science) or truth_value (logic)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce45f183-008d-4143-920b-919b3115c1d7",
   "metadata": {},
   "source": [
    "### Bag of Words Vectorization\n",
    "A bag of words vectorization is a vector representation of a document (or sentence, paragraph, etc.) that does not take into account token order. Each document is represented as a vector with dimensionality equivalent to the size of the number of unique words in the corpus. Each component in the vector represents a token, and the magnitude of the component represents how many times the token is used in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b64f15-de0d-4421-b9c5-9e7021e2710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'bigram_bow.mm' not in os.listdir():\n",
    "    # Create bigram dictionary\n",
    "    bigram_dictionary = Dictionary(bigrams_trigrams_docs)\n",
    "    bigram_dictionary.save_as_text('bigram_dict.txt')\n",
    "\n",
    "    # Create bag of words models from the bigram representations of the documents\n",
    "    bigram_bow = [bigram_dictionary.doc2bow(doc) for doc in tqdm(bigrams_trigrams_docs)]\n",
    "\n",
    "    MmCorpus.serialize('bigram_bow.mm', bigram_bow)\n",
    "else:\n",
    "    bigram_bow = MmCorpus('bigram_bow.mm')\n",
    "    bigram_dictionary = Dictionary.load_from_text('bigram_dict.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a436b2fb-736f-4f83-aa45-9b2dbdc79009",
   "metadata": {},
   "source": [
    "### Building the Tf-Idf Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae92a6ca-3e15-4a54-abcc-55aff2008762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "tfidf_bigram = TfidfModel(corpus=bigram_bow, id2word=bigram_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3f429f-4a5c-4e5b-a52e-627fc55e47a2",
   "metadata": {},
   "source": [
    "#### Visualize tf-idf Score Distribution\n",
    "Here, I visualize the tf-idf score, so I can pick a reasonable score, below which words will be disposed. Note that this distribution has a very long tail. The limits placed on the x-axis cut off some outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e838e092-6600-4c23-8d35-58359cad9724",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "idx = []\n",
    "corpus_length = len(bigram_bow)\n",
    "\n",
    "with tqdm(total=corpus_length, position=0, leave=True) as pbar:\n",
    "    for i in tqdm(range(corpus_length), position=0, leave=True):\n",
    "    #for i in range(len(bigram_bow[:10])):\n",
    "        bow = bigram_bow[i]\n",
    "        tfidf = tfidf_bigram[bow]\n",
    "        for tup in tfidf:\n",
    "            idx.append(tup[0])\n",
    "            scores.append(tup[1])\n",
    "        \n",
    "hist = sns.histplot(scores,bins=1000)\n",
    "hist.set(xlim=(0,0.1))\n",
    "hist.set_xlabel('tf-df Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd0d8c8-35d9-42a6-8a81-366248863f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.02\n",
    "words = []\n",
    "missing_words = []\n",
    "corpus_length = len(bigram_bow)\n",
    "tf_idf_bow = []\n",
    "if 'tf_idf_bow.mm' not in os.listdir():\n",
    "    # Create new bag of words representations of each document, keeping\n",
    "    # only words above a certain score.\n",
    "    with tqdm(total=corpus_length, position=0, leave=True) as pbar:\n",
    "        for i in tqdm(range(corpus_length), position=0, leave=True):\n",
    "            bow = bigram_bow[i]\n",
    "            # This list holds all the words\n",
    "            tfidf_idxs = []\n",
    "            # This list holds the words with scores below the threshold\n",
    "            low_val_words = []\n",
    "            # This list holds the words to keep in the model\n",
    "            bow_idxs = []\n",
    "            for idx, value in tfidf_bigram[bow]:\n",
    "                tfidf_idxs.append(idx)\n",
    "                if value < cutoff:\n",
    "                    low_val_words.append(idx)\n",
    "                else:\n",
    "                    bow_idxs.append(idx)\n",
    "            dropped = low_val_words + missing_words\n",
    "            for item in dropped:\n",
    "                words.append(bigram_dictionary[item])\n",
    "            # Missing words are words with zero score\n",
    "            missing_words = [idx for idx in bow_idxs if idx not in tfidf_idxs]\n",
    "            # Keeps words above score threshold\n",
    "            new_bow = [w for w in bow if w[0] not in low_val_words and w[0] not in missing_words]    \n",
    "            tf_idf_bow.append(new_bow)\n",
    "        MmCorpus.serialize('tf_idf_bow.mm', tf_idf_bow)\n",
    "else:\n",
    "    tf_idf_bow = MmCorpus('tf_idf_bow.mm')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f006cf-1b67-43fe-b56a-4d204b4b881f",
   "metadata": {},
   "source": [
    "# LDA Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3708cc-29ec-4d55-9f48-69532c31bed8",
   "metadata": {},
   "source": [
    "# Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcd21e9-bc6a-4f93-941a-b39f3296eec7",
   "metadata": {},
   "source": [
    "# Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd5a84-9d3b-42f6-9e3b-3f096754cad7",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5c7463-5c78-4258-b60a-b2c819f8e15c",
   "metadata": {},
   "source": [
    "### Tune Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902ef5f0-8738-4405-bf7e-c7c9ddc5ab8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Given a number of topics, trains an LDA model if one has not been saved\n",
    "# Otherwise, loads an equivalent LDA model.\n",
    "# Returns the LDA model.\n",
    "def train_lda(topics):\n",
    "    data_size_folder = f\"{len(bigram_bow)} Documents\"\n",
    "    # Use the same name for model folders and files\n",
    "    model_folder_file = f\"lda_model_{len(bigram_bow)}_{topics}\"\n",
    "    \n",
    "    # Check if any models for a corpus of this size have been saved\n",
    "    # If not, make a folder to store those models\n",
    "    if data_size_folder not in os.listdir():\n",
    "        print(f'Making directory for corpus with {len(bigram_bow)} documents.')\n",
    "        os.mkdir(data_size_folder)\n",
    "        \n",
    "    # Check if any models with a given number of topics have been saved for a corpus of this size\n",
    "    # If not, make a folder to store the model, train the model, and save it\n",
    "    if model_folder_file not in os.listdir(data_size_folder):\n",
    "        print(f'Training a model with {topics} topics on a corpus with {len(bigram_bow)} documents.')\n",
    "        os.mkdir(os.path.join(data_size_folder, model_folder_file))\n",
    "        model_loc = os.path.join(data_size_folder,  model_folder_file, model_folder_file)\n",
    "        lda_model = LDAModel.LdaModel(corpus=tf_idf_bow,\n",
    "                                                   id2word=bigram_dictionary,\n",
    "                                                   num_topics=topics,\n",
    "                                                   chunksize=100,\n",
    "                                                   update_every=1,\n",
    "                                                   passes=100,\n",
    "                                                   alpha='auto',\n",
    "                                                   random_state=123)\n",
    "        lda_model.save(model_loc)\n",
    "    else:\n",
    "        print(f'Loading trained model for model with {topics} topics.')\n",
    "        lda_model = LDAModel.LdaModel.load(os.path.join(data_size_folder, model_folder_file, model_folder_file))\n",
    "    return lda_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1c4bff-0801-4e70-aebd-45ebffc73e70",
   "metadata": {},
   "source": [
    "##### Coherence by Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2b5d3d-1634-4f0f-b725-ba75c572c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_by_topic = [train_lda(i) for i in range(21,50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be13acf2-6d94-4d1a-a74e-125dbc7ad796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/29 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "corpus_bow = []\n",
    "for doc in bigram_bow:\n",
    "    corpus_bow += doc\n",
    "# Takes a model\n",
    "# Returns the model's CV score\n",
    "def coherence_score(lda_model):\n",
    "    coherence_model = CoherenceModel(model=lda_model,\n",
    "                                        texts=clean_docs,\n",
    "                                        coherence='c_uci',\n",
    "                                        processes=-1)\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        score = coherence_model.get_coherence()\n",
    "    return score\n",
    "\n",
    "scores = [coherence_score(model) for model in tqdm(lda_by_topic)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f176aa-e6b2-49bb-a804-78006e750d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a6006a-b977-4e1b-b7d3-bb726feb0dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(range(len(scores)), scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a641f9-ee35-469b-bbe0-dd0353193a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd07bfd8-2e41-4543-a9b0-cdbe42076530",
   "metadata": {},
   "source": [
    "### Visualization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648b8fbb-da4d-41a7-acc7-3942cd6a3e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = gensimvis.prepare(lda_model,\n",
    "                       bigram_bow,\n",
    "                       bigram_dictionary,\n",
    "                       mds='mmds',\n",
    "                       R=topics)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b769fe60-cfe2-4736-b9d8-760f51b80fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good topic numbers\n",
    "# 12!!!!!\n",
    "# 10\n",
    "# 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73e9a3a-7542-4883-962b-096f970a30aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b86dc0-3cfa-44c3-b4f1-98dc14bffb60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c3d40a-06d4-4578-907f-0420aa9fa2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb63eb44-6eb5-46d0-84c9-671a1fda0f30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
