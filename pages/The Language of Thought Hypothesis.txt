What does it mean to posit a mental language? Or to say that thinking
occurs in this language? Just how “language-like” is
Mentalese supposed to be? To address these questions, we will isolate
some core commitments that are widely shared among LOT theorists.
Folk psychology routinely explains and predicts behavior by citing
mental states, including beliefs, desires, intentions, fears, hopes,
and so on. To explain why Mary walked to the refrigerator, we might
note that she believed there was orange juice in the refrigerator and
wanted to drink orange juice. Mental states such as belief and desire
are called propositional attitudes. They can be specified
using locutions of the form
X believes that p.
X desires that p.
X intends that p.
X fears that p.
etc.
By replacing “p” with a sentence, we specify the
content of X’s mental state. Propositional attitudes have
 intentionality
 or aboutness: they are about a subject matter.
For that reason, they are often called intentional
states.
The term “propositional attitude” originates with Russell
(1918–1919 [1985]) and reflects his own preferred analysis: that
propositional attitudes are relations to
  propositions. 
 A proposition is an abstract entity that determines
a truth-condition. To illustrate, suppose John believes that
Paris is north of London. Then John’s belief is a relation to
the proposition that Paris is north of London, and this
proposition is true iff Paris is north of London. Beyond the thesis
that propositions determine truth-conditions, there is little
agreement about what propositions are like. The literature offers many
options, mainly derived from theories of Frege (1892 [1997]), Russell
(1918–1919 [1985]), and Wittgenstein (1921 [1922]).
Fodor (1981: 177–203; 1987: 16–26) proposes a theory of
propositional attitudes that assigns a central role to mental
representations. A 
  mental representation
 is a mental item with
semantic properties (such as a denotation, or a meaning, or a
truth-condition, etc.). To believe that p, or hope that
p, or intend that p, is to bear an appropriate relation
to a mental representation whose meaning is that p. For
example, there is a relation belief* between thinkers and mental
representations, where the following biconditional is true no matter
what English sentence one substitutes for “p”:
X believes that p iff there is a mental representation
S such that X believes* S and S means that
p.
More generally:
On this analysis, mental representations are the most direct objects
of propositional attitudes. A propositional attitude inherits its
semantic properties, including its truth-condition, from the mental
representation that is its object.
Proponents of (1) typically invoke 
 functionalism to analyze A*.
Each psychological relation A* is associated with a distinctive
functional role: a role that S plays within your
mental activity just in case you bear A* to S. When
specifying what it is to believe* S, for example, we might
mention how S serves as a basis for inferential reasoning, how
it interacts with desires to produce actions, and so on. Precise
functional roles are to be discovered by scientific psychology.
Following Schiffer (1981), it is common to use the term
“belief-box” as a placeholder for the functional role
corresponding to belief*: to believe* S is to place S in
your belief box. Similarly for “desire-box”, etc.
(1) is compatible with the view that propositional attitudes are
relations to propositions. One might analyze the locution
“S means that p” as involving a relation
between S and a proposition expressed by S. It would
then follow that someone who believes* S stands in a
psychologically important relation to the proposition expressed by
S. Fodor (1987: 17) adopts this approach. He combines a
commitment to mental representations with a commitment to
propositions. In contrast, Field (2001: 30–82) declines to
postulate propositions when analyzing “S means that
p”. He posits mental representations with semantic
properties, but he does not posit propositions expressed by the mental
representations.
The distinction between 
  types and tokens is crucial for understanding
(1). A mental representation is a repeatable type that can be
instantiated on different occasions. In the current literature, it is
generally assumed that a mental representation’s tokens are
neurological. For present purposes, the key point is that mental
representations are instantiated by mental events. Here we
construe the category of 
 events broadly so as to include both
occurrences (e.g., I form an intention to drink orange juice)
and enduring states (e.g., my longstanding belief that
Abraham Lincoln was president of the United States). When mental event
e instantiates representation S, we say that S is
tokened and that e is a tokening of S.
For example, if I believe that whales are mammals, then my belief (a
mental event) is a tokening of a mental representation whose meaning
is that whales are mammals.
According to Fodor (1987: 17), thinking consists in chains of mental
events that instantiate mental representations:
A paradigm example is deductive inference: I transition from
believing* the premises to believing* the conclusion. The first mental
event (my belief* in the premises) causes the second (my belief* in
the conclusion).
(1) and (2) fit together naturally as a package that one might call
the representational theory of thought (RTT). RTT postulates
mental representations that serve as the objects of propositional
attitudes and that constitute the domain of thought
 processes.[1]
RTT as stated requires qualification. There is a clear sense in which
you believe that there are no elephants on Jupiter. However, you
probably never considered the question until now. It is not plausible
that your belief box previously contained a mental representation with
the meaning that there are no elephants on Jupiter. Fodor (1987:
20–26) responds to this sort of example by restricting (1) to
core cases. Core cases are those where the propositional
attitude figures as a causally efficacious episode in a mental
process. Your tacit belief that there are no elephants on Jupiter does
not figure in your reasoning or decision-making, although it can come
to do so if the question becomes salient and you consciously judge
that there are no elephants on Jupiter. So long as the belief remains
tacit, (1) need not apply. In general, Fodor says, an intentional
mental state that is causally efficacious must involve explicit
tokening of an appropriate mental representation. In a slogan:
“No Intentional Causation without Explicit Representation”
(Fodor 1987: 25). Thus, we should not construe (1) as an attempt at
faithfully analyzing informal discourse about propositional attitudes.
Fodor does not seek to replicate folk psychological categories. He
aims to identify mental states that resemble the propositional
attitudes adduced within folk psychology, that play roughly similar
roles in mental activity, and that can support systematic
theorizing.
Dennett’s (1977 [1981]) review of The Language of
Thought raises a widely cited objection to RTT:
In a recent conversation with the designer of a chess-playing program
I heard the following criticism of a rival program: “it thinks
it should get its queen out early”. This ascribes a
propositional attitude to the program in a very useful and predictive
way, for as the designer went on to say, one can usefully count on
chasing that queen around the board. But for all the many levels of
explicit representation to be found in that program, nowhere is
anything roughly synonymous with “I should get my queen out
early” explicitly tokened. The level of analysis to which the
designer’s remark belongs describes features of the program that
are, in an entirely innocent way, emergent properties of the
computational processes that have “engineering reality”. I
see no reason to believe that the relation between belief-talk and
psychological talk will be any more direct.
In Dennett’s example, the chess-playing machine does not
explicitly represent that it should get the queen out early, yet in
some sense it acts upon a belief that it should do so. Analogous
examples arise for human cognition. For example, we often follow rules
of deductive inference without explicitly representing the rules.
To assess Dennett’s objection, we must distinguish sharply
between mental representations and rules governing the manipulation of
mental representations (Fodor 1987: 25). RTT does not require that
every such rule be explicitly represented. Some rules may be
explicitly represented—we can imagine a reasoning system that
explicitly represents deductive inference rules to which it conforms.
But the rules need not be explicitly represented. They may
merely be implicit in the system’s operations. Only when
consultation of a rule figures as a causally efficacious episode in
mental activity does RTT require that the rule be explicitly
represented. Dennett’s chess machine explicitly represents chess
board configurations and perhaps some rules for manipulating chess
pieces. It never consults any rule akin to Get the Queen out
early. For that reason, we should not expect that the machine
explicitly represents this rule even if the rule is in some sense
built into the machine’s programming. Similarly, typical
thinkers do not consult inference rules when engaging in deductive
inference. So RTT does not demand that a typical thinker explicitly
represent inference rules, even if she conforms to them and in some
sense tacitly believes that she should conform to them.
Natural language is 
 compositional: complex 
linguistic expressions are
built from simpler linguistic expressions, and the meaning of a
complex expression is a function of the meanings of its constituents
together with the way those constituents are combined.
Compositional semantics describes in a systematic way how
semantic properties of a complex expression depend upon semantic
properties of its constituents and the way those constituents are
combined. For example, the truth-condition of a conjunction is
determined as follows: the conjunction is true iff both conjuncts are
true.
Historical and contemporary LOT theorists universally agree that
Mentalese is compositional:
Compositionality of mental representations
(COMP): Mental representations have a compositional
semantics: complex representations are composed of simple
constituents, and the meaning of a complex representation depends upon
the meanings of its constituents together with the constituency
structure into which those constituents are arranged.
Clearly, mental language and natural language must differ in many
important respects. For example, Mentalese surely does not have a
phonology. It may not have a morphology either. Nevertheless, COMP
articulates a fundamental point of similarity. Just like natural
language, Mentalese contains complex symbols amenable to semantic
analysis.
What is it for one representation to be a “constituent” of
another? According to Fodor (2008: 108), “constituent structure
is a species of the part/whole relation”. Not all parts
of a linguistic expression are constituents: “John ran” is
a constituent of “John ran and Mary jumped”, but
“ran and Mary” is not a constituent because it is not
semantically interpretable. The important point for our purposes is
that all constituents are parts. When a complex representation is
tokened, so are its parts. For example, 
intending that \(P \amp Q\) requires having a sentence in your
intention box… one of whose parts is a token of the very same
type that’s in the intention box when you intend that \(P\), and
another of whose parts is a token of the very same type that’s
in the intention box when you intend that \(Q\). (Fodor 1987: 139)
More generally: mental event \(e\) instantiates a complex mental
representation only if \(e\) instantiates all of the
representation’s constituent parts. In that sense, \(e\) itself
has internal complexity.
The complexity of mental events figures crucially here, as highlighted
by Fodor in the following passage (1987: 136):
Practically everybody thinks that the objects of intentional states
are in some way complex… [For example], what you believe when
you believe that \(P \amp Q\) is… something composite, whose
elements are—as it might be—the proposition that P
and the proposition that Q. But the (putative) complexity of
the intentional object of a mental state does not, of course,
entail the complexity of the mental state itself… LOT claims
that mental states—and not just their propositional
objects—typically have constituent structure.
Many philosophers, including Frege and Russell, regard propositions as
structured entities. These philosophers apply a part/whole model to
propositions but not necessarily to mental events during which
thinkers entertain propositions. LOTH as developed by Fodor applies
the part/whole model to the mental events themselves: 
what’s at issue here is the complexity of mental events and not
merely the complexity of the propositions that are their intentional
objects. (Fodor 1987: 142) 
On this approach, a key element of LOTH is the thesis that mental
events have semantically relevant complexity.
Contemporary proponents of LOTH endorse RTT+COMP. Historical
proponents also believed something in the vicinity (Normore 1990,
2009; Panaccio 1999 [2017]), although of course they did not use
modern terminology to formulate their views. We may regard RTT+COMP as
a minimalist formulation of LOTH, bearing in mind that many
philosophers have used the phrase “language of thought
hypothesis” to denote one of the stronger theses discussed
below. As befits a minimalist formulation, RTT+COMP leaves unresolved
numerous questions about the nature, structure, and psychological role
of Mentalese expressions.
In practice, LOT theorists usually adopt a more specific view of the
compositional semantics for Mentalese. They claim that Mentalese
expressions have 
  logical form (Fodor 2008: 21). More
specifically, they claim that Mentalese contains analogues to the
familiar logical connectives (and, or, not,
if-then, some, all, the).
Iterative application of logical connectives generates complex
expressions from simpler expressions. The meaning of a logically
complex expression depends upon the meanings of its parts and upon its
logical structure. Thus, LOT theorists usually endorse a doctrine
along the following lines:
Logically structured mental representations
(LOGIC): Some mental representations have logical
structure. The compositional semantics for these mental
representations resembles the compositional semantics for logically
structured natural language expressions.
Medieval LOT theorists used syllogistic and propositional logic to
analyze the semantics of Mentalese (King 2005; Normore 1990).
Contemporary proponents instead use the predicate calculus,
which was discovered by Frege (1879 [1967]) and whose semantics was
first systematically articulated by Tarski (1933 [1983]). The view is
that Mentalese contains primitive words—including predicates,
singular terms, and logical connectives—and that these words
combine to form complex sentences governed by something like the
semantics of the predicate calculus.
The notion of a Mentalese word corresponds roughly to the
intuitive notion of a concept. In fact, Fodor (1998: 70)
construes a concept as a Mentalese word together with its denotation.
For example, a thinker has the concept of a cat only if she has in her
repertoire a Mentalese word that denotes cats.
Logical structure is just one possible paradigm for the structure of
mental representations. Human society employs a wide range of
non-sentential representations, including pictures, maps, diagrams,
and graphs. Non-sentential representations typically contain parts
arranged into a compositionally significant structure. In many cases,
it is not obvious that the resulting complex representations have
logical structure. For example, maps do not seem to contain logical
connectives (Fodor 1991: 295; Millikan 1993: 302; Pylyshyn 2003: 424–5). Nor is
it evident that they contain predicates (Camp 2018; Rescorla 2009c),
although some philosophers contend that they do (Blumson 2012; Casati
& Varzi 1999; Kulvicki 2015).
Theorists often posit mental representations that conform to COMP but
that lack logical structure. The British empiricists postulated
ideas, which they characterized in broadly imagistic terms.
They emphasized that simple ideas can combine to form complex ideas.
They held that the representational import of a complex idea depends
upon the representational import of its parts and the way those parts
are combined. So they accepted COMP or something close to it
(depending on what exactly “constituency” amounts
 to).[2]
 They did not say in much detail how compounding of ideas was supposed
to work, but imagistic structure seems to be the paradigm in at least
some passages. LOGIC plays no significant role in their
 writings.[3]
 Partly inspired by the British empiricists, Prinz (2002) and Barsalou
(1999) analyze cognition in terms of image-like representations
derived from perception. Armstrong (1973) and Braddon-Mitchell and
Jackson (2007) propose that propositional attitudes are relations not
to mental sentences but to mental maps analogous in important
respects to ordinary concrete maps.
One problem facing imagistic and cartographic theories of thought is
that propositional attitudes are often logically complex (e.g., John
believes that if Plácido Domingo does not sing then either
Gustavo Dudamel will conduct or the concert will be cancelled).
Images and maps do not seem to support logical operations: the
negation of a map is not a map; the disjunction of two maps is not a
map; similarly for other logical operations; and similarly for images.
Given that images and maps do not support logical operations, theories
that analyze thought in exclusively imagistic or cartographic terms
will struggle to explain logically complex propositional
 attitudes.[4]
There is room here for a pluralist position that allows mental
representations of different kinds: some with logical structure, some
more analogous to pictures, or maps, or diagrams, and so on. The
pluralist position is widespread within cognitive science, which
posits a range of formats for mental representation (Block 1983; Camp 2009; Johnson-Laird
2004: 187; Kosslyn 1980; McDermott 2001: 69; Pinker 2005: 7; Sloman 1978:
144–76). Fodor himself (1975: 184–195) suggests a view on
which imagistic mental representations co-exist alongside, and
interact with, logically structured Mentalese expressions.
Given the prominent role played by logical structure within historical
and contemporary discussion of Mentalese, one might take LOGIC to be
definitive of LOTH. One might insist that mental representations
comprise a mental language only if they have logical
structure. We need not evaluate the merits of this terminological
choice.
RTT concerns propositional attitudes and the mental processes in which
they figure, such as deductive inference, reasoning, decision-making,
and planning. It does not address perception, motor control,
imagination, dreaming, pattern recognition, linguistic processing, or
any other mental activity distinct from high-level cognition. Hence
the emphasis upon a language of thought: a system of mental
representations that underlie thinking, as opposed to perceiving,
imagining, etc. Nevertheless, talk about a mental language generalizes
naturally from high-level cognition to other mental phenomena.
Perception is a good example. The perceptual system
transforms proximal sensory stimulations (e.g., retinal stimulations)
into perceptual estimates of environmental conditions (e.g., estimates
of shapes, sizes, colors, locations, etc.). Helmholtz (1867 [1925])
proposed that the transition from proximal sensory input to perceptual
estimates features an unconscious inference, similar in key
respects to high-level conscious inference yet inaccessible to
consciousness. Helmholtz’s proposal is foundational to
contemporary perceptual psychology, which constructs detailed
mathematical models of unconscious perceptual inference (Knill &
Richards 1996; Rescorla 2015). Fodor (1975: 44–55) argues that
this scientific research program presupposes mental representations.
The representations participate in unconscious inferences or
inference-like transitions executed by the perceptual
 system.[5]
Navigation is another good example. Tolman (1948)
hypothesized that rats navigate using cognitive maps: mental
representations that represent the layout of the spatial environment.
The cognitive map hypothesis, advanced during the heyday of
behaviorism, initially encountered great scorn. It remained a fringe
position well into the 1970s, long after the demise of behaviorism.
Eventually, mounting behavioral and neurophysiological evidence won it
many converts (Gallistel 1990; Gallistel & Matzel 2013; Jacobs
& Menzel 2014; O’Keefe & Nadel 1978; Weiner et al.
2011). Although a few researchers remain skeptical (Mackintosh 20002),
there is now a broad consensus that mammals (and possibly even some
insects) navigate using mental representations of spatial layout.
Rescorla (2017b) summarizes the case for cognitive maps and reviews
some of their core properties.
To what extent should we expect perceptual representations and
cognitive maps to resemble the mental representations that figure in
high-level human thought? It is generally agreed that all these mental
representations have compositional structure. For example, the
perceptual system can bind together a representation of shape and a
representation of size to form a complex representation that an object
has a certain shape and size; the representational import of the
complex representation depends in a systematic way upon the
representational import of the component representations. On the other
hand, it is not clear that perceptual representations have anything
resembling logical structure, including even predicative
structure (Burge 2010: 540–544; Fodor 2008: 169–195). Nor
is it evident that cognitive maps contain logical connectives or
predicates (Rescorla 2009a, 2009b). Perceptual processing and
non-human navigation certainly do not seem to instantiate mental
processes that would exploit putative logical structure. In
particular, they do not seem to instantiate deductive inference.
These observations provide ammunition for pluralism about
representational format. Pluralists can posit one system of
compositionally structured mental representations for perception,
another for navigation, another for high-level cognition, and so on.
Different representational systems potentially feature different
compositional mechanisms. As indicated in
 section 1.3,
 pluralism figures prominently in contemporary cognitive science.
Pluralists face some pressing questions. Which compositional
mechanisms figure in which psychological domains? Which
representational formats support which mental operations? How do
different representational formats interface with each other? Further
research bridging philosophy and cognitive science is needed to
address such questions.
Modern proponents of LOTH typically endorse the 
 computational theory of mind
 (CTM), which claims that the mind is a computational system.
Some authors use the phrase “language of thought
hypothesis” so that it definitionally includes CTM as one
component.
In a seminal contribution, Turing (1936) introduced what is now called
the 
  Turing machine: an abstract model
 of an idealized computing
device. A Turing machine contains a central processor, governed by
precise mechanical rules, that manipulates symbols inscribed along a
linear array of memory locations. Impressed by the enormous power of
the Turing machine formalism, many researchers seek to construct
computational models of core mental processes, including reasoning,
decision-making, and problem solving. This enterprise bifurcates into
two main branches. The first branch is artificial
intelligence (AI), which aims to build “thinking
machines”. Here the goal is primarily an engineering
one—to build a system that instantiates or at least simulates
thought—without any pretense at capturing how the human mind
works. The second branch, computational psychology, aims to
construct computational models of human mental activity. AI and
computational psychology both emerged in the 1960s as crucial elements
in the new interdisciplinary initiative 
 cognitive science, which
studies the mind by drawing upon psychology, computer science
(especially AI), linguistics, philosophy, economics (especially game
theory and behavioral economics), anthropology, and neuroscience.
From the 1960s to the early 1980s, computational models offered within
psychology were mainly Turing-style models. These models embody a
viewpoint known as the classical computational theory of mind
(CCTM). According to CCTM, the mind is a computational system similar
in important respects to a Turing machine, and certain core mental
processes are computations similar in important respects to
computations executed by a Turing machine.
CCTM fits together nicely with RTT+COMP. Turing-style computation
operates over symbols, so any Turing-style mental computations must
operate over mental symbols. The essence of RTT+COMP is postulation of
mental symbols. Fodor (1975, 1981) advocates RTT+COMP+CCTM. He holds
that certain core mental processes are Turing-style computations over
Mentalese expressions.
One can endorse RTT+COMP without endorsing CCTM. By positing a system
of compositionally structured mental representations, one does not
commit oneself to saying that operations over the representations are
computational. Historical LOT theorists could not even
formulate CCTM, for the simple reason that the Turing formalism had
not been discovered. In the modern era, Harman (1973) and Sellars
(1975) endorse something like RTT+COMP but not CCTM. Horgan and
Tienson (1996) endorse RTT+COMP+CTM but not CCTM, i.e.,
classical CTM. They favor a version of CTM grounded in
  connectionism, an alternative
 computational framework that differs
quite significantly from Turing’s approach. Thus, proponents of
RTT+COMP need not accept that mental activity instantiates
Turing-style computation.
Fodor (1981) combines RTT+COMP+CCTM with a view that one might call
the formal-syntactic conception of computation (FSC).
According to FSC, computation manipulates symbols in virtue of their
formal syntactic properties but not their semantic properties.
FSC draws inspiration from modern logic, which emphasizes the
formalization of deductive reasoning. To formalize, we
specify a formal language whose component linguistic
expressions are individuated non-semantically (e.g., by their
geometric shapes). We describe the expressions as pieces of formal
syntax, without considering what if anything the expressions mean. We
then specify inference rules in syntactic, non-semantic
terms. Well-chosen inference rules will carry true premises to true
conclusions. By combining formalization with Turing-style computation,
we can build a physical machine that manipulates symbols based solely
on the formal syntax of the symbols. If we program the machine to
implement appropriate inference rules, then its syntactic
manipulations will transform true premises into true conclusions.
CCTM+FSC says that the mind is a formal syntactic computing system:
mental activity consists in computation over symbols with formal
syntactic properties; computational transitions are sensitive to the
symbols’ formal syntactic properties but not their semantic
properties. The key term “sensitive” is rather imprecise,
allowing some latitude as to the precise import of CCTM+FSC.
Intuitively, the picture is that a mental symbol’s formal syntax
rather than its semantics determines how mental computation
manipulates it. The mind is a “syntactic engine”.
Fodor (1987: 18–20) argues that CCTM+FSC helps illuminate a
crucial feature of cognition: semantic coherence. For the
most part, our thinking does not move randomly from thought to
thought. Rather, thoughts are causally connected in a way that
respects their semantics. For example, deductive inference carries
true beliefs to true beliefs. More generally, thinking tends to
respect epistemic properties such as warrant and degree of
confirmation. In some sense, then, our thinking tends to cohere with
semantic relations among thoughts. How is semantic coherence achieved?
How does our thinking manage to track semantic properties? CCTM+FSC
gives one possible answer. It shows how a physical system operating in
accord with physical laws can execute computations that coherently
track semantic properties. By treating the mind as a syntax-driven
machine, we explain how mental activity achieves semantic coherence.
We thereby answer the question: How is rationality mechanically
possible?
Fodor’s argument convinced many researchers that CCTM+FSC
decisively advances our understanding of the mind’s relation to
the physical world. But not everyone agrees that CCTM+FSC adequately
integrates semantics into the causal order. A common worry is that the
formal syntactic picture veers dangerously close to
epiphenomenalism (Block 1990; Kazez 1994). Pre-theoretically,
semantic properties of mental states seem highly relevant to mental
and behavioral outcomes. For example, if I form an intention to walk
to the grocery store, then the fact that my intention concerns the
grocery store rather than the post office helps explain why I walk to
the grocery store rather than the post office. Burge (2010) and
Peacocke (1994) argue that cognitive science theorizing likewise
assigns causal and explanatory importance to semantic properties. The
worry is that CCTM+FSC cannot accommodate the causal and explanatory
importance of semantic properties because it depicts them as causally
irrelevant: formal syntax, not semantics, drives mental computation
forward. Semantics looks epiphenomenal, with syntax doing all the work
(Stich 1983).
Fodor (1990, 1994) expends considerable energy trying to allay
epiphenomenalist worries. Advancing a detailed theory of the relation
between Mentalese syntax and Mentalese semantics, he insists that FSC
can honor the causal and explanatory relevance of semantic properties.
Fodor’s treatment is widely regarded as problematic (Arjo 1996;
Aydede 1997b, 1998; Aydede & Robbins 2001; Perry 1998; Prinz 2011;
Wakefield 2002), although Rupert (2008) and Schneider (2005) espouse
somewhat similar positions.
Partly in response to epiphenomenalist worries, some authors recommend
that we replace FSC with an alternative semantic conception
of computation (Block 1990; Burge 2010: 95–101; Figdor 2009;
O’Brien & Opie 2006; Peacocke 1994, 1999; Rescorla 2012a).
Semantic computationalists claim that computational transitions are
sometimes sensitive to semantic properties, perhaps in addition to
syntactic properties. More specifically, semantic computationalists
insist that mental computation is sometimes sensitive to
semantics. Thus, they reject any suggestion that the mind is a
“syntactic engine” or that mental computation is sensitive
only to formal
 syntax.[6]
 To illustrate, consider Mentalese conjunction. This mental symbol
expresses the truth-table for conjunction. According to semantic
computationalists, the symbol’s meaning is relevant (both
causally and explanatorily) to mechanical operations over it. That the
symbol expresses the truth-table for conjunction rather than, say,
disjunction influences the course of computation. We should therefore
reject any suggestion that mental computation is sensitive to the
symbol’s syntactic properties rather than its semantic
properties. The claim is not that mental computation explicitly
represents semantic properties of mental symbols. All parties
agree that, in general, it does not. There is no homunculus inside
your head interpreting your mental language. The claim is rather that
semantic properties influence how mental computation proceeds.
(Compare: the momentum of a baseball thrown at a window causally
influences whether the window breaks, even though the window does not
explicitly represent the baseball’s momentum.)
Proponents of the semantic conception differ as to how exactly they
gloss the core claim that some computations are
“sensitive” to semantic properties. They also differ in
their stance towards CCTM. Block (1990) and Rescorla (2014a) focus upon CCTM. They argue
that a symbol’s semantic properties can impact mechanical
operations executed by a Turing-style computational system. In
contrast, O’Brien and Opie (2006) favor connectionism over
CCTM.
Theorists who reject FSC must reject Fodor’s explanation of
semantic coherence. What alternative explanation might they offer? So
far, the question has received relatively little attention. Rescorla
(2017a) argues that semantic computationalists can explain semantic
coherence and simultaneously avoid epiphenomenalist worries by
invoking neural implementation of semantically-sensitive mental
computations.
Fodor’s exposition sometimes suggests that CTM, CCTM, or
CCTM+FSC is definitive of LOTH (1981: 26). Yet not everyone who
endorses RTT+COMP endorses CTM, CCTM, or FSC. One can postulate a
mental language without agreeing that mental activity is
computational, and one can postulate mental computations over a mental
language without agreeing that the computations are sensitive only to
syntactic properties. For most purposes, it is not important whether
we regard CTM, CCTM, or CCTM+FSC as definitive of LOTH. More important
is that we track the distinctions among the doctrines.
The literature offers many arguments for LOTH. This section introduces
four influential arguments, each of which supports LOTH abductively by
citing its explanatory benefits.
 Section 5
 discusses some prominent objections to the four arguments.
Fodor (1975) defends RTT+COMP+CCTM by appealing to scientific
practice: our best cognitive science postulates Turing-style mental
computations over Mentalese expressions; therefore, we should accept
that mental computation operates over Mentalese expressions. Fodor
develops his argument by examining detailed case studies, including
perception, decision-making, and linguistic comprehension. He argues
that, in each case, computation over mental representations plays a
central explanatory role. Fodor’s argument was widely heralded
as a compelling analysis of then-current cognitive science.
When evaluating cognitive science support for LOTH, it is crucial to
specify what version of LOTH one has in mind. Specifically,
establishing that certain mental processes operate over mental
representations is not enough to establish RTT. For example, one might
accept that mental representations figure in perception and animal
navigation but not in high-level human cognition. Gallistel and King
(2009) defend COMP+CCTM+FSC through a number of (mainly non-human)
empirical case studies, but they do not endorse RTT. They focus on
relatively low-level phenomena, such as animal navigation, without
discussing human decision-making, deductive inference, problem
solving, or other high-level cognitive phenomena.
During your lifetime, you will only entertain a finite number of
thoughts. In principle, though, there are infinitely many thoughts you
might entertain. Consider:
Mary gave the test tube to John’s daughter.
Mary gave the test tube to John’s daughter’s daughter.
Mary gave the test tube to John’s daughter’s
daughter’s daughter.
⋮
The moral usually drawn is that you have the competence to
entertain a potential infinity of thoughts, even though your
performance is bounded by biological limits upon memory,
attention, processing capacity, and so on. In a slogan: thought is
productive.
RTT+COMP straightforwardly explains productivity. We postulate a
finite base of primitive Mentalese symbols, along with operations for
combining simple expressions into complex expressions. Iterative
application of the compounding operations generates an infinite array
of mental sentences, each in principle within your cognitive
repertoire. By tokening a mental sentence, you entertain the thought
expressed by it. This explanation leverages the recursive nature of
compositional mechanisms to generate infinitely many expressions from
a finite base. It thereby illuminates how finite creatures such as
ourselves are able to entertain a potential infinity of thoughts.
Fodor and Pylyshyn (1988) argue that, since RTT+COMP provides a
satisfying explanation for productivity, we have good reason to accept
RTT+COMP. A potential worry about this argument is that it rests upon
an infinitary competence never manifested within actual performance.
One might dismiss the supposed infinitary competence as an
idealization that, while perhaps convenient for certain purposes, does
not stand in need of explanation.
There are systematic interrelations among the thoughts a thinker can
entertain. For example, if you can entertain the thought that John
loves Mary, then you can also entertain the thought that Mary loves
John. Systematicity looks like a crucial property of human thought and
so demands a principled explanation.
RTT+COMP gives a compelling explanation. According to RTT+COMP, your
ability to entertain the thought that p hinges upon your
ability to bear appropriate psychological relations to a Mentalese
sentence S whose meaning is that p. If you are able to
think that John loves Mary, then your internal system of mental
representations includes a mental sentence John loves
Mary, composed of mental words John,
loves, and Mary
combined in the right way. If you have the capacity to stand in
psychological relation A* to John loves
Mary, then you also have the capacity to stand in relation
A* to a distinct mental sentence Mary loves
John. The constituent words John, loves,
and Mary make the
same semantic contribution to both mental sentences (John
denotes John, loves
denotes the loving relation, and Mary denotes
Mary), but the words are arranged in different constituency structures
so that the sentences have different meanings. Whereas John
loves Mary means that John loves Mary, Mary
loves John means that Mary loves John. By
standing in relation A* to the sentence Mary
loves John, you entertain the thought that Mary loves John.
Thus, an ability to think that John loves Mary entails an ability to
think that John loves Mary. By comparison, an ability to think that
John loves Mary does not entail an ability to think that whales are
mammals or an ability to think that \(56 + 138 = 194\).
Fodor (1987: 148–153) supports RTT+COMP by citing its ability to
explain systematicity. In contrast with the productivity argument, the
systematicity argument does not depend upon infinitary idealizations
that outstrip finite performance. Note that neither argument provides
any direct support for CTM. Neither argument even mentions
computation.
There are systematic interrelations among which inferences a thinker
can draw. For example, if you can infer p from p
and q, then you can also infer m from m and
n. The systematicity of thinking requires explanation. Why is it
that thinkers who can infer p from p and
q can also infer m from m and
n?
RTT+COMP+CCTM gives a compelling explanation. During an inference from
p and q to p, you transit from believing* mental
sentence \(S_1 \amp S_2\) (which means that p and q) to
believing* mental sentence \(S_{1}\) (which means that p).
According to CCTM, the transition involves symbol manipulation. A
mechanical operation detaches the conjunct \(S_{1}\) from the
conjunction \(S_1 \amp S_2\). The same mechanical operation is
applicable to a conjunction \(S_{3} \amp S_{4}\) (which means that
m and n), corresponding to the inference from m and
n to n. An ability to execute the first inference entails
an ability to execute the second, because drawing the inference in
either case corresponds to executing a single uniform mechanical
operation. More generally, logical inference deploys mechanical
operations over structured symbols, and the mechanical operation
corresponding to a given inference pattern (e.g., conjunction
introduction, disjunction elimination, etc.) is applicable to any
premises with the right logical structure. The uniform applicability
of a single mechanical operation across diverse symbols explains
inferential systematicity. Fodor and Pylyshyn (1988) conclude that
inferential systematicity provides reason to accept RTT+COMP+CCTM.
Fodor and Pylyshyn (1988) endorse an additional thesis about the
mechanical operations corresponding to logical transitions. In keeping
with FSC, they claim that the operations are sensitive to formal
syntactic properties but not semantic properties. For example,
conjunction elimination responds to Mentalese conjunction as a piece
of pure formal syntax, much as a computer manipulates items in a
formal language without considering what those items mean.
Semantic computationalists reject FSC. They claim that mental
computation is sometimes sensitive to semantic properties. Semantic
computationalists can agree that drawing an inference involves
executing a mechanical operation over structured symbols, and they can
agree that the same mechanical operation uniformly applies to any
premises with appropriate logical structure. So they can still explain
inferential systematicity. However, they can also say that the
postulated mechanical operation is sensitive to semantic properties.
For example, they can say that conjunction elimination is sensitive to
the meaning of Mentalese conjunction.
In assessing the debate between FSC and semantic computationalism, one
must distinguish between logical versus non-logical
symbols. For present purposes, it is common ground that the meanings
of non-logical symbols do not inform logical inference. The
inference from \(S_1 \amp S_2\) to \(S_{1}\) features the same
mechanical operation as the inference from \(S_{3} \amp S_{4}\) to
\(S_{4}\), and this mechanical operation is not sensitive to the
meanings of the conjuncts \(S_{1}\), \(S_{2}\), \(S_{3}\), or
\(S_{4}\). It does not follow that the mechanical operation is
insensitive to the meaning of Mentalese conjunction. The meaning of
conjunction might influence how the logical inference proceeds, even
though the meanings of the conjuncts do not.
In the 1960s and 1970s, cognitive scientists almost universally
modeled mental activity as rule-governed symbol manipulation. In the
1980s, connectionism gained currency as an alternative computational
framework. Connectionists employ computational models, called
neural networks, that differ quite significantly from
Turing-style models. There is no central processor. There are no
memory locations for symbols to be inscribed. Instead, there is a
network of nodes bearing weighted connections to one another.
During computation, waves of activation spread through the network. A
node’s activation level depends upon the weighted activations of
the nodes to which it is connected. Nodes function somewhat
analogously to neurons, and connections between nodes function
somewhat analogously to synapses. One should receive the
neurophysiological analogy cautiously, as there are numerous important
differences between neural networks and actual neural configurations
in the brain (Bechtel & Abramson 2002: 341–343;
Bermúdez 2010: 237–239; Clark 2014: 87–89; Harnish
2002: 359–362).
Connectionists raise many objections to the classical computational
paradigm (Rumelhart, McClelland, & the PDP Research Group 1986;
Horgan & Tienson 1996; McLaughlin & Warfield 1994; Bechtel
& Abrahamsen 2002), such as that classical systems are not
biologically realistic or that they are unable to model certain
psychological tasks. Classicists in turn launch various arguments
against connectionism. The most famous arguments showcase
productivity, systematicity of thought, and systematicity of thinking.
Fodor and Pylyshyn (1988) argue that these phenomena support classical
CTM over connectionist CTM.
Fodor and Pylyshyn’s argument hinges on the distinction between
eliminative connectionism and implementationist
connectionism (cf. Pinker & Prince 1988). Eliminative
connectionists advance neural networks as a replacement for
the Turing-style formalism. They deny that mental computation consists
in rule-governed symbol manipulation. Implementationist connectionists
allow that, in some cases, mental computation may instantiate
rule-governed symbol manipulation. They advance neural networks not to
replace classical computations but rather to model how classical
computations are implemented in the brain. The hope is that, because
neural network computation more closely resembles actual brain
activity, it can illuminate the physical realization of rule-governed
symbol manipulation.
Building on Aydede’s (2015) discussion, we may reconstruct Fodor
and Pylyshyn’s argument like so:
The argument does not say that neural networks are unable to
model systematicity. One can certainly build a neural network that is
systematic. For example, one might build a neural network that can
represent that John loves Mary only if it can represent that Mary
loves John. The problem is that one might just as well build a neural
network that can represent that John loves Mary but cannot represent
that Mary loves John. Hence, nothing about the connectionist framework
per se guarantees systematicity. For that reason, the
framework does not explain the nomic necessity of systematicity. It
does not explain why all the minds we find are systematic. In
contrast, the classical framework mandates systematicity, and so it
explains the nomic necessity of systematicity. The only apparent
recourse for connectionists is to adopt the classical explanation,
thereby becoming implementationist rather than eliminative
connectionists.
Fodor and Pylyshyn’s argument has spawned a massive literature,
including too many rebuttals to survey here. The most popular
responses fall into five categories:
We focus here on (vi).
As discussed in
 section 1.2,
 Fodor elucidates constituency structure in terms of part/whole
relations. A complex representation’s constituents are literal
parts of it. One consequence is that, whenever the first
representation is tokened, so are its constituents. Fodor takes this
consequence to be definitive of classical computation. As Fodor and
McLaughlin (1990: 186) put it: 
for a pair of expression types E1, E2, the first is a
Classical constituent of the second only if the
first is tokened whenever the second is tokened. 
Thus, structured representations have a concatenative
structure: each token of a structured representation involves a
concatenation of tokens of the constituent representations.
Connectionists who deny (vi) espouse a non-concatenative
conception of constituency structure, according to which structure is
encoded by a suitable distributed representation.
Developments of the non-concatenative conception are usually quite
technical (Elman 1989; Hinton 1990; Pollack 1990; Smolensky 1990, 1991, 1995; Touretzky 1990).
Most models use vector or tensor algebra to define
operations over connectionist representations, which are codified by
activity vectors across nodes in a neural network. The representations
are said to have implicit constituency structure: the
constituents are not literal parts of the complex representation, but
they can be extracted from the complex representation through suitable
computational operations over it.
Fodor and McLaughlin (1990) grant that distributed representations may
have constituency structure “in an extended sense”. But
they insist that distributed representations are ill-suited to explain
systematicity. They focus especially on the systematicity of thinking,
the classical explanation for which postulates mechanical operations
that respond to constituency structure. Fodor and McLaughlin argue
that the non-concatenative conception cannot replicate the classical
explanation and offers no satisfactory substitute for it. Chalmers
(1993) and Niklasson and van Gelder (1994) disagree. They contend that
a neural network can execute structure-sensitive computations over
representations that have non-concatenative constituency structure.
They conclude that connectionists can explain productivity and
systematicity without retreating to implementationist
connectionism.
Aydede (1995, 1997a) agrees that there is a legitimate notion of
non-concatenative constituency structure, but he questions whether the
resulting models are non-classical. He denies that we should regard
concatenative structure as integral to LOTH. According to Aydede,
concatenative structure is just one possible physical realization of
constituency structure. Non-concatenative structure is another
possible realization. We can accept RTT+COMP without glossing
constituency structure in concatenative terms. On this view, a neural
network whose operations are sensitive to non-concatenative
constituency structure may still count as broadly classical and in
particular as manipulating Mentalese expressions.
The debate between classical and connectionist CTM is still active,
although not as active as during the 1990s. Recent anti-connectionist
arguments tend to have a more empirical flavor. For example, Gallistel
and King (2009) defend CCTM by canvassing a range of non-human
empirical case studies. According to Gallistel and King, the case
studies manifest a kind of productivity that CCTM can easily explain
but eliminative connectionism cannot.
LOTH has elicited too many objections to cover in a single
encyclopedia entry. We will discuss two objections, both alleging that
LOTH generates a vicious regress. The first objection emphasizes
language learning. The second emphasizes
language understanding.
Like many cognitive scientists, Fodor holds that children learn a
natural language via hypothesis formation and testing.
Children formulate, test, and confirm hypotheses about the denotations
of words. For example, a child learning English will confirm the
hypothesis that “cat” denotes cats. According to Fodor,
denotations are represented in Mentalese. To formulate the hypothesis
that “cat” denotes cats, the child uses a Mentalese word
cat that denotes cats. It may seem that a regress is now in the
offing, sparked by the question: How does the child learn Mentalese?
Suppose we extend the hypothesis formation and testing model
(henceforth HF) to Mentalese. Then we must posit a meta-language to
express hypotheses about denotations of Mentalese words, a
meta-meta-language to express hypotheses about denotations of
meta-language words, and so on ad infinitum (Atherton and
Schwartz 1974: 163).
Fodor responds to the threatened regress by denying we should apply HF
to Mentalese (1975: 65). Children do not test hypotheses about the
denotations of Mentalese words. They do not learn Mentalese at all.
The mental language is innate.
The doctrine that some concepts are innate was a focal point
in the clash between rationalism versus empiricism. Rationalists
defended the innateness of certain fundamental ideas, such as god and
cause, while empiricists held that all ideas derive from sensory
experience. A major theme in the 1960s cognitive science revolution
was revival of a nativist picture, inspired by the
rationalists, on which many key elements of cognition are innate. Most
famously, Chomsky (1965) explained language acquisition by positing
innate knowledge about possible human languages. Fodor’s
innateness thesis was widely perceived as going way beyond all
precedent, verging on the preposterous (P.S. Churchland 1986; Putnam
1988). How could we have an innate ability to represent all the
denotations we mentally represent? For example, how could we innately
possess a Mentalese word carburetor that represents carburetors?
In evaluating these issues, it is vital to distinguish between
learning a concept versus acquiring a concept. When
Fodor says that a concept is innate, he does not mean to deny that we
acquire the concept or even that certain kinds of experience are
needed to acquire it. Fodor fully grants that we cannot mentally
represent carburetors at birth and that we come to represent them only
by undergoing appropriate experiences. He agrees that most concepts
are acquired. He denies that they are learned. In
effect, he uses “innate” as a synonym for
“unlearned” (1975: 96). One might reasonably challenge
Fodor’s usage. One might resist classifying a concept as innate
simply because it is unlearned. However, that is how Fodor
uses the word “innate”. Properly understood, then,
Fodor’s position is not as far-fetched as it may
 sound.[7]
Fodor gives a simple but striking argument that concepts are
unlearned. The argument begins from the premise that HF is the only
potentially viable model of concept learning. Fodor then
argues that HF is not a viable model of concept learning,
from which he concludes that concepts are unlearned. He offers various
formulations and refinements of the argument over his career. Here is
a relatively recent rendition (2008: 139):
Now, according to HF, the process by which one learns C must
include the inductive evaluation of some such hypothesis as “The
C things are the ones that are green or triangular”. But
the inductive evaluation of that hypothesis itself requires (inter
alia) bringing the property green or triangular before
the mind as such… Quite generally, you can’t represent
anything as such and such unless you already have the concept
such and such. All that being so, it follows, on pain of
circularity, that “concept learning” as HF understands it
can’t be a way of acquiring concept C…
Conclusion: If concept learning is as HF understands it, there can
be no such thing. This conclusion is entirely general; it
doesn’t matter whether the target concept is primitive (like
green) or complex (like green
or triangular).
Fodor’s argument does not presuppose RTT, COMP, or CTM. To the
extent that the argument works, it applies to any view on which people
have concepts.
If concepts are not learned, then how are they acquired? Fodor offers
some preliminary remarks (2008: 144–168), but by his own
admission the remarks are sketchy and leave numerous questions
unanswered (2008: 144–145). Prinz (2011) critiques Fodor’s
positive treatment of concept acquisition.
The most common rejoinder to Fodor’s innateness argument is to
deny that HF is the only viable model of concept learning. The
rejoinder acknowledges that concepts are not learned through
hypothesis testing but insists they are learned through other
means. Three examples:
A lot depends here upon what counts as “learning” and what
does not, a question that seems difficult to adjudicate. A closely
connected question is whether concept acquisition is a
rational process or a mere causal process. To the
extent that acquiring some concept is a rational achievement, we will
want to say that one learned the concept. To the extent that acquiring
the concept is a mere causal process (more like catching a cold than
confirming a hypothesis), we will feel less inclined to say that
genuine learning took place (Fodor 1981: 275).
These issues lie at the frontier of psychological and philosophical
research. The key point for present purposes is that there are two
options for halting the regress of language learning: we can say that
thinkers acquire concepts but do not learn them; or we can say that
thinkers learn concepts through some means other than hypothesis
testing. Of course, it is not enough just to note that the two options
exist. Ultimately, one must develop one’s favored option into a
compelling theory. But there is no reason to think that doing so would
reinitiate the regress. In any event, explaining concept acquisition
is an important task facing any theorist who accepts that we have
concepts, whether or not the theorist accepts LOTH. Thus, the learning
regress objection is best regarded not as posing a challenge specific
to LOTH but rather as highlighting a more widely shared theoretical
obligation: the obligation to explain how we acquire concepts.
For further discussion, see the entry on innateness. See also the
exchange between Cowie (1999) and Fodor (2001).
What is it to understand a natural language word? On a popular
picture, understanding a word requires that you mentally represent the
word’s denotation. For example, understanding the word
“cat” requires representing that it denotes cats. LOT
theorists will say that you use Mentalese words to represent
denotations. The question now arises what it is to understand a
Mentalese word. If understanding the Mentalese word requires
representing that it has a certain denotation, then we face an
infinite regress of meta-languages (Blackburn 1984: 43–44).
The standard response is to deny that ordinary thinkers represent
Mentalese words as having denotations (Bach 1987; Fodor 1975:
66–79). Mentalese is not an instrument of communication.
Thinking is not “talking to oneself” in Mentalese. A
typical thinker does not represent, perceive, interpret, or reflect
upon Mentalese expressions. Mentalese serves as a medium within which
her thought occurs, not an object of interpretation. We should not say
that she “understands” Mentalese in the same way that she
understands a natural language.
There is perhaps another sense in which the thinker
“understands” Mentalese: her mental activity coheres with
the meanings of Mentalese words. For example, her deductive reasoning
coheres with the truth-tables expressed by Mentalese logical
connectives. More generally, her mental activity is semantically
coherent. To say that the thinker “understands” Mentalese
in this sense is not to say that she represents Mentalese
denotations. Nor is there any evident reason to suspect that
explaining semantic coherence will ultimately require us to posit
mental representation of Mentalese denotations. So there is no regress
of understanding.
For further criticism of this regress argument, see the discussions of
Knowles (1998) and
Laurence and Margolis
 (1997).[8]
Naturalism
 is a movement that seeks to ground philosophical theorizing
in the scientific enterprise. As so often in philosophy, different
authors use the term “naturalism” in different ways. Usage
within philosophy of mind typically connotes an effort to depict
mental states and processes as denizens of the physical world, with no
irreducibly mental entities or properties allowed. In the modern era,
philosophers have often recruited LOTH to advance naturalism. Indeed,
LOTH’s supposed contribution to naturalism is frequently cited
as a significant consideration in its favor. One example is
Fodor’s use of CCTM+FSC to explain semantic coherence. The other
main example turns upon the problem of intentionality.
How does intentionality arise? How do mental states come to be
about anything, or to have semantic properties? Brentano
(1874 [1973: 97]) maintained that intentionality is a hallmark of the
mental as opposed to the physical: “The reference to something
as an object is a distinguishing characteristic of all mental
phenomena. No physical phenomenon exhibits anything similar”. In
response, contemporary naturalists seek to naturalize
intentionality. They want to explain in naturalistically
acceptable terms what makes it the case that mental states have
semantic properties. In effect, the goal is to reduce the intentional
to the non-intentional. Beginning in the 1980s, philosophers have
offered various proposals about how to naturalize intentionality. Most
proposals emphasize causal or nomic links between mind and world
(Aydede & Güzeldere 2005; Dretske 1981; Fodor 1987, 1990;
Stalnaker 1984), sometimes also invoking teleological factors
(Millikan 1984, 1993; Neander 2017l; Papineau 1987; Dretske 1988) or
historical lineages of mental states (Devitt 1995; Field 2001).
Another approach, functional role semantics, emphasizes the
functional role of a mental state: the cluster of causal or
inferential relations that the state bears to other mental states. The
idea is that meaning emerges at least partly through these causal and
inferential relations. Some functional role theories cite causal
relations to the external world (Block 1987; Loar 1982), and others do
not (Cummins 1989).
Even the best developed attempts at naturalizing intentionality, such
as Fodor’s (1990) version of the nomic strategy, face serious
problems that no one knows how to solve (M. Greenberg 2014; Loewer
1997). Partly for that reason, the flurry of naturalizing attempts
abated in the 2000s. Burge (2010: 298) reckons that the naturalizing
project is not promising and that current proposals are
“hopeless”. He agrees that we should try to illuminate
representationality by limning its connections to the physical, the
causal, the biological, and the teleological. But he insists that
illumination need not yield a reduction of the intentional to the
non-intentional.
LOTH is neutral as to the naturalization of intentionality. An LOT
theorist might attempt to reduce the intentional to the
non-intentional. Alternatively, she might dismiss the reductive
project as impossible or pointless. Assuming she chooses the reductive
route, LOTH provides guidance regarding how she might proceed.
According to RTT,
X A’s that p iff there is a mental
representation S such that X bears A* to S
and S means that p.
The task of elucidating “X A’s that
p” in naturalistically acceptable terms factors into two
sub-tasks (Field 2001: 33):
As we have seen, functionalism helps with (a). Moreover, COMP provides
a blueprint for tackling (b). We can first delineate a compositional
semantics describing how S’s meaning depends upon
semantic properties of its component words and upon the compositional
import of the constituency structure into which those words are
arranged. We can then explain in naturalistically acceptable terms why
the component words have the semantic properties that they have and
why the constituency structure has the compositional import that it
has.
How much does LOTH advance the naturalization of intentionality? Our
compositional semantics for Mentalese may illuminate how the semantic
properties of a complex expression depend upon the semantic properties
of primitive expressions, but it says nothing about how primitive
expressions get their semantic properties in the first place.
Brentano’s challenge (How could intentionality arise from
purely physical entities and processes?) remains unanswered. To
meet the challenge, we must invoke naturalizing strategies that go
well beyond LOTH itself, such as the causal or nomic strategies
mentioned above. Those naturalizing strategies are not specifically
linked to LOTH and can usually be tailored to semantic properties of
neural states rather than semantic properties of Mentalese
expressions. Thus, it is debatable how much LOTH ultimately helps us
naturalize intentionality. Naturalizing strategies orthogonal to LOTH
seem to do the heavy lifting.
How are Mentalese expressions individuated? Since Mentalese
expressions are types, answering this question requires us to consider
the type/token relation for Mentalese. We want to fill in the
schema
e and e* are tokens of the same Mentalese type iff
R(e, e*).
What should we substitute for R(e, e*)? The
literature typically focuses on primitive symbol types, and
we will follow suit here.
It is almost universally agreed among contemporary LOT theorists that
Mentalese tokens are neurophysiological entities of some sort. One
might therefore hope to individuate Mentalese types by citing neural
properties of the tokens. Drawing R(e, e*) from
the language of neuroscience induces a theory along the following
lines:
Neural individuation: e and e*
are tokens of the same primitive Mentalese type iff e and
e* are tokens of the same neural type.
This schema leaves open how neural types are individuated. We may
bypass that question here, because neural individuation of Mentalese
types finds no proponents in the contemporary literature. The main
reason is that it conflicts with 
  multiple realizability: 
 the doctrine that a single mental state type can be realized by
physical systems that are wildly heterogeneous when described in
physical, biological, or neuroscientific terms. Putnam (1967)
introduced multiple realizability as evidence against the 
 mind/brain identity theory,
 which asserts that mental state types are brain state
types. Fodor (1975: 13–25) further developed the multiple
realizability argument, presenting it as foundational to
LOTH. Although the multiple realizability argument has subsequently
been challenged (Polger 2004), LOT theorists widely agree that we
should not individuate Mentalese types in neural terms.
The most popular strategy is to individuate Mentalese types
functionally:
Functional individuation: e and
e* are tokens of the same primitive Mentalese type iff e
and e* have the same functional role.
Field (2001: 56–67), Fodor (1994: 105–109), and Stich (1983:
149–151) pursue functional individuation. They specify
functional roles using a Turing-style computationalism formalism, so
that “functional role” becomes something like
“computational role”, i.e., role within mental
computation.
Functional roles theories divide into two categories:
molecular and holist. Molecular theories isolate
privileged canonical relations that a symbol bears to other symbols.
Canonical relations individuate the symbol, but non-canonical
relations do not. For example, one might individuate Mentalese
conjunction solely through the introduction and elimination rules
governing conjunction while ignoring any other computational rules. If
we say that a symbol’s “canonical functional role”
is constituted by its canonical relations to other symbols, then we
can offer the following theory:
Molecular functional individuation: e
and e* are tokens of the same primitive Mentalese type iff
e and e* have the same canonical functional role.
One problem facing molecular individuation is that, aside from logical
connectives and a few other special cases, it is difficult to draw any
principled demarcation between canonical and non-canonical relations
(Schneider 2011: 106). Which relations are canonical for
 SOFA?[9]
 Citing the demarcation problem, Schneider espouses a holist approach
that individuates mental symbols through total functional
role, i.e., every single aspect of the role that a symbol plays
within mental activity:
Holist functional individuation: e
and e* are tokens of the same primitive Mentalese type iff
e and e* have the same total functional role.
Holist individuation is very fine-grained: the slightest difference in
total functional role entails that different types are tokened. Since
different thinkers will always differ somewhat in their mental
computations, it now looks like two thinkers will never share the same
mental language. This consequence is worrisome, for two reasons
emphasized by Aydede (1998). First, it violates the plausible
publicity constraint that propositional attitudes are in
principle shareable. Second, it apparently precludes interpersonal
psychological explanations that cite Mentalese expressions. Schneider
(2011: 111–158) addresses both concerns, arguing that they are
misdirected.
A crucial consideration when individuating mental symbols is what role
to assign to semantic properties. Here we may usefully compare
Mentalese with natural language. It is widely agreed that natural
language words do not have their denotations essentially. The English
word “cat” denotes cats, but it could just as well have
denoted dogs, or the number 27, or anything else, or nothing at all,
if our linguistic conventions had been different. Virtually all
contemporary LOT theorists hold that a Mentalese word likewise does
not have its denotation essentially. The Mentalese word cat denotes
cats, but it could have had a different denotation had it born
different causal relations to the external world or had it occupied a
different role in mental activity. In that sense, cat is a piece of
formal syntax. Fodor’s early view (1981: 225–253) was that
a Mentalese word could have had a different denotation but
not an arbitrarily different denotation: cat could not have
denoted just anything—it could not have denoted the number
27—but it could have denoted some other animal species had the
thinker suitably interacted with that species rather than with cats.
Fodor eventually (1994, 2008) embraces the stronger thesis that a
Mentalese word bears an arbitrary relation to its denotation:
cat could have had any arbitrarily different denotation. Most
contemporary theorists agree (Egan 1992: 446; Field 2001: 58; Harnad
1994: 386; Haugeland 1985: 91: 117–123; Pylyshyn 1984: 50).
The historical literature on LOTH suggests an alternative
semantically permeated view: Mentalese words are individuated
partly through their denotations. The Mentalese word cat is not a
piece of formal syntax subject to reinterpretation. It could not have
denoted another species, or the number 27, or anything else. It
denotes cats by its inherent nature. From a semantically
permeated viewpoint, a Mentalese word has its denotation essentially.
Thus, there is a profound difference between natural language and
mental language. Mental words, unlike natural language words, bring
with them one fixed semantic interpretation. The semantically
permeated approach is present in Ockham, among other medieval LOT
theorists (Normore 2003, 2009). In light of the problems facing neural
and functional individuation, Aydede (2005) recommends that we
consider taking semantics into account when individuating Mentalese
expressions. Rescorla (2012b) concurs, defending a semantically permeated approach as
applied to at least some mental representations. He proposes that
certain mental computations operate over mental symbols with essential
semantic properties, and he argues that the proposal fits well with
many sectors of cognitive
 science.[10]
A recurring complaint about the semantically permeated approach is
that inherently meaningful mental representations seem like highly
suspect entities (Putnam 1988: 21). How could a mental word have one
fixed denotation by its inherent nature? What magic ensures
the necessary connection between the word and the denotation? These
worries diminish in force if one keeps firmly in mind that Mentalese
words are types. Types are abstract entities corresponding to a scheme
for classifying, or type-identifying, tokens. To ascribe a
type to a token is to type-identify the token as belonging to some
category. Semantically permeated types correspond to a classificatory
scheme that takes semantics into account when categorizing tokens. As
Burge emphasizes (2007: 302), there is nothing magical about
semantically-based classification. On the contrary, both folk
psychology and cognitive science routinely classify mental events
based at least partly upon their semantic properties.
A simplistic implementation of the semantically permeated approach
individuates symbol tokens solely through their
denotations:
Denotational individuation: e and
e* are tokens of the same primitive Mentalese type iff e
and e* have the same denotation.
As Aydede (2000) and Schneider (2011) emphasize, denotational
individuation is unsatisfying. Co-referring words may play
significantly different roles in mental activity. Frege’s (1892
[1997]) famous Hesperus-Phosphorus example illustrates: one can
believe that Hesperus is Hesperus without believing that Hesperus is
Phosphorus. As Frege put it, one can think about the same denotation
“in different ways”, or “under different modes of
presentation”. Different modes of presentation have different
roles within mental activity, implicating different psychological
explanations. Thus, a semantically permeated individuative scheme
adequate for psychological explanation must be finer-grained than
denotational individuation allows. It must take mode of presentation
into account. But what it is to think about a denotation “under
the same mode of presentation”? How are “modes of
presentation” individuated? Ultimately, semantically permeated
theorists must grapple with these questions. Rescorla (forthcoming)
offers some suggestions about how to
 proceed.[11]
Chalmers (2012) complains that semantically permeated individuation
sacrifices significant virtues that made LOTH attractive in the first
place. LOTH promised to advance naturalism by grounding cognitive
science in non-representational computational models.
Representationally-specified computational models seem like a
significant retrenchment from these naturalistic ambitions. For
example, semantically permeated theorists cannot accept the FSC
explanation of semantic coherence, because they do not postulate
formal syntactic types manipulated during mental computation.
How compelling one finds naturalistic worries about semantically
permeated individuation will depend on how impressive one finds the
naturalistic contributions made by formal mental syntax. We saw
earlier that FSC arguably engenders a worrisome epiphenomenalism.
Moreover, the semantically permeated approach in no way precludes a
naturalistic reduction of intentionality. It merely precludes invoking
formal syntactic Mentalese types while executing such a reduction. For
example, proponents of the semantically permeated approach can still
pursue the causal or nomic naturalizing strategies discussed in
 section 7.
 Nothing about either strategy presupposes formal syntactic Mentalese
types. Thus, it is not clear that replacing a formal syntactic
individuative scheme with a semantically permeated scheme
significantly impedes the naturalistic endeavor.
No one has yet provided an individuative scheme for Mentalese that
commands widespread assent. The topic demands continued investigation,
because LOTH remains highly schematic until its proponents clarify
sameness and difference of Mentalese types.