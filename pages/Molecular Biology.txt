Despite its prominence in the contemporary life sciences, molecular
biology is a relatively young discipline, originating in the 1930s and
1940s, and becoming institutionalized in the 1950s and 1960s. It
should not be surprising, then, that many of the philosophical issues
in molecular biology are closely intertwined with this recent history.
This section sketches four facets of molecular biology’s
development: its origins, its classical period, its subsequent
migration into other biological domains, and its more recent turn to
genomics and post-genomics. The rich historiography of molecular
biology can only be briefly utilized in this shortened history (see,
for example, Abir-Am 1985, 1987, 1994, 2006; Burian 1993a; Canguillhem
1989; de Chadarevian 2002, 2003; de Chadarevian and Gaudilliere 1996;
de Chadarevian and Strasser 2002; Deichmann 2002; Fisher 2010;
Hausmann 2002; Holmes 2001; Judson 1980, 1996; Kay 1993; Marcum 2002;
Morange 1997a, 1998; Olby 1979, 1990, 1994, 2003; Powell et al. 2007;
Rheinberger 1997; Sapp 1992; Sarkar 1996a; Stegenga 2011; van Holde
and Zlatanova 2018; Witkowski 2005; Zallen 1996. Also see
autobiographical accounts by biologists, such as Brenner 2001; Cohen
1984; Crick 1988; Echols 2001; Jacob 1988; Kornberg 1989; Luria 1984;
Watson 1968, 2002, 2007; Wilkins 2003).
The field of molecular biology arose from the convergence of work by
geneticists, physicists, and structural chemists on a common problem:
the nature of inheritance. In the early twentieth century, although
the nascent field of genetics was guided by Mendel’s laws of
segregation and independent assortment, the actual mechanisms of gene
reproduction, mutation and expression remained unknown. Thomas Hunt
Morgan and his colleagues utilized the fruit fly, Drosophila
melanogaster, as a model organism to study the relationship
between the gene and the chromosomes in the hereditary process (Morgan
1926; discussed in Darden 1991; Darden and Maull 1977; Kohler 1994;
Roll-Hanson 1978; Wimsatt 1992). A former student of Morgan’s,
Hermann J. Muller, recognized the “gene as a basis of
life”, and so set out to investigate its structure (Muller
1926). Muller discovered the mutagenic effect of x-rays on
Drosophila, and utilized this phenomenon as a tool to explore
the size and nature of the gene (Carlson 1966, 1971, 1981, 2011; Crow
1992; Muller 1927). But despite the power of mutagenesis, Muller
recognized that, as a geneticist, he was limited in the extent to
which he could explicate the more fundamental properties of genes and
their actions. He concluded a 1936 essay:
The geneticist himself is helpless to analyse these properties
further. Here the physicist, as well as the chemist, must step in. Who
will volunteer to do so? (Muller 1936: 214)
Muller’s request did not go unanswered. The next decade saw
several famous physicists turn their attention to the nature of
inheritance (Keller 1990; Kendrew 1967). In What is Life, the
physicist Erwin Schroedinger (1944) proposed ways in which the
principles of quantum physics might account for the stability, yet
mutability, of the gene (see the entry on
 life)
 (Elitzur 1995; Moore 1989; Olby 1994; Sarkar 1991; for a
reinterpretation see Kay 2000). Max Delbrueck also became interested
in the physical basis of heredity after hearing a lecture by his
teacher, quantum physicist Niels Bohr (1933), which expounded a
principle of complementarity between physics and biology (McKaughan
2005; Roll-Hansen 2000). In contrast to Schroedinger, Bohr (and
subsequently Delbrueck) did not seek to reduce biology to physics;
instead, the goal was to understand how each discipline complemented
the other (Delbrueck 1949; Sloan and Fogel 2011). To investigate the
self-reproductive characteristic of life, Delbrueck used
bacteriophage, viruses that infect bacteria and then multiply very
rapidly. The establishment of “The Phage Group” in the
early 1940s by Delbrueck and another physicist-turned-biologist
Salvador Luria marked a critical point in the rise of molecular
biology (Brock 1990; Cairns et al. 1966; Fischer and Lipson 1988;
Fleming 1968; Lewontin 1968; Luria 1984; Morange 1998: Ch. 4; Stent
1968). Delbrueck’s colleague at Cal Tech, Linus Pauling,
utilized his knowledge of structural chemistry to study macromolecular
structure. Pauling contributed both theoretical work on the nature of
chemical bonds and experimental work using x-ray crystallography to
discover the physical structure of macromolecular compounds (Pauling
1939, 1970; Olby 1979; Hager 1995; Crick 1996; Sarkar 1998).
As suggested in the brief history above, experimentation figured
prominently in the rise of molecular biology (see the entry on
 experiment in biology).
 X-ray crystallography allowed molecular biologists to investigate the
structure of macromolecules.
 Alfred Hershey and Martha Chase (1952) used phage viruses to confirm
that the genetic material transmitted from generation to generation
was DNA and not proteins (see Hershey-Chase Experiment in
 Other Internet Resources).
 Muller (1927) used x-rays to intervene on and alter gene function,
thus revealing the application of methods from physics to a biological
domain (see Elof Carlson on Muller’s Research in
 Other Internet Resources).
 
Recognizing quite early the importance of these new physical and
structural chemical approaches to biology, Warren Weaver, then the
director of the Natural Sciences section of the Rockefeller
Foundation, introduced the term “molecular biology” in a
1938 report to the Foundation. Weaver wrote,
And gradually there is coming into being a new branch of
science—molecular biology—which is beginning to uncover
many secrets concerning the ultimate units of the living
cell….in which delicate modern techniques are being used to
investigate ever more minute details of certain life processes (quoted
in Olby 1994: 442).
But perhaps a more telling account of the term’s origin came
from Francis Crick, who said he started calling himself a molecular
biologist because:
when inquiring clergymen asked me what I did, I got tired of
explaining that I was a mixture of crystallographer, biophysicist,
biochemist, and geneticist, an explanation which in any case they
found too hard to grasp. (quoted in Stent 1969: 36)
This brief recapitulation of the origins of molecular biology reflects
themes addressed by philosophers, such as reduction (see
 Section 3.1),
 the concept of the gene (see
 Section 2.3),
 and experimentation (see
 Section 3.4).
 For Schroedinger, biology was to be reduced to the more fundamental
principles of physics, while Delbrueck instead resisted such a
reduction and sought what made biology unique. Muller’s shift
from Mendelian genetics to the study of gene structure raises the
question of the relation between the gene concepts found in those
separate fields of genetics. And the import of experimental methods
from physics to biology raised the question of the relation between
those disciplines.
Molecular biology’s classical period began in 1953, with James
Watson and Francis Crick’s discovery of the double helical
structure of DNA (Watson and Crick 1953a,b). Watson and Crick’s
scientific relationship unified the various disciplinary approaches
discussed above: Watson, a student of Luria and the phage group,
recognized the need to utilize crystallography to elucidate the
structure of DNA; Crick, a physicist enticed by Schroedinger’s
What is Life? to turn to biology, became trained in, and
contributed to the theory of, x-ray crystallography. At Cambridge
University, Watson and Crick found that they shared an interest in
genes and the structure of DNA (see the entry on
 scientific revolutions).
Watson and Crick collaborated to build a model of the double helical
structure of DNA, with its two helical strands held together by
hydrogen-bonded base pairs (Olby 1994). They made extensive use of
data from x-ray crystallography work on DNA by Maurice Wilkins and
Rosalind Franklin at King’s College, London, appallingly without
Franklin’s permission or even knowledge (Maddox 2002),
Crick’s theoretical work on crystallography (Crick 1988), and
the model building techniques pioneered by Pauling (de Chadarevian
2002; Judson 1996; Olby 1970, 1994, 2009).
With the structure of DNA in hand, molecular biology shifted its focus
to how the double helical structure aided elucidation of the
mechanisms of genetic replication and function, the keys to
understanding the role of genes in heredity (see the entries on
 replication and reproduction
 and
 inheritance systems).
 This subsequent research was guided by the notion that the gene was
an informational molecule. According to Lily Kay,
Up until around 1950 molecular biologists…described genetic
mechanisms without ever using the term information. (Kay
2000: 328)
“Information” replaced earlier talk of biological
“specificity”. Watson and Crick’s second paper of
1953, which discussed the genetical implications of their recently
discovered (Watson and Crick 1953a) double-helical structure of DNA,
used both “code” and “information”:
…it therefore seems likely that the precise sequence of the
bases is the code which carries the genetical
information…. (Watson and Crick 1953b: 244, emphasis
added)
In 1958, Francis Crick used and characterized the concept of
information in the context of stating the “central
dogma” of molecular biology. Crick characterized the central
dogma as follows:
This states that once “information” has passed into
protein it cannot get out again. In more detail, the transfer
of information from nucleic acid to nucleic acid, or from nucleic acid
to protein may be possible, but transfer from protein to protein, or
from protein to nucleic acid is impossible. Information means here the
precise determination of sequence, either of bases in the nucleic acid
or of amino acid residues in the protein. (Crick 1958: 152–153,
emphasis in original)
It is important not to confuse the genetic code and genetic
information. The genetic code refers to the relation between three
bases of DNA, called a “codon”, and one amino acid. Tables
available in molecular biology textbooks (e.g., Watson et al. 1988:
frontispiece) show the relation between 64 codons and 20 amino acids.
For example, CAC codes for histidine. Only a few exceptions for these
coding relations have been found, in a few anomalous cases (see the
list in a small table in Alberts et al. 2002: 814). In contrast,
genetic information refers to the linear sequence of codons along the
DNA, which (in the simplest case) are transcribed to messenger RNA,
which are translated to linearly order the amino acids in a
protein.
With the genetic code elucidated and the relationship between genes
and their molecular products traced, it seemed in the late 1960s that
the concept of the gene was secure in its connection between gene
structure and gene function. The machinery of protein synthesis
translated the coded information in the linear order of nucleic acid
bases into the linear order of amino acids in a protein. However, such
“colinear” simplicity did not persist. In the late 1970s,
a series of discoveries by molecular biologists complicated the
straightforward relationship between a single, continuous DNA sequence
and its protein product. Overlapping genes were discovered (Barrell et
al. 1976); such genes were considered “overlapping”
because two different amino acid chains might be read from the same
stretch of nucleic acids by starting from different points on the DNA
sequence. And split genes were found (Berget et al. 1977; Chow et al.
1977). In contrast to the colinearity hypothesis that a continuous
nucleic acid sequence generated an amino acid chain, it became
apparent that stretches of DNA were often split between coding regions
(exons) and non-coding regions (introns). Moreover, the exons might be
separated by vast portions of this non-coding, supposedly “junk
DNA”. The distinction between exons and introns became even more
complicated when
 alternative splicing
 was discovered the following year (Berk and Sharp 1978). A series of
exons could be spliced together in a variety of ways, thus generating
a variety of molecular products. Discoveries such as overlapping
genes, split genes, and alternative splicing forced molecular
biologists to rethink their understanding of what actually made a
gene…a gene (Portin 1993; for a survey of such complications
see Gerstein et al. 2007: Table 1).
These developments in molecular biology have received philosophical
scrutiny. Molecular biologists sought to discover mechanisms
(see
 Section 2.1),
 drawing the attention of philosophers to this concept. Also,
conceptualizing DNA as an informational molecule (see
 Section 2.2)
 was a move that philosophers have subjected to critical scrutiny.
Finally, the concept of the gene (see
 Section 2.3)
 itself has intrigued philosophers. Complex molecular mechanisms, such
as alternative splicing, have obligated philosophers to consider to
what the term “gene” actually refers. Experimentation also
figured prominently in the classical period (see
 Section 3.4);
 Matthew Meselson and Frank Stahl utilized bacteria grown with
different weights combined with centrifugation to determine how DNA,
as modeled by Watson and Crick, was replicated (Meselson and Stahl
1958; see also The Semi-Conservative Replication of DNA in
 Other Internet Resources).
 
In a 1963 letter to Max Perutz, molecular biologist Sydney Brenner
foreshadowed what would be molecular biology’s next intellectual
migration:
It is now widely realized that nearly all the “classical”
problems of molecular biology have either been solved or will be
solved in the next decade…. Because of this, I have long felt
that the future of molecular biology lies in the extension of research
to other fields of biology, notably development and the nervous
system. (Brenner, letter to Perutz, 1963)
Along with Brenner, in the late 1960s and early 1970s, many of the
leading molecular biologists from the classical period redirected
their research agendas, utilizing the newly developed molecular
techniques to investigate unsolved problems in other fields. Francois
Jacob, Jacques Monod and their colleagues used the bacteria
Escherichia coli to investigate how environmental conditions
impact gene expression and regulation (Jacob and Monod 1961; discussed
in Craver and Darden 2013; Morange 1998: Ch. 14; Schaffner 1974a;
Weber 2005; see also the entry on the
 developmental biology).
 The study of behavior and the nervous system also lured some
molecular biologists. Finding appropriate model organisms that could
be subjected to molecular genetic analyses proved challenging.
Returning to the fruit flies used in Mendelian genetics, Seymour
Benzer induced behavioral mutations in Drosophila as a
“genetic scalpel” to investigate the pathways from genes
to behavior (Benzer 1968; Weiner 1999). And at Cambridge, Sydney
Brenner developed the nematode worm, Caenorhabditis elegans,
to study the nervous system, as well as the genetics of behavior
(Brenner 1973, 2001; Ankeny 2000; Brown 2003). In subsequent decades,
the study of cells was transformed from descriptive cytology into
molecular cell biology (Alberts et al. 1983; Alberts et al. 2002;
Bechtel 2006). Molecular evolution developed as a phylogenetic method
for the comparison of DNA sequences and whole genomes; molecular
systematics sought to research the evolution of the genetic code as
well as the rates of that evolutionary process by comparing
similarities and differences between molecules (Dietrich 1998; see
also the entries on
 evolution,
 heritability, and
 adaptationism).
 The immunological relationship between antibodies and antigens was
recharacterized at the molecular level (Podolsky and Tauber 1997;
Schaffner 1993; see also the entry on the
 philosophy of immunology).
 And the study of oncogenes in cancer research as well as the
molecular bases of mental illness were examples of advances in
molecular medicine (Morange 1997b; see also the entry on
 philosophy of psychiatry).
This process of “going molecular” thus generally amounted
to using experimental methods from molecular biology to examine
complex phenomena (be it gene regulation, behavior, or evolution) at
the molecular level. The molecularization of many fields introduced a
range of issues of interest to philosophers. Inferences made about
research on model organisms such as worms and flies raised questions
about extrapolation (see
 Section 3.3).
 And the reductive techniques of molecular biology raised questions
about whether scientific investigations should always strive to reduce
to lower and lower levels (see
 Section 3.1).
In the 1970s, as many of the leading molecular biologists were
migrating into other fields, molecular biology itself was going
genomic (see the entry on
 genomics and postgenomics).
 The genome is a collection of nucleic acid base pairs within an
organism’s cells (adenine (A) pairs with thymine (T) and
cytosine (C) with guanine (G)). The number of base pairs varies widely
among species. For example, the infection-causing Haemophilus
influenzae (the first bacterial genome to be sequenced) has
roughly 1.9 million base pairs in its genome (Fleischmann et al.
1995), while the infection-catching Homo sapiens carries more
than 3 billion base pairs in its genome (International Human Genome
Sequencing Consortium 2001, Venter et al. 2001). The history of
genomics is the history of the development and use of new experimental
and computational methods for producing, storing, and interpreting
such sequence data (Ankeny 2003; Stevens 2013).
Frederick Sanger played a seminal role in initiating such
developments, creating influential DNA sequencing techniques in the
1950s and 1960s (Saiki et al. 1985; for historical treatments see
Sanger 1988; Judson 1992; Culp 1995; Rabinow 1996; Morange 1998; de
Chadarevian 2002; Little 2003; Garcia-Sancho 2012; Sanger Method of
DNA Sequencing in
 Other Internet Resources).
 Equally important was Edwin Southern’s development of a method
to detect specific sequences of DNA in DNA samples (Southern 1975).
The Southern Blot, as it came to be known, starts by digesting a
strand of DNA into many small DNA fragments; those fragments are then
separated (in a process called gel electrophoresis) based on size,
placed on filter paper which “blots” the DNA fragments on
to a new medium, and then chemically labeled with DNA probes; the
probes then allow for identification and visualization of the DNA
fragments (see also The Southern Blot in
 Other Internet Resources).
 Playing off the “southern” homonym, subsequent blotting
techniques that detect RNA and proteins came to be called Northern
blotting and Western blotting. 
In the mid 1980s, after the development of sequencing techniques, the
United States Department of Energy (DoE) originated a project to
sequence the human genome (initially as part of a larger plan to
determine the impact of radiation on the human genome induced by the
Hiroshima and Nagasaki bombings). The resulting Human Genome Project
(HGP) managed jointly by the DoE and the United States National
Institutes of Health (NIH), utilized both existent sequencing
methodologies and introduced new ones (Kevles and Hood 1992, see also
the entry on
 the human genome project).
 While the human genome project received most of the public attention,
hundreds of genomes have been sequenced to date, including the cat
(Pontius et al. 2007), the mouse (Waterson et al. 2002), rice (Goff et
al. 2002) and a flock of bird genomes (Zhang et al. 2014). One of the
most shocking results of those sequencing projects was the total
number of genes (defined in this context as stretches of DNA that code
for a protein product) found in the genomes. The human genome contains
20,000 to 25,000 genes, the cat contains 20,285 genes, the mouse
24,174, and rice 32,000 to 50,000. So in contrast to early assumptions
stemming from the classical period of molecular biology about how
genes produced proteins which in turn produced organisms, it turned
out that neither organismal complexity nor even position on the food
chain was predictive of gene-number (see the entry on
 genomics and postgenomics).
The increased attention to sequencing genomes encouraged a number of
disciplines to “go genomic”, including behavioral genetics
(Plomin et al. 2003), developmental biology (Srinivasan and Sommer
2002), cell biology (Taniguchi et al. 2002), and evolutionary biology
(Ohta and Kuroiwa 2002). What’s more, genomics has been
institutionalized with textbooks (Cantor and Smith 1999) and journals,
such as Genomics and Genome Research. And the human
genome project itself has turned its attention from a standardized
human genome to variation between genomes in the form of the Human
Genome Diversity Initiative (Gannett 2003) and the HapMap Project
(International HapMap Consortium 2003).
But just as a number of disciplines “went molecular” while
molecular biology itself was wrestling with the complexities posed by
split genes and overlapping genes, so too are fields going genomic
while genomics itself is wrestling with the complexities posed by how
a mere 20,000 genes can construct a human while a grain of rice
requires 50,000 genes (Baedke 2018; Brigandt, Green, and
O’Malley 2017; Green 2017). A related challenge was making sense
of the genetic similarity claims. For example, how to interpret the
finding that human and pumpkin genomes are 75% similar? Does this
finding tell us anything substantive about our overall similarity to
pumpkins (Piotrowska 2009)? To help answer such questions, genomics is
now supplemented by post-genomics. There is ongoing debate about what
actually constitutes post-genomics (Morange 2006), but the general
trend is a focus beyond the mere sequence of As, Cs, Ts, and Gs and
instead on the complex, cellular mechanisms involved in generating
such a variety of protein products from a relatively small number of
protein-coding regions in the genome. Post-genomics utilizes the
sequence information provided by genomics but then situates it in an
analysis of all the other entities and activities involved in the
mechanisms of transcription (transcriptomics), regulation
(regulomics), metabolism (metabolomics), and expression (proteomics).
(See ENCODE Project Consortium 2012; Germain et al. 2014; (see also
the entry on
 philosophy of systems and synthetic biology).
Developments in genomics and post-genomics have sparked a number of
philosophical questions about molecular biology. Since the genome
requires a vast array of other mechanisms to facilitate the generation
of a protein product, can DNA really be causally prioritized (see
 Section 2.3)?
 Similarly, in the face of such interdependent mechanisms involved in
transcription, regulation, and expression, can DNA alone be privileged
as the bearer of hereditary information, or is information distributed
across all such entities and activities (see
 Section 2.2)?
 And is it appropriate to extrapolate from information about other
species’ genomes to how the human genome operates (see
 Section 3.3)?
The concepts of mechanism, information, and
gene all figured quite prominently in the history of
molecular biology. Philosophers, in turn, have focused a great deal of
attention on these concepts in order to understand how they have been,
are, and should be used.
Molecular biologists discover and explain by identifying and
elucidating mechanisms, such as DNA replication, protein synthesis,
and the myriad mechanisms of gene expression. The phrase “theory
of molecular biology” was not used above and for good reason;
general knowledge in the field is represented by diagrams of
mechanisms (Machamer, Darden, and Craver 2000; Darden 2006a, 2006b;
Craver and Darden 2013; Baetu 2017). Discovering the mechanism that
produces a phenomenon is an important accomplishment for several
reasons. First, knowledge of a mechanism shows how something works:
elucidated mechanisms provide understanding. Second, knowing how a
mechanism works allows predictions to be made based upon the
regularity in mechanisms. For example, knowing how the mechanism of
DNA base pairing works in one species allows one to make predictions
about how it works in other species, even if conditions or inputs are
changed. Third, knowledge of mechanisms potentially allows one to
intervene to change what the mechanism produces, to manipulate its
parts to construct experimental tools, or to repair a broken, diseased
mechanism. In short, knowledge of elucidated mechanisms provides
understanding, prediction, and control. Given the general importance
of mechanisms and the fact that mechanisms play such a central role in
the field of molecular biology, it is not surprising that philosophers
of biology pioneered analyzing the concept of mechanism (see the entry
on mechanisms in science).
Starting in the 1990s, a number of philosophers focused squarely on
how the concept of a mechanism functions in science generally and
molecular biology specifically (Glennan and Illari 2017; see also the
entry on
 mechanisms in science).
 A number of characterizations of what a mechanism is have emerged
over the years (Bechtel and Abrahamsen 2005; Glennan 2002; Machamer,
Darden, and Craver 2000). Phyllis McKay Illari and Jon Williamson have
more recently offered a characterization that draws on the essential
features of all the earlier contributions:
A mechanism for a phenomenon consists of entities and activities
organized in such a way that they are responsible for the phenomenon.
(Illari and Williamson 2012: 120)
As an example, consider the phenomenon of DNA replication. As Watson
and Crick (1953a) famously noted upon discovery of the structure of
DNA, the macromolecule’s structure pointed to the mechanism of
DNA replication:
It has not escaped our notice that the specific pairing we have
postulated immediately suggests a possible copying mechanism for the
genetic material.
In short, the double helix of DNA (an entity with an organization)
unwinds (an activity) and new component parts (entities) bond (an
activity) to both parts of the unwound DNA helix. DNA is a nucleic
acid composed of several subparts: a sugar-phosphate backbone and
nucleic acid bases. When DNA unwinds, the bases exhibit weak charges,
properties that result from slight asymmetries in the molecules. These
weak charges allow a DNA base and its complement to engage in the
activity of forming hydrogen (weak polar) chemical bonds; the
specificity of this activity is due to the topological arrangements of
the weak polar charges in the subparts of the base. Ultimately,
entities with polar charges enable the activity of hydrogen bond
formation. After the complementary bases align, then the backbone
forms via stronger covalent bonding. The mechanism proceeds with
unwinding and bonding together (activities) new parts, to produce two
helices (newly formed entities) that are (more or less faithfully)
copies of the parent helix. (This process of “semi-conservative
replication” and the Meselson-Stahl experiment that confirmed it
are discussed in more detail in
 Section 3.4.)
Scientists rarely depict all the particular details when describing a
mechanism; representations are usually schematic, often depicted in
diagrams (see the entry on
 models in science).
 Such representations may be called a “model of a
mechanism”, or “mechanism schema”. A mechanism
schema is a truncated abstract description of a mechanism that can be
instantiated by filling it with more specific descriptions of
component entities and activities. An example is James Watson’s
(1965) diagram of his version of the central dogma of molecular
biology:
DNA → RNA → protein.
This is a schematic representation (with a high degree of abstraction)
of the mechanism of protein synthesis, which can be instantiated with
details of DNA base sequence, complementary RNA sequence, and the
corresponding order of amino acids in the protein produced by the more
specific mechanism. Molecular biology textbooks are replete with
diagrams of mechanism schemas. A mechanism schema can be instantiated
to yield a description of a particular mechanism. In contrast, a
mechanism sketch cannot (yet) be instantiated; components are (as yet)
unknown. Sketches have black boxes for missing components or grey
boxes whose function is known but whose entities and activities that
carry out that function are not yet elucidated. Such sketches guide
new research to fill in the details (Craver and Darden 2013).
The language of information appears often in molecular biology. Genes
as linear DNA sequences of bases are said to carry
“information” for the production of proteins. During
protein synthesis, the information is “transcribed” from
DNA to messenger RNA and then “translated” from RNA to
protein. With respect to inheritance, it is often said that what is
passed from one generation to the next is the
“information” in the genes, namely the linear ordering of
bases along complementary DNA strands. Historians of biology have
tracked the entrenchment of information-talk in molecular biology (Kay
2000) since its introduction.
The question for philosophers of biology is whether an analysis of the
concept of information can capture the various ways in which the
concept is used in molecular biology (e.g., Maynard Smith 2000). The
usage of “information” in the mathematical theory of
communication is too impoverished to capture the molecular biological
usage, since the coded sequences in the DNA are more than just a
signal with some number of bits that may or may not be accurately
transmitted (Sarkar 1996b,c; Sterelny and Griffiths 1999; Shannon and
Weaver 1949). Conversely, the usage in cognitive neuroscience, with
its talk of “representations” (e.g., Crick 1988) may be
said to be too rich, since the coded sequences in the DNA are also not
said to have within them a representation of the structure of the
protein (Darden 2006b). No definition of “information” as
it is used in molecular biology has yet received wide support among
philosophers of biology.
Stephen Downes (2006) helpfully distinguishes three positions on the
relation between information and the natural world:
These options may be read either ontologically or heuristically. A
heuristic reading of (1), for instance, views the talk of information
in molecular biology as useful in providing a way of talking and in
guiding research. And so the heuristic benefit of the information
concept can be defended without making any commitment to the
ontological status (Sarkar 2000). Indeed, one might argue that a vague
and open-ended use of information is valuable for heuristic purposes,
especially during early discovery phases in the development of a
field.
Philosophers’ discussions of the concept of information in
biology have also focused on its ontological reading. Three different
philosophical accounts of information serve as exemplars of
Downes’ three categories. Ulrich Stegmann (2005) provides an
example of Downes’ first category with his analysis of
template-directed synthesis. (Stegmann does explicitly allow that
components other than nucleotide sequences might contain what he calls
instructional information. However, his only example is a thought
experiment involving enzymes linearly ordered along a membrane;
nothing of the sort is known to actually exist or even seems very
likely to exist.) Stegmann calls this the sequentialization view.
Stegmann’s instructional account of genetic information requires
that the component carrying the information satisfy the following
conditions: an advance specification of the kind and order of steps
that yield a determinate outcome if the steps are carried out. On his
account, DNA qualifies as an instructional information carrier for
replication, transcription and translation. The sequence of bases
provides the order. The hydrogen bonding between specific bases and
the genetic code provide the specific kinds of steps. And the
mechanisms of replication, transcription, and translation yield
certain outcomes: a copy of the DNA double helix, an mRNA, and a
linear order of amino acids. Also, because DNA carries information for
a specific outcome, an error can occur as the mechanism operates to
produce that outcome; hence Stegmann’s account allows for errors
and error-correcting mechanisms (such as proof reading mechanisms that
correct DNA mutations). For more on this topic, see the entry on
 biological information.
Eva Jablonka (2002) is an example of Downes’ second category.
She argues that information is ubiquitous. She defines information as
follows: a source becomes an informational input when an interpreting
receiver can react to the form of the source (and variations in this
form) in a functional manner. She claims a broad applicability of this
definition. The definition, she says, accommodates information
stemming from environmental cues as well as from evolved signals, and
calls for a comparison between information-transmission in different
types of inheritance systems — the genetic, the epigenetic, the
behavioral, and the cultural-symbolic. On this view, genes have no
theoretically privileged informational status (Jablonka 2002:
583).
In line with Downes’ third category, C. Kenneth Waters argues
that information is a useful term in rhetorical contexts, such as
seeking funding for DNA sequencing by claiming that DNA carries
information. However, from an ontological perspective, Waters claims
that explication of DNA’s causal role has no need for the
concept of information. Genes, he argues, should not be viewed as
“immaterial units of information” (Waters 2000: 541). As
discussed in
 Section 2.3
 below, Waters’ focus is on stretches of DNA whose causal roles
are as actual specific difference makers in genetic mechanisms (Waters
2007). Talk of information is not needed; causal role function talk is
sufficient. (For more on Waters’ view see his entry on
 molecular genetics;
 for others who make similar points, see Sustar 2007; Weber 2005,
2006.)
The question of whether classical, Mendelian genetics could be (or
already has been) reduced to molecular biology (to be taken up in
 Section 3.1
 below) motivated philosophers to consider the connectability of the
term they shared: the gene. Investigations of reduction and scientific
change raised the question of how the concept of the gene evolved over
time, figuring prominently in C. Kenneth Waters’ (1990, 1994,
2007, see entry on
 molecular genetics),
 Philip Kitcher’s (1982, 1984) and Raphael Falk’s (1986)
work. Over time, however, philosophical discussions of the gene
concept took on a life of their own, as philosophers raised questions
independent of the reduction debate: What is a gene? And, is there
anything causally distinct about DNA? (see the entry on the
 gene)
Falk (1986) explicitly asked philosophers and historians of biology,
“What is a Gene?” Discoveries such as overlapping genes,
split genes, and alternative splicing (discussed in
 Section 1.2)
 made it clear that simply equating a gene with an uninterrupted
stretch of DNA would no longer capture the complicated
molecular-developmental details of mechanisms such as gene expression
(Downes 2004; Luc-Germain, Ratti and Boem 2015). In an effort to
answer Falk’s question, two general trends have emerged in the
philosophical literature: first, distinguish multiple gene concepts to
capture the complex structural and functional features separately, or
second, rethink a unified gene concept to incorporate such complexity.
(For a survey of gene concepts defended by philosophers, see Griffiths
and Stotz 2007, 2013.; Rheinberger and Muller-Wille 2018)
A paradigmatic example of the first line came from Lenny Moss’s
distinction between Gene-P and Gene-D (Moss 2001, 2002). Gene-P
embraced an instrumental preformationism (providing the
“P”); it was defined by its relationship to a phenotype.
In contrast, Gene-D referred to a developmental resource (providing
the “D”); it was defined by its molecular sequence. An
example will help to distinguish the two: When one talked about the
gene for cystic fibrosis, the most common genetic disease affecting
populations of Western European descent, the Gene-P concept was being
utilized; the concept referred to the ability to track the
transmission of this gene from generation to generation as an
instrumental predictor of cystic fibrosis, without being contingent on
knowing the causal pathway between the particular sequence of DNA and
the ultimate phenotypic disease. The Gene-D concept, in contrast,
referred instead to just one developmental resource (i.e., the
molecular sequence) involved in the complex development of the
disease, which interacted with a host of other such resources
(proteins, RNA, a variety of enzymes, etc.); Gene-D was indeterminate
with regards to the ultimate phenotypic disease. Moreover, in cases of
other diseases where there are different disease alleles at the same
locus, a Gene-D perspective would treat these alleles as individual
genes, while a Gene-P perspective treats them collectively as
“the gene for” the disease. (For other examples of
gene-concept dividers, see Keller’s distinction between the gene
as a structural entity and the gene as a functional entity as well as
Baetu’s distinction between the gene as a syntax-based concept
and the gene as a mapping concept (Baetu 2011; Keller 2000).)
A second philosophical approach for conceptualizing the gene involved
rethinking a single, unified gene concept that captured the
molecular-developmental complexities. For example, Eva Neumann-Held
(Neumann-Held 1999, 2001; Griffiths and Neumann-Held 1999) claimed
that a “process molecular gene concept” (PMG) embraced the
complicated developmental intricacies. On her unified view, the term
“gene” referred to “the recurring process that leads
to the temporally and spatially regulated expression of a particular
polypeptide product” (Neumann-Held 1999). Returning to the case
of cystic fibrosis, a PMG for an individual without the disease
referred to one of a variety of transmembrane ion-channel templates
along with all the epigenetic factors, i.e., nongenetic influences on
gene expression, involved in the generation of the normal polypeptide
product. And so cystic fibrosis arose when a particular stretch of the
DNA sequence was missing from this process. (For another example of a
gene-concept unifier, see Falk’s discussion of the gene as a DNA
sequence that corresponded to a single norm of reaction for various
molecular products based on varying epigenetic conditions (Falk
2001).)
Relatedly, philosophers have also debated the causal distinctiveness
of DNA. Consider again the case of cystic fibrosis. A stretch of DNA
on chromosome 7 is involved in the process of gene expression, which
generates (or fails to generate) the functional product that
transports chloride ions. But obviously that final product results
from that stretch of DNA as well as all the other developmental
resources involved in gene expression, be it in the expression of the
functional protein or the dysfunctional one. Thus, a number of authors
have argued for a causal parity thesis, wherein all developmental
resources involved in the generation of a phenotype such as cystic
fibrosis are treated as being on par (Griffiths and Knight 1998;
Robert 2004; Stotz 2006).
Waters (2007, see also his entry on
 molecular genetics),
 in reply, has argued that there is something causally distinctive
about DNA. Causes are often conceived of as being difference makers,
in that a variable (i.e., an entity or activity in a mechanism) can be
deemed causal when a change in the value of that variable would
counterfactually have led to a different outcome (see the entry on
 scientific explanation).
 According to Waters, there are a number of potential
difference makers in the mechanisms involved in developing or not
developing cystic fibrosis; that is, an individual with two normal
copies of the gene could still display signs of cystic fibrosis if a
manipulation was done to the individual’s RNA polymerase (the
protein responsible for transcribing DNA to RNA), thereby undermining
the functional reading of the stretch of DNA. So RNA polymerase is a
difference maker in the development or lack of development of cystic
fibrosis, but only a potential difference maker, since variation in
RNA polymerase does not play a role in the development or lack of
development of cystic fibrosis in natural populations. The stretch of
DNA on chromosome 7, however, is an actual difference maker.
That is, there are actual differences in natural human populations on
this stretch of DNA, which lead to actual differences in developing or
not developing cystic fibrosis; DNA is causally distinctive, according
to Waters, because it is an actual difference maker. Advocates of the
parity thesis are thus challenged to identify the other resources (in
addition to DNA) that are actual difference makers.
Recently, Paul Griffiths and Karola Stotz (2013) have responded to
this challenge by offering examples in which, depending on context,
regulatory mechanisms can either contribute additional information to
the gene products or create gene products for which there is no
underlying sequence. Thus, according to Griffiths and Stotz, to assign
a causally distinctive role to DNA, as Waters does, is to ignore key
aspects of how the gene makes its product.
In addition to analyzing key concepts in the field, philosophers have
employed case studies from molecular biology to address more general
issues in the philosophy of science, such as reduction, explanation,
extrapolation, and experimentation. For each of these philosophical
issues, evidence from molecular biology directs philosophical
attention toward understanding the concept of a mechanism for
addressing the topic.
Reduction may be understood in multiple ways depending on what it is
that is being reduced (see the entry on
 scientific reduction).
 Theory reduction pertains to whether or not theories from one
scientific field can be reduced to theories from another scientific
field. In contrast, explanatory reduction (often united with
methodological reduction) pertains to whether or not explanations that
come from lower levels (often united with methodologies that
investigate those lower levels) are better than explanations that come
from higher levels. Philosophical attention to molecular biology has
contributed to debates about both of these senses of reduction (see
the entry on
 reductionism in biology).
Philosophy of biology first came to prominence as a sub-specialty of
philosophy of science in the 1970s when it offered an apparent case
study by which to judge how theories from one field may reduce to
theories from another field. The specific question was: might classic,
Mendelian genetics reduce to molecular genetics (see the entry on
 
molecular genetics)?
 Kenneth Schaffner used and developed Ernst Nagel’s (1961)
analysis of derivational theory reduction to argue for the reduction
of classical Mendelian genetics (T2) to molecular
biology (T1) and refined it over many years
(summarized in Schaffner 1993). The goal of formal reduction was to
logically deduce the laws of classical genetics (or its improved
successor, “modern transmission genetics”
T2*) from the laws of molecular biology. Such a
derivation required that all the terms of T2* not
in T1 had to be connected to terms in
T1 via correspondence rules. Hence, Schaffner
endeavored to find molecular equivalents of such terms as
“gene”, as well as predicate terms, such as “is
dominant”. David Hull (1974) criticized formal reduction, argued
against Schaffner’s claims, and suggested, instead, that perhaps
molecular biology replaced classical genetics.
Even though Schaffner and Hull were engaged in a debate over theory
reduction, they simultaneously admitted that the question of formal
theory reduction was rather peripheral to what scientists actually did
and studied (Schaffner 1974b; Hull 1974). And indeed, while the theory
reduction debate was playing out, a number of philosophers of biology
switched attention from scientific theories to the
stuff in nature that scientists investigated. William Wimsatt
(1976) argued for a shift in the reduction debate from talk of
relations between theories to talk of decompositional explanation via
mechanisms. And Lindley Darden and Nancy Maull (1977) focused
attention on the bridges between fields formed by part-whole
relations, structure-function relations, and cause-effect
relations.
This shift in attention was a precursor to understanding the
philosophy of science through the lens of mechanisms. Darden, building
on the work of Machamer, Darden, and Craver (2000), has more recently
returned to the question of how Mendelian and molecular genetics are
related and viewed it through this lens (Darden 2005). Rather than
understanding the relationship as one of reduction, she suggests they
can be understood as relating via a focus on different working
entities (often at different size levels) that operate at different
times. Thus, the relation was one of integration of sequentially
operating chromosomal and molecular hereditary mechanisms rather than
reduction. (For an alternative but still integrative reading of the
relationship between classical genetics and molecular biology that
focuses on their shared functional units, see Baetu 2010.)
Reduction can also be about explanation and methodology. That is,
reduction can be about using reductive methodologies to dig down to
lower levels because the thought is that this exercise leads to more
reductive explanations and more reductive explanations are better than
explanations at higher levels. Alex Rosenberg (1997, 2006)
controversially divided biology into molecular biology and everything
else, which he dubbed “functional biology”.
“Reductionism”, Rosenberg argued,
is the thesis that biological theories and explanations that employ
them do need to be grounded in molecular biology and ultimately
physical science, for it is only by doing so that they can be
improved, corrected, strengthened, and made more accurate and more
adequate and completed. (Rosenberg 2006: 4)
Hence, the task of this explanatory reduction is to explain all
functional biological phenomena via molecular biology.
Critics and defenders of Rosenberg’s view have discussed
organizational and contextual features not captured by molecular
biological principles. These included orientation of the embryo in the
earth’s gravitational field and other spatial, regulatory, and
dynamical properties of developing systems (e.g., see Delehanty 2005;
Frost-Arnold 2004; Keller 1999; Laubichler and Wagner 2001; Love et
al. 2008; Robert 2001, 2004). This particular debate can be understood
as an instance of a more general debate occurring in biology and
philosophy of biology about whether investigations of lower-level
molecular biology are better than investigations of high-level systems
biology (Baetu 2012a; Bechtel and Abrahamsen 2010; De Backer, De
Waele, and Van Speybroeck 2010; Huettemann and Love 2011; Marco 2012;
Morange 2008; Pigliucci 2013; Powell and Dupre 2009; see also the
entries on
 feminist philosophy of biology,
 philosophy of systems and synthetic biology, and
 multiple realizability).
Traditionally, philosophers of science took successful scientific
explanations to result from derivation from laws of nature (see the
entries on
 laws of nature
 and
 scientific explanation).
 On this deductive-nomological account (Hempel and Oppenheim 1948), an
explanation of particular observation statements was analyzed as
subsumption under universal (applying throughout the universe),
general (exceptionless), necessary (not contingent) laws of nature
plus the initial conditions of the particular case. Philosophers of
biology have criticized this traditional analysis as inapplicable to
biology, and especially molecular biology.
Since the 1960s, philosophers of biology have questioned the existence
of biological laws of nature. J. J. C. Smart (1963) emphasized the
earth-boundedness of the biological sciences (in conflict with the
universality of natural laws). No purported “law” in
biology has been found to be exceptionless, even for life on earth (in
conflict with the generality of laws). And John Beatty (1995) argued
that the purported “laws” of, for example, Mendelian
genetics, were contingent on evolution (in conflict with the necessity
of natural laws). (For further discussion, see Brandon 1997; Lange
2000; Mitchell 1997; Sober 1997; Waters 1998; Weber 2005.) Hence,
philosophers’ search for biological laws of nature,
characterized as universal, necessary generalizations, has ceased.
Without traditional laws of nature from which to derive explanations,
philosophers of biology have been forced to rethink the nature of
scientific explanation in biology and, in particular, molecular
biology. Two accounts of explanation emerged: the unificationist and
the causal-mechanical. Philip Kitcher (1989, 1993) developed a
unificationist account of explanation, and he and Sylvia Culp
explicitly applied it to molecular biology (Culp and Kitcher 1989).
Among the premises of the “Watson-Crick” argument schema
were “transcription, post-transcriptional modification and
translation for the alleles in question”, along with details of
cell biology and embryology for the organisms in question (Kitcher
1989). An explanation of a particular pattern of distribution of
progeny phenotypes in a genetic cross resulted from instantiating the
appropriate deductive argument schema: the variables were filled with
the details from the particular case and the conclusion derived from
the premises.
Working in the causal-mechanical tradition pioneered by Wesley Salmon
(1984, 1998), other philosophers turned to understanding mechanism
elucidation as the avenue to scientific explanation in biology
(Bechtel and Abrahamsen 2005; Bechtel and Richardson 1993; Craver
2007; Darden 2006a; Glennan 2002; Machamer, Darden, and Craver 2000;
Sarkar 1998; Schaffner 1993; Woodward 2002, 2010). There are
differences between the various accounts of a mechanism, but they hold
in common the basic idea that a scientist provides a successful
explanation of a phenomenon by identifying and manipulating variables
in the mechanisms thereby determining how those variables are situated
in and make a difference in the mechanism; the ultimate explanation
amounts to the elucidation of how those mechanism components act and
interact to produce the phenomenon under investigation. As mentioned
above (see
 Section 2.1,
 see also entry on mechanisms in science), an elucidated mechanism
allows for the explanatory features of understanding, prediction, and
control.
There are several virtues of the causal-mechanical approach to
understanding scientific explanation in molecular biology. For one, it
is truest to molecular biologists’ own language when engaging in
biological explanation. Molecular biologists rarely describe their
practice and achievements as the development of new theories; rather,
they describe their practice and achievements as the elucidation of
molecular mechanisms (Baetu 2017; Craver 2001; Machamer, Darden,
Craver 2000). Another virtue of the causal-mechanical approach is that
it captures biological explanations of both regularity and variation.
Unlike in physics, where a scientist assumes that an electron is an
electron is an electron, a biologist is often interested in precisely
what makes one individual different from another, one population
different from another, or one species different from another.
Philosophers have extended the causal-mechanical account of
explanation to cover biological explanations of variation, be it
across evolutionary time (Calcott 2009) or across individuals in a
population (Tabery 2009, 2014). Tabery (2009, 2014) characterized
biological explanations of variation across individuals in a
population as the elucidation of “difference mechanisms”.
Difference mechanisms are regular causal mechanisms made up of
difference-making variables, one or more of which are actual
difference makers (see
 Section 2.3
 for the discussion of Waters’ (2007) concept of an actual
difference maker). There is regularity in difference mechanisms;
interventions made on variables in the mechanisms that change the
values of the variables lead to different outcomes in the phenomena
under investigation. There is also variation in difference mechanisms;
interventions need not be taken to find differences in outcomes
because, with difference mechanisms, some variables are actual
difference makers which already take different values in the natural
world, resulting in natural variation in the outcomes.
But philosophers have also raised challenges to the causal-mechanical
approach. One complaint has been that mechanistic explanations
don’t work well in systems biology, a field distinct from
molecular biology but that nonetheless draws on the work of molecular
biologists. While some argue that systems biology is best explained
using mechanisms (cf. Boogerd et al. 2007; Matthiessen 2017;
Richardson and Stephan 2007; Bechtel 2011; Bechtel and Abrahamsen
2013), others argue that it requires non-mechanistic explanation (cf.
Braillard 2010; Kuhlmann 2011; Silberstein and Chemero 2013).
Another complaint has been that mechanistic explanations fail to
provide a satisfactory account of the complexity of living systems
because they fail to devote serious attention to the “thoroughly
processual character of living systems” (Dupre 2012: 19). Those
who prefer a “process-oriented” ontology (Bapteste and
Dupre 2013; Bickhard 2011; Campbell 2015; Dupre 2012; Jaeger and Monk
2015; Jaeger et al. 2012; Meincke 2018; Nicholson and Dupre 2018)
argue that mechanistic accounts mistakenly assume that parts composing
a mechanism can be identified independently of the activities or
processes in which they are involved. Instead, as Anne Sophie Maincke
explains, if there are parts or substances, they must be
“reconceptualised as stabilized higher-order processes that
persist as long as stabilization can be maintained” (2018).
Processes are ontologically primary. Recent literature in molecular
biology on molecular pathways (cf. Boniolo and Campaner 2018; Brigandt
2018; Ioannides and Psillos 2017; Ross 2018) seems to be another
instantiation of this shift from mechanistic to processual
explanations. But Christopher Austin (2016) has recently challenged
the idea that processual explanations can be sufficiently grounded
“without the metaphysical underpinning of the very
mechanisms which processes purport to replace” (639).
As discussed earlier in the historical sections, molecular biologists
have relied heavily on model organisms (see the entry on
 models in science).
 The model organisms that were used to lay down the foundation of
molecular biology served as “exemplary models” in contrast
to what today are called “surrogate models”—the
distinction comes from Jessica Bolker (2009). According to Bolker,
exemplary models are “representatives of a broader group”
and the goal of using them is “to elucidate fundamental or
general biological patterns and mechanisms” (487). But making
inferences from a single exemplary model to general biological
patterns has been cause for worry. What grounds do biologists have for
believing that what is true of a mere model is true of many different
organisms? One answer, provided by Marcel Weber (2005), is that the
generality of biological knowledge obtained from studying exemplary
models can be established on evolutionary grounds. According to Weber,
if a mechanism is found in a set of phylogenetically distant
organisms, this provides evidence that it is also likely to be found
in all organisms that share a common ancestor with the organisms being
compared.
Exemplary models still play an important role in molecular biology,
but a new type of model organism, the “surrogate model”,
has become increasingly popular. According to Bolker, surrogate models
“act as proxies for specific targets” and are often used
in biomedical research “where the objective is to understand the
mechanisms and etiology of human disease, and ultimately to develop
treatments” (Bolker 2009: 489). Unlike the aim of exemplary
models, the representative aim of a surrogate model is not necessarily
to be broad. Instead, it’s to faithfully replicate a specific
target. For example, biomedical researchers frequently expose
surrogate models to harmful chemicals with the aim of modeling human
disease. However, if a chemical proves to be carcinogenic in rats, for
example, there is no guarantee that it will also cause cancer in
humans.
This difficulty of justifying the inference from rats to humans or,
more broadly, of “transferring causal generalizations from one
context to another when homogeneity cannot be presumed” (Steel
2008: 3) is known as the problem of extrapolation. Although
this problem is not unique to surrogate models, it often arises when
biomedical researchers use them to replicate human disease at the
molecular level. Consequently, philosophers who write about the
problem of extrapolation in the context of molecular biology often
focus on such models (see, for example, Ankeny 2001; Baetu 2016;
Bechtel and Abrahamsen 2005; Bolker 1995; Burian 1993b; Darden 2007;
LaFollette and Shanks 1996; Love 2009; Piotrowska 2013; Schaffner
1986; Steel 2008; Weber 2005; Wimsatt 1998).
Within the context of surrogate models, any successful solution to the
problem of extrapolation must explain how inferences can be justified
given causally relevant differences between models and their targets
(Lafollette and Shanks 1996). It must also avoid what Daniel Steel
(2008) calls the “extrapolator’s circle”, which
arises when attempting to determine whether the model and its target
are similar enough in casually relevant respects.
One way to escape the extrapolator’s circle is to black box the
mechanisms being compared and instead treat the problem of
extrapolation as a statistical problem (cf. Cook and Campbell 1979).
This method avoids the circle because it eliminates the need to know
if two mechanisms are similar. All that matters is that two outcomes
are produced to a statistically significant degree, given the same
intervention. For this reason, statistically significant outcomes in
clinical trials are at the top of the evidence hierarchy in biomedical
research (Sackett et al. 1996). One problem with relying merely on
statistics to solve the problem of extrapolation, however, is that it
cannot show that an observed correlation between model and target is
the result of intervention and not a confounder.
A different strategy for avoiding the extrapolator’s circle is
to remove the black box and compare the two mechanisms but argue that
they do not have to be causally similar at every stage for
extrapolation to be justified. This approach avoids the circle because
the suitability of a model can be established given only partial
information about the target. For example, Steel argues that only the
stages downstream from the point where the mechanisms in the model and
target are likely to differ need to be compared, since the point where
differences are likely will serve as a bottleneck through which the
eventual outcome must be produced.
Though promising, criticisms have been raised against Steel’s
mechanistic approach to extrapolation. One worry, raised by Jeremy
Howick et al. (2013), is that in order to identify the bottlenecks and
downstream differences, we must know more about the target than Steel
admits. If what we know about the mechanism in the target exceeds what
we know about the mechanism in the model, the extrapolator’s
circle will not have been avoided. Another worry with Steel’s
approach to extrapolation is that it doesn’t avoid the masking
problem. According to Julian Reiss (2010), Federica Russo (2010), and
Brendan Clarke et al. (2014), even if we establish that X
causes Y through some mechanism, this doesn’t seem to
eliminate the possibility of there being several paths that link
X to Y. For example, there may be an upstream difference
that affects the outcome but does not pass through the downstream
stages of the mechanism. (This problem is taken up again below in
 Section 3.4.)
 A third worry with Steel’s approach, which comes from Tudor
Baetu (2016), is that mechanistic accounts of the experimental model
of interest “combine data from tens or even hundreds of distinct
experimental setups” (956). The resulting big picture account of
the experimental model is an aggregate of findings that do not
describe a mechanism that actually exists in any cell or organism.
Instead, as a number of authors have also pointed out (Huber and Keuck
2013; Lemoine 2017; Nelson 2013), the mechanism of interest is often
stipulated first and then verified piecemeal in many different
experimental organisms. The result is what Mael Lemoine (2017) has
called a “theoretical chimera”, a hypothesis supported by
heterogeneous partial models. On the chimera view of extrapolation,
it’s possible that all one-to-one analogies work, and yet the
aggregate theoretical chimera model fails. 
While theoretical chimeras seem to further complicate Steel’s
mechanistic account of extrapolation, actual chimeras also raise
questions that Steel does not address. Consider, for example,
humanized mice that have been engineered to carry a “partial or
complete human physiological system” (Macchiarini et al. 2005:
1307). These genetically engineered rodents are supposed to make
extrapolation more reliable by simulating a variety of human diseases,
e.g., asthma, diabetes, cancer, etc. As Monika Piotrowska (2013)
points out, however, this raises a new problem. The question is no
longer how an inference from model to target can be justified given
existing differences between the two, but rather, in what way should
these mice be modified in order to justify extrapolation to
humans? Piotrowska has proposed three conditions that should be met in
the process of modification to ensure that extrapolation is justified.
The first two requirements demand that we keep track of parts and
their boundaries during transfer, which presupposes a mechanistic view
of human disease, but the third requirement—that the constraints
that might prevent the trait from being expressed be
eliminated—highlights the limits of using a mechanistic approach
when making inferences from humanized mice to humans. As Piotrowska
explains,
without the right context, even the complete lack of differences
between two mechanisms cannot justify the inference that what is true
of one mechanism will be true of another (Piotrowska 2013: 453).
If Piotrowska is right, Steel’s mechanistic solution to the
problem of extrapolation seems to have reached its limit. As our
ability to manipulate biological models advances, philosophers will
need to revisit the problem of extrapolation and seek out new
solutions.
The history of molecular biology is in part the history of
experimental techniques designed to probe the macromolecular
mechanisms found in living things. Philosophers in turn have looked to
molecular biology as a case study for understanding how
experimentation works in science—how it contributes to
scientific discovery, distinguishes correlation from causal and
constitutive relevance, and decides between competing hypotheses
(Barwich and Baschir 2017). In all three cases, the concept of a
mechanism is central to understanding the function of experimentation
in molecular biology (also see the entry on experimentation in
biology).
Take discovery. Throughout much of the twentieth century, philosophers
of science treated scientific discovery as if it were off-limits to
philosophical analysis; philosophers could evaluate the rational
process of confirmation, the argument went, but the psychological
process of discovery (the “aha!” moment) fell into the
realm of creativity (Popper 1965; see the entry on
 scientific discovery).
 Darden has countered with a focus on the strategies that scientists
employ to construct, evaluate, and revise mechanical explanations of
phenomena; on her view, discovery is a piecemeal, incremental, and
iterative process of mechanism elucidation. In the 1950s and 1960s,
for example, scientists from both molecular biology and biochemistry
employed their own experimental strategies to elucidate the mechanisms
of protein synthesis that linked DNA to the production of proteins.
Molecular biologists moved forward from DNA using experimental
techniques such as x-ray crystallography and model building to
understand how the structure of DNA dictated what molecules it could
interact with; biochemists simultaneously moved backward from the
protein products using in vitro experimental systems to
understand the chemical reactions and chemical bonding necessary to
build a protein. They met in the middle at RNA, ultimately leading to
Watson’s famous mechanism schema DNA → RNA → protein.
Far from being philosophically inscrutable, Darden points out that the
molecular biologists were “forward chaining” while the
biochemists were “backward chaining”, using information
about the working entities and activities that they knew about to
infer what could come next or before in the mechanism of protein
synthesis (Darden 2006a: chapters 3, 12; Craver and Darden 2013:
chapter 10).
Tudor Baetu builds on the contemporary philosophy of mechanism
literature as well to provide an account of how different experiments
in molecular biology move from finding correlations, to establishing
causal relevance, to establishing constitutive relevance (Baetu
2012b). Much recent philosophical attention has been given to the
transition from correlation to causal relevance. On a manipulationist
account of causal relevance, some factor X is determined to be
causally relevant to some outcome Y when interventions on
X can be shown to produce the change in Y. Experiments
from molecular biology often take this form, which Baetu calls
“one-variable experiments” because they involve a single
intervention made on X in the experiment to establish the
causal relevance to Y. But these one-variable experiments,
Baetu cautions, do not necessarily provide information about the
causal mechanism that links X to Y. Is X causally
relevant to Y by way of mechanism A, mechanism B,
or some other unknown mechanism? Baetu, drawing on his own
experimental research in molecular oncology, shows that scientists
probe the mechanical link by performing “two-variable
experiments”. In a two-variable experiment, two interventions
are simultaneously made on the initial factor and some component
postulated in the mechanical link, thereby establishing both causal
and constitutive relevance. 
Experiments from molecular biology have also figured into
philosophical discussions about the possibility of “crucial
experiments”. An experiment is taken to be a crucial experiment
if it is devised so as to result in the confirmation of one hypothesis
by way of refuting other competing hypotheses. But the very idea of a
crucial experiment, Pierre Duhem pointed out, assumes that the set of
known competing hypotheses contains all possible explanations of a
given phenomenon such that the refutation of all but one of the
hypotheses deductively ensures the confirmation of the hypothesis left
standing. However, if there are in fact unknown alternatives that
weren’t considered in the set of competing hypotheses, Duhem
warned, then the refutation cannot guarantee the confirmation of the
last hypothesis standing (also see the entry on Pierre Duhem). (Duhem
actually raised two problems for crucial experiments—the problem
mentioned above, as well as the problem of auxiliary assumptions,
which any hypothesis brings with it; for reasons of space, we will
only discuss the former here.) Marcel Weber (2009) has utilized a
famous experiment from molecular biology to offer a different vision
of how crucial experiments work. After Watson and Crick discovered the
double helical structure of DNA, molecular biologists turned their
attention to how that macromolecule could be replicated (see
 Section 1.2
 above). The focus was in part on the fact that the DNA was twisted
together in a helix, and so the challenge was figuring out what
process could unwind and replicate that complexly wound molecule.
Three competing hypotheses emerged, each with their own prediction
about the extent to which newly replicated DNA double helices
contained old DNA strands versus newly synthesized material:
semi-conservative replication, conservative replication, and
dispersive replication. Matthew Meselson and Frank Stahl, at Cal Tech,
devised a method for testing among these competing hypotheses (see The
Semi-Conservative Replication of DNA in
 Other Internet Resources).
 They grew E.coli on a medium that contained a unique,
heavier form of nitrogen which could be taken up into the
E.coli DNA; then they transferred that E.coli with
heavy DNA to a medium with the more common, lighter form of nitrogen
and let the E.coli replicate there. By then taking regular
samples of the replicating E.coli and examining the DNA of
the subsequent generations, they determined the extent to which the
new DNA was a mix of old DNA strands or newly synthesized material.
The result was a clear victory for semi-conservative replication, and
the Meselson-Stahl experiment became known as the “most
beautiful experiment in biology” (Meselson and Stahl 1958;
Holmes 2001). Weber argues that we should understand the quick uptake
of Meselson and Stahl’s experimental result as an instance of
inference to the best explanation (as opposed to Duhem’s
deductive characterization). Meselson and Stahl, Weber claims, took
the physiological mechanism of DNA replication and then embedded it in
an “experimental mechanism”; that experimental mechanism
then generated the data pattern of heavy-vs-light DNA in subsequent
E.coli generations. Moreover, any hypothesis of DNA
replication had to satisfy mechanistic constraints imposed by what was
already known about the physiological mechanism—that DNA was a
double helix, and that the sequence of nucleotides in the DNA needed
to be preserved in subsequent generations. So Duhem’s concern
about unknown alternatives was alleviated because known mechanistic
constraints limited the set of possible hypotheses that could generate
the phenomenon. On Weber’s reading, the mechanistic constraints
culled the set of possible hypotheses for DNA replication to
semi-conservative replication, conservative replication, and
dispersive replication; then, among that set, Meselson and Stahl
devised an experimental mechanism such that semi-conservative
replication was the best explanation of the data pattern they found.
(For a critique, see Baetu 2017.)
An overview of the history of molecular biology revealed the original
convergence of geneticists, physicists, and structural chemists on a
common problem: the nature of inheritance. Conceptual and
methodological frameworks from each of these disciplinary strands
united in the ultimate determination of the double helical structure
of DNA (conceived of as an informational molecule) along with the
mechanisms of gene replication, mutation, and expression. With this
recent history in mind, philosophers of molecular biology have
examined the key concepts of the field: mechanism, information, and
gene. Moreover, molecular biology has provided cases for addressing
more general issues in the philosophy of science, such as reduction,
explanation, extrapolation, and experimentation.