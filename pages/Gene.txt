Before dealing with the historical stages of the gene concept's
tangled development, we will have to see how it came into being. It
was only in the nineteenth century that heredity became a major
problem to be dealt with in biology (López Beltrán 2004;
Müller-Wille and Rheinberger 2007 and 2012).  With the rise of heredity as
a biological research area the question of its material basis and of
its mechanism took shape. In the second half of the nineteenth
century, two alternative frameworks were proposed to deal with this
question. The first one conceived of heredity as a force whose
strength was accumulated over the generations, and which, as a
measurable magnitude, could be subjected to statistical analysis. This
concept was particularly widespread among nineteenth-century breeders
(Gayon and Zallen 1998) and influenced Francis Galton and the
so-called “biometrical school” (Gayon 1998, 105-146).  The
second framework saw heredity as residing in matter that was
transmitted from one generation to the next. Two major trends are to
be differentiated here. One of them regarded hereditary matter as
particulate and amenable to breeding analysis. Charles Darwin, for
example, called the presumed hereditary particles
“gemmules”; Hugo de Vries, “pangenes”. None of
these nineteenth- century authors, however, thought of associating
these particles with a particular hereditary substance. They all
believed that they consisted of the very same stuff that the rest of
the organism was made of, so that their mere growth, recombination and
accumulation en masse would make visible the particular
traits for which they were responsible. A second category of
biologists in the second half of the nineteeenth century, to whom Carl
Naegeli and August Weismann belonged, distinguished the body
substance, the “trophoplasm” or “soma”, from a
specific hereditary substance, the “idioplasm” or
“germ plasm”, which was assumed to be responsible for
intergenerational hereditary continuity. However, they took this
idioplasmic substance as being, not particulate, but highly
organized. In the case of Weismann it remained intact in the germ
cells, but irreversibly differentiated in the body cells during
development. In the case of Naegeli it extended even from cell to cell
and throughout the whole body, a capillary hereditary system analogous
to the nervous system (Robinson 1979; Churchill 1987, Rheinberger 2008).
Mendel stands out among these biologists, although he worked within a
well-defined botanical tradition of hybrid research. He is generally
considered as the precursor to twentieth-century genetics (see,
however, Olby 1979 and, for a more recent discussion, Orel and Hartl
1997). As Jean Gayon has argued, Mendel's 1865 paper attacked heredity
from a wholly new angle, interpreting it not as a measurable
magnitude, as the biometrical school did at a later stage, but as
“a certain level of organization,” a “structure in a
given generation to be expressed in the context of specific
crosses.” This is why Mendel applied a “calculus of
differences,” i.e., combinatorial mathematics to the resolution
of hereditary phenomena (Gayon 2000, 77-78). With that, he introduced
a new formal tool for an analysis of hybridization experiments that
was at the same time based on a new experimental regime: the selection
of pairs of alternative and “constant” (i.e., heritable)
traits. Mendel believed that these traits were related by a
“constant law of development” to certain
“elements” or “factors” in the reproductive
cells from which organisms developed. An analysis of the distribution
of alternative traits in the progeny of hybrids could therefore reveal
something about the relationship that the underlying
“factors” entered when united in the hybrid parent
organism (Müller-Wille and Orel 2007).
The year 1900 can be seen as the annus mirabilis that gave
birth to a new discipline soon to be called genetics. During that
year, three botanists, Hugo de Vries, Carl Correns, and Erich
Tschermak, reported on their breeding experiments of the late 1890s
and claimed to have confirmed the regularities in the transmission of
characters from parents to offspring that Mendel had already presented
in his seminal paper of 1865 (Olby 1985, 109-37).  Basically, in their
experimental crosses with Zea mays, Pisum,
and Phaseolus, they observed that the elements responsible for
pairs of alternative traits, “allelomorphs” in the later
terminology of William Bateson (1902), which soon came into general
use under the abbreviation of “alleles” segregated
randomly in the second filial generation (Mendel's law of
segregation), and that these elements were transmitted independently
from each other (Mendel's law of independent assortment). The
additional observation that sometimes several elements behaved as if
they were linked, contributed to the assumption soon promoted by
Walter Sutton and by Theodor Boveri that these elements were located
in groups on different chromosomes in the nucleus. Thus the chromosome
theory of inheritance assumed that the regularities of character
transmission were grounded in cytomorphology, in particular the
nuclear morphology with its individual chromosomes keeping their
identity over the generations (Coleman 1965; Martins 1999).
Despite initial resistance from the biometrical school (Provine 1971;
Mackenzie and Barnes 1979), awareness rapidly grew that the
possibility of an independent assortment of discrete hereditary
factors according to the laws of probability was to be seen as the
very cornerstone of a new “paradigm” of inheritance (Kim
1994). This went together after an initial period of conflation by
what Elof Carlson called the “unit-character fallacy”
(Carlson 1966, ch. 4) with the establishment of a categorical
distinction between genetic factors on the one hand
and traits or
characters on the other hand. The masking effect of dominant traits over
recessive ones and the subsequent reappearance of recessive traits were
particularly instrumental in stabilizing this distinction (Falk 2001).
Furthermore, it resonated with the earlier concept of two material regimes,
one germinal and one bodily, already promoted by Naegeli and Weismann.
Yet if—as Correns stated in his first review on the new
Mendelian literature in 1901—“we cannot uphold the idea
of a permanent fixation [of the hereditary factors] in the germ plasm,
but have to assume, because of their miscibility, some mobility at
least at certain times,” and if chromosomal coupling was a
possible, but not a necessary and general mechanism of conveying
structure to inheritance, how was one to explain the successive and
regular physiological deployment of the dispositions (Anlagen)
in the orderly development of the organism? To resolve this
difficulty, Correns came up with the following, as he called it,
“heresy”:
In this way, Correns, at the beginning of the first decade of the
twentieth century, distinguished a hereditary space with an
independent logic and metrics from another, physiological and
developmental space represented by the cytoplasm. Toward the end of
the first decade of the twentieth century, after Bateson had coined
the term genetics for the emerging new field of transmission
studies in 1906, Wilhelm Johannsen codified this distinction by
introducing the notions of genotype and phenotype,
respectively, for these two spaces. In contrast to Correns, Johannsen
considered genotyope and phenotype as abstract entities, not confining
them to certain cellular spaces and remaining skeptical about the
chromosome theory of inheritance throughout his life. In addition, for
the elements of the genotype, Johannsen proposed the notion
of gene, which for him was a concept “completely free of
any hypothesis” regarding localization and material constitution
(Johannsen 1909, 124).
Johannsen's codification, which was based on microbiology's
“pure culture” approach, breeders' practices of separating
“pure lines” as well as Richard Woltereck's notion of an
innate “norm of reaction”, was gradually taken up by the
genetics community and has profoundly marked all of twentieth century
biology (Allen 2002, Müller-Wille 2007). We can safely say that
it instituted the gene as an epistemic object to be studied within
its proper space, and with that an “exact, experimental
doctrine of heredity” (Johannsen 1909, 1) that concentrated on
transmission only and not on the development of the organism in its
environment. Some historians have spoken of a “divorce” of
genetical from embryological concerns with regard to this separation
(Allen 1986; Bowler 1989). Others hold that this separation was itself
an expression of the embryological interests of early geneticists in
their search for “developmental invariants” (Gilbert 1978;
Griesemer 2000). Be that as it may, the result was that the relations
between the two spaces, once separated by abstraction, were now
elucidated experimentally in their own right (Falk 1995). Michel
Morange observed that this “division was logically
absurd”—from hindsight but—“historically and
scientifically necessary” (Morange 2001, 9).
Johannsen himself stressed that the genotype had to be treated as
independent of any life history and thus, at least within the bounds
of time in which research operated, as an “ahistoric”
entity amenable to scientific scrutiny like the objects of physics and
chemistry (Johannsen 1911, 139; cf. Churchill 1974; Roll-Hansen
1978a). “The personal qualities of any individual
organism do not at all cause the qualities of its offspring; but the
qualities of both ancestor and descendant are in quite the same manner
determined by the nature of the sexual substances,” Johannsen
claimed (Johannsen 1911, 130). Unlike most Mendelians, however, he
remained convinced that the genotype would possess an overall
architecture—as expressed in the notion of
“type”. He therefore had reservations with respect to its
particulate nature, and especially warned that the notion of
“genes for a particular character” should always be used
cautiously if not altogether be omitted (Johannsen 1911,
147). Johannsen also remained consciously agnostic with respect to the
material constitution of the genotype and its elements. He clearly
recognized that the experimental regime of Mendelian genetics,
although scientific in its character like physics or chemistry, did
neither require nor allow for any definite supposition about the
material structure of the genetic elements. “Personally,”
he wrote as late as 1923, “I believe in a great central
something as yet not divisible into separate factors,”
identifying this “something” with the specific nature of
the organism. “The pomace-flies in Morgan's splendid
experiments,” he explained, “continue to be pomace-flies
even if they loose all good genes necessary for a normal fly-life, or
if they be possessed with all the bad genes, detrimental to the
welfare of this little friend of the geneticist” (Johannsen
1923, 137).
On this account, genes were taken as abstract elements of an
equally abstract space, whose structure, however, could be explored
through the visible and quantifiable outcome of breeding experiments
based on model organisms and their mutants. This became the research
program of Thomas Hunt Morgan and his group. From the early 1910s
right into the 1930s, the growing community of researchers around
Morgan and their followers used mutants of the fruit fly Drosophila
melanogaster, constructed in ever more sophisticated ways, in
order to produce a map of the fruit flys genotype in which genes, and
alleles thereof, figured as genetic markers occupying a particular
locus on one of the four homologous chromosome pairs of the fly
(Kohler 1994). The basic assumptions that allowed the program to
operate were that genes were located in a linear order along the
different chromosomes (like "beads on a string" as Morgan put it in
1926, 24), and that the frequency of recombination events between
homologous chromosomes, that is, the frequency of crossovers during
reduction division, gave a measure of the distance between the genes,
at the same time defining them as units of recombination
(Morgan et al. 1915). 
In this practice, identifiable aspects of the phenotype, assumed to
be determined directly by genes in a consciously black-boxed manner,
were used as indicators or windows for an outlook on the formal
structure of the genotype. This is what Moss has termed the
“Gene-P” (P standing for phenotype, but also for
preformationist; Moss 2003, 45 – for the counterpart, the
“Gene-D”, see below). Throughout his career, Morgan
remained aware of the formal character of his program. As late as
1933, on the occasion of his Nobel address, he declared: “At the
level at which the genetic experiments lie it does not make the
slightest difference whether the gene is a hypothetical unit, or
whether the gene is a material particle” (Morgan 1935, 3). In
particular, it did not matter if one-to-one, or more complicated
relationships reigned between genes and traits (Waters 1994). Morgan
and his school were well aware that, as a rule, many genes were
involved in the development of a particular trait as, e.g., eye-color,
and that one gene could affect several characters. To accommodate this
difficulty and in line with their experimental regime, they embraced a
differential concept of the gene. What mattered to them was the
relationship between a change in a gene and a change in a trait,
rather than the nature of these entities themselves. Thus the
alteration of a trait could be causally related to a change in (or a
loss of) a single genetic factor, even if it was plausible in general
that a trait like eye-color was, in fact, determined by a whole group
of variously interacting genes (Roll-Hansen 1978b; Schwartz
2000).
The fascination of this gene concept consisted in the fact that it
worked, if properly applied, like a precision instrument in
developmental and evolutionary studies. On the one hand, the classical
gene allowed for the identification of developmental processes across
generations.  As a consequence, procedures of classical genetics were
soon integrated with the panoply of methods that embryologists had
developed since the end of the nineteenth to “track”
development. (Griesemer 2007). On the other hand, mathematical
population geneticists like Ronald A. Fisher, J. B. S. Haldane, and
Sewall Wright could make use of the classical gene with equal rigor
and precision to elaborate testable mathematical models describing the
effects of evolutionary factors like selection and mutation on the
genetic composition of populations (Provine 1971). As a consequence,
evolution became re-defined as a change of gene frequencies in the
gene pool of a population in what is commonly called the
“evolutionary,” “neo-Darwinian” or simply
“modern synthesis” of the late 1930s and early 1940s (Mayr
& Provine 1980, Gayon 1998). Considered as a “developmental
invariant” in reproduction, solely obeying the Mendelian laws in
its transmission from one generation to the next, the classical gene
provided a kind of inertia principle against which the effects of both
developmental (epistasis, inhibition, position effects etc.) and
evolutionary factors (selection, mutation, isolation, recombination
etc.) could be measured with utmost accuracy (Gayon 1995, 74). We will
revisit the evolutionary synthesis in the third section; for the
remainder of this section, we would like to turn to the early history
of developmental genetics, which played an important role in the
eventual “reification” of the gene.
Despite the formal character of the classical gene, it became the
conviction of many geneticians in the 1920s, among them Morgans
student Herman J. Muller, that genes had to be material
particles. Muller saw genes as fundamentally endowed with two
properties: that of autocatalysis and that
of heterocatalysis. Their autocatalytic function allowed them
to reproduce as units of transmission and thus to connect the genotype
of one generation to that of the next. Their concomitant capability of
reproducing mutations faithfully once they had occurred gave rise, on
this account, to the possibility of evolution. Their heterocatalytic
capabilities connected them to the phenotype, as units of function
involved in the expression of a particular character. With his own
experimental work, Muller added a significant argument for the
materiality of the gene, pertaining to the third aspect of the gene as
a unit of mutation. In 1927, he reported on the induction of Mendelian
mutations in Drosophila by using X-rays. He was not the first
to use radiation to induce mutations, but stands out for his
conclusion that X-rays caused mutations by altering some molecular
structure in a permanent fashion, thus giving rise to a whole
“industry” of radiation genetics in the 1930s and
1940s. 
But the experimental practice of X-raying alone could not open the
path to a material characterization of genes as units of heredity. On
the occasion of the fiftieth anniversary of the rediscovery of
Mendel's work in 1950, Muller thus had to confess: “[T]he real
core of gene theory still appears to lie in the deep unknown. That is,
we have as yet no actual knowledge of the mechanism underlying that
unique property which makes a gene a gene—its ability to cause
the synthesis of another structure like itself, [in] which even the
mutations of the original gene are copied. [We] do not know of such
things yet in chemistry” (Muller 1951, 95-96).
Meanwhile, cytological work had also added credence to the
materiality of genes-on-chromosomes. At the same time, however, it
further complicated the notion of the classical gene. During the
1930s, the cytogeneticist Theophilus Painter correlated formal
patterns of displacement of genetic loci on Morganian chromosome maps
with corresponding visible changes in the banding pattern of giant
salivary gland chromosomes of Drosophila.  Barbara McClintock
was able to follow with her microscope the
changes—translocations, inversions and deletions—induced
by X-rays in the chromosomes of Zea mays
(maize). Simultaneously, Alfred Sturtevant, in his experimental work
on the Bar-eye-effect in Drosophila at the end of the 1920s,
had shown what came to be called a position effect: the
expression of a mutation was dependent on the position which the
corresponding gene occupied in the chromosome. This finding stirred
wide-ranging discussions about what Muller had called the
heterocatalytic aspect of a gene, namely, its functional association
with the expression of a particular phenotypic trait. If a genes
function depended on its position on the chromosome, it became
questionable whether that function was stably connected to that gene
at all, or as Richard Goldschmidt later assumed, whether physiological
function was not altogether a question of the organization of the
genetic material as a whole rather than of particulate genes
(Goldschmidt 1940; cf. Dietrich 2000 and Richmond 2007).
Thus far, all experimental approaches to the new field of genetics
and its presumed elements, the genes, had remained silent with respect
to the two basic Mullerian aspects of the gene: its autocatalytic and
its heterocatalytic function. Toward the end of the 1930s, Max
Delbrück had the intuition that the question of autocatalysis,
that is, replication, could be attacked through the study of phage,
i.e., viruses replicating in bacteria. It has, however, been noted that
the phage system, which he established throughout the 1940s, largely
remained as formal as that of classical Drosophila
genetics. Seymour Benzer, for example, used this system in an entirely
“classical” manner to increase the resolving power of
genetic mapping techniques down to distances of a few nucleotide
pairs, thus preparing the ground for Francis Cricks sequence
hypothesis. Interestingly, Benzer came to the conclusion that
“gene” was a “dirty word,” as the inferred
molecular dimensions of the gene as a unit of function, recombination,
and mutation clearly differed. Consequently, he suggested referring to
genetic elements as cistrons, recons and mutons respectively (Holmes
2006).
Around the same time, Alfred Kühn and his group, as well as
Boris Ephrussi with George Beadle, were able to open a window on the
space between the gene and its presumed physiological function by
transplanting organs between mutant and wild type insects. Studying
the pigmentation of insect eyes, they realized that genes did not
directly give rise to physiological substances, but that they
obviously first initiated what Kühn termed a “primary
reaction” leading to ferments or enzymes, which in turn
catalyzed particular steps in metabolic reaction cascades. In 1941,
Kühn summarized the perspective of this kind of
“developmental-physiological genetics,” as he called
it:
Kühn viewed his experiments as the beginning of a
reorientation away from what he perceived as the new preformationism
of transmission genetics (Rheinberger 2000a). He pleaded for an
epigenetics that would combine genetic, developmental and
physiological analyses to define heterocatalysis, that is, the
expression of a gene, as the result of an interaction of two reaction
chains, one leading from genes to particular ferments and the other
leading from one metabolic intermediate to the next by the
intervention of these ferments, thus resulting in complex epigenetic
networks. But his own experimental practice throughout the 1940s led
him to stay with the completion of the pathway of eye pigment
formation in Ephestia kühniella (the
flour-moth). He did not try to develop experimental instruments to
attack the gene-enzyme relations themselves implicated in the
process. On the other side of the Atlantic, George Beadle and Edward
Tatum, working with cultures of Neurospora crassa, codified the
latter connection into the one gene-one enzyme hypothesis. But to
them, too, the material character of genes and the way these putative
entities gave rise to primary products remained elusive and beyond the
reach of their own biochemical analysis.
Thus by the 1940s, the gene in classical genetics was already far
from being a simple notion corresponding to a simple entity.
Conceiving of the gene as a unit of transmission, recombination,
mutation, and function, classical geneticists combined various aspects
of hereditary phenomena whose interrelations, as a rule, turned out
not to be simple one-to-one relationships. Due to the lack of
knowledge about the material nature of the gene, however, the
classical gene remained a largely formal and operational concept,
i.e., had to be substantiated indirectly by the successes achieved in
explaining and predicting experimental results. This lack
notwithstanding, however, the mounting successes of the various
research strands associated with classical genetics led to a
“hardening” of the belief in the gene as a discrete,
material entity (Falk 2000, 323-26).
The enzyme view of gene function, as envisaged by Kühn and by
Beadle and Tatum, though with cautious reservation, gave the idea of
genetic specificity a new twist and helped to pave the way to the
molecularization of the gene to which this section will be devoted
(see also Kay 1993). The same can be said about the findings of Oswald
Avery and his colleagues in the early 1940s. They purified the
deoxyribonuleic acid of one strain of bacteria, and demonstrated that
it was able to transmit the infectious characteristics of that strain
to another, harmless one. Yet the historical path that led to an
understanding of the nature of the molecular gene was not a direct
follow-up of classical genetics (cf. Olby 1974 and Morange 2000a).  It
was rather embedded in an over-all molecularization of biology driven
by the application of newly developed physical and chemical methods
and instruments to problems of biology, including those of
genetics. Among these methods were ultracentrifugation, X-ray
crystallography, electron microscopy, electrophoresis, macromolecular
sequencing, and radioactive tracing. At the biological end, it relied
on the transition to new, comparatively simple model organisms like
unicellular fungi, bacteria, viruses, and phage. A new culture of
physically and chemically instructed in vitro biology ensued
that in large parts did no longer rest on the presence of intact
organisms in a particular experimental system (Rheinberger 1997;
Landecker 2007).
For the development of molecular genetics in the narrower sense,
three lines of experimental inquiry proved to be crucial. They were
not connected to each other when they gained momentum in the late
1940s, but they happened to merge at the beginning of the 1960s,
giving rise to a grand new picture. The first of these developments
was the elucidation of the structure of deoxyribonucleic acid (DNA) as
a macromolecular double helix by Francis Crick and James D. Watson in
1953. This work was based on chemical information about base
composition of the molecule provided by Erwin Chargaff, on data from
X-ray crystallography produced by Rosalind Franklin and Maurice
Wilkins, and on mechanical model building as developed by Linus
Pauling. The result was a picture of a nucleic acid double strand
whose four bases (Adenine, Thymine,
Guanine, Cytosine) formed complementary pairs (A-T, G-C) that
could be arranged in all possible combinations into long linear sequences. At
the same time, that molecular model suggested an elegant mechanism for the
duplication of the molecule. Opening the strands and synthesizing two new strands
complementary to each of the separated threads respectively would suffice to
create two identical helices from one. This indeed turned out to be the case,
although the duplication process would come to be seen as relying on a
complicated molecular replication machinery. Thus, the structure of the DNA
double helix had all the characteristics that were to be expected from a
molecule serving as an autocatalytic hereditary entity (Chadarevian 2002).
The second line of experiment that formed molecular genetics was
the in vitro characterization of the process of protein
biosynthesis to which many biochemically working researchers
contributed, among them Paul Zamecnik, Mahlon Hoagland, Paul Berg,
Fritz Lipmann, Marshall Nirenberg and Heinrich Matthaei. It started in
the 1940s largely as an effort to understand the growth of malignant
tumors. During the 1950s, it became evident that the process required
an RNA template that was originally thought to be part of the
microsomes on which the assembly of amino acids took place. In
addition it turned out that the process of amino acid condensation was
mediated by a transfer molecule with the characteristics of a nucleic
acid and the capacity to carry an amino acid. The ensuing idea
that it was a linear sequence of ribonucleic acid derived from one of
the DNA strands that directed the synthesis of a linear sequence of
amino acids, or a polypeptide, and that this process was mediated by
an adaptor molecule, was soon corroborated experimentally (Rheinberger
1997). The relation between these two classes of molecules was
eventually found to be ruled by a nucleic acid triplet code,
which consisted in three bases at a time specifying one amino acid
(Kay 2000, ch. 6); hence, the sequence hypothesis and
the central dogma of molecular biology, which Francis Crick
formulated at the end of the 1950s:
With these two fundamental assumptions, a new view of biological
specificity came into play. It was centered on the transfer of
molecular order from one macromolecule to the other. In one molecule
the order is preserved structurally; in the other it becomes expressed
and provides the basis for a biological function. This transfer
process became characterized as molecular information
transfer. Henceforth, genes could be seen as stretches of
deoxyribonucleic acid (or ribonucleic acid in certain viruses)
carrying the information for the assembly of a particular
protein. Both molecules were thus thought to be colinear, and this
indeed turned out to be the case for many bacterial genes. In the end,
both fundamental properties that Muller had required of genes, namely
autocatalysis and heterocatalysis, were perceived as relying on one
and the same stereochemical principle respectively: The base
complementarity between nucleic acid building blocks C/G and A/T (U in
the case of RNA) was both responsible for the faithful duplication of
genetic information in the process of replication, and, via the
genetic code, for the transformation of genetic information into
biological function through transcription to RNA
and translation to proteins.
The code turned out to be nearly universal for all classes of
living beings, as were the mechanisms of transcription and
translation. The genotype was thus reconfigured as a universal
repository of genetic information, sometimes also addressed as
a genetic program. Talk of DNA as embodying genetic
“information,” as being the “blueprint of
life” which governs public discourse until today, emerged from a
peculiar conjunction of the physical and the life sciences during
World War II, with Erwin Schrödinger's What is Life? as a
source of inspiration (Schrödinger 1944), and cybernetics as the
then-leading discipline in the study of complex systems. It needs to
be stressed, however, that initial attempts to “crack” the
DNA code by purely cryptographic means soon ran into a dead
end. Finally it was biochemists who unraveled the genetic code by the
advanced tools of their discipline (Judson 1996; Kay 2000).
For the further development of the notion of DNA as a
“program,” we have to consider an additional third line of
experiment, aside from the elucidation of DNA structure and the
mechanisms of protein synthesis.  This line of experiment came out of
a fusion of bacterial genetics with the biochemical characterization
of an inducible system of sugar metabolizing enzymes. It was largely
the work of François Jacob and Jacques Monod and led, at the
beginning of the 1960s, to the identification of messenger RNA as the
mediator between genes and proteins, and to the description of a
regulatory model of gene activation, the so called operon-model, in
which two classes of genes became distinguished: One class was that
of structural genes. They were presumed to carry the
“structural information” for the production of particular
polypeptides. The other class was that of regulatory
genes. They were assumed to be involved in the regulation of the
expression of structural information (how this distinction became
challenged recently is discussed in Piro 2011). A third element of DNA
involved in the regulatory loop of an operon was a binding site,
or signal sequence that was not transcribed at all.
These three elements, structural genes, regulatory genes, and
signal sequences provided the framework for viewing the genotype
itself as an ordered, hierarchical system, as a “genetic
program,” as Jacob contended, not without immediately adding
that it was a very peculiar program, namely one that needed its own
products for being executed: “There is only the incessant
execution of a program that is inseparable from its realization. For
the only elements being able to interpret the genetic message are the
products of that message” (Jacob 1976, 297). If we take this
view seriously, although the whole conception looks like a circle and
has been criticized as such (Keller 2000), it is in the end the
organism which interprets or “recruits” the structural
genes by activating or inhibiting the regulatory genes that control
their expression.
The operon model of Jacob and Monod marked thus the precipitous end
of the simple, informational concept of the molecular gene.  Since the
beginning of the 1960s, the picture of gene expression has become
vastly more complicated (for the following, compare Rheinberger
2000b). Moreover, most genomes of higher organisms appear to comprise
huge DNA stretches to which no function can as yet be
assigned. “Non-coding,” but functionally specific,
regulatory DNA-elements have proliferated: There exist promoter and
terminator sequences; upstream and downstream activating elements in
transcribed or non-transcribed, translated or untranslated regions;
leader sequences; externally and internally transcribed spacers
before, between, and after structural genes; interspersed repetitive
elements and tandemly repeated sequences such as satellites, LINEs
(long interspersed sequences) and SINEs (short interspersed sequences)
of various classes and sizes. Given all the bewildering details of
these elements, it comes as no surprise that their molecular function
is still far from being fully understood (for an overview see Fischer
1995).
As far as transcription, i.e., the synthesis of an RNA copy from a
sequence of DNA, is concerned, overlapping reading frames have been
found on one and the same strand of DNA, and protein coding stretches
have been found to derive from both strands of the double helix in an
overlapping fashion. On the level of modification after transcription,
the picture has become equally complicated. Already in the 1960s it
was realized that DNA transcripts such as transfer RNA and ribosomal
RNA had to be trimmed and matured in a complex enzymatic manner to
become functional molecules, and that messenger RNAs of eukaryotes
underwent extensive posttranscriptional modification both at their
5′-ends (capping) and their 3′-ends (polyadenylation)
before they were ready to go into the translation machinery.  In the
1970s, to the surprise of everybody, Phillip Allen Sharp and Richard
J. Roberts independently found that eukaryotic genes were composed of
modules, and that, after transcription, introns were cut out
and exons spliced together in order to yield a functional
message. 
The “gene-in-pieces” (Gilbert 1978) was one of the
first major scientific offshoots of recombinant DNA technology, and
this technology has since continued to be good for unanticipated
vistas on the genome and the processing of its units. A spliced
messenger sometimes may comprise a fraction as little as ten percent
or less of the primary transcript. Since the late 1970s, molecular
biologists have become familiar with various kinds of RNA
splicing autocatalytic self-splicing, alternative splicing of one
single transcript to yield different messages and even trans-splicing
of different primary transcripts to yield one hybrid message. In the
case of the egg-laying hormone of Aplysia, to take just one
example, one and the same stretch of DNA gives rise to eleven protein
products involved in the reproductive behavior of this snail. Finally,
yet another mechanism, or rather, class of mechanisms has been found
to operate on the level of RNA transcripts. It is called messenger
RNA editing. In this case-which in the meanwhile has turned out
not just to be an exotic curiosity of some trypanosomes-the original
transcript is not only cut and pasted, but its nucleotide sequence is
systematically altered after transcription. The nucleotide replacement
happens before translation starts, and is mediated by various guide
RNAs and enzymes that excise old and insert new nucleotides in a
variety of ways to yield a product that is no longer complementary to
the DNA stretch from which it was originally derived, and a protein
that is no longer co-linear with the DNA sequence in the classical
molecular biological sense.
The complications with the molecular biological gene continue on
the level of translation, i.e., the synthesis of a polypeptide
according to the sequence of triplets of the mRNA molecule. There are
findings such as translational starts at different start codons on one
and the same messenger RNA; instances of obligatory frameshifting
within a given message without which a nonfunctional polypeptide would
result; and post-translational protein modification such as removing
amino acids from the amino terminus of the translated
polypeptide. There is another observation called protein
splicing, instances of which have been reported since the early
1990s.  Here, portions of the original translation product have to be
cleaved out (inteins) and others joined together (exteins) before
yielding a functional protein. And finally, a recent development from
the translational field is that a ribosome can manage to translate two
different messenger RNAs into one single polypeptide. François
Gros, after a life in molecular biology, has come to the rather
paradoxically sounding conclusion that in view of this perplexing
complexity, the “exploded gene” le gène
éclaté could be specified, if at all, only by
“the products that result from its activity,” that is, the
functional molecules to which it gives rise (Gros 1991, 297). But it
appears difficult, if thought through, to follow Gros' advice of such
a reverse definition, as the phenotype would come to define the
genotype.
The most recent debates concerning the structure and function of
the genome are centered around the Encyclopedia of DNA Elements
(ENCODE) project. The project aimed at identifying all functional
elements in the human genome. The results of the consortium´s work so
far make the already known deviations from the classic model of the
molecular gene as a continuous protein coding region flanked by
regulatory regions appear as the rule rather than the exception. To a
large extent ENCODE researchers found overlap of transcripts, products
derived from widely separated pieces of DNA sequence and widely
dispersed regulatory sequences for a given gene. The findings also
confirm that most of the genome is transcribed and emphasize the
importance and pervasiveness of functional non-protein-coding RNA
transcripts that has emerged during the last decade suggesting a
“vast hidden layer of RNA regulatory transactions”
(Mattick 2007). In the light of these findings a definition of the
gene has been proposed, according to which “The gene is a union
of genomic sequences encoding a coherent set of potentially
overlapping functional products.” (Gerstein et al 2007,
677). Such definitions mainly serve the purpose of solving the
annotation problem (Baetu 2012), which becomes particularly important
in the context of the increasing importance of bioinformatics and the
use of databases that requires a consistent ontology (Leonelli
2008). More controversial is the notion of function involved
here. According to the ENCODE Consortium their data enabled them
“to assign biochemical functions for 80% of the
genome.” (ENCODE Project Consortium 2012, 57), despite the fact
that according to conservative estimates only 3–8% of bases
are under purifying selection, which is usually taken to indicate
sequence function. Critics have argued that an etiological notion of
function, according to which function is a selected effect, is more
appropriate in the context of functional genomics (Doolittle et
al. 2014), whereas others maintain that any causal role of a strand of
DNA might be relevant, especially in biomedical research (see Germain
et al. 2014 for a philosophical take on the discussion). As we have
noticed for previous twists and turns in the history of the gene
concept, these developments have been driven by technological
advances, in particular in deep RNA sequencing and in identifying
protein-DNA interactions.
In conclusion, it can be said with Falk (2000, 327) that, on the
one hand, the autocatalytic property once attributed to the gene as an
elementary unit has been relegated to the DNA at large. Replication
can no longer be taken as being specific to the gene as such. After
all, the process of DNA replication is not punctuated by the
boundaries of coding regions. On the other hand, as many observers of
the scene have remarked (Kitcher 1982; Gros 1991; Morange 2001; Portin
1993; Fogle 2000), it has become ever harder to define clear-cut
properties of a gene as a functional unit with heterocatalytic
properties. It has become a matter of choice under contextual
constraints as to which sequence elements are to be included and which
ones to be excluded in the functional characterization of a gene. Some
have therefore adopted a pluralist attitude towards gene
concepts. (Burian 2004).
There have been different reactions to this situation. Scientists
like Thomas Fogle and Michel Morange concede that there is no longer a
precise definition of what could count as a gene. But they do not
worry much about this situation and are ready to continue to talk
about genes in a pluralist, contextual, and pragmatic manner (Fogle
1990, 2000; Morange 2000b). Elof Carlson and Petter Portin have as well
concluded that the present gene concept is abstract, general, and
open, despite or just because present knowledge of the structure and
organization of the genetic material has become so comprehensive and
so detailed. But they, like Richard Burian (1985), take open concepts
with a large reference potential not only as a deficit to live with,
but as a potentially productive tool in science. Such concepts offer
options and leave choices open (Carlson 1991, Portin
1993). Philosopher Philip Kitcher, as a consequence of all the
molecular input concerning the gene, already some 25 years ago praised
the “heterogenous reference potential” of the gene as a
virtue and drew the ultraliberal conclusion that “there is no
molecular biology of the gene. There is only molecular biology of the
genetic material” (Kitcher 1982, 357).
From the perspective of the autocatalytic and evolutionary
dimension of the genetic material, the reproductive function ascribed
to genes has turned out to be a function of the whole genome. The
replication process, that is, the transmission aspect of genetics as
such has revealed itself to be a complicated molecular process whose
versatility, far from being restricted to gene shuffling during
meiotic recombination, constitutes a reservoir for evolution and is
run by a highly complex molecular machinery including polymerases,
gyrases, DNA binding proteins, repair mechanisms, and more. Genomic
differences, targeted by selection, then can, but must not become
“compartmented into genes” during evolution, as Peter
Beurton has put it (Beurton 2000, 303).
On the other hand, there are those who take the heterocatalytic
variability of the gene as an argument to treat the genetic material
as a whole, hence genes as well, no longer as fundamental in its own
right, but rather as a developmental resource that needs to be
contextualized.  They claim that time has come, if not to dissolve,
then at least to embed genetics in development and even development in
reproduction—as James Griesemer suggests (Griesemer
2000)—and thus to pick up the thread where Kühn and others
left it more than half a century ago. Consequently, Moss defines
“Gene-D” (the counterpart of the previously mentioned
phenotypically defined Gene-P) as a “developmental resource
(hence the D), which in itself is indeterminate with respect to
phenotype. To be a Gene-D is to be a transcriptional unit on a
chromosome, within which are contained molecular template
resources” (Moss 2003, 46; cf. Moss 2008). On this view, these
templates constitute only one reservoir on which the developmental
process draws and are not ontologically privileged as hereditary
molecules.
With molecular biology the classical gene “went
molecular” (Waters 1994). Ironically, the initial idea of genes
as simple stretches of DNA coding for a protein became dissolved in
this process. As soon as the gene of classical genetics had acquired
material structure through molecular biology, the biochemical and
physiological mechanisms that accounted for its transmission and
expression proliferated. The development of molecular biology
itself—that enterprise which is so often described as an utterly
reductionist conquest—has made it impossible to think of the
genome simply as a set of pieces of contiguous DNA co-linear with the
proteins derived from it. At the beginning of the twenty-first
century, when the results of the 
 Human Genome Project were timely
presented on the fiftieth anniversary of the double helix, molecular
genetics seems to have accomplished a full circle, readdressing
reproduction and inheritance no longer from a purely genetic, but from
an evolutionary-developmental perspective. At the same time, the gene
has become a central category in medicine in the course of the 20th
century (Lindee 2005) and dominates discourses of health and disease
in the postgenomic era (Rose 2007). 
One of the more spectacular events in the history of
twentieth-century biology as a discipline, triggered by the rise of
genetics (mathematical population genetics in particular), was the
so-called “modern evolutionary synthesis.” In a whole
series of textbooks, published by evolutionary biologists like
Theodosius Dobzhansky, Ernst Mayr and Julian S.  Huxley, the results
of population genetics were used to re-establish Darwinian,
selectionist evolution. After the “eclipse of Darwinism”,
which had reigned around 1900 (Bowler 1983), neo-Darwinism once again
provided a unifying, explanatory framework for biology that also
included the more descriptive, naturalist disciplines like
systematics, biogeography, and paleontology (Provine 1971; Mayr &
Provine 1980; Smocoovitis 1996).
Scott Gilbert (2000) has singled out six aspects of the notion of
the gene as it had been used in population genetics up to the modern
evolutionary synthesis. First, it was an abstraction, an entity that
had to fulfill formal requirements, but that did not need to be and
indeed was not materially specified. Second, the evolutionary gene had
to result in or had to be correlated with some phenotypic difference
that could be “seen” or targeted by selection. Third, and
by the same token, the gene of the evolutionary synthesis was the
entity that was ultimately responsible for selection to occur and last
across generations. Fourth, the gene of the evolutionary synthesis was
largely equated with what molecular biologists came to call
“structural genes.” Fifth, it was a gene expressed in an
organism competing for reproductive advantage. And finally, it was
seen as a largely independent unit. Richard Dawkins has taken this
last argument to its extreme by defining the gene as a
“selfish” replicator with a life of its own, competing
with its fellow genes and using the organism as an instrument for its
own survival (Dawkins 1976; cf.  Sterelny and Kitcher 1988).
Molecular biology, with higher organisms moving center-stage during
the past three decades, has made a caricature of this kind of
evolutionary gene, and has moved before our eyes genes and whole
genomes as complex systems not only allowing for evolution to occur,
but being themselves subjected to a vigorous process of evolution. The
genome in its entirety has taken on a more and more flexible and
dynamic configuration. Evelyn Fox Keller speaks of “reactive
genomes” (Keller 2014). Not only have the mobile genetic
elements, characterized by McClintock more than half a century ago
in Zea mays, gained currency in the form of transposons
that regularly and irregularly can become excised and inserted all
over bacterial and eukaryotic genomes, there are also other forms of
shuffling that occur at the DNA level. A gigantic amount of somatic
gene tinkering and DNA splicing, for instance, is involved in
organizing the immune response. It gives rise to the production of
potentially millions of different antibodies. No genome would be large
enough to cope with such a task if not the parceling out of genes and
a sophisticated permutation of their parts had not been invented
during evolution.  Gene families have arisen from duplication over
time, containing silenced genes (sometimes called pseudogenes). Genes
themselves appear to have largely arisen from modules by
combination. We find jumping genes and multiple genes of one sort
coding for different protein isoforms. In short, there appears to be a
whole battery of mechanisms and entities that constitute what has been
called “hereditary respiration” (Gros 1991, 337).
Molecular evolutionary biologists have scarcely scratched the
surface and barely started to understand this flexible genetic
apparatus, although Jacob already put forward a view of the genome as
a dynamic body of ancestrally iterated and tinkered pieces more than
thirty years ago (Jacob 1977). Genome sequencing combined with
intelligent sequence data comparison is currently bringing out more
and more of this structure (on the history of these developments, see
García-Sancho 2012, on data-driven biology, see Stevens
2013). One of the surprising results of the Human Genome Project has
been that there are only 21,000 genes. If there is a chance to
understand evolution beyond the classical, itself largely formal,
evolutionary synthesis, it is from such perspectives of learning more
about the genome as a dynamic and modular
configuration. The purported elementary events on the basis of which
the complex machinery of genome expression and reproduction
operates—such as point mutations, nucleotide deletions,
additions, and oligonucleotide inversions—are no longer the only
elements of the evolutionary process, but solely one component in a
much wider arsenal of DNA-tinkering. Beurton concludes from all
this that the gene is no longer to be seen as the unit of evolution,
but rather as its late product, the eventual result of a long history
of genomic condensation (Beurton 2000). Others have argued that
genomic analysis is not concerned with learning about genes as
functionally or structurally defined units, but with
“identifying causal relationships between parts of genomes and
molecular products and identifying different” (Perini 2011).
Finally, recent years have seen a steady increase in evidence for
epigenetic inheritance systems (Jablonka and Raz 2009, see also the
entry on inheritance systems).
 This development has not only been promoted as a
revolution in molecular biology, defining the post-genomic era (see
Meloni and Testa 2014 for a discussion of the sociology of hype and
expectation in this respect), but has led to yet another change in the
concept of the gene, in so far as it can no longer be seen as the only
unit of inheritance and selection and the primary cause in
development. While “epigenetics” refers more generally to
processes of cell determination and differentiation — so called
“epigenetic control systems,” — epigenetic
inheritance “occurs when phenotypic variations that do not stem
from variations in DNA base sequences are transmitted to subsequent
generations of cells or organisms” (Jablonka and Raz 2009,
132). While this can include developmental interactions between mother
and offspring, social learning, symbolic communication, there is also
a more narrow concept of cellular epigenetic inheritance. It refers to
“the transmission from mother cell to daughter cell of
variations that are not the result of differences in DNA base sequence
and/or the present environment” (Jablonka and Raz 2009,
132). Cellular epigenetic inheritance systems discussed in the
literature are the transmission of chromatin marks, especially DNA
methylation, and RNAs, the inheritance of protein conformations, such
as in prions, and self-sustaining loops and chromatin inheritance in
bacteria. In multicellular organisms, especially the first type of
mechanisms can explain how differentiated cells give rise to identical
daughter cells even if the signal that initiated differentiation is
gone.
But more important for the concept of heredity is cellular
transgenerational epigenetic inheritance. In such cases “the
environment may induce epigenetic variation by directly affecting the
germline or by affecting germ cells through the mediation of the soma,
but, in either case, subsequent transmission is through the
germline” (Jablonka and Raz 2009, 133). This clearly implies
that “the epigenetic body brings the Weismannian body to an
end,” as Meloni and Testa (2014, 19) put it. Epigenetic
variation can have phenotypic effects in the generation exposed to the
stimulus, or in its offspring, which can persist for several
generations. This possibility opened a new field of interaction
between biology and the social sciences, because factors in the human
environment, from exposure to toxic compounds, via nutrition to
education, can have epigenetic effects that span several
generations. The idiom of epigenetics serves to biologize once more
social and ethnic difference, and redefines individual vulnerability
as well as responsibility and transgenerational accountability
concerning effects of lifestyles on health and disease (Meloni and
Testa 2014).
Furthermore, epigenetic inheritance in its broad as well as in its
narrow meaning has significant consequences for the understanding of
evolution and development (Jablonka & Lamb 2005). On the one hand,
when combined with the idea of genetic assimilation (Waddington 1957),
according to which genes are selected to fixate previous adaptive, but
non-genetic variation, epiegenetic inheritance helps to explain how
adaptive phenotypic responses become genetically fixed, suggesting
neo-Lamarckian views of evolution (West-Eberhard 2003). On the other
hand, the investigation of epigenetic mechanisms casts doubt on the
causal or informational primacy of genes, or DNA. As a consequence
genetic elements are treated on a par with other developmental
resources necessary for the formation of a phenotypic trait. From this
perspective, “genes” appear as processes resulting in
phenotypic outcomes that involve a great number of other resources
alongside DNA (Griffiths and Neumann-Held 1999). This view is defended
in accounts of Developmental Systems Theory (Oyama et al. 2001;
Neumann-Held and Rehmann-Sutter 2006), although this theory has come
under attack from philosophers for offering nothing “that
aspiring researchers can put to work” (Kitcher 2001, 408;
cf. Hall 2001). And, as Meloni and Testa have pointed out, while
epigenetics counters gene-centered reductionism, it leads to a
reduction of environmental influences to molecular agents. When
scientists today speak of the exposome, the suffix ‐ome
is indeed supposed to reflect the “digitization of all forms of
environmental exposure, from motherly love to toxins, from food to
class inequalities, into a single unifying category and syntax”
(Meloni and Testa 2014, 18).
We have come a long way with molecular biology from genes to
genomes to developmental systems. But there is still a longer way to
go from genomes to organisms. The developmental gene as it acquired
contours over the last twenty years from the early work of Ed Lewis
and Antonio Garcia-Bellido, and from later work by Walter Gehring,
Christiane Nüsslein-Volhard, Eric Wieschaus, Peter Gruss, Denis
Duboule, and others, allows us possibly to go a step along on this
way. As Gilbert (2000) argues, it is the exact counterpart to the gene
of the evolutionary synthesis. But we need to be more specific and
direct attention to what have been termed “developmental
genes” proper. As it turned out, largely from an exhaustive
exploitation of mutation saturation and genetic engineering
technologies, fundamental processes in development such as
segmentation or eye formation in such widely different organisms as
insects and mammals are decisively influenced by the activation and
inhibition of a class of regulatory genes that to some extent resemble
the regulator genes of the operon model. But in distinction to these
long-known regulatory genes, whose function rests on their ability to
being reversibly switched on and off according to the requirements of
actual metabolic and environmental situations, developmental genes
initiate irreversible processes. They code for so-called transcription
factors which can bind to control regions of DNA and thus influence
the rate of transcription of a particular gene or a whole set of genes
at a particular stage of development. Among them are what may be
called developmental genes of a second order which appear to control
and modulate the units gated by developmental genes of the first
order. They act as a veritable kind of “master switch” and have been
found to be highly conserved throughout evolution. An example is a
member of the pax-gene family that can switch on a whole
complex process such as eye formation from insects to
vertebrates. Most surprisingly, the homologous gene isolated from the
mouse can replace the one present in Drosophila, and when
placed in the fruit fly, switch on, not mammalian eye formation, but
insect eye formation. Many of these genes or gene families, like
the homeobox-family, are thought to be involved in the
generation of spatial patterning during embryogenesis as well as in
its temporal patterning.
Morange (2000b) distinguishes two central “hard facts”
that can be retained from this actually highly fluid and contested
research field.  The first is that the regulatory genes appear to play
a central role in development as judged from the often drastic effects
resulting from their inactivation. And second, it appears that not
only particular homeotic genes have been highly conserved between
distantly related organisms, but that they tend to come in complexes
which have themselves been structurally conserved throughout
evolution, thus once more testifying to genomic higher
order-structures. Another class of such highly conserved genes and
gene complexes is involved in the formation of components of pathways
that bring about intracellular and cell-to-cell signaling. These
processes are of obvious importance for cellular differentiation and
for embryonic development of multi-cellular organisms.
One of the big surprises of the extensive use of the technology of
targeted gene knockout has been that genes thought to be indispensable
for a particular function, when knocked out, did not alter or at least
did not significantly alter the organisms performance. This made
developmental molecular biologists aware that the networks of
development appear to be largely redundant, and that certain parts can
compensate for eventually missing parts (Mitchell 2009, Ch. 4).
 These networks are obviously
highly buffered and thus robust to a considerable extent with respect
to changing external and internal conditions. Gene products are of
course involved in these networks and their complex functions, but
these functions are by no means defined by the genes alone. Another
result, coming from embryonic gene expression studies with recently
developed chip technologies, was that one and the same gene product
can be expressed at different stages of development and in different
tissues, and that it can be implicated in quite different metabolic
and cellular functions.  Again, this multi-functionality of genes may
help to rethink what genetic and biological determination means
 (see also the discussion of biological determinism in the entry on
 feminist philosophy of biology).
These recent results seriously call into question the further
applicability of straightforward “gene-for” talk. The
discovery of developmental genes throws light on the way in which the
genome as a whole is organized as a dynamic, modular, and robust
entity. Unlike the pieces of DNA with a determinate function as
originally envisioned by molecular genetics, developmental genes
appear as highly conserved in evolution, yet highly variable and
redundant in function. They rather look like molecular building blocks
with which evolution tinkers in constructing organisms—or with
which organisms tinker in evolving. In recent years, evolutionary theories have
come to acknowledge this active role of the organism in making use of,
and even shaping highly conserved genetic mechanisms (West-Eberhard
2003; Kirschner & Gerhart 2005).
As we argued in the preceding sections, the history of
twentieth-century genetics is characterized by a proliferation of
methods for the individuation of genetic components, and, accordingly,
by a proliferation of gene definitions. These definitions appear to be
largely technology-dependent. Major conceptual changes did not precede
but followed experimental breakthroughs. Especially the contrast of
the “classical” and the “molecular” gene, the
latter succeeding the former chronologically, has raised issues of how
such alternative concepts relate semantically, ontologically, and
epistemologically. Understanding these relations might offer a chance
to convey some order to the bewildering variety of meanings inscribed
in the concept of the gene in the course of a long century.
In a now classical paper, Kenneth Schaffner argued that molecular
biology—the Watson-Crick model of DNA in
particular—effected a reduction of the laws of (classical)
genetics to physical and chemical laws (Schaffner 1969, 342). The
successes of molecular biology in identifying DNA as the genetic
material—as Watson's and Crick's discovery of the DNA structure
or the Meselson-Stahl experiment—lend empirical support,
according to Schaffner, “for reduction functions involved in the
reduction of biology as: gene1 = DNA
sequence1.” Schaffner's account was severely
criticized by David Hull, who pointed out that relations between
Mendelian and molecular terms are “many-many, and ” not
“one-one” or “many-one” relations as assumed
by Schaffner, because “phenomena characterized by a single
Mendelian predicate term can be reproduced by several types of
molecular mechanisms [... and] conversely, the same type of molecular
mechanism can produce phenomena that must be characterized by
different Mendelian predicate terms” (Hull 1974, 39).  “To
convert these many-many relations,” Hull concluded, “into
the necessary one-one or many-one relations leading from molecular to
Mendelian terms, Mendelian genetics must be modified extensively. Two
problems then arise —the justification for terming these
modifications ‘corrections’ and the transition from
Mendelian to molecular genetics ‘reduction’ rather than
‘replacement’” (Hull 1974, 43). 
To account for this difficulty and accommodate the intuition (which
Hull shared) that there should be at least some way in which it makes
sense to speak of a reduction of classical to molecular genetics,
Alexander Rosenberg adopted the notion of supervenience (coined by
Donald Davidson and going back to George Edward Moore) to describe the
relation of classical to molecular genetics. Supervenience implies
that any two items that share the same properties in molecular terms,
also have the same properties in Mendelian terms, without, however,
entailing a commitment that Mendelian laws must be deducible from the
laws of biochemistry (Rosenberg 1978). This recalls the way in which
classical geneticists related gene differences and trait differences
in the differential gene concept, where trait differences were used as
markers for genetic differences without implying a deducibility of
trait behavior, the dominance or recessivity of traits in particular,
from Mendelian laws (Schwartz 2000; Falk 2001). Interestingly, Kenneth
Waters has argued on this basis, and against Hull, that the complexity
that was revealed by molecular genetics was simply the complexity
already posited, albeit in an abstract manner, by classical
geneticists (Waters 1994, 2000). While relations between molecular
and classical genetics have proven to be non-deductive, they exist and
connect the two fields in epistemically productive ways on a
case-by-case basis (Kitcher 1984; Schaffner 1993; Darden 2005; Weber
2007).
The literature on genetics and reductionism has meanwhile become as
variegated and complex as the field of scientific activities it
attends to illuminate. In his book-length, critical assessment of that
literature, Sahotra Sarkar made an interesting move by distinguishing
five different concepts of reduction, of which he considers three to
be particularly relevant to genetics: “weak reduction,”
exemplified by the notion of heritability; “abstract
hierarchical reduction,” exemplified by classical genetics; and
“approximate strong reduction,” exemplified by the use of
“information”-based explanation in molecular genetics. The
perhaps not so surprising result with which Sarkar comes up is that
“reduction—in its various types—is scientifically
interesting beyond, especially, the formal concerns of most
philosophers of sciences” in that it constitutes a
“valuable, sometimes exciting, and occasionally indispensable
strategy in science” and thus needs to be acknowledged as being
ultimately “related to the actual practice of genetics”
(Sarkar 1998, 190). In a similar vein, Jean Gayon has expounded a
“philosophical scheme” for the history of genetics which
treats phenomenalism, instrumentalism, and realism not as alternative
systems that philosophers have to decide between, but as actual,
historically consecutive strategies employed by geneticists in their
work (Gayon 2000).
We would finally like to address briefly two issues that are
related to the problem of reduction and have occasioned repeated
discussion in the philosophical literature. The first point concerns
the notion of “information” in molecular genetics. The
inflationary early molecular use of the terms “genetic
information” and “genetic program” has been widely
criticized by philosophers and historians of science alike (Sarkar
1996, Kay 2000, Keller 2001). No one less than Gunther Stent, one of
the strongest proponents of what has been termed the
“informational school” of molecular biology, warned long
ago that talk about “genetic information” is best confined
to its explicit and explicable meaning of sequence specification, that
is, that it is best to keep it in the local confines of
“coding” instead of scaling it up to a global talk of
genetic “programming.” “It goes without saying,
” he contends, “that the principles of chemical catalysis
[of an enzyme] are not represented in the DNA nucleotide base
sequences,” and he concludes:
However, it appears to us that one should remain aware of the fact
that the molecular biological notion of a flow of information, in the
double sense of storage as well as expression in the interaction
between two classes of macromolecules, has added a dimension of
talking about living systems that helps to distinguish them
specifically from chemical and physical systems characterized solely
by flows of matter and flows of energy (Crick 1958; Maynard Smith
2000). Molecular biology, seen by many historians and philosophers of
biology as a paragon of reductionism, did not only introduce physics
and chemistry into biology, or even reduce the latter to the
former. Paradoxically, the achievements of molecular biology also
helped to find a new way of conceiving of organisms in a fundamentally
non-reductive manner. In a broader vision, this implies
“epigenetic” mechanisms of intracellular and intercellular
molecular signaling and communication in which genetic information and
its differential expression is embedded and through which it is
contextualized. Upon this view, it appears not only legitimate, but
heuristically productive to conceive of the functional networks of
living beings in a biosemiotic terminology instead of a simply
mechanistic or energetic idiom (Emmeche 1999).
The second point concerns the already mentioned
“gene-for” talk. Why has talk about genes coding for this
and that become so entrenched?  Why do genes still appear as the
ultimate determinants and executers of life?  As we have seen in the
preceding two sections, the advances in conceptualizing processes of
organismic development and evolution have thoroughly deconstructed the
view of genes that dominated classical genetics and the early phases
of molecular genetics. Why is it, to talk with Moss, that genetics is
still “understood not as a practice of instrumental reductionism
but rather in the constitutive reductionist vein” implying the
“ability to account for the production of the phenotype on the
basis of the genes” (Moss 2003, 50)? A recent empirical study by
Paul Griffiths and Karola Stotz on how biologists conceptualize genes
comes indeed to the conclusion “that the classical molecular
gene concept continues to function as something like a stereotype for
biologists, despite the many cases in which that conception does not
give a principled answer to the question of whether a particular
sequence is a gene” (Stotz, Griffiths and Knight 2004,
671). Waters provides a surprising but altogether plausible
epistemological answer to this apparent conundrum. He reminds us
forcefully that in the context of scientific work and research, genes
are first and foremost handled as entities of investigative rather
than explanatory value (Waters 2004; cf. Weber 2004, 223). It is on
the grounds of their epistemic function in research that they appear
so privileged. Waters deliberately goes beyond the question of
reductionism or anti-reductionism that has structured so much
philosophical work on modern biology, especially on genetics and
molecular biology over the past decades, and ties it into the
philosophical literature on the relationship between causation and
manipulability that has more recently gained in prominence (Waters
2007). He stresses that the successes of a gene-centered view on the
organism are not due to the fact that genes are the major determinants
of the main processes in living beings. Rather, they figure so
prominently because they provide highly successful entry points for
the investigation of these processes. The success of
gene-centrism, according to this view, is not ontologically, but first
and foremost epistemologically and pragmatically grounded (cf. Gannett
1999). 
From this, two major philosophical claims result: First, that it is
the structure of investigation rather than an all-encompassing system
of explanation that has grounded the scientific success of genetics;
and second, that the essential incompleteness of genetic explanations,
whenever they are meant to be located at the ontological level, calls
for the promotion of a scientific pluralism (Waters 2004b;
Dupré 2004; Burian 2004; Griffiths and Stotz 2006). The message
is that complex objects of investigation such as organisms cannot be
successfully understood by a single best account or description, and
that any experimentally proceeding science is basically advancing
through the construction of successful, but always partial models.
Whether and how long these models will continue to be gene-based,
remains an open question. Any answers to that question will be
contingent on future research results, not on an ontology of life.