Most contemporary discussions of reductive relations between a pair of
theories owe considerable debt to the work by Ernest Nagel. In The
Structure of Science, Nagel asserts that “[r]eduction … is the
explanation of a theory or a set of experimental laws established in
one area of inquiry, by a theory usually though not invariably
formulated for some other domain.” (Nagel 1961, 338) The general
schema here is as follows: 
Showing how these derivations are possible for “paradigm” examples
of intertheoretic reduction turns out to be rather difficult.
Nagel distinguishes two types of reductions on the basis of whether
or not the vocabulary of the reduced theory is a subset of the reducing
theory. If it is—that is, if the reduced theory \(T'\)
contains no descriptive terms not contained in the reducing theory
\(T\), and the terms of \(T'\) are understood to have
approximately the same meanings that they have in \(T\), then
Nagel calls the reduction of \(T'\) by \(T\)
“homogeneous.” In this case, while the reduction may very well be
enlightening in various respects, and is part of the “normal
development of a science,” most people believe that there is nothing
terribly special or interesting from a philosophical point of view
going on here. (Nagel 1961, 339.)
Lawrence Sklar (1967, 110–111) points out that, from a
historical perspective, this attitude is somewhat naive. The number of
actual cases in the history of science where a genuine homogeneous
reduction takes place are few and far between. Nagel, himself, took as
a paradigm example of homogeneous reduction, the reduction of the
Galilean laws of falling bodies to Newtonian mechanics. But, as Sklar
points out, what actually can be derived from the Newtonian theory are
approximations to the laws of the reduced Galilean theory. The
approximations, of course, are strictly speaking incompatible
with the actual laws and so, despite the fact that no concepts appear
in the Galilean theory that do not also appear in the Newtonian theory,
there is no deductive derivation of the laws of the one from
the laws of the other. Hence, strictly speaking, there is no
reduction on the deductive Nagelian model.
One way out of this problem for the proponent of Nagel-type
reductions is to make a distinction between explaining a theory (or
explaining the laws of a given theory) and explaining it away. (Sklar
1967, 112–113) Thus, we may still speak of reduction if the
derivation of the approximations to the reduced theory’s laws serves to
account for why the reduced theory works as well as it does in its
(perhaps more limited) domain of applicability. This is consonant with
more sophisticated versions of Nagel-type reductions in which part of
the very process of reduction involves revisions to the reduced theory.
This process arises as a natural consequence of trying to deal with
what Nagel calls “heterogeneous” reductions.
The task of characterizing reduction is more involved when the
reduction is heterogeneous—that is, when the reduced theory
contains terms or concepts that do not appear in the reducing theory.
Nagel takes, as a paradigm example of heterogeneous reduction, the
(apparent) reduction of thermodynamics, or at least some parts of
thermodynamics, to statistical
 mechanics.[1]
 For instance, thermodynamics contains the concept of temperature
(among others) that is lacking in the reducing theory of statistical
mechanics.
Nagel notes that “if the laws of the secondary science [the reduced
theory] contain terms that do not occur in the theoretical assumptions
of the primary discipline [the reducing theory] … , the logical
derivation of the former from the latter is prima facie
impossible.” (Nagel 1961, 352) As a consequence, Nagel introduces
two “necessary formal conditions” required for the reduction to take
place:
The connectability condition brings with it a number of interpretive
problems. Exactly what is, or should be, the status of the “suitable
relations,” often called bridge “laws” or bridge hypotheses? Are they
established by linguistic investigation alone? Are they factual
discoveries? If the latter, what sort of necessity do they involve? Are
they identity relations that are contingently necessary or will some
sort of weaker relation, such as nomic coextensivity, suffice? Much of
the philosophical literature on reduction addresses these questions
about the status of the bridge
 laws.[2]
The consideration of certain examples lends plausibility to the
idea, prevalent in the literature, that the bridge laws should be
considered to express some kind of identity relation. For instance,
Sklar notes that the reduction of the “theory” of physical optics to
the theory of electromagnetic radiation proceeds by
identifying one class of entities — light waves —
with (part of) another class — electromagnetic radiation. He says
“… the place of correlatory laws [bridge laws] is taken by
empirically established identifications of two classes of
entities. Light waves are not correlated with electromagnetic waves,
for they are electromagnetic waves.” (Sklar 1967, 120) In
fact, if something like Nagelian reduction is going to work, it is
generally accepted that the bridge laws should reflect the existence of
some kind of synthetic identity.
Kenneth Schaffner calls the bridge laws “reduction functions.” He
too notes that they must be taken to reflect synthetic identities
since, at least initially, they require empirical support for their
justification. “Genes were not discovered to be DNA via the analysis of
meaning; important and difficult empirical research was
required to make such an identification.” (Schaffner 1976, 614–615)
Now one problem facing this sort of account was forcefully presented
by Feyerabend in “Explanation, Reduction, and Empiricism.” (Feyerabend
1962) Consider the term “temperature” as it functions in classical
thermodynamics. This term is defined in terms of Carnot cycles and is
related to the strict, nonstatistical second law as it appears in that
theory. The so-called reduction of classical thermodynamics to
statistical mechanics, however, fails to identify or associate
nonstatistical features in the reducing theory, statistical
mechanics, with the nonstatistical concept of temperature as it appears
in the reduced theory. How can one have a genuine reduction, if terms
with their meanings fixed by the role they play in the reduced theory
get identified with terms having entirely different meanings? Classical
thermodynamics is not a statistical theory. The very possibility of
finding a reduction function or bridge law that captures the concept of
temperature and the strict, nonstatistical, role it plays in the
thermodynamics seems impossible.
The plausibility of this argument, of course, depends on certain
views about how meaning accrues to theoretical terms in a theory.
However, just by looking at the historical development of
thermodynamics one thing seems fairly clear. Most physicists, now,
would accept the idea that our concept of temperature and our
conception of other “exact” terms that appear in classical
thermodynamics such as “entropy,” need to be modified in light of the
alleged reduction to statistical mechanics. Textbooks, in fact,
typically speak of the theory of “statistical thermodynamics.” The very
process of “reduction” often leads to a corrected version of the
reduced theory.
In fact, Schaffner and others have developed sophisticated Nagelian
type schemas for reduction that explicitly try to capture these
features of actual theory change. The idea is explicitly to include in
the model, the “corrected reduced theory” such as statistical
thermodynamics. Thus, Schaffner (1976, 618) holds that \(T\)
reduces \(T'\) if and only if there is a corrected version
of \(T'\), call it \(T'^*\) such that
Much work clearly is being done here by the intuitive conception of
“strong analogy” between the reduced theory \(T'\) and the
corrected reduced theory \(T'^*\). In some cases, as
suggested by Nickles (1973) and Wimsatt (1976), the conception of
strong analogy may find further refinement by appeal to what was
referred to as the “physicist’s” sense of reduction.
Philosophical theories of reduction would have it that, say, quantum
mechanics reduces classical mechanics through the derivation of the
laws of classical physics from those of quantum physics. Most
physicists would, on the other hand, speak of quantum mechanics
reducing to classical mechanics in some kind of correspondence limit
(e.g., the limit as Planck’s constant \((h/2\pi)\) goes to zero).
Thus, the second type of intertheoretic reduction noted by Nickles fits
the following schema:
Here \(T_f\) is the typically newer, more
fine theory, \(T_c\) is the typically older,
coarser theory, and \(\varepsilon\) is a fundamental parameter appearing in
\(T_f\) .
One must take the equality here with a small grain of salt. In those
situations where Schema R can be said to hold, it is
likely not the case that every equation or formula from
\(T_f\) will yield a corresponding equation of
\(T_c\) .
Even given this caveat, the equality in Schema R
can hold only if the limit is “regular.” In such circumstances, it can
be argued that it is appropriate to call the limiting relation a
“reduction.” If the limit in Schema R is singular,
however, the schema fails and it is best to talk simply about
intertheoretic relations.
One should understand the difference between regular and singular
limiting relations as follows. If the solutions of the relevant
formulae or equations of the theory \(T_f\) are
such that for small values of \(\varepsilon\) they smoothly approach
the solutions of the corresponding formulas in
\(T_c\), then Schema R will
hold. For these cases we can say that the “limiting behavior” as
\(\varepsilon \rightarrow 0\) equals the “behavior in the limit” where
\(\varepsilon = 0\). On the other hand, if the behavior in the
limit is of a fundamentally different character than the
nearby solutions one obtains as \(\varepsilon \rightarrow 0\), then the schema will
fail.
A nice example illustrating this distinction is the following:
Consider the quadratic equation \(x^2 + x - 9\varepsilon = 0\). Think of \(\varepsilon\) as a small expansion or
perturbation parameter. The equation has two roots for any value of
\(\varepsilon\) as \(\varepsilon \rightarrow 0\). In a well-defined sense, the solutions
to this quadratic equation as \(\varepsilon \rightarrow 0\) smoothly approach
solutions to the “unperturbed” \((\varepsilon = 0)\) equation
\(x^2 + x = 0\); namely, \(x = 0, -1\). On the other hand, the equation
\(x^2\varepsilon + x - 9 = 0\) has two
roots for any value of \(\varepsilon \gt 0\) but has for its
“unperturbed” solution only one root; namely, \(x = 9\). The
equation suffers a reduction in order when \(\varepsilon = 0\). Thus, the
character of the behavior in the limit \(\varepsilon = 0\) differs
fundamentally from the character of its limiting behavior. Not all
singular limits result from reductions in order of the equations.
Nevertheless, these latter singular cases are much more prevalent than
the former.
A paradigm case where a limiting reduction of the form
\(\mathbf{R}\) rather straightforwardly holds is that of classical
Newtonian particle mechanics (NM) and the special theory of relativity
(SR). In the limit where \((v/c)^2\rightarrow 0\), SR
reduces to NM. Nickles says “epitomizing [the intertheoretic reduction
of SR to NM] is the reduction of the Einsteinian formula for
momentum,
where \(m_0\) is the rest mass, to the classical formula
\(p = m_0 v\) in the limit as
\(v\rightarrow 0\).”[3]
 (Nickles 1973, 182) 
This is a regular limit—there are no singularities or
“blowups” as the asymptotic limit is approached. As noted one way of
thinking about this is that the exact solutions for small but nonzero
values of \(|\varepsilon\)| “smoothly [approach] the unperturbed or
zeroth-order solution [\(\varepsilon\) set identically equal to zero] as
\(\varepsilon \rightarrow 0\).” In the case where the limit is singular
“the exact solution for \(\varepsilon = 0\) is fundamentally different in
character from the ‘neighboring’ solutions obtained in
the limit \(\varepsilon \rightarrow 0\).” (Bender and Orszag 1978, 324)
In the current context, one can express the regular nature of the
limiting relation in the following way. The fundamental expression
appearing in the Lorentz transformations of SR, can be expanded in a
Taylor series as
and so the limit is analytic. This means that (at least some)
quantities or expressions of SR can be written as Newtonian or
classical quantities plus an expansion of corrections in powers of
\((v/c)^2\). So one may think of this
relationship between SR and NM as a regular perturbation
problem.
Examples like this have led some investigators to think of limiting
relations as forming a kind of new rule of inference which would allow
one to more closely connect the physicists’ sense of reduction with
that of the philosophers’. Fritz Rohrlich, for example, has argued that
NM reduces (in the philosophers’ sense) to SR because the
mathematical framework of SR reduces (in the physicists’
sense) to the mathematical framework of NM. The idea is that
the mathematical framework of NM is “rigorously derived” from that of
SR in a “derivation which involves limiting procedures.” (Rohrlich
1988, 303) Roughly speaking, for Rohrlich a “coarser” theory is
reducible to a “finer” theory in the philosophers’ sense of being
rigorously deduced from the latter just in case the mathematical
framework of the finer theory reduces in the physicists’ sense to the
mathematical framework of the coarser theory. In such cases, we will
have a systematic explication of the idea of “strong analogy” to which
Schaffner appeals in his model of philosophical reduction. The
corrected theory \(T'^*\) in this context is the perturbed
Newtonian theory as expressed in the Taylor expansion given above. The
“strong analogy” between Newtonian theory \(T'\) and the
corrected \(T'^*\) is expressed by the existence of the
regular Taylor series expansion.
As noted the trouble with maintaining that this relationship between
the philosophical and “physical” models of reduction holds generally is
that far more often than not the limiting relations between the
theories are singular and not regular. In such situations,
Schema R fails to hold. Paradigm cases here include
the relationships between classical mechanics and quantum mechanics,
the ray theory of light and the wave theory, and thermodynamics and
statistical mechanics of systems in critical states.
Despite the fact that limiting relations between theories may be
singular in this way, it is (at times) useful and appropriate to think
of physical theories as forming a hierarchy related by length or
energy scales. The idea is that different theories may apply at
different length or energy scales. If one takes this idea seriously,
then it may very well be the case that each theory in this hierarchy
will be phenomenological relative to those theories at higher energies
or shorter distances. Equivalently, such a hierarchy may form a tower
of effective theories. An effective theory is one that
describes the relevant phenomena in a circumscribed domain—a
domain characterized by a range of energies, for example.
The idea of effective theories is not new. In the 19th century and
earlier, scientists developed continuum equations such as the
Navier-Cauchy equations describing the behavior of isotropic elastic
solids and the Navier-Stokes equations for incompressible viscous
fluids. These equations were, and still are, remarkably safe. This
means that once one inputs the appropriate values for a few
phenomenological parameters (such as Young’s modulus and the sheer
stress in the Navier-Cauchy equations), one arrives at equation models
that allow us to build bridges and buildings that do not collapse. It
is remarkable that a theory/model that almost entirely fails
to refer to the details of the atomic and molecular structure of a
steel beam, say, can be so successful and safe. A question of deep
philosophical interest concerns how this can be the case. The
phenomenological parameters must encode at least some details about
the atomic and molecular make up of the beam. (Hence, the “almost” in
the statement above.) 
However, this raised an important question: Can one tell a story
bridging the models at the atomic scale and those at the continuum
scale of centimeters and greater? Reductionists typically believe
that it is possible to connect, and presumably to derive, the
continuum models starting from atomic scale details. There has been a
battle for two centuries, at least, between those who are persuaded that
such a bottom-up story can be told, and those such as Duhem, Mach, and
others who have championed a top-down modeling strategies. In the 19th
century this took the form of a heated dispute between so-called
rari-constancy and multi-constancy theorists who, respectively, tried
to determine the continuum equations from top-down (ignoring unknown
micro details) considerations, and theorists trying to determine the
continuum equations with small scale atomic assumptions guiding the
constructions. In fact, surprisingly, the former
prevailed. (Todhunter and Pearson 1960; Batterman 2012) 
The debate between bottom-up, reductionist modelers and top-down,
continuum modelers receives its modern presentation, at least in part,
in the debates about the existence and nature of emergent phenomena.
One area of recent interest where this occurs is in our understanding
of effective quantum field theories.
In quantum field theory, for instance, there has been considerable
success in in showing how a theory appropriate for some range of
energy scales is related to a theory for another range via a process
of renormalization (Bain 2012). Renormalization provides a kind of limiting
relationship between theories at different scales despite the fact
that the reductive Schema R typically fails because
of divergences related to singular limits. The physics at one scale
is relatively independent of that at some higher energy (shorter
length). In effect, renormalization is a mathematical scheme for
characterizing how the structure of interactions changes with changing
scale: it turns out that the domain characterized by some lower energy
(or larger length) scale is surprisingly and remarkably decoupled from
that of higher energies (or smaller lengths). In other words, the
decoupling entails that the higher energy regime does not much effect
the behaviors and character of the lower energy regimes.
New work, more generally on the problem modeling systems at widely
different scales (10+ orders of magnitude), in nano chemistry and in
materials science, brings some hope that the all-or-nothing dichotomy
between reduction and emergence can be somewhat blunted. As noted, a
question of real philosophical interest concerns how to understand the
relative autonomy of theories and models at large scales. (Why, again,
are the continuum equations so safe for large scale modeling?)
Contemporary work in applied mathematics on so-called homogenization
theory is beginning to provide interesting connections across these
widely separated scales. (Torquato 2002; Phillips 2001)
The mathematics of renormalization is best understood as an instance
of this general strategy for homogenization or upscaling. (Batterman
2012) It is crucial for a contemporary understanding of relations
between theories. It is fair to say, however, that being able to
understand such intertheoretic relations via homogenization and
renormalization techniques does not entail the existence of reductive
relations between the theories either in the philosophers’ or
the physicists’ sense of the term. However, such an
understanding may very well lead to a more nuanced and precise
characterization of the debates about reduction and emergence.
It seems reasonable to expect something like philosophical
reductions to be possible in those situations where Schema
R holds. On the other hand, neither philosophical nor
“physical” reduction seems possible when the limiting correspondence
relation between the theories is singular. Perhaps in such cases it is
best to speak simply of intertheoretic relations rather than
reductions. It is here that much of philosophical and physical interest
is to be found. This claim and the following discussion should not be
taken to be anything like the received view among philosophers of
science. Instead, they reflect the views of the author.
Nevertheless, here is a passage from a recent paper by Michael Berry
which expresses a similar point of view.
Even within physical science, reduction between different
levels of explanation is problematic—indeed, it is almost always
so. Chemistry is supposed to have been reduced to quantum mechanics,
yet people still argue over the basic question of how quantum
mechanics can describe the shape of a molecule. The statistical
mechanics of a fluid reduces to its thermodynamics in the limit of
infinitely many particles, yet that limit breaks down near the
critical point, where liquid and vapour merge, and where we never see
a continuum no matter how distantly we observe the particles
… . The geometrical (Newtonian) optics of rays should be
the limit of wave optics as the wavelength becomes negligibly small,
yet … the reduction (mathematically similar to that of
classical to quantum mechanics) is obstructed by singularities
… .
My contention … will be that many difficulties associated
with reduction arise because they involve singular limits.
These singularities have both negative and positive aspects: they
obstruct the smooth reduction of more general theories to less general
ones, but they also point to a great richness of borderland physics
between theories. (Berry 2001, 43)
When Schema R fails this is because the mathematics
of the particular limit \((\varepsilon \rightarrow 0)\) is singular. One can ask
what, physically, is responsible for this mathematical singularity. In
investigating the answer to this question one will often find that the
mathematical blow-up reflects a physical impossibility. For instance,
if Schema R held when \(T_f\)
is the wave theory of light and \(T_c\) is the
ray theory (geometrical optics), then one would expect to recover rays
in the shortwave limit \(\lambda \rightarrow 0\) of the wave theory. On the ray
theory, rays are the carriers of energy. But in certain situations
families of rays can focus on surfaces or lines called “caustics.”
These are not strange esoteric situations. In fact, rainbows are, to a
first approximation, described by the focusing of sunlight on these
surfaces following its refraction and reflection through raindrops.
However, according to the ray theory, the intensity of the light on
these focusing surfaces would be infinite. This is part of the
physical reason for the mathematical singularities. See also the discussion of the rainbow by Pincock 2011, and Belot 2005.
One is led to study the asymptotic domain in which the parameter
\(\varepsilon\) in Schema R approaches 0. In the example
above, this is the short wavelength limit. Michael Berry (1980; 1990;
1994a; 1994b) has done much research on this and other asymptotic
domains. He has found that in the asymptotic borderlands between such
theories there emerge phenomena whose explanation requires in some
sense appeal to a third intermediate theory. This is a claim
(Batterman 2002) that when taken literally, has raised a number of
hackles in the literature. However, understood in terms of the
mathematics of characteristics and wavefronts, as was originally
intended, the current author believes some of the debates are
misdirected. The emergent structures (the rainbow itself is one of
them) are not fully explainable either in terms of the finer wave
theory or in terms of the ray theory alone. Instead, aspects of both
theories (through asymptotic investigation of the wave equations) are
required for a full understanding of these emergent phenomena. 
This fact calls into question certain received views about the nature
of intertheoretic relations. The wave theory, for example, is surely
the fundamental theory. Nevertheless, these considerations seem to
show that that theory is itself explanatorily deficient. There are
phenomena within its scope whose explanations require examining
the asymptotics of the appropriate equation. This involves
paying attention to mathematical structures called characteristics and
wavefronts. See Bóna and Slawinski 2011. These mathematical
investigations of the deep asymptotic structure of hyperbolic
equations are not at all like the straightforward derivations from
initial data that are typical of in principle derivations
often referred to in carrying out the dictates of Nagel style
explanatory reductions. A similar situation arises in the asymptotic
domain between quantum mechanics and classical mechanics where
Planck’s constant can be considered asymptotically small. (See Belot
2005 for an alternative point of view.)
There is much here worthy of further philosophical study. Some very
recent work by Butterfield (2011), Butterfield and Bouatta (2011),
Norton (2012), Menon and Callender (2012) challenges the point of view
suggested by the above discussion. These authors address issues about
the nature of infinite idealizations, reduction, and emergence. A
common theme is that it is possible to reconcile emergence and
reduction. By and large these authors adopt a Nagelian sense of
reduction as definitional extension. For a contrary point of view one
can see Batterman (2002; 2012).