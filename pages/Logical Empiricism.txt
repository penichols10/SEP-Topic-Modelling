The term ‘logical empiricism’ has no very precise
boundaries and still less that distinguishes it from ‘logical
positivism’. It is therefore hard to map.
‘Logical empiricism’  here  includes  
three groups: (1) the Vienna Circle, here taken broadly to include
those who were part of various private discussion groups, especially
that around Moritz Schlick, and also the members of the more public
Ernst Mach Society (Verein Ernst Mach), (2) the smaller, but perhaps
more influential Berlin Society for Empirical Philosophy (later called
the Berlin Society for Scientific Philosophy), and (3) those 
influenced by or who interacted with members of the first two groups and
shared an  intellectual kinship with them. Besides 
Vienna and Berlin, there were important centers of the movement in
England, France, Scandinavia, at several universities in the U.S., and even
 China. This characterization 
 includes thinkers who disagreed with doctrines
espoused by members of the original groups and even some who defined
themselves in opposition to the movement. This results in a vague
boundary, but it suffices to identify a movement in which a large
number of able philosophers self-consciously participated and to
distinguish logical empiricism from other movements.
It does not, however, distinguish logical empiricism from logical
positivism, and it is doubtful that any principled such boundary can be
drawn along doctrinal or sociological lines (Uebel 2013). Usually when distinctions are drawn, ‘logical empiricism’ is
the wider term. Members of the Berlin group never used the term
‘positivism’ about themselves, but did use it concerning
some unnamed Viennese in stressing their differences from the
latter. In any case, these differences, even if real, were
smaller than the differences within the Vienna Circle on one hand or
within the Berlin group on the other. ‘Positivist’ is
a term usually applied by opponents of various
doctrines. It was used by some of the Viennese logical
empiricists about themselves but generally with caution and in
stressing the differences between their own views and those of the
19th century positivists. The one philosopher who
would have unhesitatingly described himself as (having been) a logical
positivist was A.J. Ayer.
Another way of mapping the boundaries of logical empiricism is to list
the specific philosophers who were centrally or peripherally part of
it. This included many of the most important philosophers of the
mid-twentieth century. 
 Hans Hahn, Moritz
Schlick, Rudolf Carnap, and Otto Neurath were leaders of the Vienna
Circle, and Kurt Gödel regularly attended its meetings. The
list of its members, visitors, and interlocutors is staggering, including
 A.J. Ayer, Herbert Feigl, Philipp
Frank, Hans Hahn, Carl Hempel, Karl Menger, Richard von Mises, Ernest
Nagel, Karl Popper, W.V. Quine, Frank Ramsay, Hans Reichenbach, Alfred
Tarski, Friedrich Waismann, and Ludwig Wittgenstein, among many others. Not all of
these would admit to being part of the logical empiricist movement, of
course, but a case can be made that all contributed to it. The
Berlin Society for Empirical (or Scientific) Philosophy was, as stated,
smaller but perhaps more influential. Led by Hans Reichenbach, it
included Kurt Grelling, Walter Dubislav, Kurt Lewin,
Richard von Mises, Paul Oppenheim, and others. Hempel took his doctorate
in Berlin, working with Reichenbach until the latter was forced
to leave in 1933. Hempel also spent time in Vienna and Prague. Of course, among the foremost associates of the Berlin
Society was Albert Einstein, who was also in Berlin also until
1933.
There was also an important group of logicians in Warsaw of which
Alfred Tarski is the best known. Tarski interacted significantly
with the logical empiricists in Vienna, Berlin, and the U.S., but it is
more reasonable to classify the Polish logicians as an allied group
rather than include them within the logical empiricist movement.
Because of the catastrophic dislocations of Europe in the 1930s, the
main focus of the logical empiricism moved from central Europe to
America by the close of that decade. Erkenntnis, the main
journal of the movement, which had been edited by Reichenbach and
Carnap, ceased publication by 1940. In 1930 Feigl moved to the
U.S., and Carnap moved to Chicago in 1936. Hempel came to Chicago
and Menger to Notre Dame in 1937. The ensuing years witnessed a
massive exodus to America from central Europe. Reichenbach
arrived in the U.S. in 1938 after five years in Turkey.
Also in 1938 Gustav Bergmann and Philipp Frank emigrated. Edgar
Zilsel came in 1939. Alfred Tarski was on a visit to the U.S.
when Poland was invaded in 1939, and so he stayed. And by 1940 Richard
von Mises was also in America.
In the U.S., these exiles were joined by the Americans Nelson
Goodman, Charles Morris, W.V. Quine, Ernest Nagel, and, after the war,
by Reichenbach’s UCLA students Hilary Putnam and Wesley
Salmon. Adolf Grünbaum can also be considered as clearly in
the Reichenbach lineage. And Wilfrid Sellars was, in his early
years, a close associate of Feigl. The American incarnation of
the logical empiricist movement enjoyed generally good relations with
the American pragmatists, not only because many of the logical
empiricists had a strong pragmatist component to their philosophy, but
also because the pragmatists and logical empiricists shared a common
concern for empirical methodology in the service of social
reform. Institutionally, the movement was represented in most
major American universities, and such journals as Philosophy of
Science (with Carnap and Feigl on the Editorial Board and
Reichenbach and Schlick on the Advisory Board) and Philosophical
Studies (founded and edited for many years by Feigl and Sellars)
provided ample outlet for their publications. In addition, the
Inter-Scientific Discussion Group was founded by Philipp Frank at
Harvard. That grew into the Institute for the Unity of Science,
called by some the Vienna Circle in exile. Meanwhile in Chicago
the Encyclopedia of Unified Science was established with
Neurath, Carnap, and Morris as its editors.
But even from late 30s onward the movement was hardly limited to
America. Ayer remained in England. Wittgenstein returned to
Cambridge in 1929, but with regular visits to Vienna, including those
on which he discussed issues surrounding a strong version of
verificationism with Schlick and Waismann. Popper fled to New
Zealand in 1937, and in 1946 moved to the London School of
Economics. Neurath fled from Vienna to the Hague and then again
in 1940 to England, where he remained till his death in 1945.
Friedrich Waismann went to England in 1937. In 1939 Rose Rand, a
less well-known member of the Vienna Circle, fled to England and then
in 1954 emigrated once more to the U.S. There were
like-minded thinkers in Scandinavia (such as Jørgen
Jørgensen, Eino Kaila, and Arne Naess) and as far away as
Argentina (H.A. Lindemann) and China (Tscha Hung).
It is impossible to say when logical empiricism ceased to be
sufficiently cohesive to be identifiable as a continuing
movement. Certainly by 1960 a great many philosophers, including many
who had earlier clearly been part of the movement, were identifying
themselves in opposition to what they took to be logical
empiricism. And some members simply changed their minds or pursued
different projects. Logical empiricism probably never commanded the
assent of the majority of philosophers in either Europe or America,
and by 1970 the movement was pretty clearly over—though with
lasting influence whether recognized or not. In the 1980s there was a
resurgence of historical interest in logical empiricism. That
historical interest continues to clear away many of the caricatures
and misconceptions about the logical empiricists. Among the major
results of this work is the recognition of the tremendous variety and
subtlety of views represented within the movement and the fact that
many of the arguments later deployed by critics of logical empiricism
had been pioneered by the logical empiricists themselves.
Given the emphasis on science and its technical apparatus, social
renewal, clarity and rationality of belief, functionality, and above
all the palpable sense of doing philosophy in an importantly new way,
it is reasonable to associate logical empiricism with other forms of
European modernism in the 1920s and 30s, such as Neue Sachlichkeit in
art and the Bauhaus in architecture and design, and with mid-century
modernism as well as with political liberalism, from the New Deal to
the Great Society in the United States. There have been
recognizably modernist developments in various fields including
philosophy for centuries.
With a movement as large and complex as logical empiricism a great many
factors went into raising the questions it would address, making them
seem urgent, and making it seem as though the intellectual resources it
would need to address these questions were either at hand or could be
developed.
One long-term process with profound implications was the steady
departure of the various sciences from philosophy to form autonomous
disciplines. By early in the twentieth century mathematics,
physics, chemistry, biology, and the social sciences were all pursued
professionally and independently from philosophy. And psychology
was just separating from philosophy.
 Yes, there were polymaths who could and did pursue a science
and philosophy professionally. Those were increasingly rare, though
single-discipline scientists did from time to time make
philosophic-seeming pronouncements. But they did so from outside
the field. This pattern of steady departures raised the pressing
question: What sort of thing remained behind? Once mathematics
and the empirical sciences all left, what was left for philosophy?
The nature of philosophy was always a vexed philosophic question, but
now it was particularly insistent. Surely there was no domain of
empirical facts that philosophy could call its own. All that real
estate had been parceled out. One answer available at
the time that logical empiricism flourished was that the genuinely
philosophic remainder after the departure of the sciences is somehow
deeper than the empirical sciences and gets at matters, perhaps
cultural ones, that are more profound and important than anything that
empirical science even can address. This is either because on
this conception philosophy has a mode of access or
“evidence” that the empirical sciences do not and cannot
have, or because the very idea of fidelity to evidence and punctilious
argument is somehow small-minded.
The logical empiricists found this answer unappealing. Indeed, this
conception of philosophy is precisely what Carnap means by
‘metaphysics’. (As a consequence, what Carnap meant by
that word is different from what late twentieth and early twenty-first
century philosophers generally mean in describing their own work as
analytic metaphysics.) The logical empiricists were eager to conceive
of their enterprise as scientific and to engage in philosophy only
insofar as it was also scientific. This science need not be
empirical and need not include all that was traditional in
philosophy that had not been incorporated into the independent
sciences. The decision to be scientific can hardly be the end of the
story. It requires rather better and more detailed answers to
questions about what scientific methods are, how the mathematical (and
other apparently non-empirical sciences) fit together with the
empirical ones, and what, more precisely, philosophy’s role was.
 A
second series of developments that raised questions for logical
empiricism to address were developments in the sciences themselves,
especially the rise of non-Euclidean geometries in mathematics and the
establishment of relativity theory in physics. These posed a
serious challenge to what would otherwise be an attractive scientific
philosophy, namely some version of Kantianism. Kant had
recognized that the best of modern science was often mathematical in
character and had labored to integrate both geometry and arithmetic
into our empirical picture of the world. He had held that we
could not represent the world except as a Euclidean structure and hence
Euclidean geometry was, a priori, a permanent feature of any future
physics. The demonstration that non-Euclidean pure
geometrical structures were as consistent as Euclidean ones and that
spaces can indeed be represented as a non-Euclidean manifolds was one
half of the problem. The other half came when Einstein argued
convincingly that physical space was best described as a non-Euclidean
manifold of non-constant curvature. Plainly Euclidean geometry
could not be guaranteed a future physics. Modern mathematical
logic also posed a problem for other Kantian claims, but not in the
same wrenching way.
Many logical empiricists started out as neo-Kantians: Reichenbach,
Carnap, Schlick, and even Hempel (until he studied with Reichenbach,
who by that time had revised his view). The difficulties with
geometry and relativity certainly do not refute all forms of
neo-Kantianism, but the difficulties are quite real nonetheless. The
need is to understand how mathematics can be integrated into what is
otherwise an empirical enterprise, i.e., physics, chemistry, biology,
etc. Addressing this need was to be a major part of the logical
empiricist program.
The background of logical empiricism described so far has been confined
to the academic world, but events outside that domain shaped the
movement as well. World War I was an unmitigated disaster for
central Europe, followed by economic turmoil in the 20s
and political upheavals of the 30s. It is hard to exaggerate
these changes. Monarchies that had stood for centuries
disappeared overnight and their empires disintegrated. This level
of political convulsion had not been seen since the French Revolution,
and that earlier upheaval was comparatively confined. Cultural
changes were equally profound, and these were reflected by radical
departures in the arts such as painting, music, and architecture, and
even more importantly in new modes of living.
The logical empiricists were no mere bystanders.  They, or
at least the main leaders of the movement, were politically and
culturally engaged. Even more important, this engagement was
accompanied by the conviction that their cultures were incapable of the
necessary reform and renewal because people were in effect enslaved by
unscientific, metaphysical ways of thinking. Such ways of
thinking might be exemplified in theology, in the racial hatreds of the
day, in conceptions of property, and in traditional ideas about the
“proper” roles of men and women in society. So to
articulate a “scientific world conception” and to defend it
against metaphysics was not just to express an academic position in the
narrow sense. It was a political act as well; it was to strike a
blow for the liberation of the mind. To articulate scientific
methods and a scientific conception of philosophy was the essential
first step in the reform of society and in the emancipation of
humankind (Carnap 1958/2017, Creath 2009, Uebel 2012.
If all of this sounds like something out of the 18th century
Enlightenment, the analogy was not lost on the logical empiricists
themselves. André Carus has argued that this is
exactly what Carnap had in mind by “explication” (Carus
2007). Neurath frequently drew parallels between the logical
empiricists’ anti-metaphysical program and the earlier
Enlightenment ambitions. Certainly Kant had inveighed against the
metaphysics of his time, and the anti-metaphysical tradition remained
strong within the scientific community through the 19th
century.
The point so far was not to ask whether the logical empiricists were
right in any of this. That question will come up later. So far the
issue has been only to see the motivations that the logical
empiricists had—and from their point of view—for
addressing certain questions and for thinking that answers to those
questions were urgently needed. None of this, however, says why the
logical empiricists thought they had or could have the means to answer
these questions. To that we now turn.
Since Newton the most paradigmatic examples of empirical science were
those claims, usually quantitative ones, that were properly inferred
from or appropriately confirmed by experience. Speaking very
informally, these are the ones that we have good reason to believe or
at least better reason to believe than the available
alternatives. The problem, of course, is to specify the form of
proper inferences, the form of an appropriate confirmation relation,
and/or the structure of good reasons. The task is daunting, but
logic in a suitably broad sense seems to be the right tool. Still
speaking informally, logic seems to give us the structure of (good)
reasoning. There are other conceptions of logic, of course, but
this is a standard one and pretty well describes what the movement
needed.
If logic was the tool that was wanted, it was newly ready for
service. The progress of modern mathematical logic from Bolzano
through Russell and beyond was truly impressive. Arguably, it
could now express all parts of classical mathematics. Besides the
first order predicate calculus one would need either set theory or
higher order logic, but these were recent developments as well.
Logic, like the empirical sciences, was progressive and could be
approached cooperatively by more than one investigator. In
Our Knowledge of the External World (1914) Russell had even
positioned logic as the locus of scientific method in
philosophy. It is small wonder then that those who were looking
for something scientific in what was left of philosophy turned to
logic. Wittgenstein’s no-content theory of logic in the
Tractatus (1921/1922) was tantalizingly suggestive about how
mathematics could be integrated into an overall empirical theory of the
world. Wittgenstein also expressed a radical verificationism in
the early 1930s in his conversations with Schlick, Waismann, and other
members of the Vienna Circle. Many of the logical empiricists in
turn could see in some version of that verificationism the ideal tool
with which to carry out their anti-metaphysical program. There
was, naturally, much left to accomplish, but even with
Gödel’s results one could expect that further impressive
strides in logic could be made. Indeed, much was accomplished
even if the perfect account of scientific reasoning proved
elusive. Perfection is elusive in all the sciences, but that is
no reason for despair.
The logical empiricist movement is the sum of the interwoven
trajectories of its members, so one way of describing that movement is
to trace those various trajectories. To do so in detail for all
those involved would take rather longer than the movement lasted.
That would be inappropriate for one entry in an encyclopedia,
especially one in which entries for many of the members will appear
independently. The thumbnail sketches of the work of some
representative figures below show the breadth and international
character of the movement. While the list is long, it covers only
a small fraction of those involved and leaves out many important
thinkers.
It is not possible in an essay of this scope to trace all the
issues that the logical empiricists addressed or even to treat any one
of them with completeness. What is possible is to highlight some
salient issues, clear away some misconceptions about them, and
sketch a bit how those issues were developed over time. The first
is a related set of concerns: empiricism, verificationism, and
anti-metaphysics. The second is the logical empiricists’
treatment of logic and mathematics as analytic. Third is the
related issues of the unity of science and reduction. And
finally, comes the issue of probability. Given what has already
been said, the reader should be aware that none of the doctrines
discussed below was shared by all members of the logical empiricist
movement.
Since antiquity the idea that natural science rests importantly on
experience has been non-controversial. The only real questions about
the sources of scientific knowledge are: Are there parts of
science that do not rest on experience or rest also on something other
than experience? If so what account can we give of those
parts? And to the extent that science does rest on experience how
can we know that it does? There is another question about science
related to these, though not strictly about the sources of science, and
that is: Why, in making claims about the world, should we be scientific
as opposed to say mystical? The difficulty is that any scientific
answer to this last question would reasonably be thought to beg the
very question it purports to address.
Long before the twentieth century the prevailing opinion was that
Euclidean geometry, standard mathematics, and logic did not rest on
experience in any obvious way. They were largely presupposed in
our empirical work, and it was difficult to see what if anything might
disconfirm them. Geometry was a special case and might be handled
in different ways that we shall not discuss here. That leaves
logic and mathematics.
If Frege and Russell were right, then mathematics could be thought
of as expressing no more than logical truths and handled in whatever
way logic was to be treated. For Frege both mathematics and logic
were analytic, but that, even if true, does not provide the needed
answers. Wittgenstein’s no-content theory of logic
suggested that all of the real claims, the ones that had genuine
content, could be appropriately supported by experience, and the
logical and hence mathematical claims had no content to support.
This seemed to open the way for a thoroughgoing empiricism in
which the logical and mathematical fit in with the ordinary claims of
physics and biology in a harmonious way. The next subsection
about analyticity discusses the question of whether the needed
distinctions can be drawn.
In developing his theory of types Russell said in effect that some
expressions that seem to be sentences in fact say nothing at all.
This is because, despite appearances, they are not grammatically well
formed. Wittgenstein found this suggestive. In the
Tractatus he suggested that much else was nonsense as well
including traditional metaphysics and supposed claims about the
“higher”. When in late 1929 Wittgenstein
proposed (Waismann 1967/1979), in conversations with Schlick and
Waismann, a strict verificationism as a basis for identifying the
legitimate parts of discourse, this seemed to the logical
empiricists to be a very attractive tool for setting aside the
unscientific parts of philosophy.
This does not mean, however, that all logical empiricists or even
all members of the Vienna Circle accepted the strict verificationist
view that in order to be meaningful a claim must be implied by a finite
number of observation sentences. Even though those observation
sentences need not be true, this view had the drawback that so-called
laws of nature would not be meaningful on this criterion. Schlick
was prepared to bite the bullet and hold that laws were not statements
at all but principles of inference. Others were not prepared to
go so far and sought more liberal formulations. This more liberal or
“left” wing of the Vienna Circle included Carnap, Philipp
Frank, Hahn, and Neurath. Carnap does not seem to have been a
strict verificationist even in the Aufbau (1928/1967).
Over the years a great many different formulations of verificationist
principles ensued. Most of them came to a bad end rather quickly,
and this is sometimes taken as a convincing argument that any form of
verificationism is utterly misguided. Perhaps, but we should be
cautious. There are undoubtedly many different features joined in
any one of the proposals, and even a sequence of failures may not show
where to place the blame. The central idea behind verificationism
is linking some sort of meaningfulness with (in principle)
confirmation, at least for synthetic sentences. The actual
formulations embodied not only such a link but various particular
accounts of confirmation as well. Now confirmation is a complex
matter, and it is unlikely that we shall have the final satisfactory
account any time soon. This should not persuade us, however, that
there are no satisfactory accounts of confirmation any more than our
current lack of the final physics should convince us that there are no
physical facts of the matter. So even a string of failures in
formulating verificationist principles may mean no more than that the
embedded accounts of confirmation are too simple but the link between
meaningfulness and confirmation is nevertheless sound.
Even if we set this caution aside, there may be parts of
persistently employed strategy that lead to persistent failure.
These parts and failures might be avoidable. To see how this may
be so we will compare what is perhaps the most famous formulation of
the verificationist principle, in Ayer 1936, with a later one, in
Carnap 1956. A.J. Ayer had visited the Vienna Circle from late
1932 on into 1933, returning home for the summer term. While in
Vienna he attended meetings of the Circle and overlapped for five weeks
with Quine. Neither Carnap nor Neurath were there at the time, so
the left wing of the Circle was not fully represented. When Ayer
returned to England he published Language, Truth, and Logic in
1936. Even immediately it was widely discussed, and after the war
sales were spectacular. For many in England this book was the
epitome of logical positivism and remains so.
Ayer was careful to restrict his criterion of meaningfulness to synthetic sentences and
to demand only in principle confirmation. And the formulation seems
very natural: Confirmation is a feature that applies to sentences (or
groups of them) and not to sub-sentential parts, and for an empiricist
the content that a synthetic sentence has would be empirical
content. So it would seem that to have empirical content a
sentence, A, should either directly imply some observational
sentence or add to the observational content of some other sentence,
B. That is, the conjunction of A and B
should imply some observational sentence not implied by B
alone. This formulation may be natural, but it is also fatally
flawed. It would declare any sentence whatsoever as meaningful: For
any sentence A and any observation
sentence O, A would be meaningful because it could
be conjoined to A ⊃ O. The latter would not in
general imply O, but the conjunction would.
Other more elaborate formulations followed along the same lines, and
other more elaborate counterexamples appeared just as fast.
Hempel reviewed the situation twice within about a year (Hempel 1950
and 1951). First he concluded that it was a lively and promising
line of research and later concluded that it was not promising at
all. In retrospect it may be that the problems arise because we
were led by the fact that confirmation is a feature that applies to
whole sentences into thinking that the level at which to apply the
criterion was the level of whole sentences. Now a sentence with
meaningless parts might well pass some test especially if the test
involves its being combined with other sentences that can have
meaningless parts. So one way to avoid this difficulty is to try
to find a formulation that applies the test at the level of basic
expressions, those that can be thought of as “not having
parts” so to speak.
This is the strategy that Carnap employed in “The Methodological
Character of Theoretical Concepts” (1956).  Observational terms
are assumed to have empirical content.  Logical terms are assumed to
have none. And all defined terms are assumed to be replaced by their
definitions. If for some basic, non-logical term there is a sentence
that contains that term as its only non-logical element and if that
sentence implies some observation sentence, then that sentence has
empirical content and so does its only non-logical term. If we have
established that each term from some set, K, is empirically
significant we might test still further terms by seeing whether those
further terms can add to what is sayable with terms
from K. Carnap’s actual definition is quite complicated, but
it does seem to avoid the difficulties of its predecessors. It also
allows an account of why those predecessors ran into trouble, viz., that they
applied at the level of whole sentences (naturally enough) rather than
to elementary terms.
Not long after Carnap’s definition was published David Kaplan
devised what seemed to be counterexamples. They became fairly
well known, but they were not published until 1975. Shortly
thereafter it was shown (Creath 1976) that either Carnap’s
definition is not open to the counterexamples as presented or it can be
patched in a very natural way so that it avoids them. This does
not show that there are no counterexamples or that there are no other
features of the definition to which one might object. But it does
show that the situation is not as dire as Hempel supposed in 1951.
We need to address another issue in considering
verificationism, the persistent criticism that it is
self-undercutting. The argument for this claim goes like
this: The principle claims that every meaningful sentence is
either analytic or verifiable. Well, the principle itself is
surely not analytic; we understand the meanings of the words in it
perfectly well because we understand our own language. And we
still do not think it true, so it cannot be true in virtue of
meaning. And it is not verifiable either (whatever we choose
‘verifiable’ to mean).
This sounds more compelling than it is. Ayer understood the
principle to be a definition, defining a technical term,
‘meaning’. If so, then the sentence expressing the
principle would indeed be analytic. So the self-undercutting
charge strictly fails. But so construed and with nothing else
said about it the principle would not have the same punch as
before. Why should a metaphysician care whether his or her
utterances lack some technical feature?
Carnap explicitly takes up the “self-undercutting”
charge against verifiability in Philosophy and Logical Syntax
(1935), and he is not interested in introducing a new technical
term, ‘meaning’, so or in denying this new  technical property to
unverifiable sentences. Carnap is careful to distinguish the
language for which the verifiability principle is given from the
meta-language in which we talk about that language. This
meta-language would be the language in which the principle would be
expressed. This may seem to offer another strategy against the
“self-undercutting” charge because the principle applies to
a different language than that in which it is expressed. This is not
Carnap’s strategy. Carnap fully understands that if the
general verificationist strategy is followed, there will also be a
verificationist principle expressed in the meta-meta-language governing
the meta-language.
Carnap’s real defense of the principle was achieved by changing the
nature of the discussion. By 1935 Carnap had introduced an important
new element into his philosophy called the Principle of
Tolerance. Tolerance is a radical idea. There is no uniquely correct
logic (1934/1937 xiv–xv). Empiricism is a convention (Carnap,
1936/1937 33). Perhaps more precisely each of the various versions of
empiricism (including some sort of verificationism) is best understood
as a proposal for structuring the language of science. Before
tolerance, both empiricism and verificationism are announced as if
they are simply correct. Correspondingly, what Carnap called
metaphysics is then treated as though it is, as a matter of brute
fact, unintelligible.  But what is announced thus dogmatically can be
rejected equally dogmatically. Once tolerance is in place, alternative
philosophic positions, including metaphysical ones, are construed as
alternative proposals for structuring the language of science.
None of them is the uniquely correct one, and no theoretical
argument or evidence can show that it is. Nor can theoretical
arguments or evidence show that it is false. Neither proposals
nor languages are the sort of thing to be true or false. Instead,
proposals call for practical decisions and practical arguments rather
than for theoretical reasons or evidence. Carnap believes that
there are indeed very good practical reasons for adopting the proposal
of verificationism, for choosing a language of science in which all
substantive (synthetic) claims can, at least in principle, be brought
before the court of public experience. The reason is that if we
do not require this, the result is “wearisome
controversies” that there is no hope of resolving. That, he
thinks, is the sad history of attempts to get beyond science, and it is
just too painful.
If the proposals constituting some version of verificationism are
adopted, then in the language thus constituted it will be analytically
true that there are no synthetic sentences that are both unverifiable
and meaningful. The notion of meaning here is not some new
technical invention. Rather, ‘meaning’ is used in
something like the ordinary sense. No grammatically well-formed
sentence of this new language violates the verifiability
principle. And the principle itself is completely safe.
Thought of in this way the verifiability principle does not describe
natural language, it is not intended to. It is intended to reform
language to make it a more useful tool for the purposes of
science. Carnap is under no illusion that natural languages are
free from metaphysics. Nor is he under the illusion that
defenders of the sort of metaphysics he targets will readily step up to
the challenge of presenting precise rules of grammar and inference.
There is one other change that tolerance brings to Carnap’s
own vocabulary. Before tolerance, verificationism is stated in
such a way that violations would count only as unintelligible
gibberish. With tolerance in place, Carnap is prepared to imagine
non-empiricist languages, though of course he thinks they are very
unwise. So instead of saying that 
sentences in non-empiricist languages are meaningless, he
says that they are empirically meaningless. And that has a very different
flavor. There is no weakening of his defense of empiricism, but
it is put on a somewhat different footing.
Logic, mathematics, and mathematical geometry had traditionally
seemed to be confirmationally “different”. Indeed it
is hard to indicate any conditions under which any parts of them would
be disconfirmed. Leibniz had called them truths of reason.
Hume said that they represented relations of ideas. Kant had held
that the truths in these areas were a priori. Mathematics and
geometry were not analytic for Kant, but logic was. Kant had two
criteria of analyticity, apparently thinking them equivalent. First, in
subject-predicate sentences, an analytic sentence is one in which the
concept of the predicate is contained in that of the subject.
Second, an analytic sentence is one whose denial is
self-contradictory. This seems to include not only the sentences
whose surface logical form would be of the required sort but also those
that can be got from such logical truths by making substitutions that
were conceptually equivalent. The more modern rough analog of
this is to say that the analytic sentences are those that are true in
virtue of logic and definition.
Frege certainly developed logic beyond that which was available to
Kant, but he did not think of himself as changing the analytic status
of it. Logic is after all the only avenue we have for giving
meaning to the notion of (logical) contradiction. Of course Frege
also attempted to reduce mathematics to logic (including both first and
second order logic), and insofar as that reduction was successful it
would have implied that mathematics was analytic as well. Frege
said little of geometry, but for him it was synthetic a priori.
Carnap had not only studied with Frege, but like many of the logical
empiricists he had started out as a neo-Kantian as well. So
especially in view of Russell’s relatively more successful
attempt at reducing mathematics to logic, it was perhaps natural that
Carnap would consider both mathematics and logic as analytic.
Geometry could be handled in several different ways that we will not
discuss here. But from fairly early on there was widespread
agreement among the logical empiricists that there was no synthetic a
priori, and that logic and mathematics and perhaps much else that
seemed impervious to empirical disconfirmation should be thought of as
analytic. The point of drawing the analytic-synthetic
distinction, then, is not to divide the body of scientific truths or to
divide philosophy from science, but to show how to integrate them into
a natural scientific whole. Along the way the distinction
clarifies which inferences are to be taken as legitimate and which are
not. If, as Carnap and Neurath were, you are impressed by
Duhemian arguments to the effect that generally claims must be combined
in order to test them, the analytic-synthetic distinction allows you to
clarify which combinations of claims are testable.
If analytic, a sentence is true in virtue of the conventions of
language. In saying that, however, we must pause to confront two
widespread confusions. First, Quine alleges (1963, 385f) that the
notion of analyticity was developed and purports to explain for both
Kant and Carnap how certainty is possible. In fact
certainty has little or nothing to do with analyticity for the leading
logical empiricists. In saying that such claims are based on
convention they were explicitly calling attention to the revisability
of conventions and the sentences that owed their meanings to those
conventions. Second, nowadays any talk of convention is likely to
prompt the response: “But that cannot be! No proposition
can be made true by our conventions or decisions.” Unless
it is a proposition about conventions, this second sentence of the
response is true. But it is also completely irrelevant.
Analyticity applies to sentences rather than
propositions. Our conventions and decisions can and do affect
what expressions mean and thus what sentences mean. Once the meaning is
specified, it may well be that any sentence that has this meaning would
be true even if, for example, the point masses of the universe were
arranged quite otherwise than they in fact are. These are the
analytic sentences. No claim is being made that meaning causes
anything or that convention makes anything true. The
“making” image here is out of place. It is just that
in these cases the truth value of the sentence may well be functionally
dependent on meaning alone. If it is, then in this special sense,
truth value depends on meaning, and that depends on convention.
Other sentences whose meanings are specified might well be true or
false depending on how things in the external world, so to speak, are
arranged. In this other category of sentence the truth value is
not functionally dependent on meaning alone. They are the
synthetic sentences. Now this puts matters extremely
informally. But at least the nature of the confusions over
certainty and convention should be clear.
In the Logical Syntax of Language (1934/1937) Carnap
defined ‘analytic’ in a new way in order to circumvent
Gödel’s incompleteness results. The method used was to
distinguish between a derivation relation (the relation that holds
between some premises and what can be got from them in a finite number
of steps) and a consequence relation. The latter is an
essentially semantic relation that holds between some premises and some
other claim such that on all valuations under which the premises are
all true, so is that other claim. This definition bears a
stronger resemblance to Tarski’s account in (Tarski
1936b/1956). In any case, Carnap is able to show that for any
sentence of pure mathematics either it or its negation is a consequence
of the null set of premises. This leaves Gödel’s
results completely intact as they concerned what is provable, that is,
derivable from the null set of premises or from any one consistent
axiomatization of mathematical truths.
As noted above, another innovation of Logical Syntax is the
Principle of Tolerance. While it reflects a long-standing
attitude on Carnap’s part, the principle itself is new.
Later Carnap was to say that the Principle of Tolerance was
“perhaps better called the principle of conventionality”
(Carnap 1942, 247), that is, the conventionality of linguistic
forms. Tolerance stabilizes the verification principle as well as
Carnap’s empiricism, and it reinforces the idea that the
analytic-synthetic distinction is always relative to a particular
language (Creath 2009).
In the late 1950s Carnap began exploring (1963a and 1966) how a notion
of analyticity might be developed for novel theoretical terms where
the theories in which those terms are embedded are presented by means
of a system of postulates. It is not clear that the account he
developed was intended to supersede his earlier account. In any case
Carnap’s suggestion is as follows (where for convenience terms are
used autonymously): Let T be the totality of theoretical
postulates, and C be the totality of mixed sentences (the
sentences of the theory containing both antecedent and novel
terms). Also let
R(TC) be the Ramsey sentence for TC, that
is, the result of replacing each of the non-observational terms
in TC with predicate variables and closing that open sentence
with corresponding existential quantifiers. R(TC)
⊃ TC can, Carnap says, be thought of as the analytic
sentence for the theory, that is, a sentence that gives to the
theoretical terms of TC their meaning. Over the last decade,
this idea of Carnap’s has provoked considerable discussion that has
not yet been resolved. Whatever worries there may be concerning this
part of Carnap’s view, they are distinct from the more famous concerns
raised by Quine.
Quine began having doubts about analyticity about 1940, though he
seems not to have been firmly committed against it until later.
In any case his doubts were not published until 1951 in his famous
paper “Two Dogmas of Empiricism”. Quine’s
readers have understood his arguments in many different ways. The
most general form of his complaint is that ‘analytic’ so
far lacks the appropriate tie to observational criteria that
Carnap’s own account of theoretical terms in empirical science
would demand. More specifically, where there has been an attempt
at such a general criterion it has resulted in either a “drastic
failure as tended to admit all or no sentences as analytic, or there
has been a circularity” (Quine 1963, 404) of a kind that defines
‘analytic’ in terms that themselves lack the appropriate
empirical criteria and so can be accounted for only by appeal to
analyticity itself.
This complaint falls far short, as Quine well understood, of a proof
that Carnap’s appeal to analyticity was doomed. First, it
relies on the demand that theoretical terms must satisfy some empirical
significance criterion. Many people at the time, including some
who followed Quine in rejecting analyticity, also rejected any general
empirical significance demand for theoretical terms. Second, one could
accept the demand for theoretical terms in physics or chemistry and
deny, as Carnap did, that the demand applied to his own work.
This is because Carnap saw himself as working in an area within
metamathematics rather than in empirical linguistics. Third,
Quine did not pretend to have considered all of the possibilities for
the explication of analyticity. And so it may be possible to meet
Quine’s demands to the extent that they are legitimate.
Fourth and finally, Quine seems in Roots of Reference (1974)
to have provided an explication for ‘analytic’ that meets
his demand for empirical/behavioral criteria without inducing either
the drastic failure or the circularity envisioned above.
There is another somewhat independent thrust to Quine’s
campaign against analyticity. In the last section of “Two
Dogmas” (1951) Quine gives an extremely attractive sketch for an
alternative epistemology that apparently makes no appeal to
analyticity. Insofar as that sketch can be filled out
successfully it would constitute a dispensability argument against
analyticity. Whether it can be thus filled out, however, remains
to be seen.
Quine’s other provocative theses, including especially his
claims about the indeterminacy of translation, while relevant to his
assessment of analyticity, would carry us too far afield to consider
their ramifications here. As with most topics in philosophy there
is no uniform agreement in the literature as to whether the notion of
analyticity is or can be made sufficiently clear for use in scientific
philosophy. Nor is there such agreement that Quine’s
epistemological sketch can be satisfactorily filled out. Both
approaches have their defenders and their detractors. But between
them they seem to be the most promising avenues for integrating the
logic-mathematical part of science with the more straightforwardly
empirical parts. Since Carnap is and Quine can be argued to be
within the logical empiricist tradition, this progress toward such
unification can be counted as part of the legacy of the movement.
The commitment of some of the logical empiricists to the unity of
science has been in recent years often discussed but less often
understood. One hears in conversation that it was a sort of
rearguard action designed to preserve as much as possible of a
phenomenalist version of ontological reduction. One reads in
print that it can be refuted by the obvious fact that the various
sciences have quite distinct theoretical vocabularies (Suppes
1978). Both reactions are misplaced.
It was the left wing of the Vienna Circle, and above all Otto Neurath,
that championed the unity of science. They also promoted
physicalism, anti-foundationalism, and a generally naturalistic
viewpoint. A main focus of their activities from the late 30s was
The Encyclopedia of Unified Science edited by Neurath in
Europe and Carnap and Charles Morris in Chicago. A great many
philosophers of many different persuasions participated in that
project. The project may have been unified science, but they did
not have a completely unified view of what that project was. Here
we will discuss the Neurath and Carnap versions of it to see what their
central concerns were.
Neurath seems to have had two primary motivations to advance under
the banner of the unity of science. First, he was concerned that
there be no a priori methodological cleavage between the natural and
the social sciences. On the social scientific side he was
concerned that these sciences not condone some private, mysterious mode
of insight (empathy) whose results could not be checked against more
ordinary public observation. Such a methodology would be a harbor
for metaphysics. On the natural scientific side, he was concerned
to point out that, for Duhemian and other reasons, the situation is
much messier than is sometimes supposed, and so invidious comparisons
by natural scientists at the expense of social science were
unwarranted.
Second, because Neurath was socially and politically engaged he was
concerned that the various sciences be connected in such a way that they
could be used together to solve complex human and social
problems. For this, considerable overlap of vocabulary was
needed, and this he called a “universal jargon”.
In recent years it is sometimes claimed that Neurath meant by the unity
of science what some contemporary philosophers have defended as the
disunity of science. One cannot rule this claim out a
priori. But the often substantial differences among the current
defenses of disunity make evaluating this claim difficult. It is
fair to say, however, that Neurath was suspicious of grand hypotheses,
familiar since the 19th century to derive all of chemistry,
biology, psychology, and the social sciences (in that order) from a few
basic principles of physics. It is unclear whether this stems
from a general opposition to system building, since he was eager to
develop inferential connections among the various sciences.
Perhaps this is better expressed as an opposition to
speculative system building and to the idea that there is only
one way of systematizing our science than to systematicity as such.
Carnap’s position on unity is different from Neurath’s,
but they overlap. Carnap distinguished the unity of the language
of science from the unity of the laws of science. He wanted to
defend the former and to say what would be required for the
latter. As far as the unity of the language of science, Carnap
did in the Aufbau try to initiate a program for defining all
of scientific concepts on the basis of a very small number of basic
concepts, perhaps only one basic concept. That does afford a
certain conceptual economy, but it is now generally held by Carnap
scholars (see especially Friedman 1987 and Richardson 1998) that
ontological reduction and reduction to a phenomenalist basis was far
from his motive. Carnap explicitly acknowledged that another
system of definitions, one with a physicalist basis, might also be
possible. Instead of ontological economy and a phenomenal basis,
Carnap’s project seems to have been the more Kantian one of
indicating how semantic intersubjectivity is possible: How can it be
that, even though I have only my own experiences and you have only
yours, we can nevertheless share a common body of concepts? The
answer is given in terms of shared inferential structure and
identifying any given concept with a unique place within that shared
overall structure. This is a highly holistic conception of
concepts and it depends on thinking of the body of scientific
commitments as a whole, as a unity.
The Aufbau was largely drafted before Carnap joined the
Vienna Circle. Once there and under some influence from Neurath, Carnap
campaigned more insistently for physicalism and for the unity of
science. They seemed often to be two sides of the same coin. From 1933
onward there was a succession of monograph series with ‘Unified
Science’ in the title. Until his death in 1945, Neurath was in
each case the main editor and Carnap either the associate editor or
one of the associate editors. The International Encyclopedia of
Unified Science, begun in 1938 is undoubtedly the most famous of
these. Carnap’s own essay on this topic “Logical Foundations of
the Unity of Science” (1938) was printed as part of the very
first number in the encyclopedia.
The dates here are relevant because by the time of this essay Carnap
had already decided (Carnap 1936–37) that theoretical terms
could not in general be given explicit definitions in the observation
language even though the observation reports were already in a
physicalist vocabulary. The partially defined theoretical terms could
not be eliminated. This seems to have caused Carnap no consternation
at all, and it never seems to have occurred to him that there was any
conflict whatever between this result and the unity of science.  This
is because by this point the elimination of concepts was not the point
of the exercise; their inferential and evidential integration was.
In the 1936–37 article, “Testability and Meaning” Carnap
called the partial definitions themselves “reduction
sentences” and the system of definitions of theoretical terms,
both partial and complete, as a reduction of the theoretical terms to
the observational basis. Plainly he means by the word
‘reduction’ something other than what we currently mean,
not that there is anything univocal about current uses of the
word. By ‘reduction’ of vocabulary
A to vocabulary B Carnap means the specification of the
inferential relations that would allow us to say what sentences or
combinations of sentences in A would count as evidence for
sentences in B.
This is also the key to what Carnap means by the unity of the
language of science. The language of science is unified, no
matter how different and exotic its various technical vocabularies may
be, when each of its terms is reduced to (can be tested in) a common
public observation vocabulary. The call for the unity of the
language of science, then, amounts to no more than the demand that the
various claims of the separate sciences should be publicly testable
in a common observation language. Controversies will of course
arise as to what the observational vocabulary should be and what are
the acceptable forms of linkage. Carnap’s demand for unity
in the language of science abstracts from those controversies to
concentrate on the goal of public testability. That does not seem
to be an unreasonable demand.
The unity of the language of science so far discussed is quite a
different issue from the unity of the laws of science. And Carnap’s
attitudes toward them are quite different. The latter issue concerns
the extent to which the laws of one special science can be inferred
from those of another. Carnap tries to articulate what would be
involved in such a unification, but he nowhere says that such a unity
is either possible or mandatory. Finding any sort of inferential
connections among sets of laws would be welcome of course. But the
question of how much unity there is, if any, among the various
sciences is an empirical question that philosophers are ill equipped
to answer. Philosophers should not make pronouncements, especially in
advance of having putative laws in hand, either that scientific laws
are unified or that they are not. A certain modest deference to the
empirical facts that philosophers generally do not have, again, does
not seem unreasonable.
Taking unity as a working hypothesis, as some philosophers have done, amounts
to looking for inferential and nomological connections among
various sets of laws, but not to the assertion that such connection will be
found. Even if we accept the idea that such connections would be
welcome if found, the question of whether one should spend significant
effort in looking for them is not thereby answered. That would be
a difficult and delicate practical question of how to apportion one’s
research effort that for the purposes of this essay we must set
aside.
There are two broad approaches to probability represented in logical
empiricism. One of these, the so-called frequentist approach,
has an extensive 19th century history and was
further developed from about 1920 onward by Richard von Mises and Hans
Reichenbach. The other is the epistemic approach to
probability. This goes back at least to Laplace at the end of the
18th century. In the 20th century Rudolf
Carnap, who explored what he called logical probability, and Frank
Ramsey and Richard Jeffrey whose accounts can be distinguished from
Carnap’s and are often called subjective probability, all
defended the epistemic approach. While Ramsey visited the
Vienna Circle he was not much influenced by its members on these
matters. By contrast, Jeffrey studied and later collaborated with
Carnap but also made significant contributions of his own.
It is natural to begin thinking about probabilities with a simple
mathematical account that takes as its point of departure various games
of chance involving cards, dice, or coins. Bettors have long
noted that some outcomes are much more likely than others. In
this context it is convenient to take the probability of a kind of
outcome to be the ratio of such outcomes to all possible
outcomes. Usually for reasons of symmetry in the physical set up,
the possible outcomes are assumed to be equally likely. Where
that assumption happens to be true or nearly so the empirical results
of, say, a great many throws of a pair of dice tends to be close to
what the simple mathematical account would suggest. Conversely,
where the outcomes deviate from the expected ratios, bettors begin to
suspect that the dice, coins, and cards (or the manipulations of them)
are not all that they seem. The suspicion is that the outcomes
are not equally likely and that the simple mathematical account does
not apply.
These facts suggest both two limitations of the simple account and
the beginnings of a way around them. The first limitation is that
the account applies only where the outcomes can be partitioned into
alternatives that are equally likely. This is not the case when
dice are loaded or in such real world cases as radioactive decay or
weather forecasting. A second limitation is that the account, in
describing the possible outcomes as equally likely, implicitly appeals
to the very probability notion for which clarification was
sought. The realization that we can sometimes discover the
falsehood of the assumption of equal likelihood and make a much more
reasonable estimate of probability by making a large number of trials
is very suggestive. And from his dissertation onward Reichenbach
worked out a variety of imagined physical models that could guide ones
thinking about probability in useful ways. The result is what is
often called the frequency theory of probability (or sometimes the
statistical frequency theory or the limit frequency theory).
Even a perfectly fair coin in an odd number of flips will never
result in exactly the same number of heads and of tails. When the
coin is fair and the number of flips is even, an outcome perfectly
balanced between heads and tails is not guaranteed either. So,
even on the assumption that the probability of the coin’s coming
up heads does not change over the course of the trials, we need to be
cautious. A larger number of flips might make us more confident
that the ratio we have seen is close to the “actual” value,
but there is no finite number of flips after which we can say that the
observed ratio is exactly right. We will never make an infinite
number of flips either, and in actual cases a large finite number of
flips might so erode the coin as to bias the coin and discredit the
result. Notwithstanding these limitations on an actual series of
trials one can imagine an infinite series of trials and define a notion
of probability with respect to it. This raises its own
difficulty, namely that ratios are not defined for infinite
collections. They would be defined, however, for any finite
initial segment of such an infinite series, thus giving a sequence of
ratios. If this sequence of ratios settles down on a limit, the
probability of the coin showing a head given that it has been flipped
can be defined as the limit of the ratio of heads to total flips as the
number of flips goes to infinity.
While probability thus defined has a somewhat counterfactual
character, that is not an obvious defect. Moreover, this notion
of probability applies perfectly well to biased coins and loaded dice,
as well as to radioactive decay. On the surface at least it also
seem to avoid using the notion of probability in its own definition,
and in these respects it seems to be an important improvement over the
simple mathematical model with which we began. The definition
locates the probability objectively “out in nature” so to
speak, and this comports well with Reichenbach’s scientific
realism.
A problem that remained troublesome concerns the fact that one often
wants to assign probabilities to particular events, events that in the
nature of things cannot be repeated in all their particularity.
Thus it is unclear how a frequency theory of probability is to be
applied to such individual cases. This is often called the
problem of the single case. It is a little difficult to assess
how serious this is, because in actual practice we often have no
difficulty in making probability assignments to single cases.
Suppose we are interested in the probability of rain tomorrow.
Tomorrow will never be repeated, and we want to estimate the
probability now. What we do is to look back through the records
to find days relevantly like today and determine in what fraction of
those cases those days were followed by rainy days and use that as our
estimate. Even if we are comfortable with this practice, however,
it is another matter to say why this should give us a reasonable
estimate of the value of the limit involved in a logically impossible
infinite sequence. This problem of the single case was much
discussed, and Wesley Salmon made progress in dealing with it.
Indeed, Salmon’s account of statistical explanation can be viewed
as a substantial mitigation of the problem of the single case (W.
Salmon 1970).
There are residual difficulties in making estimates of the
probabilities on the basis of finite evidence. The problem is
that even when we are assured that the sequence of ratios has a limit,
we have no a priori grounds for saying how close the current ratio is
to that limit. We can boldly estimate the limit by means of the
so-called “straight rule”. This just takes the most
recent ratio as the desired estimate. This is a good practical
solution where the number of trials is already high, but this does not
really say why the estimate should be good, how good it is supposed to
be, or how many trials would be high enough. In addition, the
straight rule can yield counterintuitive results where the number of
trials is small.
Though there are these issues outstanding, frequency theories
define a concept of probability indispensable for quantum theories and
for a wide variety of other applications in the natural and social
sciences. It was not the only concept of probability to be
developed by the logical empiricist tradition. The primary other
such concept was the epistemic conception of probability. We will
begin with Carnap and then move to those who developed a subjectivist
account.
Carnap is addressing a different issue than was addressed by von
Mises and Reichenbach. Instead of focusing on physical phenomena
and ratios within them, Carnap focuses on arguments and takes as his
point of departure the widespread conviction that some arguments are
stronger, in varying degrees, than others, even for the same
conclusion. Similarly some bodies of evidence can give us more
reason to believe a given conclusion than would another body of
evidence. Carnap sets as his task the development of a
quantitative concept of probability that will clarify and explicate
these widespread convictions. Such a quantitative concept would
be an extraordinarily useful tool, and it would be a useful successor
to our ordinary, somewhat scattered notions of confirmation and
induction.
Carnap approaches the problem by first considering extremely limited
artificial languages and trying to find a confirmation function that
will work for that. If he succeeds he would then try to develop
an account that would work for a broader and richer range of
languages. In this his approach is like that of a physicist
developing a physical theory for the highly artificial situation of a
billiard table or air track and then broadening the theory to deal with
a wider range of cases. In Carnap’s case, however, it is
somewhat unclear what success would be in an artificial language very
much unlike our own. In any case, Carnap is not trying to describe
our linguistic habits but to clarify or even to replace them with
something more useful.
As early as Logical Syntax (Carnap 1934/1937,
244/316–17) Carnap had suggested that Wittgenstein’s remarks in
the Tractatus about ranges (Tractatus, 4.463) might
be a starting point for thinking about probability. By 1945
Carnap also distinguished the two approaches described here, insisting
that they were not competitors but were attempting to explicate two
different concepts of probability. One need not choose one as the
only concept; both concepts were useful. Reichenbach, by
contrast, never conceded that both concepts were needed and insisted
that his frequency notion could serve all epistemic purposes for which
any notion of probability is needed.
Carnap’s general strategy was first to identify a broad class
of confirmation functions, as subjectivists Ramsay and de Finetti were
also to do, and then find a natural way of limiting this class still
further. The confirmation functions have to meet some basic
mathematical conditions. The axioms that state these conditions
partially define a function, and this function can be interpreted in a
number of ways. Carnap himself lists three in Carnap
1950. In (1955), John Kemeny (one of Carnap’s
collaborators and later a co-inventor of BASIC programming language and
still later president of Dartmouth College) gave an argument that
persuaded Carnap that it was more fruitful to think of the function as
indicating fair betting quotients rather than evidential support.
This took Carnap even closer in conception to the work of such
subjectivists as Ramsey and de Finetti. Indeed, the discussion of
fair betting quotients, and related issues of Dutch book arguments had
been initiated by de Finetti.
In Logical Foundations of Probability (1950) Carnap had
discussed Bayes’ theorem and promised to expand the discussion in a
second volume. Carnap’s interest in Baysianism grew,
but that second volume never materialized, quite possibly because rapid
development of the field was still under way at the time of
Carnap’s death. As his work proceeded Carnap tended to
explain probabilities by reference to events and propositions rather
than speak overtly about sentences. A similar change appears in
the rest of Carnap’s work as well. It is not clear,
however, whether this amounts to a major change of view or a change in
what he sees as the most felicitous mode of expression. As the
years progressed Carnap tended to see the remaining differences between
himself and his subjectivist co-workers as chiefly differences in
emphasis. In any case the subjectivist tradition is now dominant
in philosophical discussions of probability (Zabell 2007, 293).
Richard Jeffrey whose own work arose out of logical empiricism carried
on that tradition for 35 years after Carnap’s death.
Jeffrey himself made major contributions including a principle for
updating ones beliefs when the evidence one learns is not
certain. The world knows this principle as “Jeffrey
conditionalization”; he called it simply “probability
kinematics”.
Popper’s view of probability, his propensity theory, differs from
either of the two approaches discussed above.  Unlike the epistemic
approach of Carnap and others, Popper was not trying to
clarify inductive relations because he did not believe that there are
inductive inferences.  Theories can be corroborated by their passing
severe tests, but they are not thereby inductively confirmed or made
more probable. For a discussion of whether there are any significant
similarities between Popper’s idea of corroboration and the ideas of
inductive confirmation that he rejects, see (Salmon 1967, 1968).
Propensities are thought of as tendencies of a physical event or state
to produce another event or state.  Because propensities are to be
features of external events and not, to use Hume’s phrase, relations
of ideas, the propensity theory and the statistical-frequency theory
are sometimes grouped together as accounts of chance.  Popper has
specifically applied propensities to single non-repeatable events
(1957), and that suggests that the concept of propensity does not
involve any essential reference to long sequences of events.  Popper
has also taken propensities as producing outcomes with a certain limit
frequency (1959).  This does suggest a rather closer tie to the
statistical frequency approach.  Later philosophers developed both
sorts of propensity theories, single-case theories and long-run
theories. (Gillies 2000) And like other approaches to probability and
induction all these views remain controversial.  While we will not
discuss the relative merits of the various approaches further, those
who are interested in Popper’s views in this area should look at the
many papers on probability, induction, confirmation, and
corroboration, and Popper’s replies, in The Philosophy of
Karl Popper (Schilpp 1974).
In 1967 John Passmore reported that: “Logical positivism,
then, is dead, or as dead as a philosophical movement ever
becomes.” (1967, 57) Earlier in the same article he had
equated logical positivism with logical empiricism, so presumably that
was dead too. At that time few would have disagreed with
Passmore, even though Carnap was still alive and active. But in
speaking of this movement Passmore was referring not to a movement but
to specific doctrines, and his interpretation of them was much
influenced by Ayer. Even so, Passmore conceded that the movement
had left a legacy and that “the spirit which inspired the Vienna
circle” persisted. It still does.
Part of the movement’s legacy lies in contemporary philosophy
of science. In the US nearly all philosophers of science can
trace their academic lineages to Reichenbach. Most were either
his students or students of his students and so on. His
scientific realism inspired a generation of philosophers, even those
clearly outside the movement. Even the reaction against various
forms of realism that have appeared in recent decades have roots in the
logical empiricist movement. Moreover, philosophers of science
are expected to know a great deal of the science about which they
philosophize and to be cautious in telling practicing scientists what
concepts they may or may not use. In these respects and others
contemporary philosophers promote a kind of naturalism, and by so doing
they follow both the precept and the example of the logical
empiricists.
There are other issues where the legacy of logical empiricism is
still visible. Two different approaches to probability are still
under discussion. One of them explores the objective chances of
external events; this investigation follows in the tradition of the
frequency theory of Reichenbach and von Mises. The second
approach has an epistemic conception of probability as exemplified by
Carnap. S.L. Zabell summarizes the current situation as
follows:
There is also a continuing concern for how the various sciences fit
together. Some have scouted theoretical unification and others a
more pluralistic model, just as the logical empiricists did.
There was for a while a vogue for the disunity of science. Some
even said that their conception of the disunity of science is just what
Neurath meant by the unity of science. Parts of the discussion
were intended as challenges to logical empiricism, but often the
arguments used were pioneered by the logical empiricists
themselves.
For the 30 years after Passmore’s report metaphysics became ever more
visible in philosophy. It was a diverse development, but in the
self-conceptions of many of its most prominent practitioners there was
no attempt to shun science or logic or to think that metaphysics had
access to facts that were deeper than or beyond those that a proper
science could reach. So the metaphysics that blossomed was not
necessarily of the sort that Carnap and others combated. Most recently
there are some in meta-ontology that want to reconsider and reconnect
with Carnap’s ontological caution.
Even in its heyday many philosophers who on either doctrinal or
sociological grounds can be grouped with the logical empiricists did
not see themselves that way. We should not expect philosophers today
to identify with the movement either. Each generation finds its place
by emphasizing its differences from what has gone before. But the
spirit of the movement still has its adherents. There are many who
value clarity and who want to understand the methodology of science,
its structure, and its prospects. There are many who want to find a
natural home within a broad conception of science for conceptual
innovation, for logic and mathematics, and for their own study of
methodology. And importantly there are those who see in science a
prospect for intellectual and social reform and who see in their own
study of science some hope for freeing us all from the merely habitual
ways of thinking “by which we are now possessed” (Kuhn
1962, 1).  These are the motives that define the movement called
logical empiricism. As Twain might have said, the reports of its death
are greatly exaggerated.