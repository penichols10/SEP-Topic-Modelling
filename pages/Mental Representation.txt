The Representational Theory of Mind (RTM) (which goes back at least to
Aristotle) takes as its starting point commonsense mental states, such
as thoughts, beliefs, desires, perceptions and imagings. Such states
are said to have “intentionality” – they are
about or refer to things, and may be evaluated with
respect to properties like consistency, truth, appropriateness and
accuracy. (For example, the thought that cousins are not related is
inconsistent, the belief that Elvis is dead is true, the desire to eat
the moon is inappropriate, a visual experience of a ripe strawberry as
red is accurate, an imaging of George Washington with dreadlocks is
inaccurate.)
RTM defines such intentional mental states as relations to mental
representations, and explains the intentionality of the former in
terms of the semantic properties of the latter. For example, to
believe that Elvis is dead is to be appropriately related to a mental
representation whose propositional content is that Elvis is
dead. (The desire that Elvis be dead, the fear
that he is dead, the regret that he is dead, etc., involve
different relations to the same mental representation.) To perceive a
strawberry is, on the representational view, to have a sensory
experience of some kind which is appropriately related to (e.g.,
caused by) the strawberry.
RTM also understands mental processes such as thinking, reasoning and
imagining as sequences of intentional mental states. For example, to
imagine the moon rising over a mountain is, inter alia, to
entertain a series of mental images of the moon (and a mountain). To
infer a proposition q from the propositions p and
if p then q is (inter alia) to have a sequence of
thoughts of the form p, if p then q, q.
Contemporary philosophers of mind have typically supposed (or at least
hoped) that the mind can be naturalized –
i.e., that all mental facts have explanations in the terms of natural
science. This assumption is shared within cognitive science, which
attempts to provide accounts of mental states and processes in terms
(ultimately) of features of the brain and central nervous system. In
the course of doing so, the various sub-disciplines of cognitive
science (including cognitive and computational psychology and
cognitive and computational neuroscience) postulate a number of
different kinds of structures and processes, many of which are not
directly implicated by mental states and processes as commonsensically
conceived. There remains, however, a shared commitment to the idea
that mental states and processes are to be explained in terms of
mental representations.
In philosophy, recent debates about mental representation have
centered around the existence of propositional attitudes (beliefs,
desires, etc.) and the determination of their contents (how they come
to be about what they are about), and the existence of phenomenal
properties and their relation to the content of thought and perceptual
experience. Within cognitive science itself, the philosophically
relevant debates have been focused on the computational architecture
of the brain and central nervous system, and the compatibility of
scientific and commonsense accounts of mentality.
Intentional Realists such as Dretske (e.g., 1988) and Fodor
(e.g., 1987) note that the generalizations we apply in everyday life
in predicting and explaining each other’s behavior (often
collectively referred to as “folk psychology”) are both
remarkably successful and indispensable. What a person believes,
doubts, desires, fears, etc. is a highly reliable indicator of what
that person will do; and we have no other way of making sense of each
other’s behavior than by ascribing such states and applying the
relevant generalizations. We are thus committed to the basic truth of
commonsense psychology and, hence, to the existence of the states its
generalizations refer to. (Some realists, such as Fodor, also hold
that commonsense psychology will be vindicated by cognitive science,
given that propositional attitudes can be construed as computational
relations to mental representations.)
Intentional Eliminativists, such as Churchland, (perhaps)
Dennett and (at one time) Stich argue that no such things as
propositional attitudes (and their constituent representational
states) are implicated by the successful explanation and prediction of
our mental lives and behavior. Churchland (1981) denies that the
generalizations of commonsense propositional-attitude psychology are
true. He argues that folk psychology is a theory of the mind with a
long history of failure and decline, and that it resists incorporation
into the framework of modern scientific theories (including cognitive
psychology). As such, it is comparable to alchemy and phlogiston
theory, and ought to suffer a comparable fate. Commonsense psychology
is false, and the states (and representations) it postulates
simply don’t exist. (It should be noted that Churchland is not
an eliminativist about mental representation tout court. See,
e.g., Churchland 1989.)
Dennett (1987a) grants that the generalizations of commonsense
psychology are true and indispensable, but denies that this is
sufficient reason to believe in the entities they appear to refer to.
He argues that to give an intentional explanation of a system’s
behavior is merely to adopt the “intentional stance”
toward it. If the strategy of assigning contentful states to a system
and predicting and explaining its behavior (on the assumption that it
is rational – i.e., that it behaves as it should, given
the propositional attitudes it should have, given its environment) is
successful, then the system is intentional, and the
propositional-attitude generalizations we apply to it are true. But
there is nothing more to having a propositional attitude than this.
(See Dennett 1987a: 29.)
Though he has been taken to be thus claiming that intentional
explanations should be construed instrumentally, Dennett (1991)
insists that he is a “moderate” realist about
propositional attitudes, since he believes that the patterns in the
behavior and behavioral dispositions of a system on the basis of which
we (truly) attribute intentional states to it are objectively real. In
the event that there are two or more explanatorily adequate but
substantially different systems of intentional ascriptions to an
individual, however, Dennett claims there is no fact of the matter
about what the individual believes (1987b, 1991). This does suggest an
irrealism at least with respect to the sorts of things Fodor and
Dretske take beliefs to be; though it is not the view that there is
simply nothing in the world that makes intentional
explanations true.
(Davidson 1973, 1974 and Lewis 1974 also defend the view that what it
is to have a propositional attitude is just to be interpretable in a
particular way. It is, however, not entirely clear whether they intend
their views to imply irrealism about propositional attitudes.)
Stich (1983) argues that cognitive psychology does not (or, in any
case, should not) taxonomize mental states by their semantic
properties at all, since attribution of psychological states by
content is sensitive to factors that render it problematic in the
context of a scientific psychology. Cognitive psychology seeks causal
explanations of behavior and cognition, and the causal powers of a
mental state are determined by its intrinsic “structural”
or “syntactic” properties. The semantic properties of a
mental state, however, are determined by its extrinsic properties
– e.g., its history, environmental or intramental relations.
Hence, such properties cannot figure in causal-scientific explanations
of behavior. (Fodor 1994 and Dretske 1988 are realist attempts to come
to grips with some of these problems.) Stich proposes a
syntactic theory of the mind, on which the semantic
properties of mental states play no explanatory role. (Stich
has since changed his views on a number of these issues. See Stich
1996.)
It is a traditional assumption among realists about mental
representations that representational states come in two basic
varieties (cf. Boghossian 1995). There are those, such as thoughts,
that are composed of concepts and have no phenomenal
(“what-it’s-like”) features (“qualia”),
and those, such as sensations, which have phenomenal features but no
conceptual constituents. (Nonconceptual content is usually defined as
a kind of content that states of a creature lacking concepts might
nonetheless
 have.[1])
 On this taxonomy, mental states can represent either in a way
analogous to expressions of natural languages or in a way analogous to
drawings, paintings, maps, photographs or movies. Perceptual states
such as seeing that something is blue, are sometimes thought
of as hybrid states, consisting of, for example, a non-conceptual
sensory experience and a belief, or some more integrated compound of
conceptual and non-conceptual elements. (There is an extensive
literature on the representational content of perceptual experience.
See the entry on the
 contents of perception.)
Disagreement over non-conceptual representation concerns the existence
and nature of phenomenal properties, the role they play in determining
the contents of sensory representations, and which kinds of properties
can be represented by non-conceptual states. Dennett (1988), for
example, denies that there are such things as qualia at all (as they
are standardly construed); while Brandom (2002), McDowell (1994), Rey
(1991) and Sellars (1956) deny that they are needed to explain the
content of sensory experience. Among those who accept that experiences
have phenomenal content, some (Dretske, Lycan, Tye) argue that it is
reducible to a kind of intentional content, while others (Block, Loar,
Peacocke) argue that it is irreducible. (See the discussion in the
next section.) A further debate concerns the non-conceptual
representability of high-level properties such as kind properties and
moral properties. (See, e.g., Dretske 1995 and Siegel 2010, and the
entry on the contents of perception.) 
Some historical discussions of the representational properties of mind
(e.g., Aristotle De Anima, Locke 1689/1975, Hume 1739/1978)
seem to assume that nonconceptual representations – percepts
(“impressions”), images (“ideas”) and the like
– are the only (or at least the main) kinds of mental
representations, and that the mind represents the world in virtue of
being in states that resemble things in it. On such a view,
all representational states have their content in virtue of their
sensory phenomenal features. Powerful arguments, however, focusing on
the lack of generality (Berkeley Principles of Human
Knowledge), ambiguity (Wittgenstein 1953) and
non-compositionality (Fodor 1981d) of sensory and imagistic
representations, as well as their unsuitability to function as logical
(Frege 1918/1997, Geach 1957) or mathematical (Frege 1884/1953)
concepts, and the symmetry of resemblance (Goodman 1976), convinced
philosophers that no theory of mind can get by with only nonconceptual
representations construed in this way. (For more discussion, see the
entry on
 nonconceputal mental content.)
There has also been dissent from the traditional claim that conceptual
representations (thoughts, beliefs) lack phenomenology. Chalmers
(1996), Flanagan (1992), Goldman (1993), Horgan and Tienson (2002),
Jackendoff (1987), Levine (1993, 1995, 2001), McGinn (1991a), Pitt
(2004, 2009, 2011, 2013), Searle (1992), Siewert (1998, 2011) and
Strawson (1994, 2010), claim that purely conceptual (conscious)
representational states themselves have a proprietary kind of
phenomenology. This view – bread and butter, it should be said,
among historical and contemporary Phenomenologists – has been
gaining momentum of late among analytic philosophers of mind. (See,
e.g., the essays in Bayne and Montague 2011 and Kriegel 2013, and
Chudnoff 2015, Farkas 2008a, Kriegel 2011, Mendelovici 2018, Montague
2016.) If this claim is correct, the question of what role
phenomenology plays in the determination of representational content
re-arises for conceptual representation; and the eliminativist
ambitions of Sellars, Brandom, Rey, et al. would meet a new obstacle.
It would also raise prima facie problems for reductive
representationalism, as well as for reductive naturalistic theories of
intentional content, and externalism in general.
The view that there is a proprietary phenomenology of conscious
thought – a cognitive (conceptual,
propositional) phenomenology – claims that there is
something it’s like to occurrently, consciously think a thought
(entertain a propositional content), which is as different from other
kinds of phenomenology (visual, auditory, etc.) as they are from each
other. Opinions diverge, however, with respect to the role such
phenomenology plays in determining the contents of
conceptual/propositional representations. Some (e.g., Siewert) claim
that it plays no such role. Others (e.g., Horgan and Tienson,
Strawson) hold that it determines only “narrow” contents,
further, “broad” contents being determined by extrinsic
relations to represented objects and properties. Still others (e.g.,
Farkas 2008b, Pitt) argue that it is the only kind of
conceptual content, insisting on a sharp distinction between content
(sense) and reference. There is also disagreement about whether or not
cognitive phenomenology determines but is distinct from
conceptual/propositional content (e.g., Pitt 2004) or is identical to
it (e.g., Pitt 2009). 
Outstanding challenges for this thesis include unconscious thought
(which seems to entail the existence of unconscious phenomenology, on
this view), indexical concepts (whose content is standardly taken to
be referentially individuated; see Pitt 2013 for an attempt to address
this challenge), and nominal concepts (concepts expressed by
utterances of names, likewise standardly referentially
individuated).
See the entries on
 consciousness and intentionality
 and
 phenomenal intentionality
 for further discussion.
Among realists about non-conceptual representations, the central
division is between representationalists (also called
“representationists” and “intentionalists”)
– e.g., Dretske (1995), Harman (1990), Leeds (1993), Lycan
(1987, 1996), Rey (1991), Thau (2002), Tye (1995, 2000, 2009) –
and phenomenalists (also called “phenomenists”)
– e.g., Block (1996, 2003), Chalmers (1996, 2004), Evans (1982),
Loar (2003a, 2003b), Peacocke (1983, 1989, 1992, 2001), Raffman
(1995), Shoemaker (1990). Representationalists claim that the
phenomenal content of a non-conceptual representation – i.e.,
its phenomenal character – is reducible to a kind of intentional
content, naturalistically construed (à la Dretske). On this
view, phenomenal contents are extrinsic properties represented by
non-conceptual representations. In contrast, phenomenalists claim that
the phenomenal content of a non-conceptual mental representation is
identical to its intrinsic phenomenal properties.
The representationalist thesis is often formulated as the claim that
phenomenal properties are representational or intentional. However,
this formulation is ambiguous between a reductive and a non-reductive
claim (though the term ‘representationalism’ is most often
used for the reductive claim. See Chalmers 2004a). As a reductive
claim, it means that the phenomenal content of an experience, the
properties that characterize what it is like to have it (i.e.,
qualia), are certain extrinsic properties it represents. For
example, the blueness one might mention in describing one’s
experience (perceptual representation) of a clear sky at noon is a
property of the sky, not of one’s experience of it. Blueness is
relevant to the characterization of one’s experience because
one’s experience represents it, not because one’s
experience instantiates it. An experience of the sky no more
instantiates blueness than a thought that snow is cold instantiates
coldness. On this view, the phenomenal content of sensory experience
is explained as its representation of extrinsic properties. (See Byrne
and Tye 2006, Dretske 1995, Harman 1990, Lycan 1987, 1996 and Tye
2014, 2015 for elaboration and defense of this “qualia
externalism.” See Thompson 2008 and Pitt 2017 for objections to
this account.) (See also the entry on
 representational theories of consciousness.)
As a non-reductive claim, it means that the phenomenal content of an
experience is its intrinsic subjective phenomenal properties, which
are themselves representational. One’s experience of the sky
represents its color by instantiating phenomenal blueness. Among
phenomenalists there is disagreement over whether non-conceptual
representation requires complex structuring of phenomenal properties
(Block and Peacocke, op. cit., Robinson 1994) or not (Loar 2003b).
So-called “Ganzfeld” experiences, in which, for example,
the visual field is completely taken up with a uniform experience of a
single color, are a standard test case: Do Ganzfeld experiences
represent anything? (It may be that doubts about the
representationality of such experiences are simply a consequence of
the fact that (outside of the laboratory) we never encounter things
that would produce them. Supposing we routinely did (and especially if
we had names for them), it seems unlikely such skepticism would
arise.)
Most (reductive) representationalists are motivated by the conviction
that one or another naturalistic explanation of intentionality (see
the next section) is, in broad outline, correct, and by the desire to
complete the naturalization of the mental by applying such theories to
the problem of phenomenality. (Needless to say, many phenomenalists
are just as eager to naturalize the phenomenal – though not in
the same way.)
The main argument for representationalism appeals to the
transparency of experience (cf. Tye 2000: 45–51). The
properties that characterize what it’s like to have a sensory
experience are presented in experience as properties of objects
perceived: in attempting to attend to an experience, one seems to
“see through it” to the objects and properties it is
experiences
 of.[2]
 They are not presented as properties of the experience itself. If
nonetheless they were properties of the experience,
perception would be massively deceptive. But perception is not
massively deceptive. In veridical perception, these properties are
locally instantiated; in illusion and hallucination, they are not. On
this view, introspection is indirect perception: one comes to know
what phenomenal features one’s experience has by coming to know
what objective features it represents. (Cf. also Dretske 1996,
1999.)
In order to account for the intuitive differences between conceptual
and sensory representations, representationalists appeal to structural
or functional properties. Dretske (1995), for example, distinguishes
experiences and thoughts on the basis of the origin and nature of
their functions: an experience of a property P is a state of
a system whose evolved function is to indicate the presence
of P in the environment; a thought representing the property
P, on the other hand, is a state of a system whose
assigned (learned) function is to calibrate the output of the
experiential system. Rey (1991) takes both thoughts and experiences to
be relations to sentences in the language of thought, and
distinguishes them on the basis of (the functional roles of) such
sentences’ constituent predicates. Lycan (1987, 1996)
distinguishes them in terms of their functional-computational
profiles. Tye (2000) distinguishes them in terms of their functional
roles and the intrinsic structure of their vehicles: thoughts are
representations in a language-like medium, whereas experiences are
image-like representations consisting of “symbol-filled
arrays.” (Cf. the account of mental images in Tye 1991.)
Phenomenalists tend to make use of the same sorts of features
(function, intrinsic structure) in explaining some of the intuitive
differences between thoughts and experiences; but they do not suppose
that such features exhaust the differences between phenomenal and
non-phenomenal representations. For the phenomenalist, it is the
phenomenal properties of experiences – qualia themselves –
that constitute the fundamental difference between experience and
thought. Peacocke (1992), for example, develops the notion of a
perceptual “scenario” (an assignment of phenomenal
properties to coordinates of a three-dimensional egocentric space),
whose content is “correct” (a semantic property) if in the
corresponding “scene” (the portion of the external world
represented by the scenario) properties are distributed as their
phenomenal analogues are in the scenario.
Another sort of representation appealed to by some phenomenalists
(e.g., Chalmers (2003), Block (2003)) is what Chalmers calls a
“pure phenomenal concept.” A phenomenal concept in general
is a concept whose denotation is a phenomenal property, and it may be
discursive (‘the color of ripe bananas’), demonstrative
(‘this color’; Loar 1990/96)), or even more
direct. On Chalmers’s view, a pure phenomenal concept
is (something like) a conceptual/phenomenal hybrid consisting of a
phenomenological “sample” (an image or an occurrent
sensation) integrated with (or functioning as) a conceptual component
(see also Balog 1999 and Papineau 2002). Phenomenal concepts are
postulated to account for the apparent fact (among others) that, as
McGinn (1991b) puts it, “you cannot form [introspective]
concepts of conscious properties unless you yourself instantiate those
properties.” One cannot have a phenomenal concept of a
phenomenal property P, and, hence, phenomenal
beliefs about P, without having experience of
P, because P itself is (in some way)
constitutive of the concept of P. (Cf. Jackson 1982,
1986 and Nagel 1974.) (The so-called “ phenomenal concept
strategy” puts pure phenomenal concepts to use in defending the
Knowledge Argument against physicalism. See Loar 1990/96, Chalmers
2004a. Alter and Walter 2007 is an excellent collection of essays on
phenomenal concepts. See Conee 1994 and Pitt 2019 for skeptical
responses to this strategy.)
Though imagery has played an important role in the history of
philosophy of mind, the important contemporary literature on it is
primarily psychological. (Tye 1991 and McGinn 2004 are notable recent
exceptions.) In a series of psychological experiments done in the
1970s (summarized in Kosslyn 1980 and Shepard and Cooper 1982),
subjects’ response time in tasks involving mental manipulation
and examination of presented figures was found to vary in proportion
to the spatial properties (size, orientation, etc.) of the figures
presented. The question of how these experimental results are to be
explained kindled a lively debate on the nature of imagery and
imagination.
Kosslyn (1980) claims that the results suggest that the tasks were
accomplished via the examination and manipulation of mental
representations that themselves have spatial properties – i.e.,
pictorial representations, or images. Others,
principally Pylyshyn (1979, 1981a, 1981b, 2003), argue that the
empirical facts can be explained in terms exclusively of
discursive, or propositional representations and
cognitive processes defined over them. (Pylyshyn takes such
representations to be sentences in a language of thought.)
The idea that pictorial representations are literally
pictures in the head is not taken seriously by proponents of
the pictorial view of imagery (see, e.g., Kosslyn and Pomerantz 1977).
The claim is, rather, that mental images represent in a way that is
relevantly like the way pictures represent. (Attention has
been focused on visual imagery – hence the designation
‘pictorial’; though of course there may be imagery in
other modalities – auditory, olfactory, etc. – as well.
See O’Callaghan 2007 for discussion of auditory imagery.)
The distinction between pictorial and discursive representation can be
characterized in terms of the distinction between analog and
digital representation (Goodman 1976). This distinction has
itself been variously understood (Fodor & Pylyshyn 1981, Goodman
1976, Haugeland 1981, Lewis 1971, McGinn 1989), though a widely
accepted construal is that analog representation is continuous (i.e.,
in virtue of continuously variable properties of the representation),
while digital representation is discrete (i.e., in virtue of
properties a representation either has or doesn’t have) (Dretske
1981). (An analog/digital distinction may also be made with respect to
cognitive processes. (Block 1983.)) On this understanding of
the analog/digital distinction, imagistic representations, which
represent in virtue of properties that may vary continuously (such as
being more or less bright, loud, vivid, etc.), would be analog, while
conceptual representations, whose properties do not vary continuously
(a thought cannot be more or less about Elvis: either it is or it is
not) would be digital.
It might be supposed that the pictorial/discursive distinction is best
made in terms of the phenomenal/non-phenomenal distinction, but it is
not obvious that this is the case. For one thing, there may be
non-phenomenal properties of representations that vary continuously.
Moreover, there are ways of understanding pictorial representation
that presuppose neither phenomenality nor analogicity. According to
Kosslyn (1980, 1982, 1983), a mental representation is
“quasi-pictorial” when every part of the representation
corresponds to a part of the object represented, and relative
distances between parts of the object represented are preserved among
the parts of the representation. But distances between parts of a
representation can be defined functionally rather than spatially
– for example, in terms of the number of discrete computational
steps required to combine stored information about them. (Cf. Rey
1981.)
Tye (1991) proposes a view of images on which they are hybrid
representations, consisting both of pictorial and discursive elements.
On Tye’s account, images are “(labeled) interpreted
symbol-filled arrays.” The symbols represent discursively, while
their arrangement in arrays has representational significance (the
location of each “cell” in the array represents a specific
viewer-centered 2-D location on the surface of the imagined
object).
See the entry on
 mental imagery
 for further discussion.
The contents of mental representations are typically taken to be
abstract objects (properties, relations, propositions, sets, etc.). A
pressing question, especially for the naturalist, is how mental
representations come to have their contents. Here the issue is not how
to naturalize content (abstract objects can’t be
naturalized), but, rather, how to specify naturalistic
content-determining relations between mental representations
and the abstract objects they express. There are two basic types of
contemporary naturalistic theories of content-determination and
causal-informational and
 functional.[3]
Causal-informational theories (Dretske 1981, 1988, 1995) hold that the
content of a mental representation is grounded in the information it
carries about what does (Devitt 1996) or would
(Fodor 1987, 1990a) cause it to
 occur.[4]
 There is, however, widespread agreement that causal-informational
relations are not sufficient to determine the content of mental
representations. Such relations are common, but representation is not.
Tree trunks, smoke, thermostats and ringing telephones carry
information about what they are causally related to, but they do not
represent (in the relevant sense) what they carry information about. A
mental representation can be caused by something it does not
represent, and can represent something that has not caused it, whereas
nothing can be caused by something that doesn’t cause it.
The main attempts to specify what makes a causal-informational state a
mental representation are Asymmetric Dependency Theories
(e.g., Fodor 1987, 1990a, 1994) and Teleological Theories
(Dretske 1988, 1995, Fodor 1990b, Millikan 1984, Neander 2017,
Papineau 1987). The Asymmetric Dependency Theory distinguishes merely
informational relations from representational relations on the basis
of their higher-order relations to each other: informational relations
depend upon representational relations, but not vice versa. For
example, if tokens of a mental state type are reliably caused by
horses, cows-on-dark-nights, zebras-in-the-mist and Great Danes, then
they carry information about horses, etc. If, however, such tokens are
caused by cows-on-dark-nights, etc. because they were caused
by horses, but not vice versa, then they represent horses (or the
property horse).
According to Teleological Theories, representational relations are
those a representation-producing mechanism has the selected
(by evolution or learning) function of establishing. For
example, zebra-caused horse-representations do not mean
zebra, because the mechanism by which such tokens are
produced has the selected function of indicating horses, not zebras.
The horse-representation-producing mechanism that responds to zebras
is malfunctioning.
See the entries on
  teleological theories of mental content
 and
  causal theories of mental content.
Functional theories (Block 1986, Harman 1973), hold that the content
of a mental representation is determined, at least in part, by its
(causal, computational, inferential) relations to other mental
representations. They differ on whether relata should include all
other mental representations or only some of them, and on whether to
include external states of affairs. The view that the content of a
mental representation is determined by its inferential/computational
relations with all other representations is holism;
the view it is determined by relations to only some other
mental states is localism (or molecularism). (The
non-functional view that the content of a mental state depends on
none of its relations to other mental states is
atomism.) Functional theories that recognize no
content-determining external relata have been called
solipsistic (Harman 1987). Some theorists posit distinct
roles for internal and external connections, the former determining
semantic properties analogous to sense, the latter determining
semantic properties analogous to reference (McGinn 1982, Sterelny
1989).
(Reductive) representationalists (Dretske, Lycan, Tye) usually take
one or another of these theories to provide an explanation of the
(non-conceptual) content of experiential states. They thus tend to be
externalists (see the next section) about phenomenological as well as
conceptual content. Phenomenalists and non-reductive
representationalists (Block, Chalmers, Loar, Peacocke, Siewert), on
the other hand, take it that the representational content of such
states is (at least in part) determined by their intrinsic phenomenal
properties. Further, those who advocate a phenomenally-based approach
to conceptual content (Horgan and Tienson, Kriegel, Loar,
Pitt, Searle, Siewert) also seem to be committed to internalist
individuation of the content (if not the reference) of such
states.
Persistent indeterminacy problems with
causal-informational-teleological theories of content determination
have motivated a growing number of (analytic) philosophers to seek a
different approach, grounded not in external relations of
representational states but in their intrinsic phenomenal properties.
This approach has come to be known as the “Phenomenal
Intentionality Research Program” (Kriegel 2013), or, simply
“Phenomenal Intentionality.” These philosophers (including
Bourget, Kriegel, Loar, Mendelovici, Montague, Pitt, Searle, Smithies
(2012, 2013a and b, 2019), Strawson and Siewert), argue that
causal-informational-teleological relations cannot yield the
fine-grained, determinate content conceptual and perceptual
representations possess, and that such content can only be delivered
by phenomenal character. The cognitive phenomenology thesis (discussed
above) is an important component of this overall approach. 
Generally, those who, like informational theorists, think relations to
one’s (natural or social) environment are (at least partially)
determinative of the content of mental representations are
externalists, or anti-individualists (e.g., Burge
1979, 1986b, 2010, McGinn 1977), whereas those who, like some
proponents of functional theories, think representational content is
determined by an individual’s intrinsic properties alone, are
internalists (or individualists; cf. Putnam 1975,
Fodor
 1981c).[5]
This issue is widely taken to be of central importance, since
psychological explanation, whether commonsense or scientific, is
supposed to be both causal and content-based. (Beliefs and desires
cause the behaviors they do because they have the contents they do.
For example, the desire that one have a beer and the beliefs
that there is beer in the refrigerator and that the
refrigerator is in the kitchen may explain one’s getting up
and going to the kitchen.) If, however, a mental
representation’s having a particular content is due to factors
extrinsic to it, it is unclear how its having that content
could determine its causal powers, which, arguably, must be intrinsic
(see Stich 1983, Fodor 1982, 1987, 1994). Some who accept the standard
arguments for externalism have argued that internal factors determine
a component of the content of a mental representation. They
say that mental representations have both “narrow” content
(determined by intrinsic factors) and “wide” or
“broad” content (determined by narrow content plus
extrinsic factors). (This distinction may be applied to the
sub-personal representations of cognitive science as well as to those
of commonsense psychology. See von Eckardt 1993: 189.)
Narrow content has been variously construed. Putnam (1975), Fodor
(1982: 114; 1994: 39ff), and Block (1986: 627ff), for example, seem to
understand it as something like de dicto content (i.e.,
Fregean sense, or perhaps character, à la
Kaplan 1989). On this construal, narrow content is context-independent
and directly expressible. Fodor (1987) and Block (1986), however, have
also characterized narrow content as radically inexpressible.
On this construal, narrow content is a kind of proto-content, or
content-determinant, and can be specified only indirectly, via
specifications of context/wide-content pairings. On both construals,
narrow contents are characterized as functions from context to (wide)
content. The narrow content of a representation is determined by
properties intrinsic to it or its possessor, such as its syntactic
structure or its intramental computational or inferential role.
Burge (1986b) has argued that causation-based worries about
externalist individuation of psychological content, and the
introduction of the narrow notion, are misguided. Fodor (1994, 1998)
has more recently urged that a scientific psychology might not need
narrow content in order to supply naturalistic (causal) explanations
of human cognition and action, since the sorts of cases they were
introduced to handle, viz., Twin-Earth cases and Frege cases, are
either nomologically impossible or dismissible as exceptions to
non-strict psychological laws.
On the most common versions of externalism, though intentional
contents are externally determined, mental representations themselves,
and the states they partly constitute, remain “in the
head.” More radical versions are possible. One might maintain
that since thoughts are individuated by their contents, and some
thought contents are partially constituted by objects external to the
mind, then some thoughts are partly constituted by objects external to
the mind. On such a view, a singular thought – i.e., a
thought about a particular object – literally contains
the object it is about. It is “object-involving.” Such a
thought (and the mind that thinks it) thus extend beyond the
boundaries of the skull. (This appears to be the view articulated in
McDowell 1986, on which there is “interpenetration”
between the mind and the world.)
See the entries on
 externalism about mental content
 and
 narrow mental content.
Clark and Chalmers (1998) and Clark (2001, 2005, 2008) have argued
that mental representations may exist entirely “outside
the head.” On their view, which they call “active
externalism,” cognitive processes (e.g., calculation) may be
realized in external media (e.g., a calculator or pen and paper), and
the “coupled system” of the individual mind and the
external workspace ought to count as a cognitive system – a mind
–in its own right. Symbolic representations on external media
would thus count as mental representations.
Clark and Chalmers’s paper has inspired a burgeoning literature
on extended, embodied and interactive cognition. (Menary 2010 is a
recent collection of essays. See also the entry on
 embodied cognition.)
The leading contemporary version of the Representational Theory of
Mind, the Computational Theory of Mind (CTM), claims that the brain is
a kind of computer and that mental processes are computations.
According to CTM, cognitive states are constituted by computational
relations to mental representations of various kinds, and cognitive
processes are sequences of such states.
CTM develops RTM by attempting to explain all psychological
states and processes in terms of mental representation. In the course
of constructing detailed empirical theories of human and other animal
cognition, and developing models of cognitive processes implementable
in artificial information processing systems, cognitive scientists
have proposed a variety of types of mental representations. While some
of these may be suited to be mental relata of commonsense
psychological states, some – so-called “subpersonal”
or “sub-doxastic” representations – are not. Though
many philosophers believe that CTM can provide the best scientific
explanations of cognition and behavior, there is disagreement over
whether such explanations will vindicate the commonsense psychological
explanations of prescientific RTM.
According to Stich’s (1983) Syntactic Theory of Mind, for
example, computational theories of psychological states should concern
themselves only with the formal properties of the objects
those states are relations to. Commitment to the explanatory relevance
of content, however, is for most cognitive scientists
fundamental (Fodor 1981a, Pylyshyn 1984, Von Eckardt 1993). That
mental processes are computations, that computations are rule-governed
sequences of semantically evaluable objects, and that the
rules apply to the symbols in virtue of their content, are central
tenets of mainstream cognitive science.
Explanations in cognitive science appeal to a many different kinds of
mental representation, including, for example, the “mental
models” of Johnson-Laird 1983, the “retinal arrays,”
“primal sketches” and “2½-D sketches”
of Marr 1982, the “frames” of Minsky 1974, the
“sub-symbolic” structures of Smolensky 1989, the
“quasi-pictures” of Kosslyn 1980, and the
“interpreted symbol-filled arrays” of Tye 1991 – in
addition to representations that may be appropriate to the explanation
of commonsense psychological states. Computational explanations have
been offered of, among other mental phenomena, belief (Fodor 1975,
2008 Field 1978), visual perception (Marr 1982, Osherson, et al.
1990), rationality (Newell and Simon 1972, Fodor 1975, Johnson-Laird
and Wason 1977), language learning and use (Chomsky 1965, Pinker
1989), and musical comprehension (Lerdahl and Jackendoff 1983).
A fundamental disagreement among proponents of CTM concerns the
realization of personal-level representations (e.g., thoughts) and
processes (e.g., inferences) in the brain. The central debate here is
between proponents of Classical Architectures and proponents
of Connectionist Architectures.
The classicists (e.g., Turing 1950, Fodor 1975, 2000, 2003, 2008,
Fodor and Pylyshyn 1988, Marr 1982, Newell and Simon 1976) hold that
mental representations are symbolic structures, which
typically have semantically evaluable constituents, and that mental
processes are rule-governed manipulations of them that are sensitive
to their constituent structure. The connectionists (e.g., McCulloch
& Pitts 1943, Rumelhart 1989, Rumelhart and McClelland 1986,
Smolensky 1988) hold that mental representations are realized by
patterns of activation in a network of simple processors
(“nodes”) and that mental processes consist of the
spreading activation of such patterns. The nodes themselves are,
typically, not taken to be semantically evaluable; nor do the patterns
have semantically evaluable constituents. (Though there are versions
of Connectionism – “localist” versions – on
which individual nodes are taken to have semantic properties (e.g.,
Ballard 1986.) It is arguable, however,
that localist theories are neither definitive nor representative of
the connectionist program (Smolensky 1988, 1991, Chalmers 1993).)
Classicists are motivated (in part) by properties thought seems to
share with language. Fodor’s Language of Thought Hypothesis
(LOTH) (Fodor 1975, 1987, 2008), according to which the system of
mental symbols constituting the neural basis of thought is structured
like a language, provides a well-worked-out version of the classical
approach as applied to commonsense psychology. (Cf. also Marr 1982 for
an application of classical approach in scientific psychology.)
According to the LOTH, the potential infinity of complex
representational mental states is generated from a finite stock of
primitive representational states, in accordance with recursive
formation rules. This combinatorial structure accounts for the
properties of productivity and systematicity of the
system of mental representations. As in the case of symbolic
languages, including natural languages (though Fodor does not suppose
either that the LOTH explains only linguistic capacities or that only
verbal creatures have this sort of cognitive architecture), these
properties of thought are explained by appeal to the content of the
representational units and their combinability into contentful
complexes. That is, the semantics of both language and thought is
compositional: the content of a complex representation is
determined by the contents of its constituents and their structural
configuration. (See, e.g.,Fodor and Lepore 2002.) 
Connectionists are motivated mainly by a consideration of the
architecture of the brain, which apparently consists of layered
networks of interconnected neurons. They argue that this sort of
architecture is unsuited to carrying out classical serial
computations. For one thing, processing in the brain is typically
massively parallel. In addition, the elements whose manipulation
drives computation in connectionist networks (principally, the
connections between nodes) are neither semantically compositional nor
semantically evaluable, as they are on the classical approach. This
contrast with classical computationalism is often characterized by
saying that representation is, with respect to computation,
distributed as opposed to local: representation is
local if it is computationally basic; and distributed if it is not.
(Another way of putting this is to say that for classicists mental
representations are computationally atomic, whereas for
connectionists they are not.)
Moreover, connectionists argue that information processing as it
occurs in connectionist networks more closely resembles some features
of actual human cognitive functioning. For example, whereas on the
classical view learning involves something like hypothesis formation
and testing (Fodor 1981c), on the connectionist model it is a matter
of evolving distribution of “weights” (strengths) on the
connections between nodes, and typically does not involve the
formulation of hypotheses regarding the identity conditions for the
objects of knowledge. The connectionist network is “trained
up” by repeated exposure to the objects it is to learn to
distinguish; and, though networks typically require many more
exposures to the objects than do humans, this seems to model at least
one feature of this type of human learning quite well. (Cf. the sonar
example in Churchland 1989.)
Further, degradation in the performance of such networks in response
to damage is gradual, not sudden as in the case of a classical
information processor, and hence more accurately models the loss of
human cognitive function as it typically occurs in response to brain
damage. It is also sometimes claimed that connectionist systems show
the kind of flexibility in response to novel situations typical of
human cognition – situations in which classical systems are
relatively “brittle” or “fragile.”
Some philosophers have maintained that connectionism entails that
there are no propositional attitudes. Ramsey, Stich and Garon (1990)
have argued that if connectionist models of cognition are basically
correct, then there are no discrete representational states as
conceived in ordinary commonsense psychology and classical cognitive
science. Others, however (e.g., Smolensky 1989), hold that certain
types of higher-level patterns of activity in a neural network may be
roughly identified with the representational states of commonsense
psychology. Still others (e.g., Fodor & Pylyshyn 1988, Heil 1991,
Horgan and Tienson 1996) argue that language-of-thought style
representation is both necessary in general and realizable within
connectionist architectures. (MacDonald & MacDonald 1995 collects
the central contemporary papers in the classicist/connectionist
debate, and provides useful introductory material as well. See also
Von Eckardt 2005.)
Whereas Stich (1983) accepts that mental processes are computational,
but denies that computations are sequences of mental representations,
others accept the notion of mental representation, but deny that CTM
provides the correct account of mental states and processes.
Van Gelder (1995) denies that psychological processes are
computational. He argues that cognitive systems are dynamic,
and that cognitive states are not relations to mental symbols, but
quantifiable states of a complex system consisting of (in the case of
human beings) a nervous system, a body and the environment in which
they are embedded. Cognitive processes are not rule-governed sequences
of discrete symbolic states, but continuous, evolving total states of
dynamic systems determined by continuous, simultaneous and mutually
determining states of the systems’ components. Representation in
a dynamic system is essentially information-theoretic, though the
bearers of information are not symbols, but state variables or
parameters. (See also Port and Van Gelder 1995; Clark 1997a, 1997b,
2008.)
Horst (1996), on the other hand, argues that though computational
models may be useful in scientific psychology, they are of no help in
achieving a philosophical understanding of the intentionality of
commonsense mental states. CTM attempts to reduce the
intentionality of such states to the intentionality of the mental
symbols they are relations to. But, Horst claims, the relevant notion
of symbolic content is essentially bound up with the notions of
convention and intention. So CTM involves itself in a vicious
circularity: the very properties that are supposed to be reduced are
(tacitly) appealed to in the reduction.
See the entries on the
 computational theory of mind
 and
 computational theory of mind.
 
To say that a mental object has semantic properties is,
paradigmatically, to say that it is about, or true or false
of, an object or objects, or that it is true or false
simpliciter. Suppose I think that democracy is dying. I am
thinking about democracy, and if what I think of it (that it is dying)
is true of it, then my thought is true. According to RTM such states
are to be explained as relations between agents and mental
representations. To think that democracy is dying is to token in some
way a mental representation whose content is that democracy is dying.
On this view, the semantic properties of mental states are the
semantic properties of the representations they are relations to.
Linguistic acts seem to share such properties with mental states.
Suppose I say that democracy is dying. I am talking about
democracy, and if what I say of it (that it is dying) is true of it,
then my utterance is true. Now, to say that democracy is dying is (in
part) to utter a sentence that means that democracy is dying. Many
philosophers have thought that the semantic properties of linguistic
expressions are inherited from the intentional mental states they are
conventionally used to express (Grice 1957, Fodor 1978,
Schiffer1972/1988, Searle 1983). On this view, the semantic properties
of linguistic expressions are the semantic properties of the
representations that are the mental relata of the states they are
conventionally used to express. Fodor has famously argued that these
states themselves have a language-like structure. (See the entry on
the
  language of thought hypothesis.)
 
(Others, however, e.g., Davidson (1975, 1982) have suggested that the
kind of thought human beings are capable of is not possible without
language, so that the dependency might be reversed, or somehow mutual
(see also Sellars 1956). (But see Martin 1987 for a defense of the
claim that thought is possible without language. See also Chisholm and
Sellars 1958.) Schiffer (1987) subsequently despaired of the success
of what he calls “Intention-Based Semantics.”)
It is also widely held that in addition to having such properties as
reference, truth-conditions and truth – so-called
extensional properties – expressions of natural
languages also have intensional properties, in virtue of
expressing properties or propositions – i.e., in virtue of
having meanings or senses, where two expressions may
have the same reference, truth-conditions or truth value, yet express
different properties or propositions (Frege 1892/1997). If the
semantic properties of natural-language expressions are inherited from
the thoughts and concepts they express (or vice versa, or both), then
an analogous distinction may be appropriate for mental
representations.