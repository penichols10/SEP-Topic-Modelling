Versions of atomism developed by seventeenth-century mechanical
philosophers, referred to hereafter as mechanical atomism, were
revivals of Ancient Greek atomism, with the important difference that
they were presumed to apply only to the material world, and not to the
spiritual world of the mind, the soul, angels and so on. Mechanical
atomism was a totally general theory, insofar as it offered an account
of the material world in general as made up of nothing other than
atoms in the void. The atoms themselves were characterised in terms of
just a few basic properties, their shape, size and motion. Atoms were
changeless and ultimate, in the sense that they could not be broken
down into anything smaller and had no inner structure on which their
properties depended. The case made for mechanical atomism was largely
prior to and independent of empirical investigation.
There were plenty of seventeenth-century versions of atomism that were
not mechanical. These tended to be less ambitious in their scope than
mechanical atomism, and properties were attributed to atoms with an
eye to the explanatory role they were to play. For instance, chemicals
were assumed by many to have least parts, natural minima, with those
minima possessing the capability of combining with the minima of other
chemicals to form compounds.
The flexibility and explanatory potential of mechanical atomism was
increased once Newton had made it possible to include forces in the
list of their properties. However, there was no way of specifying
those forces by recourse to general philosophical argument and they
were remote from what could be established empirically also. Newtonian
atomism was not fruitful as far as eighteenth-century experimental
science is concerned.
It was only in the nineteenth century that atomism began to bear
significant fruit in science, with the emergence of atomic chemistry
and the kinetic theory of gases. The way in which and the point at
which atomic speculations were substantiated or were fruitful is
controversial but by the end of the century the fact that the
properties of chemical compounds are due to an atomic structure that
can be represented by a structural formulae was beyond dispute. The
kinetic theory of gases met with impressive empirical success from
1860 until 1885 at least. However, it also faced
difficulties. Further, there was the emergence and success of
phenomenological thermodynamics, which made it possible to deal with a
range of thermal and chemical phenomena without resort to an
underlying structure of matter. Atomism was rejected by leading
scientists and philosophers such as Wilhelm Ostwald, Pierre Duhem and
Ernst Mach up to the end of the nineteenth century and beyond. By that
time atomism had been extended from chemistry and the kinetic theory
to offer explanations in stereochemistry, electro-chemistry,
spectroscopy and so on. Any opposition from scientists that remained
was removed by Jean Perrin's experimental investigations of Brownian
motion. However, the task of explaining chemical properties in terms
of atoms and their structure still remained as a task for twentieth
century science.
Twentieth-century atomism in a sense represents the achievement of the
Ancient Greek ideal insofar as it is a theory of the properties of
matter in general in terms of basic particles, electrons, protons and
neutrons, characterised in terms of a few basic properties. The major
difference is that the nature of the particles and the laws governing
them were arrived at empirically rather than by a priori philosophical
argument.
Suggested Reading: Melson (1952) is a somewhat dated but
still interesting and useful overview of the history of atomism from a
philosophical point of view.Chalmers (2009) is a history of atomism
that focuses on the relationship between philosophical and scientific
theories about atoms.
 Influential versions of Greek atomism were formulated by a range
of philosophers in the seventeenth century, notably Pierre Gassendi
(Clericuzio, 2000, 63–74) and Robert Boyle (Stewart, 1979 and
Newman 2006). Neither the content of nor the mode of argument for
these various versions were identical. Here the focus is on the
version articulated and defended by Robert Boyle. Not only was Boyle
one of the clearest and ablest defenders of the mechanical philosophy
but he was also a leading pioneer of the new experimental science, so
his work proves to be particularly illuminating as far as
distinguishing philosophical and empirical aspects of atomism are
concerned.
The mechanical philosophy differed from the atomism of the Greeks
insofar as it was intended to apply to the material world only and not
to the spiritual world. Apart from that major difference, the
world-views are alike. Fundamentally there is just one kind of matter
characterised by a property that serves to capture the tangibility of
matter and distinguish it from void. Boyle chose absolute
impenetrability as that property. There are insensibly small portions
of matter that, whilst they are divisible in thought or by God, are
indivisible as far as natural processes are concerned. Boyle,
misleadingly drawing on another tradition that will be discussed in a
later section, referred to these particles as minima
naturalia or prima naturalia. Here they are referred to
as atoms, a terminology only very rarely adopted by Boyle
himself. Each atom has an unchanging shape and size and a changeable
degree of motion or rest.  All properties of the material world are
reducible to and arise as a consequence of the arrangements and
motions of the underlying atoms. In particular, properties possessed
by macroscopic objects, both those detectable directly by the senses,
such as colour and taste, and those involved in the interaction of
bodies with each other, such as elasticity and degree of heat, are to
be explained in terms of the properties of atoms. Those properties of
atoms, their shape, size and motion, together with the impenetrability
possessed by them all, are the primary ones in terms of which the
properties of the complex bodies that they compose, the secondary
ones, are to be explained. Such explanations involve the fundamental
laws of nature that govern the motions of atoms.
Not all of the mechanical philosophers were mechanical atomists.
Descartes provides a ready example of a mechanical philosopher who was
not an atomist insofar as he rejected the void and held that particles
of matter could be broken down into smaller particles. The mechanical
philosophers were divided on the question of the existence of the
void, some sharing the opinion of the Greek atomists that void was a
pre-requisite for motion but others, like Descartes, rejecting the
void as unintelligible and hence regarding all motion as involving the
simultaneous displacement of closed loops of matter whether that
matter be continuous or particulate. Arguments at the most general
level for the intelligibility of the void and its relation to the
possibility of motion were inconclusive. In addition to the question
of the void, there is the question of whether matter is particulate
and whether there are indivisible particles called atoms. Once again,
general a priori philosophical arguments were hardly able to settle
the question.
Boyle, along with his fellow mechanical philosophers, argued for his
position on the grounds that it was clear and intelligible compared to
rival systems such as Aristotelianism and those developed in chemical
and related contexts by the likes of Paracelsus. The argument operated
at the level of the fundamental ontology of the rival philosophies.
Boyle insisted that it is perfectly clear what is intended when shape,
size and degree of motion are ascribed to an impenetrable atom and
when arrangements are ascribed to groups of such atoms. That much can
surely be granted. But Boyle went further to insist that it is
unintelligible to ascribe to atoms properties other than these primary
ones, that is, properties other than those that atoms must necessarily
possess by virtue of being portions of matter, such as the forms and
qualities of the Aristotelians or the principles of the
chemists. ‘Nor could I ever find it intelligibly made
out’, wrote Boyle, ‘what these real qualities may be, that
they [the scholastics] deny to be either matter, or modes of matter,
or immaterial substances’ (Stewart, 1979, 22). If an atom is
said to possess elasticity, for example, then Boyle is saying that the
ontological status of whatever it is that is added to matter to render
it elastic is mysterious, given that it cannot be material. This is
not to claim that attributing elasticity and other secondary
properties to gross matter is unintelligible. For such properties can
be rendered intelligible by regarding them as arising from the primary
properties and arrangements of underlying atoms. Secondary properties
can be ascribed to the world derivatively but not primitively. So the
stark ontology of the mechanical philosopher is established a priori
by appealing to a notion of intelligibility.
Explaining complex properties by reducing them to more elementary ones
was not an enterprise unique to the mechanical philosophers. After
all, it was a central Aristotelian thesis that the behaviour of
materials was due to the proportions of the four elements in them,
whilst the elements themselves owed their properties to the
interaction of the hot and the cold and the wet and the dry, the
fundamental active principles in nature. What a mechanical atomist
like Boyle needed, and attempted, to do was establish that they could
provide examples of successful mechanical reductions that were clear
and intelligible. It was to this end that Boyle stressed how the
workings of a key could be explained in terms of nothing other than
its shape and size relative to the lock and the workings of a clock
can be explained by appeal to nothing other than the properties of its
parts.
There is a basic problem with this type of illustration of and support
for the mechanical philosophy. Firstly, whilst the examples may indeed
be examples of successful reductions, they are not strict mechanical
reductions, and they are certainly not reductions to the mechanical
properties of atoms. The functioning of a key depends on its rigidity
whilst that of clocks and watches depend crucially on the weight of
pendulum bobs or the elasticity of springs. On a number of occasions
Boyle himself observed that explanations that appealed to such things
as elasticity, gravity, acidity and the like fall short of the kind of
explanations sought by a mechanical atomist (Chalmers, 1993).
 To attempt to produce examples of reduction that conform to the
ideal of the mechanical atomists is, in effect, to attempt to bolster
the arguments from intelligibility with empirical arguments. The issue
of empirical support for mechanical atomism, or any other version of
atomism, raises a fundamental problem, a problem that Maurice
Mandelbaum (1964, 88–112) has called ‘the problem of
transdiction’. How are we to reach knowledge of unobservable
atoms from knowledge of the bulk matter to which we have observational
and experimental access? Mandelbaum credits Boyle with proposing a
solution to the problem and he is endorsed by Newman (2006). Roughly
speaking, the solution is that knowledge that is confirmed at the
level of observation, that is found to apply to all matter whatsoever,
and is scale invariant can be assumed to apply to atoms also. There is
no doubt that an argument of this kind is to be found in Boyle, but it
is highly problematic and can hardly be regarded as the solution to
the epistemological problems faced by a seventeenth-century
atomist.
There is something to be said for an appeal to scale invariance along
the lines that laws that are shown to hold at the level of observation
in a way that is independent of size should be held to hold generally,
and in particular, on a scale so minute that it is beyond what can be
observed. Boyle draws attention to the fact that the law of fall is
obeyed by objects independently of their size and that the same appeal
to mechanism can be applied alike to explain the workings of a large
town clock and a tiny wristwatch (Stewart, 1979, 143). The question is
to what extent recognition of scale invariance of this kind can aid
the atomist. There is a range of reasons for concluding that it
cannot.
A key problem is that laws established at the level of observation and
experiment involve or imply properties other than the primary ones of
the mechanical atomist. As mentioned above, the mechanisms of clocks
involve the elasticity of springs, the weight of pendulum bobs and the
rigidity of gear wheels and the law of fall presupposes a tendency for
heavy objects to fall ‘downwards’. So the mechanical
atomist cannot apply knowledge of this kind, scale invariant or
otherwise, to atoms that are presumed to lack such properties. If we
are looking for an empirical case for the list of properties that can
be applied to atoms then it would appear that we need some criteria
for picking out that subset of properties possessed by observable
objects that can be applied to atoms also. Boyle offered a solution to
this problem. He suggested that only those properties that occur in
all observable objects whatsoever should be transferred to
atoms. Since all observable objects have some definitive shape and
size then atoms do also. By contrast, whilst some observable objects
are magnetic, many are not, and so atoms are not magnetic. This
strategy does not give an atomist what is needed. All observable
objects are elastic to some degree and are even divisible to some
degree and yet mechanical atoms are denied such
properties. Conversely, no observable macroscopic object is absolutely
impenetrable whereas Boyle assumes that atoms posses precisely that
property. Perhaps it should not be surprising that the mechanical
atomists of the seventeenth century lacked the resources to forge
links between their conjectured atoms and experimental findings.
Many speculations about atoms in the seventeenth century came from a
source quite distinct from mechanical atomism. That source was the
theory of natural minima which had its roots in Aristotle and that was
transformed into a detailed atomic theory mainly applicable to
chemical change.
Aristotle (On Generation and Corruption, Bk 1, Ch 10) clearly
identified what we would refer to as chemical change as a special
category presenting problems peculiar to it. It differs from mere
alteration, such as the browning of an autumn leaf, where an
identifiable material substratum persists, and from generation and
corruption, such as the transformation of an olive seed into a tree or
the decay of a rose into a heap of dust, where no identifiable
material substratum persists. The transformation of a mixture of
copper and tin into bronze, an example of what Aristotle called
combination, is intermediate between alteration and generation and
corruption. Copper and tin do not persist as such in the bronze and to
assume so would fail to make the appropriate distinction between a
combination and a mixture. Nevertheless, there is some important sense
in which the copper and tin are in the bronze because they are
recoverable from it.  Aristotle had put his finger on a central
problem in chemistry, the sense in which elements combine to form
compounds and yet remain in the compounds as components of
them. Aristotle did not use this terminology, of course, and it should
be recognised that he and the scholastics that followed him had few
examples of combination, as opposed to alteration and generation and
corruption, to draw on.  Alloys, which provided them with their stock
and just about only example, are not even compounds from a modern
point of view. The importance of combination for Aristotelians lay in
the philosophical challenge it posed.
Many scholastics came to understand combination as the coming together
of the least parts of the combining substances to form least parts of
the compound. These least parts were referred to as natural
minima. Substances cannot be divided indefinitely, it was claimed,
because division will eventually result in natural minima which are
either indivisible or are such that, if divided, no longer constitute
a portion of the divided substance. But the theory of natural minima
was developed to a stage where it involved more than that. The minima
were presumed to exist as parts of a substance quite independent of
any process of division. What is more, chemical combination was
understood as coming about via the combination of minima of the
combining substances forming minima of the compound. Talk of chemical
combination taking place ‘per minima’ became common.
Natural minima were presumed by the scholastics to owe their being
both to matter and form in standard Aristotelian fashion. A key
problem they struggled with concerned the relation of the form
characteristic of the minima of combining substances and the form of
the minima of the resulting compound. Natural minima of copper and tin
cannot remain as such in the minima of bronze otherwise the properties
of copper and tin would persist in bronze. On the other hand, the form
of copper and tin must persist in some way to account for the fact
that those metals can be recovered. A common scholastic response was
to presume that the forms of the combining minima persist in the
minima of the resulting compound but in a way that is subservient to
the form of those latter minima. Elements persist in the compound
somewhat as individual notes persist in a chord.
Whilst Aristotle and the scholastics can be given the credit for
pinpointing a fundamental problem associated with chemical change they
can hardly be credited with providing a definitive solution. It should
be recognised that adding the assumption of natural minima does not
contribute in any way to a solution to the problem posed by chemical
change. The problem of understanding how components persist in
compounds simply becomes transferred to the problem of how minima of
components persist in minima of compounds. So the extent to which
acceptance of natural minima became widespread cannot be explained in
terms of their contribution to a solution to the fundamental problem
of chemical change. There were a number of motivations for assuming
minima, all having at least their germs in Aristotle. One idea was
that a portion of a substance can resist the corrupting influence of
the surrounding medium only if there is a sufficient amount of
it. Another stemmed from the common recognition that substances must
come into contact if they are to combine. The particulate nature of
substances facilitates such contact, as Aristotle hinted (On
Generation and Corruption, 1, 10, 328a, 34). A third motivation
concerned the logical problems, dating back to Zeno, that were
understood to flow from assuming infinite divisibility.
Recognising the need to avoid problems perceived to be associated with
infinite divisibility was a point shared by proponents of natural
minima and mechanical atomists. But this one point of contact must not
blind us to the crucial differences between the two traditions.
Mechanical atoms were proposed as components of matter in
general. They were unchangeable and possessed a minimum of properties,
shape, size and a degree of motion or rest together with the
impenetrability of their component matter. The motivation for
ascribing just those properties to atoms was to provide an
intelligible account of being and change in general. By contrast,
natural minima possess properties characteristic of the substances of
which they are the minima. The minima are not unchangeable because
they are transformed into more complicated minima via chemical
combination. The minima were not basic building blocks for the
scholastics that developed this theory because their properties needed
to be traced back to their composition from the four Aristotelian
elements. Finally, the minima theory was developed as an attempt to
accommodate chemical change. It was not intended as a theory of
everything in the way that mechanical atomism was.
 Atomic theories became common in the seventeenth century. The
emerging emphasis on experiment led the proponents of those theories
to become less concerned with philosophical systems and more concerned
with the explanation of specific phenomena such as condensation and
rarefaction, evaporation, the strength of materials and chemical
change. There was an increasing tendency for atomists to borrow in an
opportunist way from both the mechanical and natural minima traditions
as well as from the alchemical tradition which employed atomistic
theories of its own as Newman (1991, 143–190 and 1994,
92–114) has documented. Thus an Aristotelian proponent of the
natural minima tradition, Daniel Sennert, whose main interest was in
chemistry in medical contexts, drew on the work of the alchemists as
well as that of the minima theorists, employed minima in physical as
well as chemical contexts, and insisted that his atomism had much in
common with that of Democritus (Clericuzio, 2000, 23–29 and
Melsen, 1952, 81–89). Boyle referred to his mechanical atoms as
natural minima and his first account of atomism involved attributing
to an atom properties distinctive of the substance it was a least part
of (Newman, 2006, 162ff, Clericuzio, 2000, 166ff) and in fact borrowed
heavily from Sennert (Newman, 1996). In subsequent writings he made it
clear that in his view least parts of substances are composed of more
elementary particles possessing only shape, size and a degree of
motion. Whether, according to Boyle, properties other than primary
mechanical ones emerge at the level of least parts or at the
macroscopic level is an issue on which contemporary commentators
disagree (Chalmers, 2009, 155–161), Chalmers, 2010, 8–9,
Clericuzio, 2000, 103–148, Newman, 2006, 179–189). The
theories of a number of atomists, such as Sebastien Basso, Etienne de
Clave and Thomas Digges, were an eclectic mixture of ingredients drawn
from mechanical atomism, minima theory and alchemy. (Clericuzio, 2000,
Melsen, 1952, Newman, 2006)
The seventeenth-century certainly witnessed the growth of a range of
experimental sciences, an occurrence of considerable epistemological
significance. However, the experimental basis for seventeenth-century
atomism remained extremely weak and none of the various versions of it
can be said to have productively informed experiment or to have been
confirmed by it, a claim that has been documented by Meinel (1988) in
his survey of the experimental basis for atomism in the seventeenth
century and is argued in detail in Chalmers (2009). Appeal to atoms to
explain the gradual wearing away of a stone, the evaporation of a
liquid, the passage of a solution through a filter paper folded
multiple times and so on dated back at least as far as Lucretius and
were hardly sufficiently powerful to convince anyone disinclined to
accept the reality of atoms. Experimental knowledge of the combination
and recovery of reacting chemicals, which certainly experienced marked
growth in the course of the seventeenth century, did not of itself
warrant the assumption that atoms were involved. Evidence revealed by
the microscope was new to the seventeenth century, of course, and did
reveal a microscopic world previously unknown. But the properties of
microscopic systems were not qualitatively distinct from macroscopic
ones in a way that aided the demonstration of the emergence of the
properties of observable systems, whether microscopic or macroscopic,
from the properties of atoms.
Suggested Readings: Clericuzio (2000) is a detailed survey of
seventeenth-century atomic theories. Stewart (1979) is a collection of
Boyle's philosophical papers related to his mechanical
atomism. Boyle's atomism is detailed in Newman(2006) and Chalmers
(2009). Debates concerning the nature and status of it are in
Chalmers(1993), Chalmers (2002), Chalmers (2009), Chalmers (2010),
Newman (2006), Newman (2010), Anstey (2002) and Pyle (2002).
The key sources of Newton's stance on atomism in his published work
are Querie 31 of his Opticks, and a short piece on acids
(Cohen, 1958, 257–8). Atomistic views also make their appearance in
the Principia, where Newton claimed “the least parts of
bodies to be—all extended, and hard and impenetrable, and
moveable, and endowed with their proper inertia” (Cajori, 1962,
399). If we temporarily set aside Newton's introduction of his concept
of force, then Newton's basic matter theory can be seen as a version
of mechanical atomism improved by drawing on the mechanics of the
Principia. Whereas mechanical atomists prior to Newton had
been unclear about the nature and status of the laws governing atoms,
Newton was able to presume that his precisely formulated three laws of
motion, shown to apply in a wide variety of astronomical and
terrestrial settings, applied to atoms also. Those laws provided the
law of inertia governing motion of atoms in between collisions and
laws of impact governing collisions. Newton also added his precise and
technical notion of inertia or mass, another fruit of his new
mechanics, to the list of primary properties of atoms. These moves
certainly helped to give precise content to the fundamental tenets of
mechanical atomism that they had previously lacked.
There is no doubt that Newton shared the assumption of the Ancient and
mechanical atomists that there is just one kind of homogeneous matter
of which all atoms are composed. This is clear from the way in which
Newton explained differing densities of observable matter in terms of
the amount of space intervening between the component atoms.  Newton
argued, for instance that the ratio of space to volume occupied by
atoms was seventeen times greater in water than in gold on the grounds
that gold is seventeen times more dense. The fact that thin gold films
transmit light convinced Newton that the atoms of gold already
contains enough space to permit the transmission of light
particles. The preponderance of space between the atoms of matter,
however bulky or solid they might appear at the observational and
experimental level, became a characteristic feature of Newtonian
atomism, as Thackray (1968) has stressed.
The picture of Newton's atomism as an elaboration and improvement of
mechanical atomism becomes untenable once the role of force in
Newton's theorising is taken into account. There is no doubting that
Newton's introduction of forces, especially the gravitational force,
into his mechanics was a major scientific success borne out by
observational and experimental evidence. Newton famously speculated in
the Preface to the Principia (Cajori, 1958, xviii), that if
all forces operative in nature, including those acting between the
smallest, unobservable, particles, were known, then the whole course
of nature could be encompassed within his mechanics.  However, the
fulfilment of such a dream would not constitute the fruition of the
mechanical philosophy because of the ontological problems posed by the
concept of force.
Newton explicitly rejected the idea that gravitation, or any other
force, be essential to matter. But the major point of mechanical
atomism had been to admit as properties of atoms only those that they
must, essentially, have as pieces of matter. It was in this way that
they had endeavoured to avoid introducing Aristotelian forms and
qualities, which they regarded as incomprehensible from an ontological
point of view. The introduction of forces as irreducible entities flew
in the face of the major aim of the mechanical philosophers for
clarity and intelligibility on ontological matters. Newton was unable
to fashion an unambiguous view on the ontological status of gravity, a
force manifest at the level of observation and experiment, let alone
forces operative at the atomic level. It is true that, in the case of
gravity, Newton had a plausible pragmatic response. He argued that,
whatever the underlying status of the force of gravity might be, he
had given a precise specification of that force with his law of
gravitation and had employed the force to explain a range of phenomena
at the astronomical and terrestrial level, explanations that had been
confirmed by observation and experiment. But not even a pragmatic
justification such as this could be offered for forces at the atomic
level.
Mechanical atomism had faced the problem of how to introduce the
appropriate kinds of activity into the world relying solely on the
shapes, sizes and motions of atoms. They had struggled unsuccessfully
to explain elasticity and gravity along such lines and chemistry posed
problems of its own. Newtonian forces could readily be deployed to
remove these problems. Newton presumed that forces of characteristic
strengths (affinities) operated between the least parts of chemicals.
What displaces what in a chemical reaction is to be explained simply
in terms of the relative strengths of the affinities
involved. Elasticity was attributed to attractive and repulsive forces
acting between particles of an elastic substance and so on.
Newton developed theories of optics and chemistry that were atomistic
in the weak sense that they sought to explain optical and chemical
properties by invoking interacting particles lying beyond the range of
observation. However, the particles were not ultimate.  Newton's
position on the least parts of chemical substances was similar to that
of Boyle and other mechanical philosophers. They were regarded as made
up of a hierarchy of yet smaller particles. So long as the smallest
particles were held together by forces, the problem of the ontological
status of the forces remained. The least parts of chemicals in
Newton's theory were akin to natural minima with the added detail that
their action was due to attractive and repulsive forces. As far as the
particles of light in Newton's optics are concerned, whether they were
ultimate or not, they too acted by way of forces and also suffered
fits of easy reflection and easy refraction, the latter being used to
explain interference phenomena such as Newton’s rings and why a
ray incident on a boundary between two refracting media can be
partially reflected and partially transmitted.
However attractive the reduction of the material world to particles
interacting by way of forces may have appeared, it must be recognised
that there was scant empirical support for the idea. This point is
clearest in the context of chemistry. The affinities presumed to act
between chemical ‘atoms’ were postulated solely on the
basis of the observed chemical behaviour of bulk substances
manipulated in the laboratory. The assumption that the chemical
behaviour of bulk substances were due to combining atoms added nothing
that made a difference to what was testable by experiment. Observed
properties of chemical substances were simply projected onto
atoms. Newtonians had not formulated a chemical atomic theory that
could be used as a basis for the prediction of chemical phenomena at
the experimental level.  Newton's optics was in an analogous
situation. However, here it can be said that that optical theory was
able to accommodate a range of optical phenomena in a coherent way
that rendered it superior to any rival. The result was the widespread
acceptance of the theory in the eighteenth century.
When Newton took for granted that there is just one kind of universal
matter and refused to include gravity as a primary property of matter
because of worries about the ontological status of force, he was
playing the role of a natural philosopher in the tradition of the
mechanical philosophy. When he offered a pragmatic justification of
his specification of the force of gravity independently of how that
force might be explained he was acting as one who sought to develop an
experimentally confirmed science independent of the kinds of ultimate
explanation sought by the mechanical philosophers. His atomism
contained elements of both of these tendencies. A sympathiser could
say that whatever the philosophical problems posed by forces,
Newtonian atomism was a speculation that at least held the promise of
explaining material phenomena in a way that mechanical atomism did not
and so experimental support in the future was a possibility. A critic,
on the other hand, could argue that, from the philosophical
perspective, the introduction of force undermined the case for the
clarity and intelligibility of mechanical atomism on which its
originators had based their case. From a scientific point of view,
there was no significant empirical support for atomism and it was
unable to offer useful guidance to the experimental sciences that grew
and prospered in the seventeenth century and beyond.
Force was to prove a productive addition to experimental science in no
uncertain manner in the eighteenth century. Force laws in addition to
the law of gravitation, involving elasticity, surface tension,
electric and magnetic attractions and so on were experimentally
identified and put to productive use. In the domain of science,
scruples about the ontological status of forces were forgotten and
this attitude spread to philosophy. Eighteenth-century updates of
mechanical atomism typically included gravity and other forces amongst
the primary properties of atoms. Acceptance of force as an ontological
primitive is evident in an extreme form in the 1763 reformulation of
Newtonian atomism by R. Boscovich (1966). In his philosophy of matter
atoms became mere points (albeit possessing mass) acting as centres of
force, the forces varying with the distance from the centre and
oscillating between repulsive and attractive several times before
becoming the inverse square law of gravitation at sensible
distances. The various short-range attractive and repulsive forces
were appealed to as explanations of the cohesion of atoms in bulk
materials, chemical combination and also elasticity. Short-range
repulsive forces varying with distance enabled Boscovich to remove the
instantaneous rebounds of atoms that had been identified as an
incoherency in Newton's own atomism stemming from their absolute
hardness and inelasticity.
While most atomists were able to rid themselves of scruples about
accepting forces as ontologically primitive, the issue of the empirical
foundation for the various unobservable forces hypothesised remained.
The best arguments that could be mounted were hypothetical-deductive.
Forces postulated at the atomic level were credited with some empirical
support if they could serve to explain observable phenomena. The form
of such arguments, as well as their inconclusiveness, can be
illustrated by Newton's demonstration in the Principia
(Bk. 2, Prop. 23) that a gas consisting of a static array of atoms
repelling each other with a force inversely proportional to their
separation would obey Boyle's law. The fact that some of these
theories did indeed reproduce the experimentally established facts was
certainly a point in their favour, but hardly served to establish them.
Whewell brought the point home by identifying competing theories of
capillarity, due to Poisson and Laplace, that were equally able to
reproduce the phenomena but which were based on incompatible atomic
force laws, as Gardner (1979, 20) has pointed out.
The problem besetting those seeking experimental support for atomic
theories is most evident in chemistry. Although many eighteenth-century
chemists espoused versions of Newtonian chemistry their chemical
practice owed nothing to it (Thackray, 1970). As philosophers they
payed lip-service to atomism but as experimental chemists they worked
independently of it. As early as 1718 Ettienne Geoffroy spelt out how
the blossoming experimental science of chemical combination, involving
extensive use of mineral acids to form an array of salts, could be
understood in terms of what substances combined with what and could be
recovered from what and to what degree. His table of the degrees of
‘rapport’ of chemical substances for each other summarised
experimental data acquired by manipulating substances in the laboratory
and became an efficient device for ordering chemical experience and for
guiding the search for novel reactions. Klein (1995) has highlighted
this aspect of Geoffroy's work and how his 1718 paper in effect
shows how a large section of the experimental chemistry of the time
could be construed as a practical tradition divorced from a speculative
metaphysics, atomistic or otherwise. Eighteenth-century tables of
affinity, modelled on Geoffroy's version, became increasingly
detailed as the century proceeded. Many of the chemists who employed
them interpreted the affinities featuring in them as representing
attractions between chemical atoms, but such an assumption added
nothing that could not be fully represented in terms of combinations of
chemical substances in the laboratory.
The fact that Newtonian atomism offered little that was of practical
utility to chemistry became increasingly recognised by chemists as the
eighteenth century progressed. The culmination of the experimental
program involving the investigation of the combination and analysis of
chemical substances was, of course, Lavoisier's system involving
chemical elements. But whatever sympathy Lavoisier may have had for
Newtonian atomism of the kind championed by Laplace, he was at pains to
distance his new chemistry from it. Substances provisionally classified
as elements were those that could not be broken down into something
simpler in the laboratory. Progress in eighteenth-century chemistry led
away from rather than towards atomism. It was not until Dalton that the
situation changed early in the nineteenth century.
The assessment that eighteenth-century atomism was ill-confirmed by
experiment and failed to give useful guidance to experimentalists is a
judgement that is fairly insensitive to what theory of confirmation one
adopts or what one might require of an adequate scientific explanation.
This situation was transformed by the emergence of Daltonian atomism, a
strong candidate for the first atomic theory that had a productive link
with experiment.
Suggested Reading: Thackray (1970) is an authoritative and
detailed account of Newton's atomism and its development in the
eighteenth century. The relation between Newton's atomism and his
mechanics is discussed in Chalmers (2009, Chapter 7).
The status of atomism underwent a transformation when John Dalton
formulated his version of chemical atomism early in the nineteenth
century. His atomic theory had implications for the way chemicals
combine by weight and, for the first time, it would seem, a direct
path was uncovered that took scientists from experimental measurement
to a property of atoms, namely, their relative weight. An assessment
of the fruitfulness and epistemological status of Dalton's atomism can
easily be distorted if we are uncritically influenced by the
recognition that Dalton's basic assumptions are in fact correct from a
modern point of view. This section will involve a summary of the basic
features of Dalton's chemistry as he published it in 1808 together
with the way in which its content can be usefully expressed using
chemical formulae introduced by Berzelius five years later. The
following sections will explore, first the issue of the
epistemological status of this early version and then the nature and
status of subsequent elaborations of chemical atomism during the first
half century of its life. These latter issues very much involve
developments in organic chemistry, issues that have been highlighted
by historians of chemistry only in the last few decades.
Dalton was able to take for granted assumptions that had become
central to chemistry since the work of Lavoisier. Chemical compounds
were understood as arising through the combination of chemical
elements, substances that cannot be broken down into something simpler
by chemical means. The weight of each element was understood to be
preserved in chemical reactions. By the time Dalton (1808) made his
first contributions to chemistry the law of constant composition of
compounds could be added to this. Proust had done much to substantiate
experimentally the claim that the relative weights of elements making
up a chemical compound remain constant independent of its mode of
preparation, its temperature and its state.
The key assumption of Dalton's chemical atomism is that chemical
elements are composed of ‘ultimate particles’ or
atoms. The least part of a chemical compound is assumed to be made up
of characteristic combinations of atoms of the component elements.
Dalton called these ‘compound atoms’. According to Dalton,
all atoms of a given substance whether simple or compound, are alike
in shape, weight and any other particular. This much already entails
the law of constant proportions. Although Dalton himself resisted the
move, Berzelius was able to show how Dalton's theory can be
conveniently portrayed by representing the composition of compounds in
terms of elements by chemical formulae in the way that has since
become commonplace. Hereafter this device is employed using modern
conventions rather than any of the various ones used by Berzelius and
his contemporaries,
As Dalton stressed, once the chemical atomic theory is accepted, the
promise is opened up of determining the relative weights of atoms by
measuring the relative weights of elements in compounds. If an atom of
element A combines with an atom of element B to form
a compound atom of compound AB, then the relative weights of
A and B in the compound as measured in the
laboratory will be equal to the relative weights of atoms of
A and B. However, there is a serious
under-determination of relative atomic weights by measurements of
combining weights in the laboratory. If the compound atom in our
example were A2B rather than AB
then the relative atomic weight of B would be twice what it
would be if the formula were AB. Dalton himself attempted to
resolve this problem with a simplicity assumption. Formulae were
always to take the simplest form compatible with the empirical
data. If there was only one compound of A and B
known then it was assumed to be AB, whilst if there were two
then a more complicated compound, A2B or
AB2 became necessary. As is illustrated by the
latter example, as well as the problem of the truth of the simplicity
assumption there was the problem of its ambiguity. Chemical atomists
were to struggle for several decades with various solutions to the
problem of arriving at definitive formulae and relative atomic
weights, as we shall see.
This deficiency of Dalton's atomism aside, links were forged between
it and experimentally determined combining weights that went beyond
the law of constant proportions to include the laws of multiple and
reciprocal proportions. If two elements combine together in more than
one way to form compounds, as is the case with the various oxides of
nitrogen and carbon, for example, then Daltonian atomism predicts that
the weights of one of the elements in each compound, relative to a
fixed weight of the second, will bear simple integral ratios to each
other. This is the law of multiple proportions, predicted by Dalton
and soon confirmed by a range of experiments. Daltonian atomism also
predicts that if the weights of elements A and B
that combine with a fixed weight of element C are x
and y respectively, then if A and B combine
to form a compound then the relative weights of A and
B in the compound will be in the ration x:y
or some simple multiple of it. This law was also confirmed by
experiment.
There is a further component that needs to be added to the content of
early atomic chemistry, although it did not originate with Dalton, who
in fact did not fully embrace it. Gay Lussac discovered experimentally
that when gases combine chemically they do so in volumes that bear an
integral ratio to each other and to the volume of the resulting
compound if gaseous, provided that all volumes are estimated at the
same temperature and pressure. For instance, one volume of oxygen
combines with two volumes of hydrogen to form two volumes of steam. If
one accepts atomism, this implies that there are some whole-number
ratios between the numbers per unit volume of atoms of various gaseous
elements at the same temperature. Following suggestions made by
Avogadro and Ampere early in the second decade of the nineteenth
century, many chemists assumed that equal volumes of gases contain
equal numbers of ‘atoms’, with the important implication
that relative weights of atoms could be established by comparing
vapour densities. As Dalton clearly saw, this can only be maintained
at the expense of admitting that ‘atoms’ can be split. The
measured volumes involved in the formation of water, for example,
entail that, if equal volumes contain equal numbers of atoms then a
water ‘atom’ must contain half of an oxygen
‘atom’. The resolution of these problems required a clear
distinction between atoms of a chemical substance and molecules of a
gas, the grounds for which became available only later in the century.
This problem aside, the empirical fact that gases combine in volumes
that are in simple ratios to each other became a central component of
chemistry, although it should be noted that at the time Gay Lussac
proposed his law, only a small number of gases were known to chemists.
The situation was to change with the development of organic chemistry
in the next few decades.
If Dalton's atomism was viewed as a contribution to natural philosophy
in the tradition of mechanical atomism, designed to give a simple and
intelligible account of the ultimate nature of the material world,
then it did not have a lot going for it. It marked a decisive break
with the idea that there is just one kind of matter, an assumption
that extended from Democritus to Newton and beyond. If Dalton's atoms
were regarded as ontologically basic, then there needed to be as many
kinds of matter as there are chemical elements.  Further, atoms of
each element needed to posses a range of characteristic properties to
account for chemical combination as well as physical aggregation and
other physical properties. As a philosophical theory of the ultimate
nature of material reality, Daltonian atomism was not a serious
contender and was not treated as such. A more significant issue is the
status of Daltonian chemistry as an experimental science. To what
extent was Daltonian chemistry borne out by and able to fruitfully
guide experiment?
A basic issue concerning the empirical statues of Daltonian atomism
was already pinpointed in an early exchange between Dalton (1814) and
Berzelius (1815). Dalton was keen to present himself as the Newton of
chemistry. In his view, just as Newton had explained Kepler’s
laws with his new mechanics, so he, Dalton, had explained the laws of
proportion with his atomism. Without atomism the joint truth of the
three laws of proportion is a mystery. Berzelius questioned the
experimental grounds for assuming anything stronger than the laws of
proportion, since, he argued, all of the chemistry could be
accommodated by the latter. That is, nothing testable by the chemistry
of the time follows from Dalton's atomic theory that does not follow
from the laws of proportion plus the experimental law of combining
volumes for gases.
Berzelius (1814) expressed his version of Daltonian chemistry using
formulae. Dalton had pictured atoms as spheres and compound atoms as
characteristic arrangements of spheres. Berzelius claimed that the two
methods were equivalent but that his method was superior because it
was less hypothetical. It is clear that Berzelius's version cannot be
both less speculative and equivalent to Dalton's theory at the same
time. But it is also clear what Berzelius intended. His point was that
the testable empirical content of the two theories were equivalent as
far as the chemistry of the time was concerned, but that his version
was less speculative because it did not require a commitment to atoms.
The symbols in Berzelian formulae can be interpreted as representing
combing weights or volumes without a commitment to atoms. A Daltonian
atomist will typically take the hydrogen atom as a standard of weight
and the atomic weight of any other element will represent the weight
of an atom of that element relative to the weight of the hydrogen
atom. On such an interpretation the formula H2O represents
two atoms of hydrogen combined with one of oxygen. But, more in
keeping with the weight determinations that are carried out in the
laboratory, it is possible to interpret atomic weights and formulae in
a more empirical way. Any sample of hydrogen whatever can be taken as
the standard, and the atomic weight of a second element will be
determined by the weight of that element which combines with it. The
formula H2O then represents the fact that water contains
two atomic weights of hydrogen for every one of oxygen. Of course,
determining atomic weights and formulae requires some decision to
solve the under-determination problem, but that is the case whether
one commits to atoms or not.
Berzelius was right to point out that as far as being supported by
and serving to guide the chemistry of the time was concerned, his
formulation using formulae served as well as Dalton's formulation
without committing to atomism. What follows from this will depend on
one's stand on confirmation and explanation in science. A
strong-minded empiricist might conclude from Berzelius’s
observation that Dalton's atomism had no place in the chemistry
of the time. Others might agree with Dalton that the mere fact that
Dalton's theory could explain the laws of proportion in a way
that no available rival theory could constituted a legitimate argument
for it in spite of the lack of evidence independent of combining
weights and volumes. Atomism could be defended on the grounds that
attempts to articulate and improve it might well fruitfully guide
experiment in the future and lead to evidence for it that went beyond
combining weights and volumes. But such articulations would clearly
require properties to be ascribed to atoms in addition to their
weight.
Berzelius himself took this latter option. He developed an atomic
theory that attributed the combination of atoms in compounds to
electrostatic attractions. He developed a ‘dualist’ theory
to bring order to compounds involving several types of molecules. For
instance, he represented copper sulphate as (CuO + SO3).
Here electropositive copper combines with electronegative oxygen but in
a way that leaves the combination slightly electropositive, whereas
electropositive sulphur combines with oxygen in a way that leaves the
combination slightly electronegative. The residual charges of the
‘radicals’ as they became known could then account for
their combination to form copper sulphate.
Berzelius's conjectures about the electrical nature of
chemical combination owed their plausibility to the phenomenon of
electrolysis, and especially the laws governing it discovered by
Faraday, which linked the weights of chemicals deposited in
electrolysis to chemical equivalents. But evidence for the details of
his atomistic theory independent of the evidence for the experimental
laws that the theory was designed to support was still lacking.
Contemporaries of Berzelius proposed other atomic theories to explain
electrical properties of matter. Ampère proposed electrical
currents in atoms to explain magnetism and Poisson showed how
electrostatic induction could be explained by assuming atomic dipoles.
In each of these cases some new hypothesis was added to atomism for
which there was no evidence independent of the phenomenon explained.
Nevertheless, the fact that there existed this range of possible
explanations all assuming the existence of atoms can be seen as
constituting evidence for atoms by those favouring inferences to the
best explanation.
In the early decades of the life of Dalton's atomic chemistry various
attempts were made to solve the problem of the under-determination of
atomic weights and formulae. We have already mentioned the appeal to
the equal numbers hypothesis and vapour densities. The fact that
chemists of the time did not have the resources to make this solution
work has been explored in detail by Brooke (1981) and Fisher (1982). A
second method was to employ an empirical rule, proposed by Dulong and
Petit, according to which the product of the specific heats and the
atomic weights of solids is a constant. The problem with this at the
time was, firstly, that some atomic weights needed to be known
independently to establish the truth of the rule, and, secondly, there
were known counter-instances. A third method for determining atomic
weights employed Mitscherlich’s proposal (Rocke, 1984, 154–6)
that substances with similar formulae should have similar crystal
structure. This method had limited application and, again, there were
counter-examples.
Our considerations so far of the status of Daltonian atomism have not
yet taken account of the area in which chemistry was to be making
spectacular progress by the middle of the nineteenth century, namely,
organic chemistry. This is the topic of the next section.
The period from the third to the sixth decades of the nineteenth
century witnessed spectacular advances in the area of organic
chemistry and it is uncontroversial to observe that these advances
were facilitated by the use of chemical formulae. Inorganic chemistry
differs from organic chemistry insofar as the former involves simple
arrangements of a large number of elements whereas organic chemistry
involves complicated arrangements of just a few elements, mainly
carbon, hydrogen, oxygen and to a lesser extent, nitrogen.
It was soon to become apparent that the specification of the
proportions of the elements in an organic compound was not sufficient
to identify it or to give an adequate reflection of its properties.
Progress became possible when the arrangements of the symbols
representing the elements in formulae were deployed to reflect
chemical properties. The historical details of the various ways in
which chemical properties were represented by arrangements of symbols
are complex. (For details see Rocke (1984) and Klein (2003)). Here we
abstract from those details to illustrate the kinds of moves that were
made.
The simplest formula representing the composition of acetic acid is
CH2O using modern atomic weights. This formula cannot
accommodate the fact that, in the laboratory, the hydrogen in acetic
acid can be replaced by chlorine in four distinct ways yielding four
distinct chemical compounds. Three of those compounds are acids that
have properties very similar to acetic acid, and in which the relative
weights of chlorine vary as 1:2:3. The fourth compound has the
properties of a salt rather than an acid. These experimental facts can
be captured in a formula by doubling the numbers and rearranging the
symbols, so that we have C2H4O2,
rearranged to read C2H3O2H. The
experimental facts can now readily be understood in terms of the
substitution of one or more of the hydrogens by chlorine, with the
three chloro-acetic acids represented as
C2H2ClO2,
C2HCl2O2H and
C2Cl3O2H and the salt, acetyl
chloride, as C2H3O2Cl. Such formulae
came to be known as ‘rational formulae’ as distinct from
the ‘empirical formula’ CH2O. Representing the
replacement of one element in a compound by another in the laboratory
by the replacement of one symbol by another in a chemical formula
became a standard and productive device that was to eventually yield
the concept of valency in the 1860s. (Oxygen has a valency of two
because two hydrogens need to be substituted for each oxygen.)
Other devices employed to fashion rational formulae involved the
notion of a radical, a grouping of elements that persisted through a
range of chemical changes so that they play a role in organic
chemistry akin to that of elements in inorganic chemistry. Series of
compounds could be understood in terms of additions, for example to
the methyl radical, CH3, or to the ethyl radical,
C2H5, and so on. ‘Homologous series’
of compounds could be formed by repeatedly adding CH2 to
the formulae for such radicals so that the properties, and indeed the
existence, of complex compounds could be predicted by analogy with
simpler ones. Another productive move involved the increasing
recognition that the action of acids needed to be understood in terms
of the replacement of hydrogen. Polybasic acids were recognised as
producing two or more series of salts depending on whether one, two or
more hydrogens are replaced. Yet another important move involved the
demand that rational formulae capture certain asymmetric compounds,
such as methyl ethyl ether, CH3C2H5O,
as distinct from methyl ether, (CH3)2O, and
ethyl ether, (C2H5)2O. By 1860, the
idea of tetravalent carbon atoms that could combine together in chains
was added. By that stage, the demand that rational formulae reflect a
wide range of chemical properties had resulted in a set of formulae
that was more or less unique. The under-determination problem that had
blocked the way to the establishment of unique formulae and atomic
weights had been solved by chemical means.
The previous section was deliberately written in a way that does not
involve a commitment to atomism. It is possible to understand the
project of adapting rational formulae so that they adequately reflect
chemical properties by interpreting the symbols as representing
combining weights or volumes as Berzelius had already observed in his
early debates with Dalton. Philosophers and historians of science have
responded in a variety of ways to this situation.
Pierre Duhem (2002), in his classic analysis of the logic of
nineteenth-century chemistry at the end of that century, construed it
as being independent of, and offering no support for, atomism. Paul
Needham (2004a, 2004b) has recently supported his case. Klein (2003,
18–20) notes that many of the pioneers of the developments in organic
chemistry referred to combining volumes or portions or proportions
rather than atoms. She attributes the productivity of the use of
formulae to the fact that they ‘conveyed a building-block image
of chemical proportions without simultaneously requiring an investment
in atomic theories, together with the simplicity of their
maneuverability on paper’ (2003, 35).
A number of chemists involved in the early advances of organic
chemistry who did adopt atomism expressed their ontological commitment
to ‘chemical atoms’. In doing so they distinguished their
theories from those brands of physical atomism that were in the
tradition of mechanical or Newtonian atomism and which sought to
explain phenomena in general, and chemistry in particular, by
reference to a few physical properties of atoms. Chemical atoms had
more in common with natural minima insofar as they were presupposed to
have properties characteristic of the substances they were atoms of.
Chemical atomism lent itself to the idea that it was developments in
chemistry that were to indicate which properties were to be attributed
to chemical atoms, as exemplified in the path that led to the property
‘valency’. Alan Rocke (1984, 10–15 and 2013) interprets the use of
formulae in organic chemistry as involving a chemical atomism that is
weaker than physical atomism but stronger than a commitment only to
laws of proportion.
Dalton's atomism had given a line on just one property of atoms, their
relative weight. But it is quite clear that they needed far richer
properties to play there presumed role in chemistry. It was to be
developments in chemistry, and later physics, that were to give
further clues about what properties to ascribe to atoms. (We have seen
how chemists came to ascribe the property of valency to them.) There
was no viable atomistic theory of chemistry in the nineteenth century
that was such that chemical properties could be deduced from it. The
phenomenon of isomerism is often regarded as a success for
atomism. (See Bird, (1998, p. 152) for a recent example.) There are
reasons to doubt this. The fact that there are chemical substances
with the same proportional weights of the elements but with widely
different chemical properties was a chemical discovery. It could not
be predicted by any atomic theory of the nineteenth-century because no
theory contained within its premises a connection between the physical
arrangement of atoms and chemical properties.Isomerism could be
accommodated to atomism but could not, and did not, predict it.
The emergence of unique atomic weights and the structural formulae
that organic chemistry had yielded by the 1860s were to prove vital
ingredients for the case for atomism that could eventually be
made. But there are reasons to be wary of the claim that atomism was
responsible for the rise of organic chemistry and the extent to which
the achievement improved the case for atomism needs to be elaborated
with more caution that is typically the case. Glymour (1980, 226–263)
offers an account of how Dalton's atomism was increasingly confirmed
and relative atomic weights established by 1860 that conforms to his
‘bootstrapping’ account of confirmation, an account that
is adopted and built on by Gardner (1979). These accounts do not take
organic chemistry into account. In one sense, doing so could in fact
help to improve Glymour's account by offering a further element to the
interlocking and mutually supporting hypotheses and pieces of evidence
that are involved in his case. But in another sense, the fact that
organic chemistry led to unique formulae by chemical means casts doubt
on Glymour's focus on the establishment of definitive atomic weights
as the problem for chemistry. There is a case for claiming
that correct atomic weights were the outcome of, rather than a
precondition for, progress in organic chemistry prior to 1860. After
all, the majority of the formulae productively involved in that
dramatic progress were the wrong formulae from a modern point of view!
For instance, use of homologous series to project properties of lower
hydrocarbons on to higher ones are not affected if the number of
carbon atoms in the correct formulae are doubled, which results from
taking 6 as the relative atomic weight of carbon, as many of the
contemporary organic chemists did.
Suggested Readings: Rocke (1984) is a detailed study of the
relevant theories in eighteenth-century chemistry whilst Klein (2003)
is a historical and philosophical analysis of the introduction of
formulae into organic chemistry. The empirical status of atomism in
nineteenth-century chemistry is discussed in Chalmers (2009, Chapters
9 and 10)
The first atomic theory that had empirical support independent of the
phenomena it was designed to explain was the kinetic theory of
gases. This discussion will pass over the historical detail of the
emergence of the theory and consider the mature statistical theory as
developed by Maxwell from 1859 (Niven, (1965, Vol. 1, 377–409, Vol. 2,
26–78) and developed further by Boltzmann (1872).
The theory attributed the behaviour of gases to the motions and
elastic collisions of a large number of molecules. The motions were
considered to be randomly distributed in the gas, while the motion of
each molecule was governed by the laws of mechanics both during and in
between collisions. It was necessary to assume that molecules acted on
each other only during collision, that their volume was small compared
with the total volume of the gas and that the time spent in collision
is small compared to the time that elapses between collisions. While
the molecules needed to be assumed to be small, they needed to be
sufficiently large that they could not move uninterrupted through the
gas. The irregular path of a molecule through the body of a gas from
collision to collision was necessary to explain rates of
diffusion.
The kinetic theory was able to explain the gas laws connecting volume,
temperature and pressure. It also predicted Avogadro’s law that
equal volumes of gases contain equal numbers of molecules and so
explained Gay Lussac's law also. This legitimated the use of vapour
densities for the determination of relative molecular weights.  This
in turn led to definitive atomic weights and formulae that coincided
with those that organic chemistry had yielded by the 1860’s. The
kinetic theory of gases also explained the laws of diffusion and even
predicted a novel phenomena that was quite counter-intuitive, namely,
that the viscosity of a gas, the property that determines its ease of
flow and the ease with which objects flow through it, is independent
of its density. Counter-intuitive or not, the prediction was confirmed
by experiment.
It was known from experiment that the behaviour of gases diverges from
the gas laws as pressure is increased and they approach
liquefaction. The gas laws were presumed to apply to ‘ideal
gases’ as opposed to real gases. The behaviour of real gases
approaches that of ideal gases as their pressure is reduced. The
kinetic theory had an explanation for this distinction, for at high
pressure the assumptions of the kinetic theory, that the volume of
molecules is small compared with the total volume of the gas they form
part of and that the time spent in collision is small compared to the
time between collisions, become increasingly inaccurate. The theory
was able to predict various ways in which a real gas will diverge from
the ideal gas laws at high pressures (Van der Waals equation) and
these were confirmed by experiments on gases approaching
liquefaction.
The kinetic theory of gases explained a range of experimental laws and
successfully predicted new ones. However, there were some key
difficulties. One of them was the departure of experimentally measured
values of the ratio of the two specific heats of a gas, measured at
constant pressure and at constant volume, from what the theory
predicted. This prediction followed from a central tenet of the theory
that energy is distributed equally amongst the degrees of freedom of a
molecule. The difficulty could be mitigated by assuming that molecules
of monatomic gases were perfectly smooth spheres that could not be set
rotating and that diatomic molecules were also smooth to the extent
that they could not be set rotating about the axis joining the two
atoms in the molecule. But, as Maxwell made clear, (Niven, 1965, Vol.
2, 433) it must be possible for molecules to vibrate in a number of
modes in order to give rise to the spectra of radiation that they emit
and absorb, and once this is admitted the predictions of the theory
clash unavoidably with the measured specific heats.
The second major difficulty stemmed from the time reversibility of the
kinetic theory. The time inverse of any process is as allowable as the
original within the kinetic theory. This clashes with the time
asymmetry of the second law of thermodynamics and the
time-directedness of the observed behaviour of gases. Heat flows
spontaneously from hot regions to cold regions and gases in contact
spontaneously mix rather than separate. It is true that defenders of
the kinetic theory such as Maxwell and Boltzmann were able to
accommodate the difficulty by stressing the statistical nature of the
theory and attributing time asymmetries to asymmetries in initial
conditions. But this meant that a fundamental tenet of thermodynamics,
the second law, was in fact only statistically true. Violations were
improbable rather than impossible.  Defenders of the kinetic theory
had no direct experimental evidence for deviations from the second
law.
The kinetic theory explained known experimental laws and predicted new
ones. That empirical success could not be accommodated by some
truncated version of the theory that avoided a commitment to atomism
in the way that use of chemical formulae could for chemistry. Insofar
as the kinetic theory explained anything at all, it did so by
attributing the behaviour of gases to the motions and collisions of
molecules. On the other hand, it did face apparent empirical
refutations as we have seen. Those wishing to assert the truth of the
kinetic theory, and hence of an atomic theory, had a case but also
faced problems.
For those inclined to judge theories by the extent to which they
fruitfully guide experiment and lead to the discovery of experimental
laws, we get a more qualified appraisal. For two decades or more the
mature kinetic theory proved to be a fruitful guide as far as the
explanation and prediction of experimental laws is concerned. But, in
the view of a number of scientists involved at the time, the kinetic
theory had ceased to bear fruit for the remainder of the century, as
Clarke (1976, 88–9) has stressed.
 It might appear that the success of the kinetic theory marked a
successful instantiation of the kind of atomism aspired to by the
mechanical or Newtonian atomists, since macroscopic phenomena are
explained in terms of atoms with just a few specified mechanical
properties. There are reasons to resist such a view. Firstly, neither
the molecules of the kinetic theory nor the atoms composing them were
ultimate particles. As we have noted, it was well appreciated that
they needed an inner structure to accommodate spectra. Secondly, it
was well apparent that the mechanical properties attributed to
molecules by the kinetic theory could not constitute an exhaustive
list of those properties. Further properties were required to explain
cohesion and chemical interaction for instance. Thirdly, and perhaps
most fundamentally, the kinetic theory was not an attempt to give an
atomic account of the ultimate structure of matter. Maxwell, for one,
was quite clear of the distinction between an atomism that made claims
about the ultimate structure of matter for some very general
metaphysical reasons, on the one hand, and a specific scientific
theory postulating atoms on the other (Niven, 1965, Vol. 2,
361–4). The kinetic theory was an example of the latter insofar
as it was proposed, not as an ultimate theory, nor as a theory of
matter in general, but as a theory designed to explain a specified
range of phenomena, in this case the macroscopic behaviour of gases
and, to a less detailed extent, of liquids and gases too. As such, it
was to be judged by the extent it was able to fulfil that task and
rejected or modified to the extent that it could not. A case for the
existence of atoms or molecules and for the properties to be
attributed to them was to be sought in experimental science rather
than philosophy.
During the half-century that followed the emergence of unique chemical
formulae and viable versions of the kinetic theory around 1860 the
content of atomism was clarified and extended and the case for it
improved by the development of atomic explanations of experimental
effects that involved connections between phenomena of a variety of
kinds, the behaviour of gases, the effect of solutes on solutions,
osmotic pressure, crystallography and optical rotation, properties of
thin films, spectra and so on. In several of these cases atomic
explanations were offered of experimental connections for which there
were no available alternative explanations so that the case for
atomism understood as an inference to the best explanation was
strengthened.
Stereo-chemistry emerged as a result of taking the structures depicted
in chemical formulae of substances to be indicative of actual
structures in the molecules of those substances. Pairs of substances
that had crystal structures that were mirror images of each other but
which were otherwise chemically identical were represented by formulae
that were themselves mirror images of each other. Optical rotation
gave independent evidence for the reality of these underlying
structures.  Some chemists were reluctant to assert that the
structures were in fact depictions of the physical arrangements of
atoms in space, a stand supported by the fact that there was still no
theory that connected physical arrangements of atoms with physical and
chemical properties.  There were eminent scientists, notably Ostwald
(1904) and Duhem (2002), who, whilst accepting that the phenomena were
indicative of some underlying structure, refused to make the further
assumption that the formulae with their structures referred to
arrangements of atoms at all. Two factors provide a rationale for
their stance. Firstly, the use of formulae in chemistry could be
accepted without committing to atomism, as we have discussed above,
and as both Ostwald and Duhem stressed. Secondly, an analogy with
electromagnetism indicates that structural features need not be
indicative of underlying physical arrangements accounting for those
structures. The electric field has the symmetry of an arrow and the
magnetic field the symmetry of a spinning disc, but there is no known
underlying physical mechanism that accounts for these
symmetries. Stereo-chemistry may not have provided a case for atomism
that was logically compelling, but it certainly enabled that case to
be strengthened.
Another set of phenomena providing opportunities to develop atomism
involved the effects of solutes on solutions. It was discovered that
effects such as the depression of freezing point and vapour pressure
and the elevation of boiling point of a solvent brought about by
dissolving a non-electrolytic solute in it are proportional to the
weight of dissolved substance and, what is more, that the relative
effects of differing solutes in a given solvent were determined by the
molecular weight of the solute. More specifically, the magnitude of
the various physical effects of a solute was dependent on the number
of gram molecules of the dissolved solute, independent of the chemical
nature of the solute. This provided a way of measuring the molecular
weight of soluble substances that complimented the method involving
the measurement of the vapour pressure of volatile ones. The strong
suggestion that these effects depended on the number of molecules per
unit volume was strengthened when it was discovered that the osmotic
pressure of a solute in a solvent obeys the gas laws. That is, the
osmotic pressure exerted by a solute in a definite volume of solvent,
measurable as the pressure exerted on a membrane permeable to the
solvent but not the solute, was exactly the same as if that same
amount of solute were to fill that same volume as a gas.
While the above could readily be explained by atomism, an anti-atomist
could still accept the experimental correlations by interpreting
molecular weights as those yielded by chemical formulae independently
of an atomic interpretation. Ostwald took that course.  The move
became less plausible once the phenomena were extended to include
solutions of non-electrolytes. For electrolytes, physical phenomena
such as modification of boiling and freezing points and osmotic
pressure could be explained in terms of the concentration of ions
rather than molecules, where the ions were the charged atoms or
complexes of atoms employed by the atomists to explain electrolysis.
This enabled new experimental connections to be forged between, for
example, osmotic pressure, and the conductivity of electrolytes. What
is more, the charges that needed to be attributed to ions to explain
electrolysis were themselves linked to the valencies of the chemists.
The atomic interpretation of electrolysis required a corresponding
atomistic interpretation of electric charge, with each monovalent ion
carrying a single unit of charge, a bi-valent ion carrying two such
units and so on.
Yet another breeding ground for atomism came in the wake of the
electromagnetic theory of light (1865) and the experimental production
of electromagnetic radiation by an electric oscillator (1888).
Helmholtz (1881) observed that optical dispersion could be readily
explained if it were assumed that the transmission of light through a
medium involved the oscillation of particles that were both massive
and charged. The adsorption and emission of spectra characteristic of
atoms also suggested that they were due to the oscillations of charged
particles on the atomic or sub-atomic scale. These assumptions in
conjunction with the kinetic theory of gases led to an explanation of
the width of spectral lines as a Doppler shift due to the velocity of
radiating molecule, making possible estimates of the velocities of
molecules that were in agreement with those deduced from the diffusion
rate of gases.
Strong evidence for the charged and massive particles assumed in an
atomic explanation of electrolysis and radiation was provided by the
experiments on cathode rays performed by J. J. Thomson (1897). The
experimental facts involving cathode rays could be explained on the
assumption that they were beams of charged particles each with the same
value for the ratio of their charge to their mass. Thomson’s
experiments enabled that ratio to be measured. A range of other
experiments in the ensuing few years, especially by Milliken, enabled
the charge on the cathode particles, electrons, to be estimated, and
this led to a mass of the electron very much smaller than that of
atoms. The fact that identical electrons were emitted from cathodes of
a range of materials under a range of conditions strongly suggested
that the electron is a fundamental constituent of all atoms.
As the considerations of the previous section indicate, there is no
doubt that those wishing to make a case for atoms were able to
steadily strengthen their case during the closing decades of the
nineteenth century. However, it is important to put this in perspective
by taking account of spectacular developments in thermodynamics which
were achieved independently of atomism, and which could be, and were,
used to question atomism, branding it as unacceptably
hypothetical.
Phenomenological thermodynamics, based on the law of conservation of
energy and the law ruling out spontaneous decreases in entropy,
supported an experimental programme that could be pursued
independently of any assumptions about a micro-structure of matter
underlying properties that were experimentally measurable. The
programme was developed with impressive success in the second half of
the nineteenth century. Especially relevant for the comparison with
atomism is the extension of thermodynamics, from the late 1870s, to
include chemistry.  Two of the striking accomplishments of the
programme were in areas that had proved a stumbling block for atomism,
namely, thermal dissociation and chemical affinity.
Gibbs (1876–8) developed a theory to account for what, from the point
of view of the atomic theory, had been regarded as
‘anomalous’ vapour densities by regarding them as
consisting of a mixture of vapours of different chemical constitution
in thermal equilibrium. The theory was able to predict relative
densities of the component vapours as a function of temperature in a
way that was supported by experiment. It is true that atomists could
not only accommodate this result by interpreting it in atomic terms
but also welcomed it as a way of removing the problems the phenomena
had caused for the determination of molecular weights from vapour
densities. But it remains the fact that the thermodynamic predictions
are independent of atomic considerations once it is recognised that
the chemical formulae needed for them can be, and were, obtained and
interpreted in a way independent of atomism. As a matter of historical
fact, Deville, the major participant in the experimental confirmation,
was opposed to atomism, as Duhem (2002, 96–7) stressed.
From the time Newton introduced the notion of forces of affinity
acting between atoms and responsible for their chemical behaviour
there had been a problem forging a link between those forces and
experimentally measurable effects. Daltonian atomists simply assumed
that atoms combine in the way required to account for the measurable
proportions of elements in compounds. The theory gave no account of
the relative strengths of chemical bonding or hints of what would
replace what in a chemical reaction. Chemical thermodynamics was able
to make headway with this problem. Considerations based on entropy
changes and heats of reaction made it possible to predict in which
direction a particular chemical reaction will proceed and to provide
an experimental measure of the affinities involved, where the
affinities are not forces between atoms but provide a measure of the
facility with which one macroscopic chemical substance combines with
another. In the late nineteenth century leading scientists such as
Ostwald, Duhem and Planck were inclined to take thermodynamics as
the model of how science should proceed, maintaining a secure
and productive relationship with experiment whilst avoiding hypotheses
of the kind involved in atomism.
The factor that is usually considered as turning the tables decisively
in favour of the atomists is Jean Perrin's experiments on Brownian
motion. Nye (1972, 145–52) has documented how Ostwald and
others conceded that the experiments settled the case in favour of
atoms.
Suggested Readings: Clarke (1976) is a detailed investigation
of the relationship between thermodynamics and the kinetic theory
which contains good summaries of both theories.  Clarke's case that
objections to the kinetic theory were based largely on scientific
grounds is contested in Nyoff (1988) which contains a good treatment
of the specific heats problem, and is further discussed in de Regt (1996).
Brownian motion is the fluctuating motion of particles of an emulsion
visible through a microscope. Two features of it led physicists in the
late nineteenth century to suspect that it was caused by the molecular
motions assumed in the kinetic theory. Those two features were its
permanence and its random character. Perrin’s experiments of
1908 were able to give precision to those suspicions. He was able to
show that the motions of the particles are indeed random, in a
technical sense, and he showed that the general features of the motion
are permanent, once equilibrium has been reached. The density
distribution and mean free path of the particles remain constant at
constant temperature. The kinetic theory had a ready explanation of
these features, attributing the randomness to the randomness of the
motions of the molecules making up the liquid in which the emulsion
was suspended, and the equilibrium conditions as a dynamic equilibrium
corresponding to the distribution of velocities formalised by Maxwell.
Those wishing to resist the conclusion that Brownian motion
constituted strong evidence for the kinetic theory needed to offer
some alternative explanation for the two features.
The randomness of the motion rules out causes, such as convection
currents in the liquid, which operate on a scale larger than the
dimensions of the particles. Causes of that kind would lead to
correlations between the motions of neighbouring particles and that is
precisely what is ruled out by a truly random motion of particles. The
permanence of the motion is a puzzle because the particles, moving
through a viscous liquid, will be slowed down, losing heat to the
liquid, suggesting that the whole motion should come to a halt just as
a sizeable object such as a cricket ball, projected into a liquid,
will be brought to rest. General, quantitative features of
Perrin’s results made life difficult for the anti-atomists, but
there was yet more to his case.
As was observed in 
 Section 5.3, 
 it had been experimentally established that the osmotic pressure of a
solute in small concentrations obeys the gas laws. A natural step from
the point of view of the kinetic theory is to assume that the
difference between the molecules of a solute distributed through the
liquid in which it is dissolved and Brownian particles all of like
size suspended in a liquid is simply one of scale. Einstein (1905,
1906,1907) was the first to stress this point and to give a detailed
account of Brownian motion as a thermal agitation. Perrin's initial
work on the density distribution of Brownian particles seems to have
been carried out in ignorance of Einstein's paper. But it was soon
brought to his attention and influenced his subsequent work with full
acknowledgment given to Einstein.
Perrin's observations revealed that the density distribution of
Brownian particles decreased exponentially with height. He was able to
explain this quantitatively by appeal to Newtonian mechanics and
statistics. Because of the decrease in their density with height, more
particles per second strike a unit area of the lower surface of a thin
horizontal layer in the liquid than will strike a unit area of the
upper surface. As a result there will be a net pressure directed
upwards. Perrin was able to derive a value for the pressure in terms of
the number of particles per unit volume, their mass and the mean of
the squares of their velocities. Equilibrium is reached when the
upwards force due to the pressure is equal to the weight of the
particles arising from the excess of the density of the material of
the particles over that of the suspending liquid. The resulting
equation, when integrated, showed the density of the particle
distribution to vary exponentially with height and also enabled Perrin
to calculate a value for the mean kinetic energy of the Brownian
particles from the measured variation in density of the particle
distribution. It transpired that the mean kinetic energy depended only
on temperature and was independent of the material of the particles,
their size and the density of the liquid in which the particles were
suspended.
Once Perrin was able to calculate the mean kinetic energy of the
Brownian particles he could support the most basic assumptions of
kinetic theory without a need to complicate matters by adding
additional hypotheses. From the point of view of the kinetic theory,
systems are in equilibrium when the mean kinetic energy of the
molecules in those systems are equal, with particle collisions being
the mechanism by means of which equilibrium is reached. Brownian
particles constitute a system that differs from the molecules
constituting a gas only quantitatively, not qualitatively. If a system
of Brownian particles is in thermal equilibrium with a gas at some
temperature, T, then, from the point of view of the kinetic
theory, the mean kinetic energy of the particles must be equal to
that of the molecules of the gas. By measuring the mean kinetic energy
of Brownian particles from the observable density distribution at
temperature, Tr, Perrin had in effect measured the mean
kinetic energy of the molecules of a gas at that temperature. That
knowledge enabled him to calculate Avogadro's number. As Perrin (1990,
104) remarked, it was with ‘the liveliest emotion’ that he
found that number to be in accord with previous, more indirect,
estimates of Avogadro's number.
Perrin stressed the extent to which the value for Avogadro’s
number yielded by his experiments on density distribution formed the
basis of a strong argument from coincidence for the kinetic theory.
Perrin posed the question of what density distribution of Brownian
particles might have been suspected prior to his experiments if the
kinetic theory is ignored. Since the particles were denser than the
liquid in which they were suspended, a reasonable assumption might be
that the particles fall to the bottom so that the density distribution
is zero. This experimental result, substituted into Perrin’s
formula, would have led to an infinitely large value for
N. Another plausible assumption might have attributed an even
distribution to the suspended particles. Such an outcome would have
led to a value of zero for N. A decrease in density with
height yields some value in between these two extremes. Prior to the
experiment, then, the range of plausible results to be expected from
feeding the measured distribution into Perrin's equation, derived on
the basis of the kinetic theory, is immense. And yet the outcome was a
number very close to that predicted by the kinetic theory.
There were yet further dimensions to Perrin's experiments. In his 1905
paper, Einstein had derived expressions for the mean displacement and
rotation of Brownian particles as a function of time on the basis of
the kinetic theory. Perrin was able to show how these predictions were
borne out by his observations of the particles. He, in effect, showed
that propositions basic to the kinetic theory, such as the
independence of orthogonal components of the velocity of particles and
the equi-partition of energy amongst their degrees of freedom, were
satisfied by the Brownian particles. What is more, it was again
possible to calculate values for N from the experimentally
determined mean displacements and rotations, and in both cases the
measured values were within a few percent of 68 ×
1022.
It was not long before Avogadro's number could be calculated by
methods not closely tied to the kinetic theory of gases. Rayleigh
speculated that the brightness of the sky is due to the scattering of
light from the sun by molecules in the atmosphere. The theory
predicted that light of shorter wavelength is scattered more
effectively than that of longer wavelength, a prediction borne out by
the blueness of the sky and the redness of sunsets. It also predicted
that the scattered light be polarised, also in conformity with
observation. It was possible to calculate Avogadro's number from the
ratio of the intensity of skylight to that of light coming direct from
the sun. Once the charge on the electron had been measured it was also
possible to calculate Avogadro's number from the relation between
current passed and weight of substance deposited in
electrolysis. Radioactivity was to provide further access to the
number. In all cases, the values for Avogadro's number agreed to a
degree that could be reconciled with the accuracy of the experiments
and the degree of approximation involved in the calculations.
There is a further important aspect of the extent to which Perrin's
experiments supported the kinetic theory. One of the major objections
raised by opponents of that theory was the fact that it implied that
the second law of thermodynamics is only statistically true. Perpetual
motion machines of the second kind become improbable rather than
impossible. It is difficult to resist the conclusion that the constant
lifting of Brownian particles against gravity refutes the unqualified
version of the second law. When a Brownian particle moves upwards then
the first law of thermodynamics, the conservation of energy, requires
that the potential energy gained by the particle must come from
somewhere. If it comes from the heat of the suspending liquid then
this is in contradiction to the second law. An opponent of the kinetic
theory and a defender of the literal truth of the second law is
required to supply some alternative source of the energy. Needless to
say, no suitable alternative was forthcoming. Once the kinetic theory
is assumed, the rising of a Brownian particle is understood as a
the result of a statistical fluctuation. What is more, the randomness and the
smallness of the scale on which the violations of the second law take
place ensures that it is not possible to employ the phenomenon to
extract useful work.
 The force of Perrin's argument for the kinetic theory, and hence
for the reality of molecules, stems from the fact that his argument
requires only the central assumptions of the theory, the equipartition
of energy and the randomness of molecular agitation, without requiring
the addition of auxiliary or simplifying assumptions. It is this fact
that made his calculations of Avogadro's number qualitatively distinct
from, and more telling than, other estimates. Ostwald cited this as
the reason for his conversion to belief in molecules (Nye, 1972,
151–152). Interestingly, the derivation of the ratio of the
principle specific heats of a gas similarly requires only the basic
assumptions of the kinetic theory cited above. That is why the clash
of the prediction with measured values spelt serious trouble for the
classical kinetic theory. Equipartition of energy breaks down for the
vibrational modes of a molecule and for rotational modes also at
temperatures sufficiently low, as Perrin (1990, 73) noted.
Suggested Readings: Perrin (1990) is an English
translation of his classic defence of atomism written in 1913. Nye
(1972) is a useful historical survey of Perrin's work on Brownian
motion. A recent philosophical analysis of the significance of
Perrin's experiments, which contains references to earlier analyses by
other philosophers, is Achinstein (2001), 243–265. Mayo (1996,
214–250) is an attempt to construe Perrin's argument as
involving bottom-up rather than top-down reasoning. Doubts about the
use of Perrin's experiments by philosophers are raised by van Fraassen
(2009), to which Chalmers (2011) is a response.
If we take atomism to involve the claim that the properties of
macroscopic matter arise as a result of the combinations and motions
of tiny particles, then it is a position confirmed by the time of the
Solvay Conference in 1911 in a way that left little room for sensible
doubt. But if we take atomism in a stronger sense, to mean a theory
that explains all of the properties of macroscopic matter in terms of
underlying particles with specified properties and governed by
specified laws, then it must be denied that atomism had reached its
objective in 1911. There were identifiable inadequacies and gaps in
the specification of the properties of atoms and the electrons and
protons that compose them and there were to an increasing extent
problematic experimental results that were eventually to lead to a
radical change in the laws that were presumed to govern the behaviour
of atomic and sub-atomic particles.
Acceptance of the kinetic theory implied acceptance of the existence
of atoms and molecules with a well-defined mass. However, it was
perfectly clear that they must have further properties. For example,
they needed properties that would explain chemical combination, and,
specifically, the notion of valency. They also needed properties that
would account for spectra. Answers to these challenges were
forthcoming in the form of the electron structure of the atom and the
quantum mechanics that governs it. There is a sense in which
contemporary physics, with its account of the properties of atoms and
molecules in terms of their electron structure and the explanation of
many macroscopic phenomena in terms of the atomic and molecular
structures underlying them, comes close to the ideal of Democritus. A
general account of the properties of the material world is offered in
terms of underlying particles with a few well-defined properties
governed by well-defined laws. The difference between the contemporary
situation and the ideals of Democritus or the mechanical philosophers
lies in the epistemological access to the general atomistic
theory. The contemporary theory became possible only as a result of
centuries of scientific development. The quantum mechanical laws
governing the atomic world were responses to quite specific problems
revealed by experiment in areas such as black-body radiation, emission
and absorption spectra, the specific heats of gases and
radioactivity. The properties ascribed to electrons, for instance,
such as their charge and half-integral spin, were themselves responses
to quite specific experimental findings involving discharge tube
phenomena and spectra.  Atomism, which began its life as speculative
metaphysics, has become a securely established part of experimental
science.