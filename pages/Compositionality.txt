There are many theses called ‘the principle of
compositionality’. The following can serve as a common reference
point:
Important variants of the compositionality principle will be presented
below in a form most similar to (C) to facilitate
 comparisons.[1]
 When formulating more precise versions it is crucial to keep the
pre-theoretical intuitions that led many to accept compositionality
firmly in mind.
The principle of compositionality is normally taken to quantify over
expressions of some particular language L:
Questions of structure and constituency are settled by the
syntax of L, while the meanings of simple expressions
are given by the lexical semantics of L.
Compositionality entails (although on many elaborations is not
entailed by) the claim that syntax plus lexical semantics determines
the entire semantics for L.
It makes a big difference whether L is a natural or an
artificial language. Syntactic and semantic questions about a natural
language are settled by and large through empirical investigation;
syntactic and semantic questions about an artificial language are
settled usually by checking what the appropriate stipulations are.
Prima facie, natural languages might turn out not to be
compositional, whereas many artificial languages were designed to meet
such a requirement. (Compositionality is a bonus when it comes to
proof-checking in computer languages, or inductive proofs in logical
calculi.) Unless explicitly noted, talk of compositionality is to be
taken as talk of compositionality of some particular natural language,
or of natural languages in general.
If thought is a kind of language, we can raise the question whether it
is compositional. Thought would not have to be much like
Swahili or the language of set theory for the question to make sense,
but we do need the assumptions that thoughts have meanings (and so,
presumably, are not themselves meanings) and that they have meaningful
constituents. These assumptions follow from
 the language of thought hypothesis
 (see entry). Those who reject this hypothesis may still speak of the
compositionality of thought—but only in an extended sense.
What would such an extended sense be? The key to generalizing
compositionality for non-linguistic representational systems is to
relax the syntactic ideas of constituency and structure. Consider, for
example, the No-Left-Turn sign:
This could be viewed as a complex sign decomposable into meaningful
features—the shape, the color pattern, the arrow, etc. These
features are the analogues of simple expressions: they appear in many
other complex signs and they appear to contribute more or less
uniformly to their meanings.
Once we have an initial grip on what counts as a constituent and how
constituents compose we can legitimately raise the question whether
this system of representations is
 compositional.[2]
 We may even be able to answer
 it.[3]
There is a major debate within the philosophy of mind between
proponents of classical cognitive architecture and proponents of
 connectionism
 (see entry). The debate is typically presented as a debate about
compositionality, but it is not exactly about that. The issue tends to
be whether there are such things as meaningful constituents of thought
(perhaps in the extended sense in which traffic signs can be said to
have meaningful constituents), and if there are, whether these
contribute the same thing (presumably their meaning) to all thoughts
in which they occur. If the answer to the first question is negative,
the question of compositionality does not arise. If the answer to the
first question is positive, the second is independent of
compositionality. (It could be that thought-constituents contribute
always the same thing to a thought of which they are constituents, but
these contributions, even together with the way the constituents are
combined, severely underdetermine the meaning of the thought. And it
could be that thought-constituents contribute different things to
different thoughts—depending, perhaps, on the surroundings of
the thinking—but these variable contributions, plus the way the
constituents are combined, fully determine the meaning of the
thought.) The debate about connectionism is more closely related to
the question whether reverse compositionality holds (cf.
 section 1.6.4.)
The principle of compositionality is not committed to a specific
conception of meaning. In fact, it is frequently announced as a
principle that is applicable to whatever a semantic theory might
assign to expressions of a language. Furthermore, although the
reference of an arbitrary expression is definitely not
something one would normally call its ‘meaning’, versions
of the following principle are frequently called ‘the
principle of compositionality’:
(I use the word ‘reference’ here roughly the way Frege
used ‘Bedeutung’ after 1892. But it could also be
taken the way Lewis uses ‘extension’. The differences are
significant, but they do not matter for present purposes.) To avoid
confusion, we should call this the principle of compositionality
of reference, and
 (C)
 the principle of compositionality of meaning; when I speak
of compositionality unqualified, what is meant is always the
latter. Since the arguments in favor of compositionality tend
to be based on general considerations about linguistic
understanding—which, I shall suppose amounts to nothing more or
less than understanding what linguistic expressions
 mean[4]—proponents
 of (C\(_{\textit{ref}}\)) have a choice to make. They can advocate
(C\(_{\textit{ref}}\)) on different grounds or they can claim that an
appropriate theory of the sort that assigns (relative to a variety of
contextually determined
 factors[5])
 references to expressions can serve as a theory of meaning.
Formalizations of
 (C)
 typically make no assumptions about what meanings are. This way we
achieve generality and stay clear of dogmatic pronouncements. Still,
it is a mistake to abandon all constraints for that turns
compositionality into a vacuous requirement. It is trivial that we can
compositionally assign something to each expression of a
language (for example, if expressions serve as their own meanings,
semantics is certainly compositional!) but it does not follow that it
is trivial to adequately assign meanings to
them.
The point applies to more subtle attempts to trivialize
compositionality as well. Consider a famous result due to Zadrozny
(1994). Given a set S of strings generated from an arbitrary
alphabet via concatenation and a meaning function m which
assigns the members of an arbitrary set M to the members of
S, we can construct a new meaning function \(\mu\) such that
for all \(s, t \in S \mu(s{.}t) = \mu(s)(\mu(t))\) and \(\mu(s)(s) =
m(s)\). What this shows is that we can turn an arbitrary meaning
function into a compositional
 one,[6]
 as long as we replace the old meanings with new “meanings” from which they
are uniformly
 recoverable.[7]
What is not clear is whether these new “meanings” really are meanings. After all, someone ignorant of m could assign entities to members of S following \(\mu\). Such a person would know what each expression in S “means” but would not necessarily know what any of them means. (For further discussion
of Zadrozny’s result, see Kazmi and Pelletier (1998),
Westerståhl (1998), Dever (1999).)
Compositionality obviously constrains what meanings might be. But the
constraints apply only to the meanings of complex
expressions—for all
 (C)
 tells us the meanings of simple expressions could be tables and
chairs. For let the meanings of complex expressions be interpreted
logical forms, i.e., phrase structure trees with the meanings of the
constituent lexical items assigned to their terminal nodes. In a
fairly straightforward sense the meanings of lexical items are then
parts of the meanings of complex expressions in which they occur, and
so the meanings of complexes are determined from the relevant tables
and chairs together with their syntactic mode of composition; for
similar remarks see Horwich (1997).
That compositionality does not constrain lexical meaning might appear
paradoxical at first, but the source of paradox is just instability in
how the label ‘compositionality’ is used. Sometimes
compositionality is said to be that feature in a language (or
non-linguistic representational system) which best explains the
productivity and systematicity of our understanding; cf. Fodor 2001:
6.
 (C)
 is but one of the features such explanations use—others include
the context-invariance of most lexical meaning, the finiteness of the
lexicon, the relative simplicity of syntax, and probably much else.
These features together put significant constraints on what
lexical meanings might be; cf. the papers collected in Fodor and
Lepore (2002) and Szabó (2004).
Much of what was said above about the need to constrain what counts as
meaning applies to structure as well. Janssen (1983) has a proof that we can turn
any meaning assignment on a recursively enumerable set of expressions
into a compositional one, as long as we can replace the syntactic
operations with different ones. If we insist—as we
should—that any acceptable semantic theory must respect what
syntax tells us about the structure of complex expressions, this
result says nothing about the possibility of providing an adequate
compositional semantics; cf. Westerståhl
 1998.[8]
 The moral of the result is that although commitment to
compositionality requires allegiance to no particular sect of
syntacticians, one cannot be oblivious to syntactic evidence in
semantic theorizing.
(C)
 does not require the kind of tight correspondence between syntax and
semantics we intuitively associate with compositionality. To
illustrate this, consider a view, according to which the meaning of a
declarative sentence s is the set of possible worlds where
s is true. According to such a view, tautologies are
synonymous, even though (since Rudolf presumably has some tautological
beliefs and lacks others) sentences resulting from embedding
tautologies under ‘Rudolf believes that…’ are not.
(I also assume a straightforward semantics for propositional attitudes
without hidden indexicals or tacit quantification.) Intuitively, this
is a violation of compositionality. Still, the semantics is
not in conflict with
 (C):
 tautologies might differ structurally or in the meaning of their
constituents, which could explain how their embedding can yield
non-synonymous sentences; cf. Carnap 1947 and Lewis 1970.
To rule a semantic theory like this one non-compositional, we need to
demand that the meaning of a complex expression be determined by its
immediate structure, and the meanings of its
immediate constituents. (The immediate structure of an
expression is the syntactic mode its immediate constituents are
combined. e is an immediate constituent of \(e'\) iff e
is a constituent of \(e'\) and \(e'\) has no constituent of which
e is a constituent.)
Call the strengthened principle local compositionality, and
 (C)
global compositionality; when unqualified,
‘compositionality’ should be taken as global. The local
principle is more intuitive, and semanticists frequently presuppose
it. In fact, some theorists assume not only
(C\(_{\textit{local}}\)),
but also that concatenation is uniformly interpreted as functional
application, or perhaps as conjunction; cf. Pietroski (2005, 2012).
Others insist that the relevant notion of structure
(C\(_{\textit{local}}\)) appeals to is the one apparent on the surface
of sentences, and accordingly, our syntax should not postulate
movement or empty elements; cf. Jacobson (2002, 2012). Clearly, appeal
to our ability to understand novel expressions in itself provides no
direct support for these strong claims.
Intuitively, if a language is compositional it cannot contain a pair
of non-synonymous complex expressions with identical structure and
pairwise synonymous constituents. This should follow from the fact
that the same structure and the same meanings of constituents cannot
determine more than one meaning within a language. But to
ensure that the inference really holds, we need to rule out a certain
reading of
 (C).
 
Fine (2007) advocates the following view: ‘Cicero’ and
‘Tully’ are synonyms, but ‘Cicero is Cicero’
and ‘Cicero is Tully’ are not, despite the fact that these
sentences do have the same structure. The meaning-difference arises
from the fact that the former sentence encodes semantic co-reference
but the latter does not. All this is fully compatible with English not
being a counterexample to (C\(_{\textit{coll}}\)):
On Fine’s view, what the constituents of ‘Cicero is
Cicero’ collectively mean goes beyond what the
constituents of ‘Cicero is Tully’ do. The collective
meaning comprises the individual meanings plus certain
meaning-relations that hold among them. Call the weak principle
(C\(_{\textit{coll}}\)) collective compositionality;
following usual practice
 (C)
 will be understood as distributive compositionality. 
It is clear that the meanings of complex expressions depend on the
individual meanings of their parts. Thus, the only chance for
(C\(_{\textit{coll}}\)) to be true is if the collective meaning of
constituents (whatever that might be) determines each of the
individual meanings of constituents. This is certainly so on
Fine’s view: he thinks the meaning of ‘Cicero is
Cicero’ depends on its structure (this is why it is not
synonymous with ‘Is Cicero Cicero?’), on the individual
meanings of its constituents (this is why it is not synonymous with
‘Cicero is Caesar’), and in addition on the
intended co-reference relation between the subject and the object,
which is an aspect of the collective meaning of its constituents (this
is why it is not synonymous with ‘Cicero is Tully’).
Given the distributive reading,
 (C)
 rules out only the existence of a pair of non-synonymous complex
expressions with identical structure and pairwise synonymous
constituents within a single language. This is a problem
because the existence of such a pair is a clear violation of what we
normally mean by determination even if the expressions belong to
distinct languages. 
Here is an illustration from Szabó (2000b). Suppose English is
compositional. Take two of its non-synonymous sentences—say,
‘Elephants are grey’ and ‘Julius Caesar was murdered
on the ides of March’—and define Crypto-English as the
language with the same expressions, the same syntax and almost the
same semantics as English. The only difference is that if a
sentence is synonymous in English with one of the two designated
sentences, then it is synonymous with the other in Crypto-English. We
assumed English is compositional and hence that there is no pair of
non-synonymous complex expressions in English with identical structure
and pairwise synonymous constituents. Trivially, the same must hold
for Crypto-English as well. But intuitively, Crypto-English is
not compositional. The structure and the meanings of
constituents of the Crypto-English sentence ‘Elephants are
grey’ cannot determine what this sentence means in
Crypto-English—if they did then the structure and the meanings
of constituents of the English sentence ‘Elephants are
grey’ would have to determine what ‘Julius Caesar was
murdered on the ides of March’ means in English.
If we want a better match with our intuitions, we must demand more
from a compositional language than the mere existence of a
function from structures and the meanings of parts to the
meanings of wholes. One possibility would be to put constraints on
this function—we could demand, for example that it be
computable, or perhaps even that the computation be reasonably quick.
But the above example shows that such a strengthening would not solve
the problem: if computing the meanings of complex expressions is easy
in English, it will not be hard in Crypto-English either. We might
instead opt for the following strengthening of
 (C):
Call the strengthened principle cross-linguistic
compositionality, and
 (C)—when
 ‘determine’ is simply read as ‘functionally
determine’ and we may have different functions for different
languages—language-bound compositionality. Note that
formal languages that are designed to satisfy language-bound
compositionality may nonetheless violate cross-linguistic
compositionality simply because their syntax or the meanings of their
constituents violate some universal constraint on human languages.
Note also that whatever the epistemic status of other versions of the
principle of compositionality might be, cross-linguistic
compositionality is clearly an empirical
 hypothesis.[9]
When speaking of compositionality unqualified, I will always mean
language-bound compositionality. Again, the stronger principle is much
closer to our pre-theoretic intuitions and it is often tacitly assumed
in practice. But the traditional considerations in favor of
compositionality support the weaker thesis only.
The fact that natural languages contain indexicals forces us to
distinguish between two notions of meaning. On the one hand,
expressions have a standing meaning fixed by convention and
known to those who are linguistically competent. On the other hand,
expressions in use are associated with occasion meanings
which is discerned by interpreters in part on the basis of contextual
information. The terminology is from Quine (1960). Kaplan (1977) uses
the terms character and content but he makes a
number of substantive assumptions about what these are which I intend
to abstract from. Thus, we should not assume that occasion meanings
are structured entities built from objects, properties, and relations,
or even that the occasion meanings of declarative sentences are always
propositions. Also, we need not assume that standing meanings are
functions from contexts to occasion meanings, or even that they
determine in context what occasion meanings are.
When we speak of meaning, usually we have standing meaning in mind.
But not always—when a contract specifies that within its main
text ‘current edition of building code’ means the 2012
edition of the Florida Building Code, it obviously fixes occasion
meaning. Corresponding to these two notions of meaning, there are two
versions of the principle of compositionality. Since occasion-meaning
is determined, in part, by context (C\(_{\textit{occ}}\)) must be
relativized to
 context:[10]
Let’s call expressions whose occasion meaning sometimes deviates
from their standing meaning context-dependent. The scope of
context-dependent lexical items is a matter of controversy. On the one
extreme, there are semantic minimalists who think these
include only a handful expressions: the personal and demonstrative
pronouns, a few adverbs (e.g., ‘here’, ‘now’,
‘next’), and a few adjectives (e.g.‘actual’,
‘present‘, ‘local’); cf. Cappelen and Lepore
(2005). On the other
extreme are radical contextualists who think essentially all
lexical items are context-dependent; e.g., Searle (1980). As usual,
most theorists are somewhere in the middle—taking heat from both
sides that their view is untenable.
Radical contextualism is sometimes seen as a challenge to
compositionality, more precisely, to
 (C\(_{\textit{occ}}\));
 cf. Cohen (1986), Lahav (1989), Fodor (2001). It shouldn’t be.
An effective argument from context-dependence against
 (C\(_{\textit{occ}}\))
 would need to show that there is at least one complex expression in
L whose occasion meaning varies with context, while the
occasion meanings of its constituents all remain the same. The usual
considerations against compositionality typically omit the second
part. Take for example Searle’s observation that “[t]he
sort of thing that constitutes cutting the grass is quite different
from e.g., the sort of thing that constitutes cutting a cake”
(Searle 1980: 222). What follows from this? Nothing more than the fact
that the occasion meaning of ‘cut’ is sensitive to a
feature of the context in which it is used, in particular, to its
linguistic environment. This is fully compatible with
 (C\(_{\textit{occ}}\)).
 
Of course, we should not insist that the occasion meaning of
‘cut’ depends on nothing but its standing meaning and the
linguistic environment in which it occurs. As Searle himself
emphasized, ‘cut the grass’ can pick out one sort of thing
if we are using it in a context of selling strips of grass turf and
another it we are using in a context of selling lawn mowers. Arguably,
this shows that the occasion meaning of ‘cut’ depends on
extra-linguistic factors as well. No matter: this too is fully
compatible with
 (C\(_{\textit{occ}}\)).
 Compositionality demands nothing more than that all
context-dependence be accounted for via context-dependence in the
lexicon and it takes no stance of how much and what kind of lexical
context-dependence there might be; cf. Szabó (2010), Lasersohn
(2012), and Recanati
 (2012).[11]
We have distinguished several interpretations for the seemingly simple
claim that a certain language is compositional and we picked a fairly
natural one. Thus, we proposed to read
 (C)
 as being about meaning (as opposed to reference or some other value
one might assign to expressions), that it postulates functional
determination of meaning within a particular language (as opposed to
across a class of languages), and that the determinants of the meaning
of a complex expression are its entire structure (as opposed to just
its immediate structure) and the meanings of its constituents
individually (as opposed to collectively). We saw that
 (C)
 remains ambiguous even after these clarifications, for there are at
least two kinds of meaning it could be about (standing meaning and
occasion meaning). If it is intended to be about occasion meaning, it
must involve a suppressed quantification over contexts (just as it
contains a suppressed quantification over languages). 
There are a number of principles worth mentioning that are often
discussed along with (and are occasionally confused with) the
principle of compositionality. It is useful to see which, if any of
them is equivalent to
 (C).
 
Consider first the often cited principle that says that substitution
of synonyms is always meaning-preserving. As stated, the principle
requires clarification. For one thing, not every case of replacement
counts as substitution: the expression we replace with its synonym
within a larger expression must be a constituent of the larger
expression. Otherwise, as Geach pointed out, the synonymy of
‘Plato was bald’ with ‘Baldness was an attribute of
Plato’ would guarantee the synonymy of ‘The philosopher
whose most eminent pupil was Plato was bald’ and ‘The
philosopher whose most eminent pupil was baldness was an attribute of
Plato’; (Geach 1965: 110).
In addition, we need to separate two issues: whether substitution of
synonyms can turn a meaningful expression into a meaningless one, and
whether it can turn a meaningful expression into an expression with a
different meaning. The principle that rules out the former possibility
was first proposed by Husserl (1913: 318), and it is usually stated in
terms of the notion of a semantic category. Two expressions
belong to the same semantic category just in case they are
intersubstitutable within any meaningful expression salva
significatione (without loss of meaningfulness). According to
Husserl’s principle:
(H) is a rather controversial—intuitively, there are many
synonyms that are not everywhere intersubstitutable. For example,
‘likely’ and ‘probable’ mean pretty much the
same even though ‘Jacques is likely to leave’ is
meaningful while ‘Jacques is probable to leave’ is
arguably not; cf. Gazdar (1985:
 32).[12]
 And—more controversially—there might be synonyms that are
almost nowhere intersubstitutable: ‘quick’ and
‘quickly’ are good candidates.
(C) even combined with (H) does not rule out the possibility that some lexical items are syncategorematic, i.e. that they are meaningless but contribute nevertheless to the meanings of larger expressions in which they occur. Formal languages often contain such expressions. For example, in the language of the propositional calculus, the Boolean connectives are not assigned anything by the interpretation function. Rather, semantic clauses specify the interpretation of complex expressions in which they occur as the main connective. 
The principle that rules out the possibility that substitution of
synonyms could turn a meaningful expression into one with a different
meaning comes in two versions:
Assuming the language under discussion has a grammar that requires
that each constituent of a meaningful complex expression be itself
meaningful (S\(_{\textit{plural}}\)) is stronger than
 (C)—it
 is equivalent to local, distributive, language bound
compositionality of meaning. Assuming in addition that the language
satisfies
 (H),
 (S\(_{\textit{singular}}\)) is equivalent to
(S\(_{\textit{plural}}\)); cf. Hodges (2001: Theorem 4).
The fact that compositionality—at least in the weak form it is usually stated and supported by arguments from productivity—does not require substitutivity is important. Substituting a complex expression for a simple one within some expression effects the structure of that expression, and since compositionality allows meaning to depend on structure, such a substitution can
affect meaning. Consider one of Quine’s arguments; cf. (Quine 1953: 143). ‘It is necessary that the number of planets is greater than seven’ is
false, ‘It is necessary that eight is greater than seven’ is true, even though
‘the number of planets’ and ‘eight’ have the same extension, so the semantics of English cannot be extensional.(Quine uses ‘nine’ because he thought Pluto was a planet.) The argument assumes that the extension of a definite description is whatever uniquely satisfies it—a claim rejected by proponents of quantificational accounts of definite
descriptions (e.g. Russellians). But even if we grant this assumption as well as the compositionality of English, the argument still fails: the difference
in truth-value could be the result of the different syntactic structure of
the two sentences.
Sometimes the claim that L is compositional is presented
directly as a claim about the relationship between its syntax and
semantics. The following thesis is often called the rule-to-rule
principle: 
How strong a claim (RR) is depends on what counts as a rule. If an
arbitrary function deserves that name, the rule-to-rule principle is
stronger than
 (C):
 it is equivalent to local, distributive, language bound
compositionality of meaning. But if we insist—quite
plausibly—that a semantic rule must be computable (or perhaps
easily computable) the rule-to-rule principle is stronger than that.
And if we assume that rules must have some sort of psychological
reality, (RR) says something completely different from
 (C).
 
Ordinarily when we say that something determines something else we
think of the former as being causally or explanatorily prior to the
latter. Although the principle of compositionality is usually not
understood in this way, sometimes philosophers read it as a principle
that asserts the priority of word meaning over sentence meaning, or
more generally, the priority of the meanings of lexical items over the
meanings of complex expressions:
(P) is often thought to be in tension with the idea that each
expression has the meaning it does in virtue of the way it is used
within some linguistic community. The conflict is supposed to arise
because (i) the use of an expression is exhausted by its employment in
speech acts, and (ii) it is sentences, not words, that can be employed
to make speech acts. Against this, it can be argued that
referring is among the speech acts speakers routinely perform
and that this speech act is done with words, not sentences. One might
try to replace (i) with a stronger claim, for example, that the use of
an expression is exhausted by its employment in asserting,
asking, commanding, and a few of other speech acts
not including referring. But even if true the stronger claim
may not save the argument against (P) because, at least prima
facie, we can make assertions uttering isolated words; cf.
Stainton (2006). Davis (2003) develops a detailed theory of meaning
that combines (P) with a version of the use theory of meaning.
To say that there is no easy argument against
 (P)
 is a far cry from saying that it must be true. It is important to
keep in mind that
 (P)
 is significantly stronger than
 (C)
 and that the usual arguments in favor of compositionality cannot by
themselves justify it. (For some arguments against (P), see Szabó (2019).) 
In section 60 of the Foundations of Arithmetic (1884) Frege
famously declares that only within a complete sentence do words have
meaning. This has come to be referred to in the literature as
Frege’s context principle. Frege writes that “it
is enough if the sentence as whole has meaning; thereby also its parts
obtain their meanings” (Frege [1884] 1950: section
 60).[13]
 On the face of it, this asserts that words have their meanings in
virtue of the meaning of sentences in which they occur as
constituents. This is incompatible with
 (P),
 but not with
 (C).
 Even if words are meaningful only because they occur as constituents
within sentences, there could still be a function (perhaps even a
single function across all possible human languages) that maps the
structure of a sentence and the meanings of its constituent words to
the meaning of that sentence.
There is an alternative way to construe Frege’s principle, a way
that makes it a determination claim, not a primacy claim. To state it
in a form that matches the generality of
 (C)
 we should drop the talk of words and sentences, and talk instead
about complex expressions and their constituents:
Like the principle of compositionality, (F\(_{\textit{all}}\)) can be
interpreted as a claim about reference or meaning, locally or
globally, collectively or distributively, in a language-bound manner
or cross-linguistically. Compositionality is about bottom-up
meaning-determination, while the context principle about top-down
meaning-determination. As long as it is not understood as a causal or
explanatory relation determination can be symmetric, so any version of
 (C)
 is compatible with the corresponding version of
(F\(_{\textit{all}}\)). 
There is a strengthening of
 (F\(_{\textit{all}}\)),
 according to which the meaning of an expression is determined not
only by the meanings of all expressions in which it occurs as
a constituent, but by the meaning of any one of these
expressions:
(F\(_{\textit{any}}\)) is an immediate consequence of the converse of
 (C)—sometimes
 called reverse compositionality—according to which the
meaning of a complex expression determines the structure of the
expression and the meanings of its constituents. (Fodor (1998b), Fodor
& Lepore (2001), Pagin (2003) advocate reverse compositionality;
Patterson (2005), Robbins (2005), Johnson (2006) are among its
opponents. The debate is complex, in part because at least some
proponents of reverse compositionality advocate it only for the
language of thought; cf. Fodor 2001.)
(F\(_{\textit{any}}\))
 is a very strong thesis and most standard semantic theories are
incompatible with it. Take, for example, a simple Carnapian semantics
that assigns to each sentence the set of possible worlds where it is
true. Suppose we are considering a language that contains the standard
logical operators, and so any sentence is a constituent of a
necessarily true sentence. Since the meaning of a necessary truth is
the set of all possible worlds, this set would have to determine the
meanings of all sentences in the language, which is absurd.
A principle of intermediate strength between
 (F\(_{\textit{all}}\))
 and
 (F\(_{\textit{any}}\))
 is (F\(_{\textit{cofinal}}\)):
(A cofinal set of expressions is a set such that any expression occurs
as a constituent in at least one member of the set. Except for very
odd languages, the set of all expressions within the language in which
some given expression occurs as a constituent is one of many cofinal
sets of expressions, so
 (F\(_{\textit{all}}\))
 follows from (F\(_{\textit{cof}}\)) but not the other way around.
That (F\(_{\textit{cof}}\)) follows from
 (F\(_{\textit{any}}\))
 but not the other way around is trivial.) 
One interesting feature of (F\(_{\textit{cofinal}}\)) is that it
appears to be in conflict with a Quine’s thesis of the
indeterminacy of translation (taken as a thesis that implies the
indeterminacy of meaning). Assume that the set of all observation
sentences is cofinal within a reasonably large fragment of a natural
language and that the meaning of an observation sentence is identical
to its stimulus meaning—(F\(_{\textit{cofinal}}\)) ensures then
that the meanings of all the words are determined within our fragment.
There has been an attempt to show that
(F\(_{\textit{cofinal}}\)) follows from less controversial claims, and
perhaps even from claims that Quine himself was committed to; cf.
Werning (2004). The heart of Werning’s argument is the
Extension Theorem; cf. Theorem 14 in Hodges 2001. The theorem
states that a meaning assignment to a cofinal set of expressions that
satisfies
 (H)
 and
 (S\(_{\textit{singular}}\))
 has a unique extension to a meaning assignment to all expressions
that satisfies
 (H),
 (S\(_{\textit{singular}}\)) as well as its converse (there is a generalized result
mentioned in Hodges 2012: 257). The extra assumptions needed to get
from the Extension Theorem to a denial of indeterminacy remain
questionable; cf. Leitgeb (2005).
The claim that L is compositional is often taken to mean that
the meaning of an arbitrary complex expression in L is built up
from the meanings of its constituents in L—call this the
building principle for L. This is a fairly strong
claim, at least if we take the building metaphor seriously. For then
the meanings of complex expressions must themselves be complex
entities whose structure mirrors that of the sentence; cf. Frege
(1892, 1919). This
presumably entails but is not entailed by local distributive
cross-linguistic compositionality of meaning.
Montague (1970) suggested a perspicuous way to capture the principle
of compositionality formally. The key idea is that compositionality
requires the existence of a homomorphism between the
expressions of a language and the meanings of those expressions.
Let us think of the expressions of a language as a set upon which a
number of operations (syntactic rules) are defined. Let us require
that syntactic rules always apply to a fixed number of expressions and
yield a single expression, and let us allow that syntactic rules be
undefined for certain expressions. So, a syntactic algebra is
a partial algebra \(\mathbf{E} = \langle E, (F_{\gamma})_{\gamma \in
\Gamma}\rangle\), where E is the set of (simple and complex)
expressions and every \(F_{\gamma}\) is a partial syntactic operation
on E with a fixed arity. The syntactic algebra is interpreted
through a meaning-assignment m, a function from E to
M, the set of available meanings for the expressions of
E.
Consider now F, a k-ary syntactic operation on E.
m is F-compositional just in case there is a
k-ary partial function G on M such that whenever
\(F(e_1 ,\ldots ,e_k)\) is defined,
(In English: there is a partial function G from the meanings of
\(e_1 ,\ldots ,e_k\) to the meaning of the expression built from \(e_1
,\ldots ,e_k\) through an application of the syntactic rule
F.)
Finally, we can say that m is compositional
simpliciter just in case m is F-compositional
for each syntactic operation in E. Whenever m
is compositional, it induces the semantic algebra \(\mathbf{M} =
\langle M, (G_{\gamma})_{\gamma \in \Gamma}\rangle\) on M, and
it is a homomorphism between E and
M; cf. Westerståhl (1998). (For details,
variants, and formal results, see Janssen (1983, 1997), Hodges (2001), and
Pagin & Westerståhl (2010a). For generalizations that cover
languages with various sorts of context-dependence, see Pagin (2005),
Pagin & Pelletier (2007), and Westerståhl (2012).)
Since there are no restrictions on what m assigns to members of
E, the formal statement captures both compositionality of
reference and compositionality of meaning. As stated, the principle
captures local distributive language-bound compositionality: it
requires that each application of each syntactic rule within a
language be matched by an application of an appropriate semantic
function. To capture cross-linguistic compositionality is easy: all we
need to say is that the expressions within E are the
expressions of all possible human languages. (Of course, if we allow
the syntactic algebra to contain expressions of different languages,
we may want to insist that syntactic operations map expressions of a
language onto complex expressions of the same language and that they
remain undefined for cases when their argument positions are filled by
expressions from different
 languages.[14])
 
To capture global compositionality is more complicated. Here is an
attempt. Let us say that the expressions e and \(e'\) are
local equivalents just in case they are the results of
applying the same syntactic operation to lists of expressions such
that corresponding members of the lists are synonymous. (More
formally: for some natural number k there is a k-ary
F in E, and there are some expressions
\(e_1, \ldots, e_k\), \(e_1 ', \ldots, e_k '\) in E,
such that \(e = F(e_1 ,\ldots ,e_k)\), \(e' = F(e_1 ',\ldots ,e_k
')\), and for every \(1\le i \le k\), \(m(e_i) = m(e_i ')\).) It is
clear that m is locally compositional just in case locally
equivalent pairs of expressions are all synonyms. Let us say that the
expressions e and \(e'\) are global equivalents just
in case they are the results of applying the same syntactic operation
to lists of expressions such that corresponding members of the lists
are either (i) simple and synonymous or (ii) complex and globally
equivalent. (Here is the recursive definition more formally. Let us
say that the expressions e and \(e'\) are 1-global
equivalents just in case they are synonymous simple expressions.
Let us say that the expressions e and \(e'\) are n-global
equivalents just in case for some natural number k there
is a k-ary F in E, and there are some
expressions \(e_1 ,\ldots ,e_k, e_1 ',\ldots ,e_k '\) in E,
such that \(e = F(e_1 ,\ldots ,e_k), e' = F(e_1 ',\ldots ,e_k ')\),
and for every \(1 \le i \le k\) there is a \(1 \le j \lt n\) such that
\(e_i\) and \(e_i '\) are j-global equivalents. Finally, let us
say that the expressions e and \(e'\) are global
equivalents just in case for some natural number n they
are n-global
 equivalents.)[15]
 I suggest that m is globally compositional just in case
globally equivalent pairs of expressions are all synonyms.
Collective compositionality is a further weakening of global
compositionality. It could be formalized using the same trick. Thus,
we can say that m is collectively compositional just in case
collectively equivalent pairs of expressions are all synonyms, where
we define collective equivalence exactly like global equivalence with
one difference. In the recursive step we demand not only that \(e_i\)
and \(e_i '\) be j-collective equivalents but also that the
very same semantic relations should hold among \(e_1 ,\ldots ,e_k\)
and among \(e_1 ',\ldots ,e_k '\). Thus, we leave space for the
possibility that ‘Cicero is Cicero’ is not collectively
equivalent to ‘Cicero is Tully’, even though they have the
same structure and their proper constituents are all collectively
equivalent; see
 section 1.4.
 
The simplest argument for compositionality is that it is supported by
intuitions many claim to have about meaning and structure. Although
there are interesting putative counterexamples (see
 section 4.2)
 they probably can be explained away through modest revisions of our
syntactic and/or semantic theories. This defense is reasonable but
much too modest. For even if it succeeds in convincing some who
aren’t already convinced, it leaves us all in the dark
why compositionality is true. Defenders of compositionality
should do better than this.
The argument most often used to support compositionality is based on
productivity. It goes back (at least) to Frege, who claimed that 
the possibility of our understanding sentences which we have never
heard before rests evidently on this, that we can construct the sense
of a sentence out of parts that correspond to words. (Frege [c. 1914]
1980: 79) 
The argument is an inference to the best explanation, which can be
expanded and rephrased without assuming that meanings are Fregean
 senses.[16]
To bolster the claim that we do, in fact, understand complex
expressions we never heard before, philosophers often appeal to
unboundedness: although we are finite beings we have the
capacity to understand each of an infinitely large set of complex
expressions. Although there are dissenters—e.g., Ziff (1974) 
and Pullum & Scholz (2010)—the claim that natural languages 
contain infinitely many complex expressions is
 plausible.[17]
 But it is equally plausible that nobody who reads this entry the
first time has ever encountered this very sentence before, and
consequently, the detour through cardinality considerations seems
superfluous. Occasionally, the fact that natural languages are
learnable is also used to argue for compositionality. This is
not an independent argument: the reason it is remarkable that we can
learn a natural language is that once we have learnt it our
understanding is productive. If we could not understand expressions we
never encountered before, without detailed empirical study we could
not rule out the hypothesis that we learned the language in question
by rote.
The first thing to point out about the argument from productivity is
that it is an argument in favor of
 (C)—global
 distributive language-bound compositionality of meaning. As it
stands, it provides no reason for believing anything this principle
does not entail; in particular it cannot establish
 (C\(_{\textit{ref}}\)),
 (C\(_{\textit{local}}\)), or
 (C\(_{\textit{cross}}\)).
 
The argument can be criticized on the ground that considerations of
this sort simply cannot establish a universal claim. Suppose someone
suggests that the complex expression e is a counterexample to
 (C).
 The fact that we tend to understand all sorts of complex expressions
we never heard before does not mean that we would understand e
on the first encounter. But suppose we would. Still, even if in
general we tend to understand complex expressions we never heard
before in virtue of our knowledge of their structure and the meanings
of their simple constituents, we might understand e in some
other way. General considerations of productivity cannot rule out
isolated exceptions to compositionality. 
Isolated putative
exceptions are called idioms— and natural languages contain many of them. (Jackendoff (1997) estimates the number of English idioms to be around twenty-five thousand. Defenders of the compositionality of natural languages sometimes argue that the syntactic complexity of these expressions is only apparent, and hence, they can be viewed as lexical items. But unless we are given clear
non-semantic grounds for singling out idioms, the move is
question-begging. Such criteria have been proposed, but they tend to
be rather controversial; cf. Nunberg, Sag, and Wasow (1994).)
If we lower our sights and seek to prove nothing more than the claim
that natural languages by and large obey global distributive
language-bound compositionality of meaning, the argument from
productivity is reasonably strong.
Another argument in favor of compositionality is based on
systematicity, the fact that there are definite and
predictable patterns among the sentences we understand. For example,
anyone who understands ‘The rug is under the chair’ can
understand ‘The chair is under the rug’ and vice
versa. This is also an inference to the best explanation, and can
be summarized as follows:
Although the arguments from productivity and systematicity are usually
alluded to in the same breath, they are very different
considerations. Unlike the main premise of the former, the main
premise of the latter is anything but obvious. Particular instances
are plausible enough: it seems reasonable that anyone who can
understand ‘The dog is asleep’ and ‘The cat is
awake’ can also understand ‘The dog is awake’ and
‘The cat is asleep’, and that anyone who can understand
‘black dog’ and ‘white cat’ can also
understand ‘black cat’ and ‘white dog’. But do
all who understand ‘within an hour’ and ‘without a
watch’ also understand ‘within a watch’ and
‘without an hour’? And do all who understand
‘halfway closed’ and ‘firmly believed’ also
understand ‘halfway believed’ and ‘firmly
closed’? As Johnson (2004) argues, the claim that natural
languages are systematic presupposes a natural non-overlapping
linguistic categorization of all the expressions. The existence of
such a categorization is a bold empirical hypothesis.
Fodor (1998b) does offer an empirical argument in favor of
systematicity. The idea is that if complex expressions could be
understood without understanding their constituents then it is unclear
how exposure to a corpus made up almost entirely of complex
expressions could suffice to learn the meanings of lexical items. But,
as a matter of empirical fact, children learn the meanings of words by
encountering them almost exclusively within other expressions.
However, as Robbins (2005) points out, this observation can at best
lead one to conclude that understanding a suitably large set
of complex expressions in which a given expression occurs as a
constituent suffices for understanding the constituent itself. It does
not show that understanding any complex expression suffices
for understanding its constituents.
The arguments from productivity and systematicity differ in what they
aim to prove. First, the argument from systematicity proves something
weaker than (any version of) compositionality. If we run the argument
for the pair of sentences ‘The dog is asleep’ and
‘The cat is awake’ we can conclude that the meanings of
‘the dog’, ‘the cat’, ‘is asleep’
and ‘is awake’ plus predication determine the meaning of
‘The dog is awake’. It does not follow that the
meanings of ‘the dog’ and ‘is awake’ plus
predication do that. Second, if this problem can be fixed somehow, the
argument from systematicity proves not only global, but
local compositionality: it tells us that the meanings of
immediate constituents and immediate structure fix the meanings of
complex expressions. Finally, if successful, the argument from
systematicity proves not only a version of the compositionality
principle, but also reverse compositionality. We are invited to
conclude that the meaning of an arbitrary complex expression
determines its immediate structure and the meanings of its immediate
constituents; cf.
 section 1.6.4,
Fodor & Lepore 2001: 59;
Pagin 2003: 292.
As with the argument from productivity, the argument from
systematicity is unable to screen out isolated counterexamples. Still,
it is a reasonably strong consideration in favor of the claim that
natural languages by and large obey language-bound
distributive local compositionality of meaning and its reverse.
By far the most popular reason for believing in compositionality is
that it works. Linguists have adopted various versions of the
principle as a working hypothesis and developed semantic theories on
their basis. These theories have provided intuitively satisfactory
explanations for certain data, such as the validity or invalidity of
inferences or contrasts in acceptability between minimal pairs.
Moreover, whenever it was suggested that certain phenomena require
abandonment of the principle, it was subsequently shown that this is
not so: reasonably elegant and comparatively natural compositional
theories were just around the corner; cf.
 section 4.2.
 
Despite its popularity, this is not a very good reason to believe in
compositionality. The fact that compositional semantic theories can
explain certain things does not show that they explain those things
because they are compositional. Do we have reason to think
that without assuming compositionality we would not be able to explain
the same things? It seems to me that we have no such reason:
semanticists have focused on whether they can hold on to
compositionality while providing satisfactory explanations, not on
whether they have to embrace compositionality in order to
provide satisfactory explanations. We are not entitled to assume that
adopting compositionality as a working hypothesis has in any way
contributed to explanatory success in semantics. 
A much more promising methodological argument for compositionality
goes as follows. The fact that we are able to communicate in real time
makes it overwhelmingly likely that the computational complexity of
the interpretation algorithm we employ is relatively low. In fact, it
seems reasonable to think that semantic theories with minimal
complexity are, other things being equal, preferable. And there are
certain results that show that, under certain conditions, semantics
theories that conform to a certain strengthening of
 (C)
 will be minimally complex; cf. Pagin (2012). Alas, the conditions in
question tend to be unrealistic for natural languages. Nonetheless, we
can think of them as idealizations, which makes adoption of
compositionality as a working hypothesis still a reasonable one. 
The arguments from productivity and systematicity are persuasive. Many
linguists and philosophers have suggested that the explanation of
these phenomena that presupposes compositionality is not only the
best, but also the only one imaginable. So, before I survey some of
the putative counterexamples to compositionality from the semantic
literature, to bolster the imagination I will discuss a simple
non-linguistic case where our understanding is productive and
systematic despite apparent lack of compositionality in the system of
representations.
Consider the Algebraic notation for
 chess.[18]
 Here are the basics. The rows of the chessboard are represented by
the numerals \(\b{1},\b{2}, \ldots ,\b{8}\); the columns are represented by the
lower case letters \(\ba, \bb, \ldots ,\bh\). The squares are identified by
column and row; for example \(\b{b5}\) is at the intersection of the
second column from the left and the fifth row from the top. Upper case
letters represent the pieces: \(\bK\) stands for king, \(\bQ\) for
queen, \(\bR\) for rook, \(\bB\) for bishop, and \(\bN\) for
knight. Moves are typically represented by a triplet consisting of an
upper case letter standing for the piece that makes the move and a
sign standing for the square where the piece moves. There are five
exceptions to this: (i) moves made by pawns lack the upper case letter
from the beginning, (ii) when more than one piece of the same type
could reach the same square, the sign for the square of departure is
placed immediately in front of the sign for the square of arrival,
(iii) when a move results in a capture an \(\bx\) is placed
immediately in front of the sign for the square of arrival, (iv) the
symbol \(\b0\hy\b0\) represents castling on the king’s side, (v)
the symbol \(\b{0\hy 0\hy 0}\) represents castling on the queen’s
side. \(\bpl\) stands for check, and \(\bpl\bpl\) for mate. The rest of the
notation serves to make commentaries about the moves and is
inessential for understanding it.
Someone who understands the Algebraic notation must be able to follow
descriptions of particular chess games in it and someone who can do
that must be able to tell which move is represented by
particular lines within such a description. Nonetheless, it is clear
that when someone sees the line \(\b{Bb5}\) in the middle of such a
description, knowing what \(\bB, \bb\), and \(\b{5}\) mean will not be
enough to figure out what this move is supposed to be. It must be a
move to \(\b{b5}\) made by a bishop, but we don’t know which bishop
(not even whether it is white or black) and we don’t know which
square it is coming from. All this can be determined by following the
description of the game from the beginning, assuming that one knows
what the initial configurations of figures are on the chessboard, that
white moves first, and that afterwards black and white move one after
the other. But staring at \(\b{Bb5}\) itself will not help.
The first moral of the example is that we can have productive and
systematic understanding of representations even if we do not
understand complex representations merely by understanding
their simple components and the way those components are combined. The
reason this could happen is that all who understand the system know
certain things (e.g., the initial configuration of pieces and the
order of moves) from which they can figure out the missing information
(e.g., which figure is moving and from where).
The second moral is that—given certain assumptions about meaning
in chess notation—we can have productive and systematic
understanding of representations even if the system itself is not
compositional. The assumptions in question are that (i) the
description I gave in the first paragraph of this section fully
determines what the simple expressions of chess notation mean and also
how they can be combined to form complex expressions, and that (ii)
the meaning of a line within a chess notation determines a move. One
can reject (i) and argue, for example, that the meaning of \(\bB\) in
\(\b{Bb5}\) contains an indexical component and within the context of
a description, it picks out a particular bishop moving from a
particular square. One can also reject (ii) and argue, for example,
that the meaning of \(\b{Bb5}\) is nothing more than the meaning of
‘some bishop moves from somewhere to square
\(\b{b5}\)’—utterances of \(\b{Bb5}\) might carry extra
information but that is of no concern for the semantics of the
notation. Both moves would save compositionality at a price. The first
complicates considerably what we have to say about lexical meanings;
the second widens the gap between meanings of expressions and the
information conveyed by their utterances. Whether saving
compositionality is worth either of these costs (or whether there is
some other story to be told about our understanding of the Algebraic
notation) is by no means clear. For all we know, Algebraic notation
might be non-compositional.
We now discuss briefly four famous putative counterexamples to the
compositionality of English from the semantic literature. The list is
supposed to be representative but by no means exhaustive. (For a more
systematic survey of how compositionality problems are typically
solved in formal semantics, see Zimmerman 2012.) In each case, we also
indicate what reasonable responses to the challenges might look
like.
Putative counterexamples to
 (C)
 are always complex expressions whose meaning appears to depend not
only on the meanings of their constituents and on their structure but
on some third factor as well. Sometimes this third factor is
linguistic context: what a complex expression means seems to depend in
part on how it is embedded into a sentence
 (§ 4.2.1)
 or a sequence of sentences
 (§ 4.2.2).
 In other cases the third factor is extra-linguistic: the setting in
which the complex expression is used
 (§ 4.2.3)
 or someone’s beliefs about what the expression means
 (§ 4.2.4).
Such putative counterexamples are not all on the same level. Although
they all violate the letter of
 (C),
 some could be more easily reconciled with productivity and
systematicity than others. If it turned out that in order to interpret
an embedded sentence, one needs information about the embedding
sentence as well we would have to conclude that the algorithm for
calculating the meanings of complex expressions is more complicated
then we thought. But a complicated algorithm is still an algorithm and
the core explanation of how we understand complex expressions would
remain untouched. By contrast, if it turned out that in order to
interpret a sentence we must know all sorts of ephemeral
non-linguistic facts we would have to conclude that the fact that we
can reliably understand all sorts of unfamiliar sentences is a
mystery. Those who accept putative counterexamples of this latter kind
must provide alternative explanations for productivity and
systematicity.
Consider the following minimal pair:
A good translation of (1) into a first-order language is (1′).
But the analogous translation of (2) would yield (2′), which is
inadequate. A good translation for (2) would be (2″) but it is
unclear why. We might convert ‘\(\neg \exists\)’ to the
equivalent ‘\(\forall \neg\)’ but then we must also
inexplicably push the negation into the consequent of the embedded
conditional.
This gives rise to a problem for the compositionality of English,
since is seems rather plausible that the syntactic structure of (1)
and (2) is the same and that ‘if’ contributes some sort of
conditional connective—not necessarily a material
conditional!—to the meaning of (1). But it seems that it cannot
contribute just that to the meaning of (2). More precisely, the
interpretation of an embedded conditional clause appears to be
sensitive to the nature of the quantifier in the embedding
sentence—a violation of compositionality (the problem is
originally raised in Higginbotham 1986).
One response might be to claim that ‘if’ does not
contribute a conditional connective to the meaning of either (1) or
(2)—rather, it marks a restriction on the domain of the
quantifier, as the paraphrases under (1″) and (2″)
 suggest:[19]
But this simple proposal (however it may be implemented) runs into
trouble when it comes to quantifiers like ‘most’. Unlike
(3′), (3) says that those students (in the contextually given
domain) who succeed if they work hard are most of the students (in the
contextually relevant domain):
Section 6.4. of Stalnaker (2014) contains a detailed defense of the claim that this problem and related ones are properly handled if we use the semantics of Stalnaker (1975) for indicative conditionals. But the predictions depend essentially on the fact that on this account, conditionals are context-dependent and their interpretation is constrained by certain pragmatic principles. There are many alternative approaches on the market and the question how best account for these examples remains controversial.[20]
Consider the following minimal pair from Barbara Partee:
There is a clear difference between (4) and (5)—the first one is
unproblematic, the second markedly odd. This difference is plausibly a
matter of meaning, and so (4) and (5) cannot be synonyms. Nonetheless,
the first sentences are at least truth-conditionally equivalent. If we
adopt a conception of meaning where truth-conditional equivalence is
sufficient for synonymy, we have an apparent counterexample to
compositionality.
Few would insist that the first sentences of (4) and (5) are really
synonymous. What is interesting about this example is that even if we
conclude that we should opt for a more fine grained conception of
meaning, it is not immediately clear how that will account for the
contrast between these sentences. The difference is obviously due to
the fact that ‘one’ occurs in the first sentence of (4),
which is available as a proper antecedent for ‘it’ and
that there is nothing in the first sentence of (5) that could play a
similar role. Some authors have suggested that the right way to
approach this problem is to opt for a dynamic conception of meaning,
one that can encode anaphoric possibilities for subsequent sentences
(cf. Heim 1982; Groenendijk & Stokhof 1990, 1991; and Chierchia
1995).
Interesting though these cases might be, it is not at all clear that
we are faced with a genuine challenge to compositionality, even if we
want to stick with the idea that meanings are just truth-conditions.
For it is not clear that (5) lacks the normal reading of (4)—on
reflection it seems better to say that the reading is available even
though it is considerably harder to get. (Contrast this with an
example due to—I think—Irene Heim: ‘They got
married. She is beautiful.’ This is like (5) because the first
sentence lacks an explicit antecedent for the pronoun in the second.
Nonetheless, it is clear that the bride is said to be beautiful.) If
the difference between (4) and (5) is only this, it is no longer clear
that we must accept the idea that they must differ in meaning.
Suppose a Japanese maple leaf, turned brown, has been painted green.
Consider someone pointing at this leaf uttering (6):
The utterance could be true on one occasion (say, when the speaker is
sorting leaves for decoration) and false on another (say, when the
speaker is trying to identify the species of tree the leaf belongs
to). The meanings of the words are the same on both occasions and so
is their syntactic composition. But the meaning of (6) on these two
occasions—what (6) says when uttered in these
occasions—is different. As Charles Travis, the inventor of this
example puts it: “…words may have all the stipulated
features while saying something true, but also while saying something
false” (Travis 1994: 171–172; see also Travis 1996 and Lahav 1989).
There are many possible responses to this challenge. One is to deny
the relevant intuition. Perhaps the leaf really is green if it is
painted green and (6) is uttered truly in both situations.
Nonetheless, we might be sometimes reluctant to make such a true
utterance for fear of being misleading. We might be taken to falsely
suggest that the leaf is green under the paint or that it is not
painted at all (cf. Sainsbury 2001 and Berg 2002).
The second option is to point out that the fact that a sentence can
say one thing on one occasion and something else on another is not in
conflict with its meaning remaining the same. Do we have then a
challenge to compositionality of reference, or perhaps to
compositionality of content? Not clear, for the reference or content
of ‘green’ may also change between the two situations.
This could happen, for example, if the lexical representation of this
word contains an indexical
 element.[21]
 If this seems ad hoc, we can say instead that although there
is no context-dependent expression in (6) it can still be used to make
both true and false assertions. Perhaps the compositionally determined
occasion meanings are impoverished (perhaps not even propositional),
which is why they tend to be different from what speakers assert (cf.
Bach 1994 and Soames 2005).
Perhaps the most widely known objection to compositionality comes from
the observation that even if e and \(e'\) are synonyms, the
truth-values of sentences where they occur embedded within the clausal
complement of a mental attitude verb may well differ. So, despite the
fact that ‘eye-doctor’ and ‘ophthalmologist’
are synonyms (7) may be true and (8) false if Carla is ignorant of
this fact:
So, we have a case of apparent violation of compositionality; cf.
Pelletier (1994).
There is a sizable literature on the semantics of
 propositional attitude reports
 (see entry). Some think that considerations like this show that there
are no genuine synonyms in natural languages. If so, compositionality
(at least the language-bound version) is of course vacuously true.
Some deny the intuition that (7) and (8) may differ in
truth-conditions and seek explanations for the contrary appearance in
terms of
 implicature.[22]
 Some give up the letter of compositionality but still provide
recursive semantic
 clauses.[23]
 And some preserve compositionality by postulating a hidden indexical
associated with ‘believe’ (for example, Richard 1990, Crimmins & Perry
1989, and Crimmins 1992).