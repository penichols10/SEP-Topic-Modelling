Wesley Charles Salmon was born in Detroit on 9 August 1925, to Wallis,
a mechanical and electrical engineer, and Ruth Springer Salmon, a
schoolteacher. After completing primary and secondary school in
Detroit, and studying at Wayne University (now Wayne State University)
from 1943–44, in 1944 Salmon moved to the University of Chicago.
Taken with admiration for the charismatic minister of his
family’s Methodist church, Salmon entered the University of
Chicago Divinity School with the intention to become a minister
himself. However, as he writes in the autobiographical note published
in What? Where? When? Why? (1982), once in Chicago he left
his “belief in virtually every tenet of Christianity, as well as
all confidence in the social value of religion” (Salmon 1982c:
281). He then decided to turn to philosophy and in 1947 obtained his
MA in Philosophy with a thesis on Whitehead’s conception of
freedom.
In Chicago, Salmon was quite unhappy with the Thomistic focus of the
Philosophy department, where the scientifically oriented philosophy he
favored was somewhat disregarded, in spite of the presence there of
Rudolf Carnap, with whom at that time he did not have much
interchange. Closer interaction between them took place in 1963, while
Salmon was visiting the University of Minnesota Center for Philosophy
of Science for the Winter and Spring terms, and was given the
opportunity to pay a one week visit to Carnap in Los Angeles, together
with Herbert Feigl, and Grover Maxwell. That visit was the occasion
for daily discussions on the nature of confirmation and inductive
logic. Such a stimulating exchange fueled many writings, including the
masterpiece The Foundations of Scientific Inference (1967a),
whose fiftieth anniversary edition was re-published in 2017 with an
introduction by Christopher Hitchcock.
After receiving his MA, Salmon decided to move to UCLA. Although he
claimed to have taken that decision attracted by the warm Californian
climate, his choice proved decisive, because there he encountered Hans
Reichenbach, who supervised his Ph.D. dissertation on John Venn, and
who exercised a dominant influence on all of his subsequent work.
After obtaining his Ph.D. in 1950, Salmon held teaching positions at
Washington State College; UCLA; Northwestern University, and Brown
University.
In 1963 Salmon moved to the recently founded (1960) department of
History and Philosophy of Science of Indiana University, where his
philosophical attitude underwent an important turn, that he so
describes:
At Indiana, for the first time, I found myself dealing with graduate
students and colleagues who had strong scientific backgrounds and a
fair degree of scientific sophistication. It became essential for me
to treat philosophy of science, not as a discipline looking inward
upon other branches of philosophy, but as one which looks outward
toward the various scientific disciplines. […] I came to the
conviction, which I still hold, that philosophy which remains out of
contact with other disciplines runs the great risk of becoming quite
sterile. (Salmon 1982c: 282)
As a result of such developments, besides continuing to work on the
issues which had always been on his mind, namely induction,
confirmation, probability and the nature of scientific inference,
Salmon became deeply interested in the philosophy of space and time,
publishing the introductory book Space, Time, and Motion
(1975a), which can be considered a pearl of clarity; the collection
Zeno’s Paradoxes (1970a); and a number of subsequent
papers. During his years at Indiana University Salmon started working
also on explanation, a subject that remained at the heart of his
production for the rest of his life.
After his first marriage ended in divorce, in 1971 Salmon married
Merrilee Ashby, herself a philosopher of science. In 1973 Salmon and
his wife left Indiana and took positions at the University of Arizona,
to stay there until 1981 when they both moved to the University of
Pittsburgh. There, in 1983 Salmon was appointed University Professor
at the Philosophy department as a successor of Carl Gustav Hempel, a
position he held until his retirement in 1999. In Pittsburgh
philosophy of science was flourishing, thanks to a large community
belonging not only to the department of Philosophy, but also the
department of History and Philosophy of Science, the Center for
Philosophy of Science, and the Philosophy department of Carnegie
Mellon University. As a member of such a lively philosophical
community Salmon enjoyed a most congenial atmosphere for twenty
extremely productive years, leading to the publication of the book
Scientific Explanation and the Causal Structure of the World
(1984), a milestone of the literature on scientific explanation and
the apex of Salmon’s work on the topic, the collection, edited
with Philip Kitcher, Scientific Explanation (1989), which
included as a chapter the essay “Four Decades of Scientific
Explanation”, to be published one year later as a book (1990a),
and a long series of articles which later merged into the two
collections Causality and Explanation (1998) and Reality
and Rationality, edited by P. Dowe and M.H. Salmon, which
appeared posthumously in 2005.
Along the years Salmon gave lectures and courses as a visiting
professor in many universities and other prestigious institutions
around the world, including the university of Bologna, where in 1988
he delivered a lecture course on “Forty years of scientific
explanation”; the university of Melbourne (1978), and Kyoto
University, where in the Spring 2000 he last taught for one term.
Salmon was also very active in promoting philosophy of science, and
served as President of the Logic, Methodology and Philosophy of
Science division of the International Union of History and Philosophy
of Science (now IUHPST: International Union of History and Philosophy
of Science and Technology) from 1996 to 2000.
On 22 April 2001 Salmon died in a car accident on his way home from a
family visit in Indiana. 
From his first book Logic (1963a), which was translated into
Spanish, Japanese, Italian, Portuguese, and German, to his last
publication, the article “The Causal Structure of the
World” (2010), Salmon published fifteen authored and edited
books, and over two hundred articles, reviews and comments of other
authors. All of his publications, written in a crystal clear style,
are characterized by rigorous argumentation and original thought.
Introductory books like Logic, Space, Time and
Motion, and Foundations of Scientific Inference are
masterpieces which have served as reference books for generations of
students and researchers operating in various fields, while
Statistical Explanation and Statistical Relevance,
Scientific Explanation and the Causal Structure of the World,
Explanation and Causality, and Reality and
Rationality are unparalleled sources of inspiration for
philosophers of science working on explanation, causality, and
scientific rationality.
Salmon regarded probability as an essential component of science and
human knowledge at large, and induction as the fundamental ingredient
of scientific method. In so doing, he followed in the steps of his
mentor Reichenbach, of whom he said: 
Just as Hume was the great empiricist of the eighteenth century, so it
may be, will Reichenbach be remembered as the great empiricist of the
twentieth century. (Salmon 1979b: 2–3) 
David Hume is undoubtedly a major source of inspiration for both
Reichenbach and Salmon, who placed themselves in the empiricist
tradition, and shared Hume’s conviction that prediction is the
main task of science, to be fulfilled using induction. By embracing a
probabilistic version of inductivism Reichenbach and Salmon intended
to make a step forward in the direction indicated by Hume. 
Reichenbach was one of the first to criticize the verifiability theory
of meaning embraced by the Viennese school on account that 
it would be illusory to imagine that the terms ‘true’ or
‘false’ ever express anything else than high or low
probability values. (Reichenbach 1936: 156) 
This brought probability to the foreground. Reichenbach embraced an
empirical approach to probability and maintained that probability
values must be determined on the basis of experience alone. What can
be extracted from experience are frequencies, and it is on their basis
that probability must be fixed. Reichenbach called the method for
obtaining probability values induction by enumeration. This
consists in counting the relative frequency of a given attribute in
the initial section of a sequence of observations and inferring that
such frequency will persist approximately for the rest of the sequence
when this is indefinitely prolonged. This procedure forms the content
of the canon according to which the frequency interpretation
prescribes fixing prior probabilities, a tenet Reichenbach called
Rule of induction:
if the sequence has a limit of the frequency, there must exist an
n such that from there on the frequency \(f^{i} (i \gt n)\)
will remain within the interval \(f^{n} \pm \delta\), where \(\delta\)
is a quantity that we can choose as small as we like, but that, once
chosen, is kept constant. Now if we posit that the frequency \(f^{i}\)
will remain within the interval \(f^{n} \pm \delta\), and we correct
this posit for greater n by the same rule, we must finally come
to the correct result. (Reichenbach 1935 [1949: 445])
Every probability statement obtained by means of this method is a
wager, or a posit, namely a statement allowing us to infer
unknown from known frequencies in such a way that, if the sequence of
observations has a limit, the method will assure convergence to a
unique result.
Salmon wholeheartedly embraced the frequency interpretation, and in
particular Reichenbach’s version of it, which embodies a way of
making single case probability assignments. The key notion in this
regard is that of weight, taken to represent the predictive
value of sentences referring to single events. The idea is that the
weight assigned to such sentences derives from the probabilities
attached to the reference class to which the event in
question belongs. The reference class should obey a criterion of
homogeneity, namely it should be chosen so as to include as
many cases as possible similar to the one under consideration,
excluding dissimilar ones. While Reichenbach recommended choosing
“the narrowest class for which we have reliable
statistics” (Reichenbach 1938: 316) Salmon opted for “the
broadest homogeneous reference class”, because 
we do not want to try to refer single cases to classes that are too
narrow, for if we do we will not have enough evidence upon which to
base our inference […] at the same time, we want our reference
class to contain other relevant cases, not irrelevant ones.
Statistical relevance is the key concept here. (Salmon 1967a:
91) 
The reference class containing all and only the relevant properties is
objectively homogeneous. Salmon is well aware that
it would be most unrealistic to suppose that we can fulfill the
requirement of selecting the broadest homogeneous reference class in
all cases in which we have to make practical decisions about single
events. We may suspect that a given reference class is inhomogeneous,
but not know of any way to make a relevant partition of it. Under
these circumstances let us say that the class is epistemically
homogeneous. […] Sometimes we know that a reference class
is inhomogeneous, but it would simply be impractical to carry out a
relevant subdivision. […] Under these circumstances, let us say
the reference class is practically homogeneous. (Salmon
1967a: 92)
As we shall see
 (§ 5.2),
 in addition to playing a crucial role in scientific inference by
providing the key to singular prediction, statistical relevance is no
less essential for Salmon’s conception of explanation, and the
same holds for reference class homogeneity.
Deeply convinced of the indispensability of induction, Salmon
maintained that 
there is a crucial sense in which the logic of science is inescapably
inductive, and that a justification of induction is essential to a
full understanding of the logic of science. (Salmon 1968a: 24) 
To Popper’s uncompromising deductivism he opposed the claim that
corroboration, the core of Popper’s falsificationist
methodology, is a non-demonstrative kind of inference:
“Modus tollens without corroboration is empty;
modus tollens with corroboration is induction” (Salmon
1968a: 28; see also Salmon 1981 [1988a]). A number of Salmon’s
papers published in the Fifties and Sixties argued against various
attempts to deny or dissolve the problem of justifying induction,
including those of A.J. Ayer, M. Black and P. Strawson (Salmon 1957,
1963b, 1965, 1978b). By contrast, he took seriously Hume’s
argument showing the impossibility of giving a logical solution to the
problem of the justification of induction. Adopting Herbert
Feigl’s distinction between validation and vindication (Feigl
1950), Salmon embraced a pragmatic approach according to which
induction is justified in terms of the knowledge-extending function to
which it happens to be necessary.
In a similar vein, Reichenbach (1935 [1949] and 1938) had set himself
the task of justifying induction as the best possible method for
prediction and action. Given his commitment to frequentism, he meant
to show that the rule of induction satisfies the “principle of
the greatest number of successes”, namely that it leads us to
act in the most successful way possible. Accordingly, the rule of
induction is justified on account that if successful
predictions are attainable at all, its application will assure the
attainment of that goal. Reichenbach’s justification relies on
the self-correcting character of the method, whose repeated
application guarantees convergence to a unique value in the long run
whenever there exists a limit of the relative frequency. However, his
argument applies to a whole class of asymptotic rules, all of which
satisfy the convergence property. The thing is that one can add to the
relative frequency a corrective term which modifies the observed
frequency to obtain a slightly different posit. The particularity of
the rule of induction is that it makes no use of the corrective term.
Well aware of the fact that his argument applied to an infinite class
of asymptotic rules, Reichenbach privileged the rule of induction on
the grounds of its descriptive simplicity. This applies to
descriptions (statements) which are semantically equivalent, as
opposed to inductive simplicity which applies when one of two
or more descriptions, or hypotheses, which are not semantically
equivalent, is believed to be more likely to be true. 
Salmon praised Reichenbach’s approach for not assuming that
there exists an order in nature: 
the whole force of the justification is that the use of induction is
reasonable whether or not nature is uniform, whatever may be meant by
the assertion “Nature is uniform”. (Salmon 1953: 48) 
Nevertheless, he found fault with Reichenbach’s argument for the
justification of the rule of induction. Both descriptive and inductive
simplicity apply to statements, not to rules, and even if one wanted
to apply them to rules, one would have to consider that asymptotic
rules do not converge uniformly, which means that they “are not
in any sense empirically equivalent” (Salmon 1963c: 28).
Simplicity would be a sufficient criterion for justifying the choice
of a particular rule from the class of asymptotic rules only if such
rules were empirically equivalent, but they are not. Salmon set
himself the task of spelling out the conditions under which all
asymptotic rules could be rejected except the rule of induction. 
His first move was to fix two regularity, or
normalizing, conditions imposing that no relative frequency
\(m/n\) observed in any initial section of a sequence can be negative,
and that for every possible n so obtained all the corresponding
values of \(m_i\) must add up to one. He then added a requirement of
linguistic invariance stating that the inferences made by
means of a given rule must be invariant with respect to the language
in which the evidence taken into consideration is expressed. In other
words, what matters to inductive inferences is the content of evidence
statements, not their linguistic form. For a while Salmon thought that
these requirements could fulfill the purpose for which they had been
proposed. He was able to demonstrate that the normalizing conditions
could eliminate a large subclass of Reichenbach’s asymptotic
rules (Salmon 1956), and that the criterion of linguistic invariance
could do the same with all the methods belonging to Carnap’s
continuum, except the one Carnap called the straight
rule—alias the rule of induction (Salmon 1961). Among other
things, the criterion of linguistic invariance allowed Salmon to
suggest an original solution to Goodman’s paradox (Salmon
1963c).
Salmon’s hope to have solved the problem left open by
Reichenbach’s argument for the justification of inductive
methods was soon disappointed, especially after the criticism raised
by Ian Hacking (1965 and 1968). Hacking showed that the three
conditions of consistency, symmetry and
invariance taken together are necessary and sufficient to
select the rule of induction among the class of asymptotic rules.
Hacking’s consistency and invariant conditions are stronger
versions of Salmon’s requirements of regularity and linguistic
invariance, while the symmetry condition (corresponding to the
property usually called exchangeability) requires that for
any given value of the relative frequency observed in a sample, the
posited value of the limiting frequency must be insensitive to the
order in which the items of the sample were observed to occur. This
requirement is commonly accepted by the upholders of the subjective
interpretation of probability, but for a frequentist like Salmon, who
held that the evaluation of probability must be entirely based on
empirical data, symmetry (exchangeability) can only mean a factual
assumption on the nature of the population under study. This clashes
with a justification in tune with Reichenbach’s perspective. As
Salmon observed, Reichenbach’s rule of induction 
is a method for primitive knowledge, and this is what he was
attempting to justify. Thus, he would argue, since we have no results
of previous inductions to establish these factual assumptions, we are
not entitled to make them. (Salmon 1991: 117)
In order to overcome this difficulty Salmon put forward a new argument
based on Reichenbach’s distinction between a context of
discovery and a context of justification. The decision
to examine a certain sequence to calculate the relative frequency of
its attributes, Salmon claimed, belongs in the context of discovery,
and in addition 
the use of the rule of induction to arrive at a value to
posit is also part of the context of discovery; at the same time, it
looks like part of the context of justification as well, for the posit
is justified by virtue of the rule of induction. (Salmon 1991:
117–118) 
The interplay between the two contexts offers the key to attempt a
solution, because assumptions introduced as hypotheses at the
discovery level are to be confirmed or rejected at the justification
level. The symmetry condition would then represent a wager in the
context of discovery, to be tested in the context of justification.
The conclusion Salmon reached is that:
Reichenbach sought to solve Hume’s problem of the justification
of induction by means of a pragmatic vindication that relies heavily
on the convergence properties of his rule of induction. His attempt to
rule out all other asymptotic methods by an appeal to descriptive
simplicity was unavailing. We found that important progress in that
direction could be made by invoking normalizing conditions
(consistency) and methodological simplicity (as a basis for
invariance), but that they did not do the whole job. I am proposing
that, in the end, Reichenbach’s own distinction between
discovery and justification holds the key to the solution. (Salmon
1991: 119)
The methodological simplicity mentioned in this passage was
suggested to Salmon by John Clendinnen. It is a version of simplicity
that applies to rules instead of statements, and requires the adoption
of “the simplest system of predicting rules which are compatible
with, and exemplified in, the set of known facts” (Clendinnen
1982: 20). Salmon accepted Clendinnen’s suggestion as a
significant, if not decisive, contribution to the justification of
induction (Salmon 1982c and 1991).
In The Foundations of Scientific Inference Salmon compared
the major interpretations of probability against three criteria:
admissibility, ascertainability, and applicability.
Admissibility requires satisfaction of the formal properties
of probability spelled out by the probability calculus;
ascertainability demands that there must exist a method of
ascertaining the values of probability; applicability
stipulates that the concept of probability must have a practical
predictive import.
Laplace’s classical interpretation was deemed defective
with respect to admissibility, because it suffers from the difficulty
known as “Bertrand’s paradox”, which applies to
problems involving an infinite range of possibilities, in which case
the classical definition can lead to conflicting probability values.
Salmon also considered the subjective interpretation,
according to which probability “is simply a measure of degree of
belief”, to be inadmissible (Salmon 1967a: 68). Salmon dealt
separately with the interpretation he called personalistic,
which he attributed to Savage. The distinctive feature of this
approach—matching what is meant by the subjective interpretation
today – is that it appeals to the betting scheme in order to
determine probability values, and imposes coherence on systems of
bets. Salmon claimed that the personalistic outlook satisfies the
criteria of admissibility and ascertainability, but does not comply
with applicability because it 
leaves entirely unanswered our questions about inductive inference. It
tolerates any kind of inference from the observed to the
unobserved. (Salmon 1967a: 82) 
Applicability also represented an obstacle for the logical
interpretation, which was reputed “to provide no basis for
expecting the probable in preference to the improbable” (Salmon
1967a: 79). In Salmon 1967a and elsewhere, notably Salmon 1967b, 1969
[2005], and 1975b [2005], Salmon discussed in great detail
Carnap’s inductive logic, which he rated highly for shedding
light on a number of important aspects of confirmation and induction.
At the same time, he deemed that 
the conception of inductive logic as based upon a logical relation is
fundamentally misconceived […] although deductive logic
requires and exploits logical relevance relations, induction is
involved with factual relevance relations instead. (Salmon 1969 [2005:
202]) 
Moreover, Salmon disagreed with Carnap’s justification of
induction in terms of inductive intuition (Carnap 1968). Salmon held
the frequency interpretation to come closest to satisfying
all of the adequacy requirements, although it faced serious
difficulties with ascertainability, especially in connection with
single case probability attributions. However, Salmon deemed the
solution promising in terms of weight referred to homogeneous
reference classes
 (§ 2.1).
 In addition, he regarded the frequency theory as the only
interpretation offering a viable approach to the justification of
induction. 
In 1979d Salmon discussed the propensity interpretation put
forward by Popper to solve the single case problem arising within
quantum mechanics. The idea is that probability should be taken as a
dispositional property of the experimental set-up, or the generating
conditions of experiments, liable to be reproduced over and over again
to form a sequence (Popper 1959). Salmon argued that Popper’s
proposal fares no better than the frequentist way of attaching
probability values to single occurrences of events. In fact, the
difficulties faced by frequentists in connection with identifying
homogeneous reference classes are displaced rather than solved by the
propensity account, which requires completeness of information in
order to describe the chance set-up generating propensities.
In addition, Salmon endorsed Paul Humphreys’ objection that
propensities cannot qualify as probabilities because their
dispositional character ascribes them a peculiar asymmetry that goes
in the opposite direction from that characterizing inverse
probability, making the propensity theory inapplicable to Bayes’
rule (Humphreys 1985). To take Salmon’s example, 
suppose we are given a set of probabilities from which we can deduce
that the probability that a certain person died as a result of being
shot through the head is ¾. It would be strange, under these
circumstances, to say that this corpse has a propensity (tendency?) of
¾ to have had its skull perforated by a bullet. (Salmon 1979d:
213) 
While clashing with the symmetrical character of probabilities, the
asymmetry of propensities matches that of the causal relation. In view
of this, Salmon preferred to take propensities to be probabilistic
causal tendencies rather than probabilities (see
 § 5.6).
Further reading: On
Reichenbach see Salmon 1979a; Glymour & Eberhardt 2008 [2016]; and
the 2011 special issue of the journal Synthese 181(1). On the
interpretation of probability see Gillies 2000; Galavotti 2005; and
Hájek 2002 [2012]. The debate on context of discovery vs.
context of justification is surveyed in Schickore 2014.
Salmon tackled the issue of confirmation of scientific hypotheses in
Chapter VII of The Foundations of Scientific Inference and in
a number of papers including 1973 and 1975b [2005]. He called
attention to the difference between absolute confirmation,
taken to mean making highly probable, and incremental, or
relevance confirmation, taken to mean increasing the
probability of some hypothesis. According to Salmon, the distinction,
already made by Carnap in Logical Foundations of Probability,
was largely overlooked by subsequent literature, although failure to
appreciate it can yield puzzling results. For instance, Carnap showed
that it can be the case that two pieces of evidence incrementally
confirm a given hypothesis, while their conjunction may disconfirm it.
In addition, Salmon pointed out that a piece of evidence that
logically entails the falsity of the conjunction of two hypotheses
could incrementally confirm each of them. In view of this and other
puzzles, Salmon emphasized that “we have said very little when
we have stated merely that a hypothesis h has been confirmed by
evidence i” (Salmon 1975b [2005: 229]). Conversely, by
appealing to Bayes’ rule “we can aspire to a much fuller
understanding of relations of confirmation (in both the absolute and
the relevance senses)” (Salmon 1975b [2005: 236]).
Like Reichenbach, Salmon assigned Bayes’ rule a crucial role in
the confirmation of scientific hypotheses, convinced that the
hypothetico-deductive (H-D) method dear to logical empiricism
“is a gross oversimplification” (1968b [2005: 72]) unable
to capture essential aspects of confirmation. His main reason for
rejecting the H-D method was its inability to accommodate
plausibility judgments, even though many examples from the
history of science show that they have entered the choice between
alternative hypotheses. It is precisely for that reason, Salmon
argued, that many philosophers have denied that plausibility
considerations belong to the logic of science, or claimed that they
are part of the context of discovery rather than the context of
justification. On the contrary, for Salmon plausibility considerations
“are pervasive in the sciences; they play a
significant—indeed, indispensable—role”
(Salmon 1990d [2005: 98]). In particular, they can contribute to
fixing the prior probability of hypotheses and hence fit very well in
the Bayesian scheme and as such belong in the context of
justification. Incidentally, Salmon considered Reichenbach’s
distinction between a context of discovery and a context of
justification, which a number of authors deem debatable, not only
fruitful, but even essential for a proper understanding of the nature
of scientific knowledge.
Salmon followed Reichenbach in embracing an objective version of
Bayesianism according to which prior probabilities should be
determined on the basis of objective and empirical criteria. Prior
probabilities represent the “best estimates of the frequencies
with which certain kinds of hypotheses succeed” (Salmon 1990d
[2005: 102]), and must be fixed by means of (objective) criteria such
as simplicity, symmetry and analogy. Analogy, in particular, is apt to
suggest comparisons with other similar theories, whose rate of success
in the past can be assessed. Salmon conceded that priors are sometimes
the expression of rough estimates, and can be stated in terms of
interval probabilities. In any case, he regarded prior probabilities
as an essential component of hypothesis confirmation, as testified by
many cases offered by the history of science. The fruitfulness of
assuming a Bayesian perspective in studying the history of science is
strongly emphasized:
Without the Bayesian analysis, one could say that the study of the
history of science might have some (at least marginal) heuristic value
for the scientist and philosopher of science; but with the Bayesian
analysis, the data provided by the history of science constitute,
in addition, an essential segment of the evidence relevant to
the confirmation or disconfirmation of hypotheses. Philosophers of
science and creative scientists ignore this fact at their peril.
(Salmon 1970c [2005: 92])
Salmon identified the decisive advantage of the Bayesian method over
the H-D with its being a tool for theory comparison, for which reason
he recommended adopting its formulation in terms of ratios:
where \(T_1\) and \(T_2\) are two alternative hypotheses, and E is a
piece of evidence. Given the pivotal importance Kuhn ascribed to
theory comparison, Salmon regarded the Bayesian method as a way of
bridging the gap between the vision of science as an objective and
rational enterprise, embraced by logical empiricists, and Kuhn’s
critique of such view (Salmon 1990d [2005]). 
Further reading: On
confirmation see Crupi 2013 [2016] and Talbott 2001 [2016].
Hájek & Hitchcock (eds.) 2016 contains a number of essays on
probability, confirmation, probabilistic causation, and related
issues.
Salmon took an interest in the philosophy of space and time while
attending Reichenbach’s courses as a graduate student at UCLA,
and continued working on the topic for many years. His first
publication in the field was the collection of essays Zeno’s
Paradoxes (1970a), including contributions by Abner Shimony,
Bertrand Russell, Henri Bergson, Max Black, J.O. Wisdom, James
Thomson, Paul Benacerraf, G.E.L. Owen, and Adolf Grünbaum, plus a
long introduction and an appendix on “Sets and Infinity”
by Salmon himself. As stated at the beginning of the introduction,
Salmon’s goal in bringing together the essays forming the
collection was systematic rather than historical. His motivation lay
in the conviction that after a long period in which Zeno’s
paradoxes had been regarded as mere sophisms, from the middle of the
Nineteenth and even more in the Twentieth century they had become the
focus of sophisticated philosophical discussion. The articles included
in the book tackle five of Zeno’s arguments, namely four
paradoxes of motion and the paradox of plurality. At the heart of the
paradoxes lies the problem of defining the notions of continuum and
infinity. According to Salmon, such paradoxes cannot be solved in
purely logical or mathematical terms because “it is also
necessary to show how the abstract mathematical system can be used for
the description of concrete physical reality” (Salmon 1970a:
16). It is worth noting that one of the chapters consists of a few
pages taken from Russell’s Our Knowledge of the External
World (1929) outlining his at-at theory of motion, which later
played a key role within Salmon’s theory of causal processes
(see
 § 5.5.2).
In 1975 Salmon published a second book called Space, Time and
Motion: A Philosophical Introduction (1975a) which contains a
masterly introduction to the topic. The four chapters of the book lead
the non-specialist reader through the essentials of the philosophy of
space and time from non-Euclidean geometries and Zeno’s
paradoxes to special relativity and simultaneity. The book, which
presupposes only an elementary background knowledge of geometry and
mathematics on the part of the reader, is meant as an invitation to
pursue the topics addressed in a more systematic and specialized way.
Salmon fully achieved this purpose, whose treatment of thorny subjects
succeeded in being both “scientifically sound” and
“intuitive and easy” (Salmon 1988c: 276).
Salmon addressed simultaneity in other writings, including the article
“The Philosophical Significance of the One-Way Speed of
Light” (1977c) which contains a defense of simultaneity’s
conventionality, dating back to Einstein and argued for in detail by
Reichenbach (1928 [1957]). After surveying a number of experimental
methods devised for the measurement of the speed of light, from
Galileo to J. Bradley, H.L. Fizeau, and J.L. Foucault, Salmon went on
to discuss synchrony, arguing that a conventional element is involved
in all of such accounts. Since simultaneity rests on synchrony and
ultimately on the one way speed of light, by showing that the latter
cannot be ascertained experimentally Salmon intended to “give
substance to the abstract conventionality issue” (Salmon 1977c:
255), namely to Einstein’s claim that the relation between the
speed of light in two different directions is not a matter that can be
established empirically, but only by convention.
Further reading: For a
classical introduction to the philosophy of space and time see
Grünbaum 1973. A more recent account is to be found in Malament
2012. A collection of essays on the key problems of the philosophy of
physics is contained in Batterman (ed.) 2013.
Salmon tackled explanation in a highly original manner which left a
most remarkable legacy. He worked intensively on the topic for the
last thirty years of his life, refining his approach in response to
comments and critiques raised by a number of authors, and shedding new
light on various details. At the turn of the Seventies, when Salmon
published his groundbreaking paper “Statistical
Explanation” (Salmon 1970b [1971]) the literature was dominated
by Hempel’s “covering law” model of explanation,
which held sway since the Forties, and is often referred to as the
received view. Salmon’s motivation for developing an
alternative view of explanation stemmed from his dissatisfaction with
certain aspects of Hempel’s account. In Four Decades of
Scientific Explanation (Salmon 1990a) Salmon discussed a number
of counterexamples showing that the requirements Hempel imposed on
explanation, and more in particular the requirement of maximal
specificity and the requirement of high inductive
probability of the explanandum, are neither necessary nor
sufficient to identify a tenable account of scientific explanation. In
short, Salmon’s discontents focused on the impossibility of
explaining low probability events, the secondary role assigned to
causality, the thesis of the symmetry between explanation and
prediction, the epistemic relativization of statistical explanation,
and the insufficient importance ascribed to the notion of relevance.
Unlike Hempel, Salmon did not regard statistical explanation as
somewhat incomplete compared to deductive explanation, taken as the
optimum. By contrast, his approach to explanation takes statistical
generalizations to be the general case, with explanations making use
of universal generalizations as a special case. A pivotal role is
assigned to the notion of relevance, more particularly statistical
relevance, placed at the core of the Statistical-Relevance
(or S-R) model. Equal importance is assigned to causality
interpreted in a probabilistic fashion. By the Seventies,
probabilistic causality had already attracted the attention of a
number of authors including Hans Reichenbach (see Reichenbach 1956);
building on this work Salmon intended to revive the mechanistic ideal
of explanation, convinced that the time had come to put the
“cause” back into “because”.
The notions of statistical relevance and probabilistic
causality inform the two components of Salmon’s theory,
namely the S-R model, aimed at identifying the network of statistical
relations holding between the properties relevant to the events to be
explained, and causal explanation, aimed at locating such
events within the mechanisms responsible for their occurrence (Salmon
1984).
According to the S-R model, in order to explain an event one must
exhibit all the factors that are statistically relevant to its
happening, without mentioning irrelevant elements, so that an
explanatory account should include only information that is genuinely
explanatory. In order to accomplish this task, the event explanandum
must be referred to a homogeneous reference class, namely to
the class containing all and only the relevant properties. Homogeneity
is obtained through statistically relevant partitions of
non-homogeneous reference classes into mutually exclusive and
exhaustive sub-classes. Statistical relevance, providing the tool to
attain homogeneity, is defined as follows. Let A stand for the
reference class including some event of which one wants to establish
the probability of possessing the property B; \(p (B \mid A)\)
be the probability of B within the reference class A,
and C be another property by which the class A can be
divided into two sub-classes \((A \amp C)\) and \((A \amp \msim C)\);
the property C is statistically relevant with respect to
B in A if and only if \(p (B \amp C \mid A) ≠ p (B
\mid A)\). A homogeneous partition of a reference class does not admit
further relevant partitions, and the resulting sub-classes must be
maximal, in the sense that no irrelevant properties are retained. 
In some cases partitioning a reference class A can result in
two sub-classes \((A \amp C)\) and \((A \amp \msim C)\) both
homogeneous with respect to a given property B. This means that
property B is held by all elements belonging to class \((A \amp
C)\), but none of the elements belonging to \((A \amp \msim C)\). In
all other cases the class A will be partitioned into k
homogeneous sub-classes \((A \amp C_{k})\) such that
for \(i ≠ j\). Such a procedure is the content of a rule of
multiple homogeneity which “expresses the fundamental
condition for adequate explanation of particular events” (Salmon
1970b [1971: 59]). Fulfillment of this rule guarantees against the
inclusion of irrelevant information in the explanatory account. Once
the process leading to the specification of a homogeneous reference
class is completed, the event to be explained is associated with a
probability distribution. The shift from a non-homogeneous to a
homogeneous reference class embodies the explanatory power of the S-R
model, according to which 
an explanation is an assemblage of factors that are statistically
relevant to the occurrence of the event to be explained, accompanied
by an associated probability distribution. (Salmon 1979c: 68) 
The shift in question involves an increase in information, although
not necessarily an increase in probability. Given that Salmon does not
require relevance to be positive, the procedure described can result
in a higher as well as a lower probability of the explanandum. In this
perspective what counts for the sake of explanation is not high
probability, as required by Hempel, but being in a position to assert
that the probability distribution associated with the explanandum
reflects the most complete and detailed information attainable. This
information is conveyed by a homogeneous partition of the reference
class, together with a statement specifying to which cell of that
partition the explanandum event belongs.
As observed
 (§ 2.1),
 the objective homogeneity of the reference class is an ideal not free
from difficulties, because one can hardly ever be sure to have taken
into account all relevant information. Well aware of this, Salmon
admits that in practice 
we often lack full knowledge of the properties relevant to a given
attribute, so we do not know whether our reference class is
homogeneous or not. (Salmon 1970b [1971: 44]) 
Therefore, in most cases use is made of epistemically
homogeneous reference classes, namely those relative to a given
knowledge situation.
Salmon rejected Hempel’s tenet that an explanation is an
argument, which he deemed the “third dogma of empiricism”
(Salmon 1977a [1998]). His main objection was that although
irrelevancies are harmless to arguments, they are fatal to
explanations. In Salmon’s words:
Inference, whether deductive or inductive, demands a requirement of
total evidence—a requirement that all relevant evidence
be mentioned in the premises. This requirement, which has substantive
importance for inductive inferences, is automatically satisfied for
deductive inferences. Explanation, in contrast, seems to demand a
further requirement—namely, that only considerations
relevant to the explanandum be contained in the explanans. (Salmon
1977a [1998: 104])
Furthermore, arguments are not well suited to account for explanatory
asymmetry, for there is a disparity of temporal asymmetry in
explanations and in arguments that makes the latter fit for prediction
and retrodiction, but unfit for explanation. Take for instance a lunar
eclipse, which can be predicted on the basis of the laws of motion and
a suitable set of initial conditions holding prior to it. The same
eclipse can also be retrodicted making use of posterior conditions and
the same laws. However, explanations are temporally asymmetric in a
very specific sense: they go from antecedent conditions to subsequent
events. Such asymmetry is not embodied by arguments, which often move
in the opposite direction. There emerges a divergence between
prediction, which is an inferential activity, and explanation, which
is not. 
The asymmetry of explanation reflects that of causation, which for
Salmon is the key to a satisfactory explanatory account. While
knowledge of correlations is usually sufficient to bolster prediction,
explanation requires more, namely establishing causal relations
between events. A favorite example mentioned by Salmon is that of
“the barometer and the storm”. Based on the correlation
between the behavior of the barometer and the occurrence of storms,
one can predict a storm after having observed a sudden drop in the
barometer, but nobody would say that the drop in the barometer
explains the storm. In similar situations one would look for causal
information, such as the sudden drop in atmospheric pressure in the
area surrounding the occurrence of the storm. On the one hand, not all
statistically relevant properties convey causal information; on the
other, statistical correlations themselves invoke an explanation. In
view of this, in the course of the Seventies Salmon gradually came to
the conclusion that the S-R model cannot substantiate a satisfactory
account of explanation. In a paper entitled “Why Ask
‘Why?’?” he maintained:
I no longer believe that the assemblage of relevant factors provides a
complete explanation—or much of anything in the way of an
explanation. We do, I believe, have a bona fide explanation of an
event if we have a complete set of statistically relevant factors, the
pertinent probability values, and causal explanations of the relevance
relations (1978a [1998: 137]). 
First of all, a theory of probabilistic causal explanation requires
that a distinction can be made between statistical and causal
relevance. The tool for that purpose is the screening off
relation, defined as follows. Going back to the example of the
barometer and the storm, let B stand for the barometer drop,
S for the storm, and P for the drop in atmospheric
pressure. There is a correlation between B and S, so
that \(p (S \mid B) \gt p (S)\)—namely B is statistically
relevant to S. But if we take into account P, we see
that \(p (S \mid P \amp B) = p (S \mid P)\), namely B becomes
irrelevant to S in the presence of P. This means that
P screens off B from S. It should not pass
unnoticed that the screening off relation is asymmetrical; in our
example B does not screen off P from S, for \(p
(S \mid P \amp B) ≠ p (S \mid B)\). Salmon formulates a
screening off rule requiring that those properties which are
screened off by other properties are removed from the reference class.
This makes screening off the canon guiding the search for homogeneous
reference classes. Moreover, by virtue of its asymmetric character the
screening off relation forges a bridge between statistical and causal
relevance, obviously taken in a probabilistic sense.
Embedded in the Principle of the common cause, that Salmon
borrowed from Reichenbach, screening off has the capacity to discern
genuine from spurious causal links. In Reichenbach’s
formulation, such principle states that “if an improbable
coincidence has occurred, there must exist a common cause”
(Reichenbach 1956: 157). Reichenbach named the structure underlying
the principle of the common cause conjunctive fork, to be
defined as follows. Take two events A and B which happen
simultaneously more frequently than would be expected on the basis of
pure chance. Then we have that \(p (A \amp B) \gt p (A) \times p
(B)\), namely the two events are not independent. If in the presence
of a third event C the correlation between A and
B is absorbed, so that the two events become reciprocally
independent if taken relative to C and \(\msim C\), we have a
conjunctive fork. For a conjunctive fork the following relations
hold:
In other words, the common cause C screens off irrelevant
properties from their effects. Salmon offers many examples of
conjunctive forks. Here is one: 
Suppose that two siblings contract mumps at the same time, and assume
that neither caught the disease from the other. The coincidence is
explained by the fact that they attended a birthday party and, by
virtue of being in the same locale, both were exposed to another child
who had the disease. This would constitute a typical example of a
conjunctive fork. (Salmon 1984: 164) 
Conjunctive forks possess a peculiar asymmetry, namely they are open
to the future, never to the past. In Salmon’s words: 
Since the statistical relations found in conjunctive forks are said to
explain otherwise improbable coincidences, it follows that such
coincidences are explained only in terms of common causes, never
common effects. (Salmon 1984: 163) 
Conjunctive forks provide the connection between the S-R model and
causal explanation.
For several years Salmon thought that the combination of screening off
and the common cause principle offered a solid basis on which causal
explanation could be made to rest, but he later revised his position,
after criticism from a number of authors including Bas van Fraassen
(1977 and 1982). In order to cope with the difficulties besetting
conjunctive forks Salmon formulated another type of fork, called
interactive. Interactive forks depict causal interactions
whose effects remain correlated even in the presence of the common
cause; in other words the common cause does not screen off one effect
from the other. The structure of interactive forks is analogous to
that of conjunctive forks, with the difference that in this case the
equality 
does not hold, and in its place we have the inequality
To exemplify interactive forks Salmon mentions Compton scattering:
If, for example, an energetic photon collides with an electron in a
Compton scattering experiment, there is a certain probability that a
photon with a given smaller energy will emerge, and there is a certain
probability that the electron will be kicked out with a given kinetic
energy. […] However, because of the law of conservation of
energy, there is a strong correspondence between the two energies:
their sum must be close to the energy of the incident photon. Thus,
the probability of getting a photon with energy \(E_1\) and an
electron with energy \(E_2\), where \(E_1 + E_2\) is approximately
equal to \(E\) (the energy of the incident photon), is much greater
than the product of the probabilities of each energy occurring
separately. (Salmon 1978a [1998: 133])
Although, in the absence of screening off, in interactive forks the
common cause does not absorb the dependency between the effects, they
play a crucial role within Salmon’s approach. Before this can be
clarified some more notions must be introduced.
The cornerstone of Salmon’s theory of probabilistic causality is
the notion of a causal process, defined as a spatio-temporal
continuous entity having the capacity to transmit “information,
structure and causal influence” (Salmon 1994b [1998: 253]; see
also Salmon 1984: 154–157). Processes are responsible for causal
propagation, and provide the links connecting causes to
effects. By opting for continuous processes, instead of causal chains
conceived as collections of events, Salmon diverged from other
theories of probabilistic causality, such as those put forward by
Patrick Suppes, Irving John Good and Hans Reichenbach. Salmon was
deeply convinced that the notion of a causal process can account for
many puzzling cases, which other theories find difficult to handle
(see Salmon 1980 [1998]). Furthermore, he regarded causal processes as
“the kinds of causal connections Hume sought but was unable to
find”, holding that “such connections do not violate
Hume’s strictures against mysterious powers” (Salmon 1990b
[1998: 71]). In order to characterize causal propagation, or
transmission, Salmon borrowed from Reichenbach the concept of mark
transmission, and from Bertrand Russell the at-at theory
of causal propagation.
Reichenbach introduced the mark method in The Philosophy
of Space and Time (1928 [1957]) to distinguish causal processes
from pseudo-processes. Unlike the latter, causal processes have the
capacity to transmit marks, namely various sorts of signals or
information. Salmon’s favorite example is that of a rotating
spotlight placed at the center of a circular room, which casts a spot
of light on the wall. The light beam that travels from the source to
the wall is a causal process, whereas the light spot that moves around
the wall is a pseudo-process. In order to convince us that the light
ray is a genuine process, Salmon invites us to consider what happens
if a red filter is placed near the source of the beam: the color of
the beam will turn red, and so will the spot on the wall. By
interposing a red filter between the source and the beam a mark has
been introduced, which is then transmitted along the beam. By
contrast, the moving spot on the wall is not a causal process, because
it lacks the capacity to transmit information; if for instance someone
were to place a piece of red cellophane on the wall at some point, the
light would turn red when the beam hit the cellophane, but the color
would not be retained as the spot moves on. 
As Salmon emphasized, it is the ability to transmit marks
that characterizes processes, not the fact that they actually do
so:
a process is causal if it is capable of transmitting a mark,
whether or not it is actually transmitting one. The fact that
it has the capacity to transmit a mark is merely a symptom of the fact
that it is actually transmitting something else. That other something
I described as information, structure, and causal influence. (Salmon
1994b [1998: 253])
In order to account for mark transmission without violating the
strictures of Hume’s critique and being exposed to the suspicion
of introducing some occult causal power, Salmon drew inspiration from
the theory of causal lines put forward by Russell in
Human Knowledge: Its Scope and Limits (1948). According to
Salmon, Russell came very close to conceiving a notion of a causal
process similar to his own, when he stated that
a causal line may always be regarded as the persistence of
something—a person, table, a photon, or what not. Throughout a
given causal line, there may be constancy of quality, constancy of
structure. […] That there are such more or less self-determined
causal processes is in no degree logically necessary, but is, I think,
one of the fundamental postulates of science. (Russell 1948: 459,
quoted from Salmon 1984: 144)
Although not completely satisfied with some aspects of Russell’s
account, Salmon retained his at-at theory of motion, which he
summarized as follows: 
to move from A to B is simply to occupy the intervening
points at the intervening instants. It consists in being at
particular points of space at corresponding moments. (Salmon
1984: 153) 
Applying the at-at theory to causal processes having the capacity to
transmit marks results in stipulating that the transmission of a mark
from a given point A in a process to some point B in the
same process simply consists in the fact that it appears at each point
between A and B. Incidentally, as observed earlier
 (§ 4)
 Salmon found Russell’s at-at theory of motion a satisfactory
solution to Zeno’s arrow paradox (Salmon 1984: 151–153).
To the definition of processes in terms of the capacity to transmit a
mark, Salmon added a counterfactual clause, to the effect that: 
A mark that has been introduced into a process by means of a
single intervention at point A is transmitted to point B
if and only if it occurs at B and at all stages of the process
between A and B without additional interventions.
(Salmon 1977b [1998: 197], italics original; see also Salmon 1984:
148) 
This move was made necessary to obviate a problem connected with a
counterexample suggested by Nancy Cartwright. This can be summarized
as follows: back to the example of the rotating spotlight, suppose
that a few nanoseconds before a piece of red cellophane is placed on
the wall and turns the moving spot red someone places a red lens on
the rotating beacon, so that the beam remains red, not because of the
cellophane placed on the wall, but because of the lens placed near the
light source. In Salmon’s words: 
in such a case the spot turns red owing to a local
interaction and remains red without any additional local
interaction. With or without the intervention on the wall, the
spot of light moving around the wall would have been red from that
point on. Considerations of such cases required a counterfactual
formulation of the principle of mark transmission. (Salmon 1994b
[1998: 252]) 
Notably, Salmon did not associate counterfactuals with the semantic
view in terms of possible words, adopting instead an experimental
interpretation according to which counterfactual statements make
reference to observed statistical relations which can be tested by
controlled experiments (Salmon 1984: 149–150). 
Salmon’s view of processes has been criticized in various ways.
In particular, Philip Kitcher (1989) found the experimental view of
counterfactuals no more satisfactory than the attempts made by various
authors to justify counterfactuals on semantic grounds. Phil Dowe also
expressed dissatisfaction with Salmon’s account of causal
processes, and suggested mark transmission to be abandoned in favor of
an alternative definition in terms of conserved quantities,
later adopted by Salmon.
The process theory of causality in terms of conserved quantities
proposed by Dowe is based on the following:
A world line is “the collection of points on a
space-time (Minkowski) diagram which represents the history of an
object”, and a conserved quantity “is any
quantity universally conserved according to current scientific
theories” (Dowe 1992: 210; see also Dowe 1995 and 2000).
Examples of conserved quantities mentioned by Dowe are mass-energy,
linear momentum, angular momentum, and charge. Salmon regarded with
favor Dowe’s proposal and accepted a slightly modified version
of it, according to which
A process transmits a conserved quantity between A and
B \((A ≠ B)\) if and only if it possesses [a fixed
amount of] this quantity at A and at B and at every
stage of the process between A and B without any
interactions in the open interval (A,B) that involve an exchange of
that particular conserved quantity. (Salmon 1997: 462, italics
original)
Salmon deemed a decisive advantage of conserved quantities over mark
transmission the fact that causal processes are defined in terms of
characteristics they actually possess, rather than of their
capacity to transmit marks. This makes it unnecessary to
appeal to counterfactuals. Moreover, the conserved quantities approach
allows one to handle intersections that are untreatable by the mark
method, such as those of Y and \(\lambda\) types. These obtain whenever two
processes merge into one—for instance, when a snake swallows a
mouse—or two processes emerge from one—for instance, when
a single-celled organism splits into two cells. A major difference
with Dowe’s approach is Salmon’s insistence upon
causal transmission, as opposed to the mere possession of
conserved quantities, so that “a process is causal if and only
if it transmits a conserved quantity” (Salmon 2010: 10; see also
Salmon 1997). The capacity to transmit conserved quantities provides
an easy way to distinguish causal from pseudo processes, which do not
have that capacity. The at-at theory of transmission is retained, to
mean that a conserved quantity “is at the appropriate
place at the appropriate stage in the process” (Salmon
2010: 10). The mark method keeps an important role as a tool for
detecting causal relationships. 
Another fundamental ingredient of Salmon’s theory of causality
is the notion of production. Causal production takes place
whenever two processes intersect each other in such a way that their
structure is modified, and the modification will be propagated by
processes until another interaction takes place. The kind of
intersection between processes that gives rise to causal production is
described by interactive forks. 
Processes and conjunctive and interactive forks are the ingredients of
the mechanistic picture of the world that Salmon had in mind.
Conjunctive and interactive forks represent two different ways in
which processes can intersect one another, and fulfill different tasks
within Salmon’s perspective. Thanks to their screening off
capacity, conjunctive forks detect order and asymmetries among the
causal connections forming mechanisms, whereas interactive forks are
the sites of causal production. The temporal asymmetry characterizing
conjunctive forks is absent from interactive forks, which describe
physical interactions taking place at a certain moment in a certain
place. Interactive forks are more basic than conjunctive forks,
because they express a causal concept which can be “explicated
without recourse to other causal concepts” (Salmon 1994b [1998:
249]). Salmon called attention to the fact that
there is a striking difference between conjunctive common causes on
the one hand and causal processes and interactions on the other.
Causal processes and causal interactions seem to be governed by basic
laws of nature in ways that do not apply to conjunctive forks.
[…] Although I am not prepared to argue the case in detail, it
seems plausible to suppose that all fundamental physical
interactions can be regarded as exemplifications of the
interactive fork. Conjunctive common causes are not nearly as closely
tied to the laws of nature. […] in contrast to causal processes
and causal interactions, conjunctive forks depend crucially on de
facto background conditions. (Salmon 1982a [1998: 299])
Unlike conjunctive forks, causal processes and interactions cannot be
defined in purely statistical terms. To stress this crucial
difference, Salmon referred to statistical causality in terms
of screening off and conjunctive forks, and aleatory
causality in terms of processes and interactive forks. Aleatory
causality “places primary emphasis on the mechanisms of
causality” (Salmon 1990c [1998: 207]) and informs us about how
phenomena fit within the mechanisms responsible for their occurrence.
Statistical regularities are all that is needed by statistical
causality, whereas aleatory causality requires more, namely the
possibility to speak of tendencies that are both causal and
probabilistic. Salmon thought that propensities, taken as causal
probabilistic tendencies
 (§ 2.3),
 prove fruitful in that connection, and held that: 
if we think of propensities as probabilistic causes, we can use the
concepts of causal processes and causal mechanisms in order to explain
the mechanisms of probabilistic causality. (Salmon 1990c [1998: 205])
To the two kinds of causality described by Salmon there correspond two
different levels of explanation: (1) the S-R model, which concerns
connections between kinds of events, and (2) causal mechanical
explanation, which bears upon single events. It should not pass
unnoticed that while explanation of the first level, based on
statistical correlations, provides a good basis for prediction,
explanation in terms of aleatory causality traces back the history of
events after their occurrence. In the Eighties, Salmon thought that
only aleatory causality could offer an adequate understanding of
causality and regarded the causal mechanical level as genuine
explanation, assigning an ancillary role to the S-R level. 
In a penetrating discussion Christopher Hitchcock observed that
Salmon’s causal mechanical explanation was too weak, because it
envisaged a geometrical network of processes and interactions but did
not convey any hint as to what properties should be taken as
explanatory. In agreement with Woodward (1984), Hitchcock held that
explanation should answer
“what-if-things-had-been-different” questions, and claimed
that 
a successful account of explanation had better make the relation of
explanatory relevance look roughly like that of counterfactual
dependence. (Hitchcock 1995: 311) 
According to Hitchcock 
our demand that explanations provide relevant information
requires something stronger—that we be told which
earlier properties the properties specified in the explanandum depend
upon. (Hitchcock 1995: 311) 
a requisite Salmon’s definition of processes and causal
transmission does not meet. By way of a counterexample, Hitchcock
observed that based on Salmon’s theory a blue spot impressed on
a billiard ball by a stick on which a player had put blue chalk would
count as a mark transmitted by a causal process, and furthermore one
would be unable to tell why the linear momentum of the
billiard ball should be included in the explanation of its movement,
whereas the blue mark should not. 
In reply to Hitchcock, Salmon revised his position by ascribing equal
significance to the two levels of explanation. In “A Reply to
Two Critiques” he wrote: 
I would now say (1) that statistical relevance relations, in the
absence of connecting causal processes, lack explanatory import and
(2) that connecting causal processes, in the absence of statistical
relevance relations, also lack explanatory import. […] Both are
indispensable. (Salmon 1997: 476) 
The causal model in terms of processes was compared to a telephone
network which exhibits the lines of communications and the
connections, but does not convey any information about the messages
that are sent. In order to identify the properties pertinent to given
outcomes one needs information on statistical relevance relations; the
procedure of singling out such properties results from an interplay
between causal and statistical relevance information. On the one hand,
a network of causal processes and interactions serves to exclude
irrelevant factors which are not there at the considered place and
time; on the other, the network has to be filled with statistical
relevance relations linking the properties present. 
Contextually, Salmon admitted that counterfactual considerations play
a role within explanation, and reaffirmed a close connection between
statistical relevance relations and counterfactuals. To take
Salmon’s example, 
when asserting that a window was shattered because it was struck by a
baseball traveling at a considerable velocity, we presumably have in
mind that the window would not have broken if the intersection with
the baseball had not occurred. (Salmon 1997: 475) 
Such a counterfactual is deemed unproblematic because it is supported
by well-established assertions of statistical relevance, which are
nothing other than reports of observed relative frequencies.
Salmon termed his conception of explanation ontic, as opposed
to the epistemic conception heralded by Hempel, and the erotetic
approach defended by van Fraassen (Salmon 1985a [1998] and 1982b
[1998]). According to the ontic viewpoint events are explained by
showing how they fit into the mechanisms operating in the world, and
the causal processes and interactions of which mechanisms are
compounded are physical entities. In Salmon’s words:
To understand the world and what goes on in it, we must expose its
inner workings. To the extent that causal mechanisms operate, they
explain how the world works. […] A detailed knowledge of the
mechanisms may not be required for successful prediction; it is
indispensable to the attainment of genuine scientific understanding.
(Salmon 1984: 133)
Albeit he never gave up the task of developing an objective and
realistic concept of explanation, in the Nineties Salmon granted the
importance of pragmatic considerations in connection with causal
explanation. In this vein, he admitted that causal analysis can be
performed at different levels of detail, depending on the
circumstances. In some cases, phenomena can be analyzed in great
detail, based on scientific theories, but more often they can be
examined at varying degrees of abstraction, determined by the context.
In “A Realistic Account of Causation”, published
posthumously in 2002, Salmon argued that 
the major obstacle to the creation of a fully objective and realistic
theory of cause-effect relations is the fact that the instances we
tend to select are highly context dependent. (Salmon 2002: 123) 
After examining a number of examples he concluded that
“cause-effect statements are almost always—if not
always—context dependent” (Salmon 2002: 125). 
That said, Salmon retained the idea of a complete causal
structure including all the processes and interactions operating
in a certain space-time region. Such a concept bears some resemblance
to Peter Railton’s “ideal explanatory text” (Railton
1981), the difference being that Railton has a text in mind, while
Salmon’s complete causal structure is a physical entity. The
most important feature characterizing the complete causal structure is
its objectivity: “the complete causal structure is a fact of
nature that exists quite independently of our knowledge or
interests” (Salmon 2002: 126). 
The role played by scientific theories within Salmon’s
mechanistic view should not pass unnoticed. Both causal production and
processes defined in terms of conserved quantities include an appeal
to theories and laws of nature, because only “our current
theories tell us what quantities to think of as conserved”
(Salmon 1994b [1998: 258]). Similarly, to describe causal interactions
one must appeal to laws such as the conservation of energy and
momentum. This led Salmon to conclude: 
I realize that the theory I am proposing has a highly reductionistic
flavor. It seems to me that my account should hold in the natural
sciences—including biology, but not quantum mechanics—I am
not confident that it is suitable for psychology and the social
sciences. (Salmon 2002: 131) 
A similar conclusion, which further clarifies Salmon’s attitude
towards reductionism, is to be found in “The Causal Structure of
the World”:
Finally, although my tone in this talk has been rather reductionistic,
I do not hold a reductionist point of view. It is quite possible that
other kinds of causation are present in such areas as psychology and
sociology, where human intentions and interrelations are involved.
Physical causation must apply at the basic level of perception and
communication, but there may be more. I would not commit myself to a
reductionist—or antireductionist—viewpoint unless I
had at least an acceptable solution to the mind-body problem, and that
is something I don’t have at present, and I doubt that
I’ll ever find one in my lifetime. (Salmon 2010: 12)
Further reading: On
Hempel’s theory of explanation see Hempel 1965 and Fetzer 2010
[2017]. On the notion of probabilistic causality and the relative
debate see Hitchcock 1997 [2016] and Beebe, Hitchcock & Menzies
(eds.) 2009. Salmon’s view of a causal process and Dowe’s
conserved quantities theory are discussed in Dowe 2000. The debate on
explanation is surveyed in Woodward 2003 [2017].
Causal explanation, probability, and the Bayesian method are the key
ingredients of Salmon’s view of rationality, which is strictly
entrenched with his version of realism. A further ingredient is the
notion of propensity taken as causal tendency (see
 § 2.3).
 Salmon’s “Rationality and Objectivity in Science”
(1990d [2005]) defines three grades of rationality standing in
different relationships with objectivity. The first kind of
rationality has no connection with objectivity and is identified with
the coherence of degrees of conviction, a key requirement of the
Bayesian method. This is called static rationality. Bayesian
conditionalization, offering a tool to update one’s degree of
conviction, shapes a stronger type of rationality called
kinematic. The highest grade of rationality is called
dynamic, and involves a closer connection with
objectivity.
Dynamic rationality is dealt with in some detail in “Dynamic
Rationality: Propensity, Probability, and Credence” (1988b
[2005]). This kind of rationality revolves around the tenet that
rational action must obey the maxim: Respect the frequencies.
For it to be accomplished, credence, or the degree of conviction
leading to action, must be empirically informed, namely it should take
into account objective facts. Salmon holds that explanation has a
crucial role in that connection because causal knowledge of phenomena
provides the optimal basis for action. The notion of propensity is
deemed no less fundamental and is assigned the purpose of bridging the
gap between objective probabilities, namely frequencies, and personal
probabilities on whose basis people act. Personal probabilities should
not be construed in a subjective sense, not only because for Salmon
subjectivism is not an admissible interpretation of probability
 (§ 2.3),
 but more generally because subjective opinions cannot be taken to
offer good grounds for rational action. Rational action can only be
based on rational degrees of conviction, the emphasis being on
rational as opposed to merely subjective.
As Salmon observed, 
it is the operations of physical devices having […]
propensities—chance setups, including our own actions—that
produce the actual short-run frequencies, on which our fortunes
depend, as well as the long-run frequencies which I am calling
probabilities. (Salmon 1988b [2005: 148]) 
Assigning a propensity to a chance setup amounts to making a causal
hypothesis, which can be evaluated by means of Bayes’ rule.
Since Salmon’s objective Bayesianism
 (§ 3.2)
 holds that prior probabilities are determined on the basis of
observed frequencies, the latter are the cornerstone on which the
whole procedure of confirming the hypotheses about propensities rests.
To sum up, dynamic rationality
consists in the attempt to use propensities—i.e., probabilistic
causes—as the weighting factors that occur in the formula for
expected utility. Since we cannot be sure that our choices and
decisions will be fully efficacious in bringing about desired results,
it is reasonable to rely on the strengths of probabilistic causes.
This line of thought treats our voluntary choices, decisions, and
actions as probabilistic causes of what happens as a result of our
deliberations. Dynamic rationality involves a
propensity-driven view of objective probabilities and
short-run frequencies. (Salmon 1988b [2005: 150])
Deeply convinced of the tenability of scientific realism, Salmon
argued in favor of its compatibility with empiricism. His argument
revolves around what he calls the key question, phrased as
follows: 
Do we have empirical evidence that gives us just about as much reason
to believe in the existence of such entities as molecules, atoms,
ions, and subatomic particles as we have for our belief in
middle-sized material objects? (Salmon 2005: 32) 
The possibility of combining empiricism and realism is made to depend
on an affirmative answer to this question, which Salmon claimed had
not been given serious consideration by most philosophers of science,
including van Fraassen, whose constructive empiricism implies a
negative answer to the key question, and therefore the incompatibility
between empiricism and realism.
In order to provide an affirmative answer to the key question, Salmon
once again drew inspiration from Reichenbach’s work. In
Experience and Prediction Reichenbach tackled the issue of
our knowledge of the external world by means of a fictional example.
He imagined a “cubical world” whose inhabitants cannot
penetrate either the walls or the ceiling, made of a translucent
substance. In Salmon’s words:
As a result of a complicated arrangement of lights and mirrors,
shadows of the birds outside of the cube are projected onto the
ceiling and the left-hand wall. The inhabitants of the cubical world
can see the shadows cast by the external birds, but they cannot see
the actual birds, mirrors, or lighting system. For beings in the
cubical world, the birds are truly unobserved entities. (Salmon 1999:
303)
After careful observations, a scientist in the cubical world notices
there are correlations between the shadows on the ceiling and those on
the wall. After having repeatedly observed such correlations, the
scientist infers the probabilistic hypothesis that there must exist
something responsible for the shadows seen on the walls and ceiling.
The reasoning leading to that conclusion is nothing other than an
inference to a common cause. Although the principle of the
common cause (see
 § 5.4.1)
 was introduced by Reichenbach in The Direction of Time,
published posthumously in 1956, and does not appear at all in
Experience and Prediction (1938), Salmon conjectured that the
idea was already there, as testified by Reichenbach’s tenet that
any physicist who happens to repeatedly observe some coincidences
“will not believe in a matter of chance but will look for a
causal connection” (Reichenbach 1938: 121).
For Salmon, the fundamental role played by the principle of the common
cause in connection with realism was testified by a great many
examples in the history of science, his favorite being Perrin’s
ascertainment of Avogadro’s number around 1912. As recounted by
Mary Jo Nye in Molecular Reality (1972), after Perrin showed
that Avogadro’s number could be determined by means of thirteen
different methods, all producing results in close agreement, the
community of physicists was convinced of the reality of atoms and
molecules. According to Salmon, Perrin’s conclusion was reached
by means of an inductive reasoning which “can appropriately be
schematized as a type of common cause argument” (Salmon 1985b
[2005: 17]). From this and other examples Salmon concluded that a
powerful method for inferring hypotheses from empirical evidence of
regularities, the principle of the common cause provides a tool for
inferring unobservables from observables.
This, however, is only part of the story. Salmon believed that the
answer to the key question “lies in connecting the common cause
principle and Bayes’s theorem” (Salmon 1994a [2005: 25]).
Although crediting him for having grasped the same idea, Salmon found
Reichenbach’s treatment incomplete, because it contains only
sketchy remarks in that regard. He therefore set himself the task of
working out a detailed argument, spelled out in “Ornithology in
a Cubical World” (1999) and “An Empiricist Argument for
Realism” published as Chapter 3 of Reality and
Rationality. In Salmon’s own words, his purpose was: 
to show how the considerations that convinced serious physical
scientists of the reality of atoms and molecules in the early years of
the twentieth century provide a philosophically sound argument for
realism that does not exceed the bounds of empiricism. (2005: x) 
In a nutshell, the fruitfulness of combining Bayes’ rule with
the principle of the common cause amounts to the fact that those
hypotheses which embody information on common causes are apt to be
assigned higher prior probabilities than hypotheses which involve mere
coincidences.
Salmon reached this conclusion by scrutinizing the way scientists
achieved a number of important findings, such as the existence of
atoms and molecules. His in-depth analysis of case studies belonging
to the history of science, carried out with respect to both
microscopic objects and the submicroscopic domain, confers on
Salmon’s argument for realism a peculiar significance and
originality.
Further reading: The volume
Reality and Rationality (Salmon 2005) contains many articles
on Salmon’s views on rationality and realism.