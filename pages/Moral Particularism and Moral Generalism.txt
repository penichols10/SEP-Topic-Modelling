Aristotle might reasonably be characterized as the
“forefather” of particularism. Aristotle famously
emphasizes that ethical inquiry is mistaken if it aims for “a
degree of exactness” too great for its subject matter, and added
that moral generalizations can hold only “for the most
part”. Moreover, Aristotle tirelessly emphasizes that ethics
ultimately concerns particular cases, that no theory can fully address
them all, and that “judgment depends on perception” (NE,
1109b). These ideas have all deeply inspired contemporary
particularists (John McDowell is a prominent case, though he does not
tend to label himself as a particularist; see McDowell 1981, 1998).
Whether Aristotle should ultimately be interpreted as a particularist
is a matter of debate (Irwin 2000; Leibovitz 2013).
Interestingly, no single major historic figure is most obviously
characterized as the “forefather” of generalism. This is
presumably because the most important historic generalists in effect
defended generalism by defending specific moral theories or
principles. The two most important traditions here are the
deontological tradition which owes so much to Kant, and the
consequentialist tradition which owes so much to the British
utilitarians (Bentham, Mill and Sidgwick). Nonetheless, each of these
traditions substantially enriched the generalist approach with a
wealth of ideas and distinctions which need not be restricted to the
theories in which they were originally formulated.
The Kantian tradition puts enormous weight on the idea that morality
must be principled and that the ultimate principle of morality must be
one we can know a priori. According to Kant, the moral law as
applied to imperfect agents who are subject to temptations, provides
what he called a “categorical imperative”—an
imperative whose rational authority is not dependent on the
agent’s contingent ends. Kant provided several formulations of
the categorical imperative. The so-called “universal law”
formulation holds that one must always act so that one’s maxim
could at the same time be willed as a universal law. The humanity
formulation holds that one must always act so as to treat humanity,
whether in one’s own person or that of another, always as an end
and never merely as a means. The Kantian tradition emphasizes common
sense moral ideas like respect and dignity, and provides a distinctive
interpretation of the role of universalizability in moral thought. On
some readings of Kant, the moral law must itself be
constitutive of being a rational agent at all. This idea has,
in turn, been enormously influential, especially in the late twentieth
and early twenty-first century. 
Consequentialism enriched the generalist framework in other ways. Most
notably, perhaps, consequentialists have often distinguished between
two very different kinds of principles, corresponding to two
rather different roles they may play. On the one hand, there
are principles—call them “standards”—which
provide the deepest explanation of why certain actions are
right or wrong. On the other hand, there are the principles which
ordinary agents ought to follow in their day to day deliberations.
Such principles are “guides”. Consider a simple analogy
with the stock market. The principle which explains what counts as
success might simply be “buy low and sell high”, but this
principle is woefully inadequate as a guide to making investment
decisions in real time. A principle like “have a diversified
portfolio” seems much more suitable for the latter role.
Each of these traditions (the Kantian and the consequentialist) faces
a number of prima facie powerful objections. It is therefore
perhaps not surprising that there was ultimately a reaction against
the broader generalist aspirations that these theories embodied. On
some readings, one of the earliest particularists in the modern sense
was Ewing, who in The Morality of Punishment (1929) argued
that consequentialism and deontology were the only plausible
principled conceptions of morality, that neither was defensible, and
that morality was therefore not principled (cf. Lind &
Brännmark 2008 interview with Dancy, who explicitly characterizes
Ewing in this way at p. 10).
Just one year after Ewing in effect defended a fairly radical form of
moral particularism, W.D. Ross argued for a more moderate form. Ross
occupies a very interesting place in the history of particularism, as
he has served as both an inspiration and a foil to modern
particularists. Ross put forward a battery of “prima
facie duties” specifying types of conduct—for example
acts of gratitude—that are always, in some sense, obligatory.
The obligation in question need not be an all things considered one,
however, since a conflicting prima facie duty might, in the
circumstances, be more important.
Setting aside whether Ross thought anything theoretically useful could
be said about how to adjudicate conflicts of prima facie
duty, he did not think it useful to try to formulate exceptionless
principles with regard to all things considered duty (cf. Postow
2006). Ross thus appears to be a generalist about prima facie
duty, but a defender of particularism about overall duty. Some
contemporary particularists, however, insist on going beyond Ross and
casting doubt even on principles of prima facie duty, or
principles specifying which considerations are pro tanto (or
“contributory” in Jonathan Dancy’s terminology)
reasons.
Jonathan Dancy has done more than anyone to articulate and defend an
especially radical form of particularism. Although Ross was both a
foil and an inspiration for Dancy, R.M. Hare was a more immediate
opponent. Hare’s prescriptivism drew on ideas from both the
Kantian and the consequentialist tradition. Hare defended a strong
form of universalizability which can be traced to Kant, but Hare then
argued that universalizability lent support not to a deontological
moral theory, but to a form of consequentialism. Indeed it led to a
form of consequentialism which emphasized the distinction between
standards and guides (cf. Hare 1963).
In the introduction of Moral Reasons, Dancy summarizes his
conclusions as the “mirror image” of Hare’s. Perhaps
most notably, Dancy objected to an idea which he took to be implicit
in Hare’s universalizability principle, that if a consideration
is a reason in one context then it is a reason with the same valence
in any possible context in which it occurs. (This reading of
Hare is open to objection. See McNaughton and Rawlings (2000) for
discussion.) Dancy calls this idea, which he also attributes to Ross,
“atomism” in the theory of reasons and argues against it
and in favour of what he calls “holism”.
As Dancy’s early work came to fruition, it inspired his
then-colleague David McNaughton to advance distinct but complementary
arguments for particularism. McNaughton was also heavily influenced by
the work of John McDowell, who had argued that it was an advantage of
his own brand of moral realism that it did not presuppose generalism
(see McDowell 1981; see also Blackburn 1981). In Moral
Vision, McNaughton defended a form of moral realism which he
argued lent support to particularism. He also argued that
particularism better accounts for moral conflict, fits reasonably well
with ordinary practice and can explain why we might reasonably be
suspicious of the very idea of a moral expert.
The work of Dancy and McNaughton inspired a host of other philosophers
to carry forward the particularist research programme, sometimes in
rather different directions. This eventually led to a wide variety of
views all going under the heading of “particularism”. Nor
were the challenges posed by these many moral particularisms ignored
by those with more generalist sympathies. Woken from their generalist
slumbers, they began to develop arguments for generalism which did not
depend on the correctness of any particular moral principle(s). This
generated a healthy debate, the contours of which the rest of this
entry will outline.
Particularists are united in their opposition to moral principles and
generalists are united in their allegiance to them. What, though, is a
moral principle? At least three conceptions of principles are worth
distinguishing. First, there are principles qua
standards. Standards purport to offer explanations
of why given actions are right or wrong, why a given consideration is
a reason with a certain valence and weight, why a given character
trait is a virtue, and the like. An especially robust metaphysical
spin on this conception understands standards as being
truth-makers for moral propositions (cf. Armstrong 2004).
Second, there are principles qua guides. These
purport to be well suited to guiding action. Third, there are
principles purporting to play both of these roles
simultaneously—action-guiding standards.
It is not hard to see these different conceptions of moral principles
at work in the history of moral philosophy. In the utilitarian
tradition in moral philosophy, the principle of utility (however it is
formulated) is characteristically understood as a standard. Even if
some politically minded utilitarians see advantages to using the
principle of utility to guide public choice and justification, moral
philosophers tend to follow Mill in thinking that the principle is
seldom apt for use in individual moral decision-making. They thus deny
that it should be understood as a principle qua
guide. Indeed, some utilitarians go further and argue that
the principle of utility is self-effacing, in the sense that
it recommends its own rejection (cf. Railton 1984; see also Parfit
1984). Utilitarians instead hold that various maxims of common sense
morality should be understood as heuristics which work well enough for
normal human beings, so there is room in this picture for principles
qua guides.
Kant, on the other hand, seemed to have understood the categorical
imperative as a kind of action-guiding standard. Kant’s
discussion of examples in the Groundwork (1785) and his
characterization of the Formula of Universal Law as appropriate method
for testing our maxims makes clear that he thinks of it as
appropriately guiding the actions of the morally virtuous agent.
Equally clearly, the categorical imperative is to be understood as the
most fundamental explanation of why given actions are right or wrong,
and so also counts as a principle qua standard. On a
constitutivist reading, the categorical imperative is meant to play
both of these roles in virtue of its constituting our rational agency.
Finally, it is worth noting in this context that a principle can
function usefully as a guide even if its application requires
judgment and sensitivity; principles qua guides need not be
algorithmic.
Principles can also be distinguished in terms of their scope.
Some principles have purely non-moral antecedents (e.g., the principle
of utility), whereas others use moral concepts in both their
antecedents and consequents (e.g., “if an action is just then it
is morally permissible”). Finally, principles can be
distinguished in terms of whether they are in some sense
“hedged”, including a ceteris paribus clause of
some kind (e.g., “other things equal, lying is wrong”), or
unhedged.
One might be a particularist or a generalist about moral principles
understood in any of these ways. Whether being a particularist or
generalist about principles in one sense drives one to be a
particularist or a generalist about principles in another sense is not
a trivial question. Further complicating matters, there is more than
one way to oppose principles (however those principles are conceived.)
Last, the form a particularist’s opposition takes might
reasonably vary across different types of principle. Let us now review
different ways one might oppose principles.
The simplest form of opposition, Principle Eliminativism,
simply denies that there are any moral principles. Of course, it must
be borne in mind here and below that a principle eliminativist may
deny that there are any principles of one sort, while allowing for
principles of another sort. For example one might be an eliminativist
about principles purporting to give the application conditions for
moral predicates in entirely non-normative terms (McNaughton 1988) or
an eliminativist about exceptionless principles (Little 2000).
Principle Scepticism holds, more modestly, that we do not
have sufficient reason to believe there are any moral principles.
Principled Particularism holds that while any given moral
truth is explained by a moral principle, no finite set of moral
principles can explain all the moral truths (Holton 2002).
Anti-Transcendental Particularism, which at one point at
least was Dancy’s favoured gloss of the view, holds that moral
thought and judgment does not depend on the supply of a suitable stock
of moral principles. Finally, Principle Abstinence asserts a
more practical opposition to moral principles, holding that we ought
not be guided by moral principles. For each of these forms of
particularism, there is a corresponding form of generalism which is
simply the denial of the particularist thesis in question.
Although this taxonomy entails that the logical space for
particularist (and generalist) views is wide and heterogeneous, it
would be a mistake to assume that all of the positions which can be
derived from a matrix constructed on the basis of these distinctions
really are distinct in any deep way. For example, Principle
Eliminativism about principles qua guides
arguably is equivalent to Principle Abstinence about
principles tout court.
The debate between particularists and generalists is often framed in
metaphysical terms. In this guise, the debate primarily concerns
principles conceived as standards. Moral principles might then be
thought of as true and law-like generalizations about moral properties
or, alternatively, as nomic regularities involving moral properties.
To be clear, generalizations in the relevant sense need not actually
be instantiated to be true; plausibly, the moral law would still be
true in a world with no agents and hence with no right or wrong
actions. This view has been developed, in different ways, by McKeever
and Ridge (2006), Väyrynen (2006, 2009) and Lance and Little
(2007), though it is not universally accepted (see Robinson 2008).
Thus understood, particularism is often taken to have an affinity with
non-reductive or non-naturalist views in ethics. Furthermore, it has
been noted that contemporary particularism arose alongside a
resurgence of interest in non-naturalism (cf. Little, 1994). To a
degree, this is understandable. After all some naturalist
views appear to entail generalism. A form of reductive naturalism
according to which being morally right is metaphysically identical
with being an act which maximizes human happiness appears to entail a
robust utilitarian principle and so, a fortiori, to entail
generalism. Furthermore, if one denies any kind of reduction of moral
properties to natural properties then it becomes more difficult to see
how any informative statements connecting moral and non-moral
properties could be sufficiently law-like to count as principles.
Nevertheless, one ought not to take it as given that non-naturalists
must be particularists or that naturalists must be generalists. Both
classic and contemporary non-naturalists have endeavored to defend
principles (Moore 1922 [1903]; Shafer-Landau 2003). Furthermore, there
may be room for particularists to embrace naturalism by claiming that
particular reasons are always grounded in some particular natural
property instance while maintaining particularism by claiming that
there are no law-like generalizations connecting moral and non-moral
property types. Perhaps because the commitments and resources of
non-naturalist and naturalist views in metaethics (and even how
properly to distinguish these views from each other) remains
contested, one cannot uncontroversially map the
generalism/particularism debate onto the naturalism/non-naturalism
debate.
Non-naturalism would seem to have less bearing on a further question:
whether there are moral principles connecting one moral property to
another. This issue is at play in Ross’s rejection of
Moore’s claim that the right action is the action which
maximizes the good. Even if Ross relied on Moore’s own
“open question” strategy to challenge Moore’s
utilitarian principle, it is nevertheless the case that Moore’s
principle is at least consistent with non-naturalism. More recently,
philosophers sympathetic to particularism have divided over the
availability of intra-moral principles. Some are content to allow that
a claim such as, “the fact that an action is just is a reason in
its favour”, is true and informative (cf. McNaughton and Rawling
2000). Others propose a more radical particularism according to which
even intra-moral principles—that is, principles linking one
moral concept with another—are unavailable (cf. Dancy 2004: ch
7).
Naturalists and non-naturalists typically share a commitment to
supervenience. Roughly put, supervenience says that, necessarily,
there can be no moral difference without some natural (or non-moral)
difference. There are various ways to interpret supervenience and its
metaphysical significance. So long as we reject a global error theory,
though, supervenience seems to guarantee some necessarily true
universal generalizations involving moral predicates.
Nevertheless, it is generally agreed by all sides that such
“supervenience functions” should not count as moral
principles. The grounds offered for this are various, but include that
such generalizations contain much potentially irrelevant information,
that they are massively disjunctive, and that they lack explanatory
import (cf. Little 2000; Dancy 2004; McKeever and Ridge 2005). The
idea is that to be a successful moral principle (qua
standard) requires more than a true or even necessary connection
between the descriptive and the moral. The connection must be
explanatory and not cite irrelevant features in the antecedent either.
Even those who gesture at supervenience in mounting arguments for
generalism concede that a successful argument requires significant
additional semantic or epistemological premises (see below, and cf.
Jackson, Pettit, & Smith 2000).
While particularism has strong affinities with non-naturalism, the
most prominent argument for particularism—the argument from the
holism of reasons—has proceeded from a more targeted and
specific claim about the metaphysics of moral reasons (see McNaughton
1988; Dancy 1993; Little 2000). According to holism about reasons, a
consideration that counts as a reason in one case may not count as a
reason in another case, or may count as a reason, but in a different
direction. By way of illustration, the fact that a remark would be
funny might be, in one case a reason for making the remark, in another
case a reason against making the remark, and in still another case no
reason at all. In short it depends on context. Importantly, holism is
meant to be a universal and modal claim. It says that for any
consideration that is a reason it is possible that that consideration
might behave differently in another case. Thus understood, holism is
consistent with the possibility that some considerations are, as a
matter of fact, reasons (of the same force and direction) in every
case. Holism also seems to presuppose that the considerations that are
reasons are not brutely unique; the insight of holism—if it is
an insight—is built upon the thought that a consideration which
is a reason in one case is repeatable in other cases. Only if, for
example, the funniness of a remark is a consideration that is
repeatable can we say, as the holist wants us to, that the funniness
of a remark is a reason in one case but is not a reason in
another.
Those attracted to holism about reasons agree that it is typically
specific elements of context that further account for whether a
consideration counts as a reason, and a rich language for
characterizing context has been developed. For example, a putative
reason might be defeated, enabled, or intensified by specific elements
of the context. To continue the previous example, the fact that a
genuinely funny remark would also be offensive might defeat whatever
reason-giving force that the humor might otherwise have had. On this
reading, the humor of the remark is no reason at all; not just a
reason that is outweighed. To vary the case again, the fact that
one’s audience will appreciate a (non-offensive) funny remark
enables the humor to be a reason. Here the humor is a reason, but only
against the background of a receptive audience; the background
functions as an “enabler”. Finally, the fact that a funny
remark will break an unduly somber mood may intensify the force of
humor as a reason. Humor itself is especially apt, but only because
the mood is unduly somber. Other factors could function as
attenuators, weakening the force of a reason (see Dancy 2004: ch 5).
Holism depends crucially on the sustainability of the distinction
between the particular considerations that count as reasons and the
contextual factors (defeaters, enablers, etc.) that impact whether a
consideration counts as a reason. Context-sensitivity without such a
distinction would be unable to explain why a feature which is a reason
in one context can fail to be a reason in another. Moreover, we need
to know why the relevant features should not be “hoovered
up” into the content of the reasons themselves. After all,
atomists need not be simple atomists. A hedonist who held that
pleasure and pain are both always reasons and the only reasons, would
certainly count as an atomist. But atomists can allow for significant
pluralism and complexity. One way to do so is to insist that the
considerations that a holist calls, variously, reasons, defeaters,
enablers, and so on are all but parts of a larger more complex
“whole” reason. Such views have been proposed by Bennett
(1995), Crisp (2000), Hooker (2003) and Raz (2000), and rejected by
Dancy (2004: 6.2). One worry about the atomists’ appeal to whole
reasons is that if reasons are identified with large complexes of
facts, then the same reason may seldom recur across cases and the
claim that agents act for reasons may fall under threat (Price
2013).
Setting aside whether holism is true, does it support particularism?
Generalists have rejected the inference on the grounds that holism
leaves open the possibility that the behaviour of reasons, defeaters,
enablers, and intensifiers/attenuators is codifiable (see
Väyrynen 2004; McKeever and Ridge 2005). They also observe that
some paradigmatic generalists seem to have exploited this logical
space. For example, Kant arguably thinks that the fact that a course
of action would advance someone’s happiness is of variable moral
significance, counting in favor whenever the happiness and its
purchase is consistent with the categorical imperative and counting as
no reason at all otherwise (McKeever and Ridge 2005). Particularists
have countered that even if holism is logically consistent with
principles it would nevertheless render them “cosmic
accidents” (Dancy 2004: 82). If this were right it would be
enough to cast doubt on the generalist tradition in moral philosophy.
Why should the heart of a discipline be the search for cosmic
accidents?! Generalists counter that whether principles are cosmic
accidents depends entirely upon underlying metaphysical issues and not
on whether principles tolerate holism. For example, if the property of
being good is identical to the property of being non-malicious
pleasure, then the associated holism tolerating principle does not
look to be a cosmic accident (see McKeever and Ridge 2006: 2.2).
Selim Berker (2007) has challenged the particularist argument from
holism in another way. He argues that the conjunction of holism with
what he calls the particularist’s
“noncombinatorialism” about the ways reasons combine to
fix an overall verdict leaves the particularist with no coherent
notion of a reason for action. To understand noncombinatorialism, one
must first understand the idea of a “combinatorial
function”. A combinatorial function takes as input all the
reasons and their valences in a given situation and gives the
rightness or wrongness of actions in that situation as an output.
Noncombinatorialism simply asserts that the combinatorial function for
reasons cannot be finitely expressed (and so, in particular, is not
additive). Berker argues that particularists are committed to
noncombinatorialism, but that this leaves them with no coherent notion
of a reason for action. Particularists typically gloss being a reason
as “counting in favour” of that for which the
consideration is a reason. Berker’s point is that talk of a
consideration “counting in favour” of something is itself
hard to make intelligible once we abandon a combinatorial conception
of how reasons combine to fix an overall verdict. We are left with a
metaphor that cannot be cashed out in any helpful way. Particularists
could of course abandon the noncombinatorial conception of reasons,
but Berker argues that this would commit them to the truth of numerous
exceptionless principles, thus compromising their particularism. (For
critical discussion of Berker’s argument, see Lechler 2012.)
So far we have focused on generalist replies to metaphysical arguments
for particularism. Generalists are not without positive metaphysical
arguments for their own views, though. Most notably, so-called
“constitutivists” sometimes invoke premises about the
metaphysics of rational agency to argue for generalism. Kantian
constitutivists are the most influential and clear-cut instance of
this style of argument. Christine Korsgaard, for example, argues that
the categorical imperative is constitutive of rational agency
(Korsgaard 2008, 2009). The rough idea is that the principles of
practical reasons unify us as agents, and allow us to take control
over our representations of the world and our movements (Korsgaard
2008: 9). Insofar as simply being a rational agent commits one to the
relevant principle(s), this strategy for defending generalism is also
meant to be especially effective at silencing sceptical challenges,
e.g., classic “why be moral?” challenges. The thought is
that the sceptic has no coherent perspective from which to reject the
relevant principles.
Whatever the success of these metaphysical arguments, some
particularists have worried that an excessive focus on metaphysics
threatens to lead us astray—not to falsehood so much as
misplaced emphasis. The ontological status of moral laws and the
grounding of moral reasons ought not predominate the particularist
program. Instead, the particularist should emphasize how their account
of moral psychology makes sense of moral development and competence
(see Bakhurst 2008, 2013). 
Particularists and generalists typically share a commitment to moral
knowledge. This common ground is not strictly entailed by either view.
For example, proponents of Hare’s universal prescriptivism will
insist that moral thought is principled even if, in their rejection of
the truth-aptness of moral language, they deny that there is moral
knowledge. On the other side, a fictionalist might reject moral
knowledge while insisting that the moral fiction is itself devoid of
principled structure, just as the particularist insists. Nevertheless,
both generalists and particularists do in fact typically see moral
thought and judgment as achieving (sometimes) significant success and
in this context the shared commitment to moral knowledge is not
surprising. 
Particularists and generalists often treat moral knowledge as being on
a par with other types of commonly accepted knowledge. Just as we can
know that our internet connection is running slow, that the milk is on
the verge of going stale, or that our friend is annoyed by the story
just told at his expense, so too we can know that it would be wrong
refuse directions to the person who is lost, that our co-worker was
courageous to criticize her supervisor, and that the American justice
system treats many people unfairly. Because the commitment to moral
knowledge is a shared one, many of the arguments both for and against
particularism have sought to use it for dialectical leverage. The
questions at stake include whether particularism or generalism best
explains our capacity to achieve moral knowledge and whether
particularism or generalism best models the person who has and uses
moral knowledge.
Some moral knowledge, it is agreed, involves the transmission or
extension of other moral knowledge already achieved. If Solomon tells
me the treaty is unjust, I may know this by relying on his testimony.
If every member of the Diogenes Society whom I have met is honest,
then I may know that Walter, who is also a member, is honest. Here, I
rely on an induction from my other moral knowledge. While highly
interesting, these types of knowledge are typically regarded as
derivative (Zangwill 2006) and are therefore set aside in arguments
over moral particularism. The question is what explains our most basic
moral knowledge. How strong an assumption can be made about our moral
knowledge while remaining on ground common to both generalists and
particularists? Obviously, particularists will not grant that we have
knowledge of moral principles, and the point of surest agreement is
that we sometimes know the moral status of a particular case, e.g.,
that this act was wrong. However, most arguments both for and against
particularism deploy somewhat stronger assumptions. 
First, one may make a stronger assumption about the objects of moral
knowledge. Of special interest is the possibility that we can know
general truths about morality even while such general truths fall
short of counting as moral principles. For example, while
particularists will deny that there is any exceptionless moral
principle to the effect that pain is bad, many sympathetic to
particularism would agree that, as a general matter, pain is bad and
that we can know this. On a deflationary reading, one might treat the
claim that pain is bad as a useful heuristic, a reminder that pain has
often been bad in the past and may well be so in the future (Dancy
1993). Alternatively, that pain is bad might capture an interesting
metaphysical fact about pain; its default status is the status of
being bad. We can understand default status in terms of an explanatory
asymmetry. When pain is not bad there must be something that explains
why it is not, but when pain is bad there needn’t be any further
explanation of what makes pain bad (Dancy 1993, 2004). Finally, one
might try to invest such generalizations with real explanatory power
while insisting they remain exception laden. That pain is bad is a
kind of defeasible generalization, where this amounts the claim that
pain is bad in a privileged set of worlds (Little 2000; Lance and
Little 2004; for critical discussion of each of these possibilities,
see Stangl 2006).
Second, one may make a stronger assumption about the scope of moral
knowledge, at least for some people. Some people, one may assume, are
(or become) especially good at acquiring moral knowledge; they have a
measure of practical wisdom or expertise. Their knowledge thus readily
extends not just to their actual circumstances but to a broad array of
novel circumstances as they arise. In so far as this is so, we should
like to have a good explanation not only of how humans acquire
specific knowledge but also of how they develop over time into more
competent moral knowers (Bakhurst 2005, 2013).
Two models of moral knowledge predominate in defences of
particularism: a perceptual model and a skill based model. According
to the perceptual model successful moral judgment is properly
analogous to sense perception even if it is not, literally, a form of
sense perception (McDowell 1979, 1985; McNaughton 1988). Moral
judgment on this view depends upon a range of sensibilities, developed
through experience and acculturation. Once developed, however, one can
just “see”, for example, that a certain response is
merited by a situation. As John McDowell puts it, 
Occasion by occasion, one knows what to do, if one does, not by
applying universal principles, but by being a certain kind of person:
one who sees situations in a certain distinctive way. (McDowell 1979:
350)
Since sensibilities may be more or less refined, the perceptual model
appears to fit well with the idea that there are both moral novices
and experts. Whatever further account is to be given of these
sensibilities, the resulting knowledge is not dependent upon any
deduction from general moral principles, at least not one
transparently and readily available to the knower. Generalists have
observed that similar perceptual metaphors seem equally apt in domains
that admit of principles (McKeever and Ridge 2006: ch 4). For example,
one might just “see” that a sentence is ungrammatical even
if grammaticality is rule-governed. Furthermore, some who develop and
defend a perceptual model are not led by it to particularism (Audi
2013). So the perceptual model of moral knowledge seems not to
establish particularism, but it was likely never meant to. Instead,
the perceptual model is intended to offer more indirect support. If
our moral experts reliably achieve moral knowledge without
self-consciously adverting to moral principles, then the generalist
inherits at least some burden to explain why principles should be a
centerpiece of moral theorizing.
Two related constraints confront the development of the perceptual
model and may threaten the particularism the model is taken to
support. The first is that the model must extend to prospective and
hypothetical cases. Particularists and generalists typically agree
that we sometimes have knowledge that if we were to perform an action
(say maintain a confidence) in our actual circumstances, that action
would be right. This seems essential if moral knowledge is to precede
and guide virtuous conduct. Similarly, if slightly more
controversially, we sometimes have knowledge that a certain course of
conduct would be right in some hypothetical circumstance. While one
might try to account for such cases by appeal to inductive reasoning
from past actual cases, this is not, in fact, how proponents of the
perceptual model have proceeded. The second constraint arises from the
fact that moral properties are grounded in other properties. For
example, it barely makes sense to say that an action’s wrongness
is a brute fact about it; if wrong an action must be wrong on account
of some otherwise specifiable features it has. While this
“because constraint” admits of various explications, the
basic idea is common ground and widely thought to be a priori
(Zangwill 2006).
Generalists argue that, once spelled out, the perceptual model is not
the a posteriori epistemology it might first have seemed but
instead commits us to a priori intuitions relating moral
features to their grounds. Particularists may grant that basic moral
knowledge is a priori knowledge of “what is a moral
reason for what” (Dancy 2004: 141) while maintaining that the
object of knowledge remains particularized and does not implicate
principles. One challenge for the particularist is that “what is
a reason for what” in a particular case is contingent, and so
the particularist risks being committed to a priori knowledge
of contingent truths. In his defense of particularism, Dancy has been
willing to embrace this apparent consequence (Dancy 2004; for
criticism, see McKeever and Ridge 2006). Other particularists have
sought to avoid the implication (Leibovitz 2014).
Particularists sometimes pursue a somewhat different model of moral
knowledge, one that likens the practically wise agent to a person who
has a developed skill. Just how different this model is from the
perceptual one must depend upon how each is spelled out. But whereas
the perceptual model directs our attention first to how the virtuous
person understands her situation, the skill model draws attention to
the knower as agent, someone who classifies actions, agents, and
states of affairs as falling under moral categories, who reasons to
moral conclusions, and ultimately puts their knowledge into outward
action. How might this skill be understood and, relatedly, how much of
an account of it does the particularist owe? Some particularists
(sometimes) demur. For example, Dancy described the virtuous agent as
someone possessed of a “contentless ability” to discern
what matters when it matters (Dancy 1993: 50). But many sympathetic to
particularism would want to say more (Garfield 2000; Leibovitz 2014).
One might liken the skill of the virtuous person to a behavioral
ability, such as the ability to ride a bicycle. While this analogy
could prove apt, it risks underrating the extent to which the virtuous
person’s action is rich with judgment and reasoning and is not
simply a sequence of successful performance.
One way to develop the skill model is to urge that the skill of the
virtuous person is akin to the skill of the person with conceptual
competence and then rely on Wittgensteinian rule-following
considerations to urge that conceptual competence cannot be fully
understood in terms of rules or principles. This approach lends itself
to a form of particularism that is less hostile to principles. The
claim is not that there are no principles but that practical wisdom
cannot be fully reduced to principles (McDowell 1979). A
perhaps similar strategy can be pursued by focusing on principles of
reasoning and urging that valid principles of reasoning cannot be
treated simply as objects of propositional knowledge akin to premises
(Carroll 1895; Thomas 2011). One seeming consequence of these
strategies is that particularism it not something distinctive of
morality and other cognate domains. Particularism would be true
everywhere we apply concepts or everywhere we reason. Some might
welcome such global particularism, but we would have lost what for
some was initially attractive—that particularism seemed to
identify something distinctive (even if not unique) about morality. A
second worry is that particularism may no longer pose the threat to
traditional moral theory that is sometimes supposed. If the
categorical imperative were shown to have the same status as modus
ponens, Kantians could sleep easily.
Another way to develop the skill model could urge that the skill in
question is the skill of applying (or reasoning with) generalizations
of a certain kind. Here the claim may be that moral principles (or
generalizations) require judgment to apply, or are defeasible, or come
with implicit ceteris paribus clauses. This commits the
particularist to principles of a kind, while also allowing both that
morality is importantly distinctive and that some traditional moral
theorists have erred by seeking a sort of principle that is not to be
found. This path leads to interesting intermediate positions that are
certainly more friendly to principles than Dancy’s particularism
while at the same time concerned to emphasize the limitations of
principles. For example, Richard Holton (2002) suggests that sound
moral principles are conditionals containing an implicit
“that”s it’ clause. The dictum that lying is wrong
is then more perspicuously expressed as the claim that if an action
amounts to lying and “that”s it’ then the action is
wrong. In this context, “that’s it” expresses the
idea that no other moral principle, given the facts at hand,
supersedes the principle that lying is wrong.
A different but similar idea is developed by Mark Lance and Margaret
Little, who advance a model of true but defeasible moral
generalizations. Here, the claim that lying is morally wrong is
elaborated as the claim that lying is wrong under privileged or normal
conditions. Conditions might fail to be privileged for any number of
reasons—perhaps because the murderer is at the door looking for
your helpful information or, less dramatically, we might be playing a
game in which deception is part of the fun. As this last possibility
suggests, Lance and Little’s proposal seems more expansive than
Holton’s in so far as they allow that a moral generalization
might fail to govern a situation not only in the case that it is
superseded by other moral principles but because the circumstances
might be such that the point of the moral generalization is simply
lost. (See Little 2000 and Lance and Little 2004. For discussion of
the skill needed to apply generalizations see Garfield 2000 and Thomas
2011. For discussion of reasoning with defaults see Horty 2007.)
One interesting question for this approach is whether the skill part
of the equation can be further explicated in terms of principles, even
if these further principles are grasped only implicitly. This issue
has received significant attention from philosophers outside the
particularism debate who are interested in the question whether
knowledge-how can be reduced to knowledge-that (Ryle 1946; Stanley
2011). Perhaps surprisingly, the literature on particularism has not
(to our knowledge) drawn significantly from that debate.
Some generalists, agreeing with particularists that moral knowledge
presupposes a sensitivity to the moral landscape and skill in
deploying what appear to be ceteris paribus laden principles,
argue that such sensitivity and skill is possible only if the
landscape itself is sufficiently patterned (McKeever and Ridge 2006)
This argument is supposed to allow for holism about reasons, and so
the relevant patterning consists in there being a finite number of
considerations that can function as reasons, and that these can be
affected by a finite number of enablers and defeaters operating in
regular, “principled” ways. Particular pieces of moral
knowledge, on this argument, presuppose only “default moral
principles” which specify a feature which ground reasons,
ceteris paribus. A full array of exceptionless principles,
the argument continues, are presupposed by practical wisdom,
characterized as including a capacity for reliably acquiring moral
knowledge in a full range of novel circumstances.
Some resist this argument in its entirety on grounds that we can
regularly gain knowledge in other areas without recourse to principles
(Schroeder 2009). Some charge that the second stage of the argument
depends on overly strong assumptions about the extent of practical
wisdom (Schroeder 2009), and that more modest forms of practical
wisdom can be explained without recourse to exceptionless principles.
Some argue that a proper account of hedged moral principles is enough
(Väyrynen 2009); others prefer to see moral wisdom as a skill
which, while wide ranging, can fail in utterly novel circumstances
(Leibowitz 2014). Still others have worried that the argument relies
on inflated assumptions about what is required for justification and
knowledge, for example that the knower must be in a position to
affirmatively rule out any possible defeaters (Thomas 2011).
A recurring charge against generalism is that it assumes an outmoded
deductive-nomological (D-N) account of successful explanation.
According to that account, any successful explanation must take a
deductive structure in which a covering law is identified that,
together with empirical information, could yield a conclusion
expressing the phenomenon to be explained. For many reasons, the D-N
model is now widely thought to be misguided.
On behalf of the generalist, one might make two points. First, it is
not clear that a generalist argument from practical wisdom needs to
assume that all successful explanations must conform to the
D-N model. The argument draws upon claims about the person of (highly
ideal) practical wisdom and asks how best to explain her reliability.
Second, while the argument does insist that we must credit the
virtuous agent with at least an implicit grasp of a principle, it is
less clear that the argument must treat this principle as functioning
as the major premise in a deduction. Likewise, we might credit an
agent with grasping modus ponens to explain her logical success
without thereby assuming that she uses modus ponens as a premise.
Generalists sometimes invoke premises about the nature of moral
concepts or about the meanings of moral words to argue for their view.
Ultimately these arguments appeal to what can be derived from a
certain kind of competence—either semantic or conceptual
competence. It is probably no accident that purely semantic/conceptual
arguments to settle the debate over particularism/generalism have been
monopolized by generalists. If the generalist could show that
semantic/conceptual competence commits one to some specific moral
principle(s), or to the existence and availability of some such
principles, then that would already be enough to establish an
ambitious form of generalism. By contrast, a particularist who showed
only that such competencies do not yet commit one either to some
specific moral principle(s) or to the existence and availability of
such principles would not yet have established a very ambitious form
of particularism. For that negative conclusion is logically consistent
with the availability of a convincing epistemological, practical or
metaphysical argument for the existence and availability of suitable
moral principles.
Generalists can and have proceeded in one of two main ways here.
First, they might argue that semantic/conceptual competence directly
commits one to the truth of some specific moral principle(s). Second,
they might argue that such competence commits one only to the weaker
thesis that if there are any substantive moral truths then
there must be some true moral principle(s). This second thesis is
weaker both in that it takes a conditional form, so that an error
theorist could endorse it but deny the existence of any true moral
principles and in that it does not entail that there is some
specific moral principle(s) to which one is committed insofar as one
thinks there are substantive moral truths. Consider each of these
strategies in turn.
The most ambitious and straightforward version of the first strategy
is effectively just to argue for a form of analytic naturalism in
meta-ethics. For example, consider the meta-ethical theory that
“is morally right” just means “is an action
which maximizes happiness”, where “happiness” is
itself cashed out in purely naturalistic terms. Any convincing
argument for that theory would provide a way of carrying out a very
ambitious version of the first of the two strategies discussed above.
Clearly, insofar as that theory is correct, semantic competence with
“is morally right” is enough to commit one to the thesis
that, necessarily, an action is morally right if and only if it
maximizes happiness, and that certainly looks like just the
right sort of generalization to function as a principle qua
standard in the sense laid out in
 section 2.
Of course, this strategy for defending generalism is for good reason a
highly controversial one. For a start, nobody has come close to
offering a fully reductive definition of predicates like “is
morally right” which has met with widespread assent. Moreover,
some philosophers are sceptical of the very idea that knowing the
meaning of a word (or possessing a concept) is already enough, in
principle, to know how to live a good life. In way, this concern about
pulling a highly substantive rabbit out of a purely
semantic/conceptual hat can be seen as what lies behind one of the
historically most influential arguments against analytic naturalism,
namely G.E. Moore’s “Open Question Argument” (Moore
1922 [1903]). Finally, anyone who is initially sympathetic to
particularism is very unlikely to find analytic naturalism an ex
ante plausible view, given how trivially it entails a very robust
form of generalism. There is a sense, then, in which this strategy for
defending generalism, however sound it might turn out to be, is
unlikely to convince anyone who needs convincing (cf. Jackson, Pettit,
& Smith 2000—the argument there seems ultimately to turn
into a version of this first strategy).
A less ambitious form of the first strategy focuses on so-called
“thick” evaluative words or concepts. Such words/concepts
in some sense have both specific descriptive and normative contents.
Concepts associated with virtues and vices are classic examples of
thick evaluative concepts—concepts like courage, justice,
fairness and generosity are all paradigm cases. The argument for
generalism focusing on these concepts takes the same form as the more
ambitious argument just canvassed. That is, the argument derives a
commitment to moral principle(s) from mere conceptual/semantic
competence.
However, the intended conclusion of an argument in this style is more
modest. For here the relevant principles do not take one from a purely
descriptive antecedent to a purely normative all things considered
consequent, as with (e.g.) the principle of utility. Rather, the
relevant principles here take one from an antecedent deploying a thick
evaluative concept (like the concept of justice) to a consequent
deploying a thin normative concept (like the concept of a reason).
Such an argument might maintain, for example, that competence with the
concept of justice commits one to a moral principle of the form,
“if an action is just then there is at least some reason to
perform it (namely, its justice)”.
Even this modest form of generalism is not uncontroversial. Dancy, for
example, argues that even thick evaluative features can vary in their
normative valence from one context to another, going so far as to
maintain that “almost all the standard thick concepts…are
of variable relevance” (Dancy 2004: 121). Insofar as this sort
of view is as much as semantically/conceptually coherent, there can be
no straightforward derivation of moral principles of the sort
canvassed above from mere semantic/conceptual competence. Of course,
there may be more “hedged” principles linking thick
evaluative concepts with thin normative concepts—principles
which either enumerate or quantify over further conditions which must
be met before the application of a thick evaluative predicate entails
the application of a thin normative predicate. In effect, this is just
the point about holism not entailing particularism again. However, it
is also unclear just how one would plausibly argue that insofar as
such hedged principles are not vacuous, they really do follow from
mere semantic/conceptual competence.
There may also be some interesting asymmetries between virtue concepts
and vice concepts which are relevant to how we should think about
these arguments. In an interesting series of papers, Rebecca Stangl
has argued for a view she calls “asymmetrical virtue
particularism” (Stangl 2010). On this view, an action is right,
all things considered, insofar as it is overall virtuous. However, the
virtues of an action in any specific respect (justice, courage, or
whatever) can vary in normative valence. However, vices on
this view are invariable—they always count against an action.
The deeper explanation of this asymmetry, on Stangl’s view, is
that virtues have “targets” at which they aim, whereas
vices are simply tendencies to miss the relevant targets. Vices are
thus parasitic on virtues but not vice-versa. Thus a given virtue
(e.g., mercy) can sometimes be wrong-making because it helps explain
why the agent (badly) misses the target associated with some other
virtue (e.g., justice). By contrast, Stangl argues, a vice always is a
tendency to miss some relevant target, and so is therefore always to
that extent wrong-making. Insofar as Stangl makes a convincing case
for this asymmetry (and obviously a lot more could be said about
this), we should be less sympathetic to arguments which hold that
there is a semantic/conceptual link between the virtuousness of an
action in a specific way and the presence of an associated reason for
action.
 Moreover, this more modest form of generalism presupposes that our
thick concepts of justice, courage, generosity, and the like must be
genuinely evaluative concepts. But this is controversial. Pekka
Väyrynen (2013), for instance, argues that the evaluations we
typically associate with thick terms such as “just” and
“courageous” are conversational implications which arise
from our use of those words in a wide range of contexts. Very roughly,
the idea is that evaluative content is a kind of generalized content
which is explained pragmatically. For example, it may become common
knowledge that only people who disapprove of the sexually explicit
tend to use the word “lewd”. In that case, someone who
uses that word thereby implies that she disapproves of the sexually
explicit – otherwise, why use the word “lewd” instead of
“sexually explicit”, given that one’s interlocutors
will reasonably infer from the use of the former that one disapproves
of the sexually explicit.
If this is right, then the status of thick words as evaluative depends
on contingent facts about the pragmatics of our use of those words.
There is then an important sense in which thick terms are, on this
view, descriptive in their semantic content. So although there is a
broader kind of speaker competence which involves understanding the
conversational defaults associated with the relevant words, this is
not the kind of semantic competence that could ground an argument for
generalism or particularism. Semantic competence with thick words is
also unlikely to commit one to any interesting moral principles.
Depending on our views of concepts, this view about thick language can
allow that some thinkers' concepts of justice, generosity, and courage
may be evaluative. But that is unlikely to be essential to those
concepts, nor will the capacity forthought about justice and other
thick notions depend on having genuinely evaluative thick concepts
(Väyrynen 2013: 123–4, 206). In that case, competence with concepts
like justice, courage, and generosity is also unlikely to commit us to
any interesting moral principles.  Moreover, the more modest form of
generalism may require that a concept isn't a concept of justice, or
courage, or generosity, unless it is evaluative. In that case the
pragmatic view of thick evaluative language would support the view
that there are no thick evaluative concepts and, therefore, no such
thing as competence with thick evaluative concepts.
However, generalists do not have a monopoly on arguments which take
theses about thick evaluative concepts/predicates as their main
premise. Some particularists argue that thick evaluative concepts are
“shapeless” with respect to the descriptive (see
especially McDowell 1981). Others take a more metaphysical approach,
and argue that thick evaluative properties are “irreducibly
thick” in a way that puts pressure on the generalist. Indeed,
some go so far as to suggest that this even undermines some important
forms of supervenience (see, e.g., Roberts 2011). Whether these
arguments are forceful may depend on the extent to which the argument
that there really are thick evaluative concepts or properties
in the needed sense can avoid begging the question. In this context,
it is not enough that no “shape” at the descriptive level
is built into the meaning of evaluative concepts. Such a weaker
shapelessness thesis would seem to rule out only principles that are
both analytic and reductive. But it seems compatible with the
possibility that someone who did know the extension of evaluative
concepts could then discover a unity or “shape” to that
extension which could be expressed using descriptive concepts. To rule
out this possibility would seem to require a stronger shapelessness
thesis according to which the extension of evaluative terms, properly
understood, has no shape at all. Generalists will want to see an
argument for this stronger thesis. Perhaps more importantly, though,
settling whether this stronger version of the shapelessness thesis is
true would seem to require more than a priori theorizing
about moral concepts and more than semantic theorizing about
evaluative terms. (For discussion of shapelessness and the metaethical
lessons to be drawn from it see Väyrynen 2014 and Miller
2003.)
So much for the first of the two strategies for giving a
semantic/conceptual argument for generalism canvassed above. What
about the second? Recall that the second strategy is less ambitious
insofar as it aims to establish only a conditional thesis linking
substantive moral truth to the existence of some moral principle(s) or
other. The guiding idea here is that a proper analysis of our moral
concepts will reveal that deploying those concepts to make a
substantive moral judgment commits one to the existence and truth of
some moral principle(s) or other which somehow explains the truth of
that judgment. Crucially, though, this commitment to the existence of
some such moral principle(s) does not entail that the speaker is
committed to any particular moral principle(s), or even to
the possibility in principle of discovering what the relevant
principle(s) are.
A modified version of T.M. Scanlon’s contractualist theory of
“what we owe one another” (Scanlon 1998) helps to
illustrate this strategy. Scanlon himself does not intend his theory
as a conceptual analysis, in part because there are strands of moral
thinking, like our thoughts about the moral status of nonhuman animals
and the environment and certain forms of moralizing about human
sexuality, which do not fit very well into his proposed framework.
However, a version of his theory which was offered as an analysis of
our moral concepts would provide a clear illustration of the strategy
for defending generalism under consideration. On Scanlon’s view,
to be morally wrong in the sense of “wrong” associated
with what he calls the “morality of what we owe one
another” is to be forbidden by principles for the general
regulation of human behaviour which nobody could reasonably reject.
The notion of the “reasonable” is a thick evaluative
concept, so the view is not a reductive one. If the view were to be
understood as following directly from an analysis of our moral
concepts, then it would follow that anyone who makes a substantive
moral judgment that some action is morally wrong would thereby be
committed to the existence of at least a range of moral principles
(the “reasonable” ones) which are such that they all
forbid the action in question. At the same time, making such a
judgment does not entail that one can articulate what the relevant
principle(s) is (are), or even that they are such that one could in
principle discover them.
Another way of arguing for this sort of view is to take a broader
focus on normative and evaluative language. On some views (e.g., Ridge
2014), all uses of “good”, “reason”,
“ought” and “must” advert to
standards of some kind, but the context of utterance
determines the relevant kind of standards. Sometimes, as in moral
contexts, the relevant standards will be normative in some rich sense.
Other times, the relevant standards will be purely conventional, as
when we discuss what one ought to do as a matter of etiquette. In
other contexts the standards will be purely strategic/instrumental, as
when we discuss what move one ought to make in a game of chess, say,
or what military strategy is best, but where one can sincerely make
these judgments while finding chess a total waste of time or being a
committed pacifist. The view aims to accommodate the
context-sensitivity of the relevant words without implausibly
postulating a brute ambiguity across the wide variety of contexts in
which such words are used. As with the conceptual version of
Scanlon’s view, this view is also one on which making a
substantive moral judgment commits one to the existence of a
range of moral standards which require the relevant action (or count
the relevant consideration as a reason, or whatever). 
An attraction of this strategy is that it draws its plausibility from
high level semantic features of words like “good”,
“reason”, “ought” and “must” which
are not specific to normative contexts. It is therefore perhaps
especially unlikely to beg the question against the particularist.
This stands in sharp contrast with the attempt to derive specific
moral principles from mere competence with moral words or
concepts.
As the taxonomy of
 section 2 above
 emphasized, whether moral principles are necessary for moral
understanding or moral explanation is not the only debate between
particularists and generalists. Distinct questions remain about the
place and value of principles in guiding moral decision-making and
action and in interpersonal justification. Generalists typically see a
larger and more important role for principles to play in these
contexts. Particularists typically find at least some sympathy with
David McNaughton’s claim that moral principles are “at
best useless and at worst a hindrance” (McNaughton 1988: 191).
In considering this aspect of the debate, it is helpful to treat as
common ground the idea that it is at least possible for an agent to be
(in some sense) guided by a principle. This assumption has, of course,
been challenged, most prominently by some readings of
Wittgenstein’s arguments concerning rule-following (Kripke
1982). If guidance by principle were utterly impossible, then
questions about the value and importance of principled guidance would
be largely moot. For similar reasons, it is helpful to assume, at
least provisionally, that an agent can eschew being guided by
principles and yet still act rationally and for reasons and with some
measure of consistency.
Against this background, we may distinguish two questions. First, we
might ask whether guidance by principles constitutes a superior
strategy for acting well as compared to guidance by particular
judgments untutored by principles. One familiar way to understand the
superiority of a strategy is in terms of its reliability at leading an
agent to act rightly and for morally good reasons (McKeever and Ridge
2006; Väyrynen 2008). Second, we might ask whether guidance by
principles enables us to secure morally valuable goods (or avoid
significant moral evils) that would otherwise be out of reach. If
particularism tells us to eschew guidance by principles and if doing
so comes with significant costs, then, to use Brad Hooker’s
phrase, there is something “bad” about particularism
(Hooker 2000, 2008). Similarly, if generalism tells us to use
principles and this has serious costs, then there is something bad
about generalism.
These questions leave one familiar and related question largely to the
side. This is the question whether there is something inherently
morally valuable about being a “person of principle”
independent of the content of those principles and how, more
specifically, they lead one to act. Generalists may, but need not,
subscribe to such a view, and even particularists could (consistently
with holism) allow that across some range of contexts being principled
is, itself, a favoring consideration. Turning to the first question
just noted, how might principles constitute a good strategy for moral
action?
Most ambitiously, the ultimate principles qua
standards—that is, the principles which provide the deepest
explanations of why right actions are right—could be well suited
to guiding action directly. Arguably this is the view we find in Kant
and in many modern Kantian moral theories. The categorical imperative
is both the ultimate standard of right action and at the same time is
well suited to guide the decision-making of a conscientious moral
agent. This view of principled guidance ought to be distinguished from
a distinct meta-ethical view according to which an ultimate moral
standard must, if it is to be valid at all, be such that agents can be
(in some sense) guided by it (Bales 1971; Smith 2012). Such a view may
be attractive to those (such as Kant) who think that moral principles
must comport with autonomy and that morality is a species of
rationality. It may also be attractive to those who believe that moral
principles must provide reasons on which agents can act. But even a
very ambitious generalist model of principled guidance need not
subscribe to this meta-ethical view. Kant, at least in some passages,
encourages optimism about our ability to apply the ultimate standard
of right and wrong directly to our individual decisions. Other
philosophers within the generalist tradition, such as Ross, defend
principles which look, on their face, to be eminently usable, and if
Ross is correct that such principles are ultimate standards, then one
might feel entitled at least to a weak presumption in favour of the
claim that using them would be a good strategy.
When we consider other candidates for the ultimate moral principle,
however, many find reasons to be sceptical that the ambitious model
just canvassed will carry us very far. This has been a recurring worry
for act consequentialism and, for that reason, many of the most
influential attempts to deal with it have emerged from philosophers
working in that tradition. The basic idea is that the consequences of
our action are so many, so various, and (often) so far reaching that
agents cannot figure out in a timely fashion what the right act is by
directly using a consequentialist principle. Using the
consequentialist principle in this sense must of course include
gathering the facts about the consequences, not just applying the
principles to the facts as one believes or knows them to be. (For
discussion of weaker and stronger senses in which an agent might
“use” a principle, see Smith 2012.) Properly understood,
the worry here is not that the act consequentialist principle provides
no guidance whatsoever; it may point quite clearly to the kinds of
information that must be gathered and heeded. The worry is that
attempts to follow the principle will not reliably lead to morally
right action. Moreover, the worry is not simply that the principle
fails to constitute a complete and reliable strategy. Any model of
principled guidance—even one such as Kant’s—is
liable to require that we rely also on cognitive and emotional powers
that go beyond the principle itself. The worry is that our normal
cognitive and emotional powers together with the principle do not
yield a reliable strategy for performing morally right actions.
Instead of concluding that principled guidance is hopeless, many act
consequentialists have instead proposed that we replace the project of
being guided by the ultimate moral standard (assuming this for the
moment to be some form of act consequentialism) and instead be guided
by some more tractable set of principles. According to such
“indirect” consequentialism, the principles we typically
employ in deliberation are not the ultimate standards of right
conduct. However, an agent who employs them in deliberation will
regularly and systematically act rightly. Such proposals have been a
staple of consequentialist thinking dating back at least to the work
of Mill and Sidgwick. An especially well known recent version of the
idea is defended by R.M. Hare, who calls reliance on such principles
“intuitive moral thinking”. By contrast, “critical
moral thinking” proceeds in terms of the actual standards of
moral conduct (Hare 1981). Importantly, neither indirect models of
principled guidance nor the worry that inspires them need be married
to a consequentialist view of moral standards. Kantian moral
philosophers have sometimes stressed the need for
“mid-level” principles (Hill 1989, 1992). Even
particularists about standards could consistently embrace the use of
such an indirect strategy and so embrace a kind of generalism about
moral guidance, though so far as we know no one has actually adopted
this position.
Discussions of indirect consequentialism often proceed as if the
correct moral standard could, in principle, be applied directly to any
given circumstance and, if so applied, would indicate the morally
right action(s) to take. Leaving aside whether this is true of (some)
consequentialist principles, many claim that it is not true of other
candidate moral standards. Consider, for example, principles such as
“all persons must be treated as moral equals”, or
“property rights must be respected”, or, to borrow a less
morally loaded example from Onora O’Neill, “teachers must
assign work appropriate to their students’ abilities”
(O’Neill 1996: 73–77). Such principles may not yield
determinate guidance in concrete circumstances even given a full array
of non-moral facts. To be properly applied, such principles may
require additional moral judgment. We must determine just
which individuals are persons and what it is to treat persons as moral
equals. We must determine which claims to property correspond to valid
rights and what invasions of property amount to a failure to respect
those rights. May an exhausted runner harmlessly trespass in order to
cool off beneath the shade of another person’s tree? We must
even decide how difficult is too difficult when it comes to
challenging students. The obstacle to using the standard as a direct
guide to conduct is not that our cognitive resources come up short,
but that the standard is itself not yet sufficiently determinate. This
situation presents an opportunity for principles to play a guiding
role by helping to fill in the normative content of higher level
standards. (Whether such guiding principles would themselves count as
non-ultimate standards is a question we here set aside.) Importantly,
however, guiding principles in this sense need not make fully
determinate the higher level principles that they help to fill out.
They may, instead, explicitly identify further questions to be
settled—whether by other principles or by judgment. For example,
the principle that a duly convicted criminal ought to receive only the
amount of punishment he deserves is highly abstract. How ought we to
determine whether a punishment is deserved? The further principle that
the punishment ought to be proportional to the crime may direct us to
find a way to proportionately rank less and more serious crimes, and
it may thus point us part, but not all, of the way towards complying
with the higher level principle. 
We now have at least three accounts of how principles might
figure in a reliable strategy for acting well. But why think that
principles do or must figure in the best strategies for moral action?
Or, taking the other side, why think that principles are useless or
even counterproductive? If one could establish or assume a specific
generalist account of moral standards, this would open up many lines
of argument for guidance by principle. The same would be true if one
could establish particularism about standards. However, such
assumptions are not dialectically available in the
generalism/particularism debate. Accordingly, we here focus on
arguments that are largely neutral about the content of the moral
domain and whether it is “principled”.
Some generalists argue that moral principles help avoid “special
pleading”—interpreting one’s moral duties in ways
that favour one’s own interests and in ways that go beyond what
a reasonable accommodation of self-interest would allow. Agents who
engage in special pleading do not do so consciously, but rather think
they are impartially assessing what morality demands in their
circumstances. The adoption of moral principles might be thought to
help with this problem. For one thing, principles can be adopted and
internalized well before any conflict with the agent’s interests
arises. Having internalized the relevant principle well in advance may
make it easier to avoid special pleading when a conflict does arise
(cf. McKeever and Ridge 2006: 202–203). Furthermore, a practice
of articulating these principles publicly endows them with symbolic
meaning. Violating explicitly endorsed principles or adding caveats in
an ad hoc manner to suit one’s interests can come to
stand for our lacking the right kind of commitment to morality more
generally (see Nozick 1993: 29; McKeever and Ridge 2006:
204–205). Anecdotally, some people seem to think New
Years’ resolutions work in this way, and George Ainslie has
provided a body of empirical evidence that such public resolutions can
help motivate agents to (e.g.) stop smoking in a way that somehow
prevents the thought that “one cigarette won’t make any
non-negligible difference” from undermining their resolve
(Ainslie 1975 and Ainslie 1986).
Particularists agree that special pleading is a problem but they do
not think that principles afford the proper solution to that problem.
Instead, they typically suggest that one simply needs to “look
harder” at the case at hand to avoid such special pleading:
…the remedy for poor moral judgment is not a different style of
moral judgment, principle-based judgment, but just better moral
judgment. There is only one real way to stop oneself distorting things
in one’s favour, and that is to look again, as hard as one can,
at the reasons present in the case, and see if really one is so
different from others that what would be required of them is not
required of oneself. The method is not infallible, I know; but then
nor was the appeal to principle. (Dancy 2013)
Generalists worry that the exhortation to look again is simply
unrealistic, given human nature, and is therefore not only fallible
but unlikely to do much good. If so, then even if principles are far
from infallible rejecting them wholesale is premature. The best way to
avoid special pleading could involve an array of more specific
strategies with principles playing some significant role.
On the other side particularists worry that reliance on principles
breeds inflexibility and a problematic tendency to shoehorn a
morally complex situation into some more familiar set of categories.
McNaughton describes such inflexibility as a “serious
vice” and claims that reliance on principles is partly to blame
(McNaughton 1988: 203). Dancy remarks that, 
We all know the sort of person who refuses to make the decision here
that the facts are obviously calling for, because he cannot see how to
make that decision consistent with one he made on a different
occasion. (Dancy 1993: 64) 
Importantly, this worry cannot be dismissed simply on the grounds that
generalists can (and do) allow judgment to also play a role in our use
and application of principles; the worry is that the use of principles
has a distorting influence of its own. One interesting and empirically
minded proposal for evaluating the force of the particularist’s
concern looks to the literature on the comparative success of rules
and expert judgment in other domains (Zamzow 2015). Much of this
literature suggests that rules outperform expert judgment (see Grove
et al. 2000).
Let us turn now to a second family of arguments for principled
guidance. Setting aside whether principles are a winning strategy for
the individual aiming at virtuous action, one might think that our
collective use of principles enables us to achieve morally valuable
goods. One such argument appeals to the value of predictability
(Hooker 2000, 2008). Successful cooperation and coordination yield
enormous benefits yet it requires an ability to predict the behaviour
of others and a willingness to rely on those predictions when making
one’s own choices. If principled guidance supports
predictability, so much the better for principles. Not surprisingly,
particularists have questioned whether principles are necessary for
predictability. “People are quite capable of judging how to
behave case by case, and in a way that would enable us to predict what
they will in fact do” (Dancy 2004: 83). The key issue is
comparative. Is the person guided by principles thereby more
predictable than the person who eschews principles? Someone who
rejects moral rules altogether and always just tries to judge each
case on its own merits plausibly is less predictable than someone who
has internalized and follows a set of moral principles. But as we saw
above, assessing the force of this generalist argument would benefit
from consideration of careful empirical research. One challenge for
generalists who might further develop this argument is that it stands
in some tension with other themes stressed by generalists, for
example, that principles can incorporate various hedges and so exhibit
the kind of flexibility particularists embrace (Väyrynen 2008)
and that principles are often indeterminate and must be supplemented
by judgment. To be consistent, generalists will need to show not only
that guidance by crude principles makes one more predictable, but that
guidance by a combination of hedged principles and judgment makes one
more predictable than guidance by judgment alone.
A very different practical argument for generalism has roots in the
Kantian tradition and has recently been advanced by Stephen Darwall
(2013, see also Darwall 2006). He contends that publicly formulable
principles are necessary for us to realize a valuable form of
interpersonal accountability in our shared moral life. He further
argues that such accountability is necessary for moral obligations
(though not necessarily for moral reasons). Within the framework here
developed, one might see Darwall’s argument as a defense of
generalism about standards but with the argument restricted
to standards of moral obligation. Alternatively, one might
see it as a practical argument for attempting to formulate shared
public principles because, if we fail to do so (or fail to continue to
do so), we will lose something that we take to be valuable about
morality, namely the respect for persons that is inherent in a
practice of interpersonal accountability (Darwall 2013: especially
183–191). Darwall’s argument fits very well with Kantian
contractualism of the sort defended by T.M. Scanlon, which emphasizes
the value of our being able to justify ourselves to others and sees
principles as mediating justification. It might also be instructive to
compare Darwall’s argument with some of the ideas found in the
tradition of discourse ethics associated with Jürgen Habermas
(see, e.g., Habermas 1990). An important challenge for this argument
is to persuasively establish the premise that accountability (or
interpersonal justification) must advert to principles. Particularists
may allow that accountability is an important value while urging that
the interpersonal process of holding one another accountable can
proceed entirely in terms of the reasons, defeaters, enablers, and
intensifiers that are at play in the case at hand.