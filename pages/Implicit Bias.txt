While Allport’s (1954) The Nature of Prejudice
remains a touchstone for psychological research on prejudice, the study
of implicit social cognition has two distinct and more recent sets
of
 roots.[3]
The first stems from the
distinction between “controlled” and
“automatic” information processing made by cognitive
psychologists in the 1970s (e.g., Shiffrin & Schneider 1977). While
controlled processing was thought to be voluntary, attention-demanding,
and of limited capacity, automatic processing was thought to unfold
without attention, to have nearly unlimited capacity, and to be hard to
suppress voluntarily (Payne & Gawronski 2010; see also Bargh 1994).
In important early work on implicit cognition, Fazio and colleagues
showed that attitudes can be understood as activated by either
controlled or automatic processes. In Fazio’s (1995)
“sequential priming” task, for example, following exposure
to social group labels (e.g., “black”, “women”,
etc.), subjects’ reaction times (or “response
latencies”) to stereotypic words (e.g., “lazy” or
“nurturing”) are measured. People respond more quickly to
concepts closely linked together in memory, and most subjects in the
sequential priming task are quicker to respond to words like
“lazy” following exposure to “black” than
“white”. Researchers standardly take this pattern to
indicate a prejudiced automatic association between semantic concepts.
The broader notion embedded in this research was that subjects’
automatic responses were thought to be “uncontaminated” by
controlled or strategic responses (Amodio & Devine 2009).
While this first stream of research focused on automaticity, a
second stream focused on (un)consciousness. Many studies demonstrated
that awareness of stereotypes can affect social judgment and behavior
in relative independence from subjects’ reported attitudes
(Devine 1989; Devine & Monteith 1999; Dovidio & Gaertner 2004;
Greenwald & Banaji 1995; Banaji et al. 1993). These studies were
influenced by theories of implicit memory (e.g., Jacoby & Dallas
1981; Schacter 1987), leading to Greenwald & Banaji’s
original definition of “implicit attitudes” as
introspectively unidentified (or inaccurately identified) traces of
past experience that mediate favorable or unfavorable feeling, thought,
or action toward social objects. (1995: 8)
The guiding idea here, as Dovidio and Gaertner (1986) put it, is
that in the modern world prejudice has been “driven
underground,” that is, out of conscious awareness. This idea has
led to the common view that what makes a bias implicit is that a person
is unwilling or unable to report it. Recent findings have challenged
this view, however (§3.1)
What a person says is not necessarily a good representation of the
whole of what she feels and thinks, nor of how she will behave.
Arguably, the central advance of research on implicit social cognition
is the ability to assess people’s thoughts, feelings, and
behavior without having to ask them directly, “what do you
think/feel about X?” or “what would you do in X
situation?” 
Implicit measures, then, might be thought of as instruments that
assess people’s thoughts, feelings, and behavior indirectly, that
is, without relying on “self-report.” This is too quick,
however. For example, a survey that asks “what do you think of
black people” is explicit and direct, in the sense that the
subject’s judgment is both explicitly reported and the subject is
being directly asked about the topic of interest to the researchers.
However, a survey that asks “what do you think about
Darnell” (i.e., a person with a stereotypically black name) is
explicit and indirect, because the subject’s judgment is
explicitly reported but the content of what is being judged (i.e., the
subject’s attitudes toward race) is inferred by the researcher.
The distinction between direct and indirect measures is also relative
rather than absolute. Even in some direct measures, such as personality
inventories, subjects may not be completely aware of what is being
studied.
In the literature, “implicit” is used to refer to at
least four distinct things (Gawronski & Brannon 2017): (1) a
distinctive psychological construct, such as an “implicit
attitude,” which is assessed by a variety of instruments; (2) a
family of instruments, called “implicit measures,” that
assess people’s thoughts and feelings in a specific way (e.g., in
a way that minimizes subjects’ reliance on introspection and
their ability to respond strategically); (3) a set of cognitive and
affective processes—“implicit processes”—that
affect responses on a variety of measures; and (4) a kind of evaluative
behavior—e.g., a categorization judgment—elicited by
specific circumstances, such as cognitive load. In this entry, I
will use “implicit” in the senses of (2) and (4), unless
otherwise noted. One virtue of this approach is that it allows one to
remain agnostic about the nature of the phenomena implicit
measures
 assess.[4]
Consider Frank again.
His implicit gender bias may be assessed by several different
instruments, such as sequential priming or the “Implicit
Association Test” (IAT; Greenwald et al. 1998). The IAT—the
most well-known implicit test—is a reaction time measure. In a
standard IAT, the subject attempts to sort words or pictures into
categories as fast as possible while making as few errors as possible.
In the images below, the correct answers would be left, right, left,
right.
Image 1
Image 2
Image 3
Image 4
All images are copyright of 
Project Implicit
and reproduced here with permission.
An IAT score is computed by comparing speed and error rates on the
“blocks” (or trials) in which the pairing of concepts is
consistent with common stereotypes (images 1 and 3) to the blocks in
which the pairing of the concepts is inconsistent with common
stereotypes (images 2 and 4). If he is typical of most subjects, Frank
will be faster and make fewer errors on stereotype-consistent trials
than stereotype-inconsistent trials. While this
“gender-career” IAT pairs concepts (e.g.,
“male” and “career”), other IATs, such as the
“race-evaluation” IAT, pair a concept to an evaluation
(e.g., “black” and “bad”). Other IATs assess
body image, age, sexual orientation, and so on. As of 2019,
approximately 26 million IATs have been taken (although it is unclear
if this number represents 26 million unique participants or 26 million
tests taken or started; Lai p.c.). One review (Nosek et al. 2007),
which tested over 700,000 subjects on the race-evaluation IAT, found
that over 70% of white participants more easily associated black faces
with negative words (e.g., war, bad) and white faces with positive
words (e.g., peace, good). The researchers consider this an implicit
preference for white faces over black
 faces.[5]
Although the IAT remains the most popular implicit measure, it is
far from the only one. Other prominent implicit measures, many of which
are derivations of sequential priming, are semantic priming (Banaji
& Hardin 1996) and the Affect Misattribution Procedure (AMP; Payne
et al. 2005). Also, a “second generation” of
categorization-based measures (like the IAT) has been developed. For
example, the Go/No-go Association Task (GNAT; Nosek & Banaji 2001)
presents subjects with one target object rather than two in order to
determine whether preferences or aversions are primarily responsible
for scores on the standard IAT (i.e., the ease of pairing good words
with white faces and bad words with black faces, or the difficulty of
pairing good words with black faces and bad words with white faces;
Brewer 1999).
A notable advance in the psychometrics of implicit bias has been the
advent of multinomial (or formal process) models, which identify
distinct processes contributing to performance on implicit measures.
For example, elderly people tend to show greater bias on the
race-evaluation IAT compared with younger people, but this may be due
to their having stronger preferences for whites or having weaker
control over their biased responding (Nosek et al. 2011). Multinomial
models, like the Quadruple Process Model (Conrey et al. 2005), are used
to tease apart these possibilities. The Quad model identifies four
distinct processes that contribute to responses: (1) the automatic
activation of an association; (2) the subject’s ability to
determine a correct response (i.e., a response that reflects
one’s subjective assessment of truth); (3) the ability to
override automatic associations; and (4) general response biases (e.g.,
favoring right-handed responses). Multinomial modeling has made
clear that implicit measures are not “process pure,” i.e.,
they do not tap into a single unified psychological process.
While there is not consensus about what implicit measures capture
(§2), it is clear that they provide at least three kinds of
information (Gawronski & Hahn 2019). The first is information about
dissociation with more explicit, direct measures. Correlations between
implicit and explicit measures tend to be relatively low (r =
.2–.25; Hofmann et al. 2005; Cameron et al. 2012), although these
relations are significantly affected by methodological practices, such
as comparing non-corresponding implicit and explicit measures (e.g., an
implicit measure of gender stereotypes and an explicit “feelings
thermometer” toward women). It is important to note the breadth
of research in this vein; dissociations between implicit and explicit
measures are found in the study of personality (e.g., Vianello et al.
2010), attitudes toward alcohol (e.g., de Houwer et al. 2004), phobias
(Teachman & Woody 2003), and more. Second, implicit measures can be
used as dependent variables in experiments. Theories about the
formation and change of attitudes, for example, have focused on
differential effects of manipulations, such as counter-attitudinal
information, on implicit and explicit measures (e.g., Gawronski &
Bodenhausen 2006; Petty 2006). Third, implicit measures are used to
predict behavior. Philosophers have been especially interested in the
relationship between implicit bias and discriminatory behavior,
particularly when the discriminatory behavior conflicts with a
person’s reported beliefs (as in the “Frank” case
above). Studies report relationships between implicit bias and behavior
in a huge variety of social contexts, from hiring to policing to
medicine to teaching and more (for an incomplete list see Table 1 in
Jost et al. 2009). There is also voluminous, varied, and on-going
discussion about how well implicit measures predict behavior, along
with several related critical assessments of the information implicit
measures provide (§5).
“Implicit bias” is a term of art, used in a variety of
ways. In this entry, the term is used to refer to the family of
evaluative judgments and behavior assessed by implicit measures (e.g.,
categorization judgments on an IAT). These measures mimic some
relevant aspects of judgment and decision-making outside the lab
(e.g., time pressure). But what do these measures measure? With some
blurry boundaries, philosophical and psychological theories can be
divided into five groups. Implicit measures might provide information
about attitudes (§2.1), implicit processes (§2.2), beliefs
(§2.3), traits (§2.4), or situations (§2.5). 
The idea that people’s attitudes are the cause of implicit
bias is pervasive. The term “attitudes” tends to be used
differently in psychology and philosophy, however. In psychology,
attitudes are akin to preferences (i.e., likings and dislikings); the
term does not refer to propositional states per se (i.e., mental states
that are thought to bear a relationship to a proposition), as it does
in philosophy. Most attitudinal theories of implicit bias use the term
in the psychologist’s sense, although variations will be noted
below.
2.1.1 Dual Attitudes in Psychology
Early and influential theories posited that people hold two distinct
attitudes in mind toward the same object, one implicit and the other
explicit (Greenwald & Banaji 1995; Wilson et al. 2000).
“Explicit attitudes” are commonly identified with verbally
reported attitudes, in this vein, while “implicit
attitudes” are those that a person is unwilling or unable to
report. Evidence for theories of dual attitudes stems largely from two
sources. The first are anecdotal reports of surprise and consternation
that people sometimes express after being informed of their performance
on an implicit measure (e.g., Banaji 2011; Krickel 2018). These
experiences suggest that people discover their putative implicit
attitudes by taking the relevant tests, just like one learns about
one’s cholesterol by taking the relevant tests. The second source
of evidence for dual-attitude views are dissociations between implicit
and explicit measures (§1.2). These suggest that implicit and
explicit measures may be tapping into distinct representations of the
same attitude-object (e.g., “the elderly”).
A central challenge for theories of this sort is whether people
truly are unaware of their implicit biases, and if so, in what way
(e.g., if people are unaware of the source, content, or behavioral
effects of their attitudes; §3.1). There may be reasons to posit
unconscious representations in the human mind independent of whether
people are or are not aware of their implicit biases, of course. But if
people are aware of their implicit biases, then implicit measures are
most likely not assessing unconscious “dual”
attitudes. 
2.1.2 Dual Attitudes in Philosophy
Some philosophers have proposed that implicit measures assess a
distinct kind of “action-oriented” attitude, which is
different from ordinary attitudes, but not necessarily in terms of
being unconscious. The core idea here is that implicit attitudes link
representations with behavioral
 impulses.[6]
 Gendler’s (2008a,b, 2011, 2012) account
of “alief,” a sui generis mental state comprised
of tightly woven co-activating representational (R), affective
(A), and behavioral (B) components, is emblematic of
this approach. Gendler argues that the R-A-B components of
alief are “bundled” together or “cluster” in
such a way that when an implicitly biased person sees a black face in a
particular context, for example, the agent’s representation will
automatically activate particular feelings and behaviors (i.e., an
R–A–B cluster). This is in contrast to the
“combinatoric” nature of ordinary beliefs and desires, that
is, that any belief could, in principle, be combined with any desire.
So while the belief that “that is a black man” is not fixed
to any particular feelings or behavior, an alief will have content
like, “Black man! Scary! Avoid!”
 “To have an alief”, Gendler writes, is
According to Gendler, aliefs explain a wide array of otherwise
puzzling cases of belief-behavior discordance, including not only
implicit bias, but also phobias, fictional emotions, and bad habits
(2008b: 554). In fact, Gendler suggests (2008a: 663) that aliefs are
causally responsible for much of the “moment-by-moment
management” of human behavior, whether that behavior is
belief-concordant or not.
Critics have raised a number of concerns about this approach, in
particular whether putative aliefs form a unified kind (Egan 2011;
Currie & Ichino 2012; Doggett 2012; Nagel 2012; Mandelbaum 2013).
Others have proposed alternate conceptions of action-oriented dual
attitudes. Brownstein and Madva (2012a,b; see also Madva and Brownstein
2018 and Brownstein 2018), for example, propose that implicit attitudes
are comprised of F-T-B-A components: the perception of a
salient Feature triggers automatic low-level feelings of
affective Tension, which are associated in turn with specific
Behavioral responses, which either do or do not
Alleviate the agent’s felt tension. This approach shares
with Gendler’s the idea that aliefs/implicit attitudes differ in
kind from beliefs/explicit attitudes. Moreover, the difference between
these putative kinds of states is not necessarily the agent’s
introspective access to them. Gendler proposes that while paradigmatic
beliefs update when the agent requires new relevant information,
paradigmatic aliefs don’t. In contrast, Brownstein and Madva
argue that implicit attitudes do update in the face of new
information—this is the feed-forward function of
“alleviation”—and thus can automatically yet flexibly
modify and improve over time. Thus, for Brownstein and Madva, implicit
attitudes are implicated not only in bias and prejudice, but also in
skillful, intelligent, and even ethical
 action.[7]
But while implicit attitudes aren’t ballistic,
information-insensitive reflexes, on Brownstein and Madva’s view,
they also don’t update in the same way as ordinary attitudes.
Brownstein and Madva draw the distinction in terms of two key features.
First, implicit attitudes are paradigmatically insensitive to the
logical form in which information is presented. For example, subjects
have been shown to form equivalent implicit attitudes on the basis of
information and the negation of that information (e.g., Gawronski et
al. 2008). Second, implicit attitudes fail to respond to the semantic
contents of other mental states in a systematic way; they appear to be
“inferentially impoverished.” For example, implicit
attitudes are implicated in behaviors for which it is difficult to give
an inferential explanation (e.g., Dovidio et al. 1997) and implicit
attitudes change in response to irrelevant information (e.g., Gregg et
al. 2006; Han et al. 2006). Levy (2012, 2015)—who argues that
implicit attitudes are “patchy endorsements”—makes
similar claims about the ways in which implicit attitudes do and do not
update, although he does not argue that these kinds of states are
“action-oriented” in the way that Gendler and Brownstein
and Madva do. Debate about these findings is ongoing (§2.3).
2.1.3 Single Attitudes
Some theories posit the existence of a singular representation of
attitude-objects. According to MODE (“Motivation and Opportunity
as Determinants”; Fazio 1990; Fazio & Towles-Schwen 1999;
Olson & Fazio 2009) and the related MCM (“Meta-Cognitive
Model”; Petty 2006; Petty et al. 2007), attitudes are
associations between objects and “evaluative knowledge” of
those objects. MODE posits one singular representation underlying the
behavioral effects measured by implicit and explicit tests. Thus, MODE
denies the distinction between implicit and explicit attitudes. The
difference between implicit and explicit measures, then, reflects a
difference in the control that subjects have over the measured
behavior. Control is understood in terms of motivation and opportunity
to deliberate. When an agent has low motivation or opportunity to
engage in deliberative thought, her automatically activated
attitudes—which might be thought of as her “true”
attitudes—will guide her behavior and judgment. Implicit measures
manufacture this situation (of low control due to low motivation and/or
opportunity to deliberate). Explicit measures, by contrast, increase
non-attitudinal contributions to test performance. MODE therefore
provides empirically-testable predictions about the conditions under
which a person’s performance on implicit and explicit measures
will converge and diverge, as well as predictions about the conditions
under which implicit and explicit measures will and will not predict
behavior (see Gawronski & Brannon 2017 for review). 
Influenced by dual process theories of mind, RIM
(“Reflective-Impulsive Model”; Strack & Deutsche 2004)
and APE (“Associative-Propositional Evaluation”; Gawronski
& Bodenhausen 2006, 2011) suggest that implicit measures assess
distinctive cognitive processes. The central distinction at the heart
of both RIM and APE is between “associative” and
“propositional” processes. Associative processes are said
to underlie an impulsive system that functions according to classic
associationist principles of similarity and contiguity. Implicit
measures are thought of as assessing the momentary accessibility of
elements or nodes of a network of associations. This network produces
spontaneous evaluative responses to stimuli. Propositional processes,
on the other hand, underlie a reflective system that validates the
information provided by activated associations. Explicit measures are
thought to capture this process of validation, which is said to operate
according to agents’ syllogistic reasoning and judgments of
logical consistency. In sum, the key distinction between associative
and propositional processes according to RIM and APE is that
propositional processing alone depends on an agent’s assessment
of the truth of a given
 representation.[8]
APE in particular aims to explain the interactions
between and mutual influences of associative and propositional
processes in judgment and behavior.
RIM and APE bear resemblance to the dual attitudes theories in
philosophy discussed above. Indeed, Bodenhausen & Gawronski (2014:
957) write that the “distinction between associative and
propositional evaluations is analogous to the distinction between
‘alief’ and belief in recent philosophy of
epistemology.” It is important to keep in mind, however, that RIM
and APE are not attitudinal theories. APE, for example, posits two
distinct kinds of process—associative and propositional
processes—that give rise to two kinds of evaluative responses to
stimuli—implicit and explicit. It does not posit the existence of
two distinct attitudes or two distinct co-existing representations of
the same entity. It is also important to note that the distinction
between associative and propositional processes can be understood in at
least three distinct senses: as applying to the way in which
information is learned, stored, or expressed (Gawronski et al. 2017).
At present, evidence is mixed for dissociation between associative and
propositional processing in the learning and storage of information,
while it is stronger for dissociation in the behavioral expression of
stored information (Brownstein et al. 2019). 
Some have argued that familiar notions of belief, desire, and
pretense can in fact explain what neologisms like “implicit
attitudes” are meant to elucidate (Egan 2011; Kwong 2012;
Mandelbaum 2013). Most defend some version of what Schwitzgebel (2010)
calls Contradictory Belief (Egan 2008, 2011; Huebner 2009; Gertler
2011; Huddleston 2012; Muller & Bashour 2011; Mandelbaum 2013,
2014,
 forthcoming).[9]
Drawing
upon theories of the “fragmentation” of the mind (Lewis
1982; Stalnaker 1984), Contradictory Belief holds that implicit and
explicit measures both reflect what a person believes, and that these
different sets of beliefs may be causally responsible for different
behavior in different contexts (Egan 2008). In short, if a person
behaves in a manner consistent with the belief that black men are
dangerous, it is because they believe that black men are dangerous
(notwithstanding what they say they believe).
In the psychological literature, De Houwer and colleagues defend a
view that can be thought of as supporting Contradictory Belief
(Mitchell et al. 2009; Hughes et al. 2011; De Houwer 2014). On this
model,
 propositions[10]
have
three defining features: (1) propositions are statements about the
world that specify the nature of the relation between concepts (e.g.,
“I am good” and “I want to be good” are
propositions that involve the same two concepts—“me”
and “good”—but differ in the way that the concepts
are related); (2) propositions can be formed rapidly on the basis of
instructions or inferences; and (3) subjects are conscious of
propositions (De Houwer 2014). On the basis of data consistent with
these criteria—for example, responses on implicit measures are
affected by one-shot instruction—De Houwer (2014) argues that
implicit measures capture propositional states (i.e.,
 beliefs).[11]
This claim represents an
application of Mitchell and colleagues’ (2009) broader argument
that all learning is propositional (i.e., there is no case in
which learning is the result of the automatic associative linking of
mental representations). One reason philosophers have been interested
in this view is due to its resonance with classic debates in the
philosophy of mind between empiricists and rationalists, behaviorists
and cognitivists, and so on.
Another belief-based approach argues that implicit biases should be
understood as cognitive “schemas.” Schemas are clusters of
culturally shared concepts and beliefs. More precisely, schemas are
abstract knowledge structures that specify the defining features and
attributes of a target (Fiske & Linville 1980). The term
“mother”, for example, invokes a schema that attributes a
collection of attributes to the person so labelled (Haslanger 2015). On
some accounts, schemas are “coldly” cognitive (Valian
2005), and so in the psychologist’s sense, they are not
attitudes. Rather, schemas are tools for social categorization, and
while schemas may help to organize and interpret feelings and
motivations, they are themselves affectless. One advantage of focusing
on schemas is that doing so emphasizes that implicit bias is not a
matter of straightforward antipathy toward members of socially
stigmatized groups.
A separate version of the generic belief approach stems from recent
work in the philosophy of language. This approach focuses on
stereotypes that involve generalizing extreme or horrific behavior from
a few individuals to groups. Such generalizations, such as “pit
bulls maul children” or “Muslims are terrorists”, can
be thought of as a particular kind of generic statement, which Leslie
(2017) calls a “striking property generic”. This subclass
of generics is defined by having predicates that express properties
that people typically have a strong interest in avoiding. Building on
earlier work on the cognitive structure and semantics of generics
(Leslie 2007, 2008), Leslie notes a particularly insidious feature of
social stereotyping: even if just a few members of what is perceived to
be an essential kind (e.g., pit bulls, Muslims) exhibit a harmful or
dangerous property, then a generic that attributes the property to the
kind likely will be judged to be true. This is only the case with
striking properties, however. As Leslie (2017) points out, it
takes far fewer instances of murder for one to be considered a murderer
than it does instances of anxiety to be considered a worrier. Striking
property generics may thus illuminate some social stereotypes (e.g.,
“black men are rapists”) better than others (e.g.,
“black men are athletic”). Beeghly (2014), however,
construes generics as expressions of cognitive schemas, which may
broaden the scope of explanation by way of generic statements. In all
of these cases, generics involve an array of doxastic properties.
Generics involve inferences to dispositions, for example (Leslie 2017).
That is, generic statements about striking properties will usually be
judged true if and only if some members of the kind possess the
property and other members of the kind are judged to be disposed to
possess it.
The most explicit defense of Contradictory Belief has been via a
theory of “Spinozan Belief Fixation” (SBF; Gilbert 1991;
Egan 2008, 2011; Huebner 2009; Mandelbaum 2011, 2013, 2014, 2016).
Proponents of SBF are inspired by Spinoza’s rejection of the
concept of the will as a cause of free action (Huebner 2009: 68), an
idea which is embodied in what they call the theory of “Cartesian
Belief Fixation” (CBF). CBF holds that ordinary agents are
capable of evaluating the truth of an idea (or representation, or
proposition) delivered to the mind (via sensation or imagination)
before believing or disbelieving it. Agents can choose to believe or
disbelieve P, according to CBF, in other words, via
deliberation or judgment. SBF, on the other hand, holds that as soon as
an idea is presented to the mind, it is believed. Beliefs on this view
are understood to be unconscious propositional attitudes that are
formed automatically as soon as an agent registers or tokens their
content. For example, one cannot entertain or consider or imagine the
proposition that “dogs are made out of paper” without
immediately and unavoidably believing that dogs are made out of paper,
according to SBF (Mandelbaum 2014). More pointedly, one cannot
entertain or imagine the stereotype that “women are bad at
math” without believing that women are bad at math. As Mandelbaum
(2014) puts it, the automaticity of believing according to SBF explains
why people are likely to have many contradictory beliefs; in order to
 reject P, one must already believe P.[12]
SBF is strongly revisionist with respect to the ordinary concept of
belief (but see Helton (forthcoming) for a similarly spirited but less
revisionist 
 view).[13]
Notwithstanding this, the central line of debate about SBF’s
account of implicit bias—as well as about belief-based accounts
of implicit social cognition generally—focuses on the fact that
people’s performance on implicit measures is sometimes
unresponsive to the kinds of reinforcement learning based interventions
that ought to affect associative processes and/or states; meanwhile,
performance on implicit measures sometimes appears to be responsive to
the kinds of logical and persuasion based interventions thought to
affect doxastic states
(e.g., de
Houwer 2009,
2014; Hu et al. 2017; Mann & Ferguson 2017; Van Dessel et al. 2018;
for additional discussion see Mandelbaum 2013, 2016; Gawronski et al.
2017; Brownstein et al. 2019). Caution is needed in drawing strong
conclusions about cognitive structure from these behavioral data,
however (Levy 2015; Madva 2016c; Byrd forthcoming; Brownstein et al 2019). As
noted above (§1.2), implicit measures are not process-pure.
Modeling technique for disentangling the multiple causal contributions
to performance on implicit measures may help to move these debates
forward (e.g., Conrey et al. 2005; Hütter & Sweldens
2018).
As is the case with terms like “attitude” and
“propositional,” psychologists and philosophers tend to use
the term “trait” in different ways. In psychology,
trait-like constructs are stable over time and across situations. If
you have always disliked eating pork, and never eat it no matter the
context, then your feelings toward pork are trait-like. If you
sometimes decline to eat pork but sometimes indulge, depending on the
company or your mood, then your feelings are more
“state”-like. In the psychologist’s sense,
significant evidence suggests that implicit bias is more state-like
than trait-like. Multiple longitudinal studies have found that
individuals’ scores on implicit measures vary significantly over
days, weeks, and months, much more so than individuals’ scores on
corresponding explicit measures (Cooley & Payne 2017; Cunningham et
al. 2001; Devine et al. 2012; Gawronski et al. 2017). Of course, the
significance of this depends on one’s theory of implicit bias. If
implicit measures are theorized to capture spontaneous affective
reactions (as APE suggests; §2.2), then contextual and temporal
variability in performance should be predicted (because, for
example, one’s immediate reactions to images of women leaders
will likely be different after watching a documentary about Ruth Bader
Ginsburg than after watching Clueless). However, if implicit
measures are meant to “diagnose” stable features of
individuals like political party affiliation, then far less variation
should be expected. Another possibility is that measurement error
contributes significantly to the instability of scores on implicit
measures. The fact that methodological improvements have in some cases
improved the temporal stability of participants’ performance
supports this idea (e.g., Cooley and Payne 2017).
In philosophy, “trait” is used more often in the context
of anti-representationalist, dispositional theories of mind. While
representationalists define concepts like “belief” in terms
of internal, representational structures of the mind, dispositionalists
define concepts like “belief” in terms of tendencies to
behave in certain ways (and perhaps also to feel and think in certain
ways). Building upon Ryle (1949/2009), Schwitzgebel (2006/2010, 2010,
2013) advances a dispositional theory of attitudes (in the
philosophical sense, that is, a theory that claims that beliefs,
desires, hopes, etc. are dispositions). On his view, attitudes have a
broad (or “multitrack”) profile, including dispositions to
feel, think, and speak in specific ways. The dispositional profile of a
given attitude is determined by the folk-psychological stereotype for
having that attitude, not by what’s inside the agent’s
metaphoric “belief box.” For example, to establish that
Jordan believes that women make good philosophers, one would look to
what Jordan says about women philosophers, to her judgments about which
philosophers are good and which aren’t, to her hiring practices,
her gut feelings around men and women philosophers, etc. Agents with
implicit biases pose an interesting challenge to dispositionalists,
since these agents often match only part of the relevant
folk-psychological stereotypes. For example, Jordan might say that she
believes that women make good philosophers but fail to read any women
philosophers (or, recall Frank;
 §1).
On Schwitzgebel’s
“gradualist dispositionalism,” Jordan and Frank would be
“in-between believers,” agents who partly match the
relevant folk-psychological stereotypes for the attitudes in
question.
A related trait-based approach treats the results of indirect
measures as reflective of elements of attitudes, rather than
as assessing attitudes or biases themselves (Machery 2016, 2017). On
Machery’s view, attitudes (in the psychologist’s sense,
that is, preferences) are dispositions and are comprised of various
bases, including feelings, associations, behavioral impulses, and
propositional states like beliefs. (In contrast to Schwitzgebel,
Machery holds a representationalist view of belief, but a
dispositionalist view of attitudes.) To have a racist attitude, on this
picture, is to be disposed to display the relevant mix of these bases,
that is, to display the feelings, associations, etc. that together
comprise the attitude. Implicit measures, then, are said to capture one
of the psychological bases (e.g., her associations between concepts) of
the agent’s overall attitude. Explicit questionnaire measures
capture another psy­chological basis of the agent’s attitude,
behavioral measures yet another basis, and so on. Implicit measures,
then, do not assess “implicit attitudes,” and indeed,
Machery denies that attitudes divide into implicit and explicit kinds.
Rather, implicit measures quantify elements of attitudes. In part, this
proposal is meant to explain some of the key psychometric properties of
implicit measures, such as their instability over time and the fact
that some implicit measures correlate poorly with each other (§5).
These findings are consistent with the notion that different implicit
measures quantify different psychological bases of attitudes, Machery
argues.
One advantage of thinking of implicit biases as traits is that it is
consistent with the way in which personality attributions readily admit
of vague cases. Just as we might say that Frank is partly
agreeable if he extols the virtues of compassion yet sometimes
treats strangers rudely, we might say that Frank is partly
prejudiced. Dispositional theories capture this intuition. On the
other hand, trait-based theories of implicit bias face long-standing
challenges to dispositionalism in the philosophy of mind. One such
challenge is that traits are explanatory as generalizations, not as
token causes of judgment and behavior
(Carruthers 2013). Another is the specter of circularity arising from the simultaneous use of an agent’s behavior to both define her disposition and to point to what her disposition predicts (Bandura,
1971; Cervone et al. 2015; Mischel 1968; Payne et al. 2017). In both
cases, the question for dispositionalism is whether it truly helps to
explain the data, or merely repackages outwardly observed
patterns in new terms.
The most common way people think and write about implicit biases is
as attributes of persons. Another possibility, though, is that
implicit biases are attributes of situations. Although
psychologists have been debating person-based and situation-based
explanations throughout the history of implicit social cognition
research (Payne & Gawronski 2010; Murphy & Walton 2013; Murphy
et al. 2018), the situationist approach has gained steam due to Payne
and colleagues’ (2017) “bias of crowds” model.
Borrowing from the concept of the “wisdom of crowds,” this
approach suggests that differences between situations explains the
variance of scores on implicit measures, rather than differences
between individuals. A helpful metaphor used by Payne and colleagues is
doing “the wave” at a baseball game. Where a person is
sitting in the bleachers, in combination with where the wave is at a
given time, is likely to outperform most individual differences (e.g.,
implicit or explicit feelings about the wave) in predicting whether a
person sits or stands. Likewise, what predicts implicit bias are
features of people’s situations, not features of their
personality. For example, living in a highly residentially segregated
neighborhood might be expected to outpredict racial implicit bias
compared to individual-level factors, such as beliefs and
personality.
The bias of crowds model is aimed at making sense of five features
of implicit bias which are otherwise challenging to make sense of
together, namely: (1) average group-level scores of implicit bias are
very robust and stable; (2) children’s average scores of implicit
bias are nearly identical to adults’ average scores; (3)
aggregate levels of implicit bias at the population level (e.g.,
regions, states, and countries) are both highly stable and strongly
associated with discriminatory outcomes and group-based disparities;
yet, (4) individual differences in implicit bias have small-to-medium
zero-order correlations with discriminatory behavior; and (5)
individual test-retest reliability is low over weeks and months. (See
Payne et al. 2017 for references.) Another advantage of the bias of
crowds model is that it coalesces well with calls in philosophy for
focusing more on “structural” or “systemic”
bias, rather than on the biases in the heads of individuals
(§5).
One challenge for the bias of crowds model is explaining how
systemic biases interact with and affect the minds of individuals,
however. Payne and colleagues appeal to the idea of the
“accessibility” of concepts in individuals’ minds,
that is, the “likelihood that a thought, evaluation, stereotype,
trait, or other piece of information” becomes activated and
poised to influence behavior. The lion’s share of evidence, they
argue, suggests that the concepts related to implicit bias are
activated due to situational causes. This may be, but it does not
explain (a) how situations activate concepts in individuals’
minds (Payne and colleagues are explicitly agnostic about the format of
cognitive representations that underlie implicit bias); and (b) how
situational factors interact with individual factors to give rise to
biased actions (Gawronski & Bodenhausen 2017; Brownstein et al.
2019).
Philosophical work on the epistemology of implicit bias has focused
on three related
 questions.[14]
First, do we have knowledge of our own implicit biases, and if so, how?
Second, do the emerging data on implicit bias demand that we become
skeptics about our perceptual beliefs or our overall status as
epistemic agents? And third, are we faced with a dilemma between our
epistemic and ethical values due to the pervasive nature of implicit
bias?
Implicit bias is typically thought of as unconscious (§2.1.1),
but what exactly does this mean? There are several possibilities: there
might be no phenomenology associated with the relevant mental states or
dispositions; agents might be unaware of the content of the
representations underlying their performance on implicit measures, or
they might be unaware of the source of their implicit biases or the
effects those biases have on their behavior; agents might be unaware of
the relations between their relevant states (e.g., that their implicit
and explicit evaluations of a given target conflict); and agents might
have different modes of awareness of their own minds (e.g.,
“access” vs. “phenomenal” awareness; Block
1995). Gawronski and colleagues (2006) argue that agents typically lack
“source” and “impact” awareness of their
implicit biases, but typically have
“content”
 awareness.[15]
Evidence for
content awareness stems from “bogus pipeline” experiments
(e.g., Nier 2005) in which participants are led to believe that
inaccurate self-reports will be detected by the experimenter. In these
experiments, participants’ scores on implicit and explicit
measures come to be more closely correlated, suggesting that
participants are aware of the content of those judgments detected by
implicit measures and shift their reports when they believe that the
experimenter will notice discrepancies. Additional evidence for content
awareness is found in studies in which experimenters bring implicit
measures and self-reports into conceptual alignment (e.g., Banse et al.
2001) and studies in which agents are asked to predict their own
implicit biases (Hahn et al. 2014). Indeed, Hahn and colleagues (2014)
and Hahn and Gawronski (2019) have found that people are good at
predicting their own IAT scores regardless of how the test is
described, how much experience they have taking the test, and how much
explanation they are given about the test before taking it. Moreover,
people have unique insight into how they will do on the test, insight
which is not explained by their beliefs about how people in general
will perform.
Hahn and colleagues’ data do not determine, however, whether
agents come to be aware of the content of their implicit biases through
introspection, by drawing inferences from their own behavior, or from
some other source (see Berger forthcoming for discussion). This is important
for determining whether the awareness agents have of their implicit
biases constitutes self-knowledge. If our awareness of the content of
our implicit biases derives from inferences we make based on (for
example) our behavior, then the question is whether these inferences
are justified, assuming knowledge entails justified true belief. Some
have suggested that the facts about implicit bias warrant a
“global” skepticism toward our capacities as epistemic
agents (Saul 2012; see
 §3.2.2).
If this is
right, then we ought to worry that our inferences about the content of
our implicit biases, from all the ways we behave on a day-to-day basis,
are likely to be unjustified. Others, however, have argued that people
are typically very good interpreters of their own minds (e.g.,
Carruthers 2009; Levy 2012), in which case it may be more likely that
our inferences about the content of our implicit biases are
well-justified. But whether the inferences we make about our own minds
are well-justified would be moot if it were shown that we have direct
introspective access to our biases.
One sort of skeptical worry stems from research on the effects of
implicit bias on perception
 (§3.2.1).
This
leads to a worry about the status of our perceptual beliefs. A second
kind of skeptical worry focuses on what implicit bias may tell us about
our capacities as epistemic agents in general
 (§3.2.2).
Compared with participants who were first shown pictures of white
faces, those who were primed with black faces in Payne (2001) were
faster to identify pictures of guns as guns and were more likely to
misidentify pictures of tools as guns. This finding has been directly
and conceptually replicated (e.g., Payne et al. 2002; Conrey et al.
2005) and is an instance of a broader set of findings about the effects
of attitudes and beliefs on perception (e.g., Barrick et al. 2002;
Proffitt 2006). Payne’s findings are chilling particularly in
light of police shootings of unarmed black men in recent years, such as
Amadou Diallo and Oscar Grant. The findings suggest that agents’
implicit associations between “black men” and
“guns” may affect their judgment and behavior by affecting
what they see. In addition to the moral implications, this may be cause
for a particular kind of epistemic concern. As Siegel (2012, 2017,
forthcoming) puts it, the worry is that implicit bias introduces a
circular structure into belief formation. If an agent believes that
black men are more likely than white men to have or use guns, and this
belief causes the agent to more readily see ambiguous objects in the
hands of black men as guns, then when the agent relies upon visual
perception as evidence to confirm her beliefs, she will have moved in a
vicious circle.
Whether implicit biases are cause for this sort of epistemic concern
depends on what sort of causal influence social attitudes have on
visual perception. Payne’s weapons bias findings would be a case
of “cognitive penetration” if the black primes make the
images of tools look like images of guns, via an effect on perceptual
experience itself (Siegel 2012, 2017, forthcoming). This would
certainly introduce a circular structure in belief formation. Other
scenarios raise the possibility of illicit belief formation without
genuine cognitive penetration. Consider what Siegel calls
“perceptual bypass”: the black primes do not cause the
tools to look like guns (i.e., the prime does not cause a change in
perceptual experience), yet some state in the agent, such as a
heightened state of anxiety, is affected by the black prime and causes
the agent to make a classification error. This will count as a case of
illicit belief formation inasmuch as the agent’s social attitudes
cause her to be insensitive to her visual stimuli in a way that
confirms her antecedent attitudes (Siegel 2012). Other scenarios might
allay the worry about illicit belief formation. For example, what
Siegel calls “disowned behavior” proposes the same route to
the classification error as “perceptual bypass,” except
that the agent antecedently regards her error as an error. Empirical
evidence can help to sort through these possibilities, though perhaps
not settle between them conclusively (e.g., Correll et al. 2015).
A broader worry is that research on implicit bias should cause
agents to mistrust their knowledge-seeking faculties in general.
“Bias-related doubt” (Saul 2012) is stronger than
traditional forms of skepticism (e.g., external world skepticism) in
the sense that it suggests that our epistemic judgments are not just
possibly but often likely mistaken. Implicit biases are likely
to degrade our judgments across many domains, e.g., professors’
judgments about student grades, journal submissions, and
job
 candidates.[16]
Moreover, as
Fricker (2007) points out, the testimony of members of stigmatized
groups is likely to be discounted due to implicit bias, which, Saul
suggests, can magnify these epistemic failures as well as create
others, such as failing to recognize certain questions as relevant for
inquiry (Hookway 2010). The key point about these examples is that our
judgments are likely to be affected by implicit biases even when
“we think we’re making judgments of scientific or
argumentative merit” (Saul 2012: 249; see also Welpinghus forthcoming).
Moreover, unlike errors of probabilistic reasoning, these effects
generalize across many areas of day-to-day life. We should be worried,
Saul argues,
whenever we consider a claim, an argument, a suggestion, a question,
etc from a person whose apparent social group we’re in a position
to recognize. (Saul 2012: 250).
Bias-related doubt may be diminished if successful interventions can
be developed to correct for epistemic errors caused by implicit bias.
In some cases, the fix may be simple, such as anonymous review of job
candidate dossiers. But other contexts will certainly be
more
 challenging.[17]
More generally,
Saul’s account of bias-related doubt takes a strongly pessimistic
stance toward the normativity of our unreflective habits. “It is
difficult to see”, she writes, “how we could ever properly
trust [our habits] again once we have reflected on implicit bias”
(2012: 254). Others, however, have stressed the ways in which
unreflective habits can have epistemic virtues (e.g., Arpaly 2004;
Railton 2014; Brownstein & Madva 2012a,b; Nagel 2012; Antony 2016).
Squaring the reasons for pessimism about the epistemic status of our
habits with these streams of thought will be important in future
research.
Gendler (2011) and Egan (2011) argue that implicit bias creates a
conflict between our ethical and epistemic aims. Concern about
ethical/epistemic dilemmas is at least as old as Pascal, as Egan points
out, but is also incarnated in contemporary research on the value of
positive illusions (i.e., beliefs like “I am brilliant!”
which may promote well-being despite being false; e.g., Taylor &
Brown 1988). The dilemma surrounding implicit bias stems from the
apparent unavoidability of stereotyping, which Gendler traces to the
way in which social categorization is fundamental to our
cognitive
 capacities.[18]
For agents who
disavow common social stereotypes for ethical reasons, this creates a
conflict between what we know and what we value. As Gendler puts
it,
if you live in a society structured by racial categories that you
disavow, either you must pay the epistemic cost of failing to encode
certain sorts of base-rate or background information about cultural
categories, or you must expend epistemic energy regulating the
inevitable associations to which that information—encoded in ways
to guarantee availability—gives rise. (2011: 37)
Gender considers forbidden base rates, for example, which are useful
statistical generalizations that utilize problematic social knowledge.
People who are asked to set insurance premiums for hypothetical
neighborhoods will accept actuarial risk as a justification for setting
higher premiums for particular neighborhoods but will not do so if they
are told that actuarial risk is correlated with the racial composition
of that neighborhood (Tetlock et al. 2000). This “epistemic
self-censorship on non-epistemic grounds” makes it putatively
impossible for agents to be both rational and equitable (Gendler 2011:
55, 57).
Egan (2011) raises problems for intuitive ways of diffusing this
dilemma, settling instead on the idea that making epistemic sacrifices
for our ethical values may simply be worth it. Others have been more
unwilling to accept that implicit bias does in fact create an
unavoidable ethical-epistemic dilemma (Mugg 2013; Beeghly 2014; Madva
2016b; Lassiter & Ballantyne 2017; Puddifoot 2017). One way of
diffusing the dilemma, for example, is to suggest that it is not social
knowledge per se that has costs, but rather that the
accessibility of social knowledge in the wrong circumstances has
cognitive costs (Madva 2016b). The solution to the dilemma, then, is
not ignorance, but the situation-specific regulation of stereotype
accessibility. For example, the accessibility of social knowledge can
be regulated by agents’ goals and habits (Moskowitz & Li
2011). Readers interested in ethical-epistemic dilemmas due to implicit
bias should also consider related scholarship on “moral
encroachment” (e.g., Basu & Schroeder 2018; Gardiner
2018).
Most philosophical writing on the ethics of implicit bias has
focused on two distinct (but related) questions. First, are agents
morally responsible for their implicit biases
 (§4.1)?
Second, can agents change their implicit
biases or control their effects on their judgments and
behavior
 (§4.2)?
Researchers working on moral responsibility for implicit bias often
make two key distinctions. First, they distinguish responsibility for
attitudes from responsibility for judgments and behavior. One can, that
is, ask whether agents are responsible for their putative (§2)
implicit attitudes as such, or whether agents are responsible for the
effects of their implicit attitudes on their judgments and behavior.
Most have focused on the latter question, as will I. A second important
distinction is between being responsible and holding
responsible. This distinction can be glossed in a number of different
but related ways. It can be glossed as a distinction between
blameworthiness and actual expressions of blame; between backward- and
forward-looking responsibility (i.e., responsibility for things one has
done in the past versus responsibility for doing certain things in the
future); and between responsibility as a form of judgment versus
responsibility as a form of sanction. Most have focused on the former
of these disjuncts (being responsible, blameworthiness, etc.) via three
kinds of approaches: arguments from the importance of awareness or
knowledge of one’s implicit biases
 (§4.1.1);
arguments from the importance of control
over the impact of one’s implicit biases on one’s judgment
and behavior
 (§4.1.2);
and arguments from
“attributionist” and “Deep Self”
considerations
 (§4.1.3;
see Holroyd et al. 2017 for a
more in-depth review of theories of moral responsibility and implicit
bias).
It is plausible that conscious awareness of our implicit biases is a
necessary condition for moral responsibility for those biases. Saul
articulates the intuitive idea, suggesting that we
abandon the view that all biases against stigmatised groups are
blameworthy … [because a] person should not be blamed
for an implicit bias that they are completely unaware of, which results
solely from the fact that they live in a sexist culture. (2013: 55,
emphasis in original)
Saul’s claim appears to be in keeping with folk psychological
attitudes about blameworthiness and implicit bias. Cameron and
colleagues (2010) found that subjects were considerably more willing to
ascribe moral responsibility to “John” when he was
described as acting in discriminatory ways against black people despite
“thinking that people should be treated equally, regardless of
race” compared to when he was described as acting in
discriminatory ways despite having a “sub-conscious dislike for
African Americans” that he is “unaware of
having”.
Recalling the evidence that people often do have awareness of their
implicit biases
 (§3.1),
it would seem that
typical agents are responsible for those biases on the basis of the
argument from awareness. However, if the question is whether agents are
blameworthy for behaviors affected by implicit biases (rather than for
having biases themselves), then perhaps impact awareness is what
matters most (Holroyd 2012). That said, lacking impact awareness of the
effects of implicit bias on our behavior may not exculpate agents from
responsibility even in principle. One possibility is that implicit
biases are analogous to moods in the sense that being in an
introspectively unnoticed bad mood can cause one to act badly (Madva
2018). There is debate about whether unnoticed moods are exculpatory
(e.g., Korsgaard 1997; Levy 2011). One possibility is that bad moods
and implicit biases both diminish blameworthiness, but do not undermine
it as such. This claim depends in part on moral responsibility
admitting of degrees.
One problem with focusing on impact awareness, however, as Holroyd
(2012) points out, is that we may be unaware of the impact of a great
many cognitive states on our behavior. The focus on impact awareness
may lead to a global skepticism about moral responsibility, in other
words. This suggests that impact awareness may not serve as a good
criterion for distinguishing responsibility for implicit biases from
responsibility for other cognitive states, notwithstanding whether
global skepticism about moral responsibility is defensible.
A second way to unpack the argument from awareness is to focus on
what agents ought to know about implicit bias, rather than
what they do know. This approach indexes moral responsibility
to one’s social and epistemic environment. For example, Kelly
& Roedder (2008) argue that a “savvy grader” is
responsible for adjusting her grades to compensate for her likely
biases because she ought to be aware of and compelled by research on
implicit bias. In a similar spirit, Washington & Kelly (2016)
compare two hypothetical egalitarians with equivalent psychological
profiles, the only difference between them being that the “Old
School Egalitarian” is evaluating résumés in 1980
and the “New Egalitarian” is doing so in 2014. While
neither has heard of implicit bias, Washington & Kelly argue that
the New Egalitarian is morally culpable in a way that the Old School
Egalitarian isn’t. Only the New Egalitarian could have, and ought
to have, known about his likely implicit biases, given the comparative
states of art of psychological research in 1980 and 2014. The
underlying intuition here is that assessments of responsibility change
with changes in an agent’s social and epistemic environment.
A third way of unpacking the argument from awareness is to focus on
the way in which an attitude does or does not integrate with a variety
of the agent’s other attitudes once it becomes conscious (Levy
2012; see
 §2.1).
On this view, attitudes
that cause responsible behavior are available to a broad range of
cognitive systems. For example, in cognitive dissonance experiments
(e.g., Festinger 1956), agents attribute confabulatory reasons to
themselves and then tend to act in accord with those self-attributed
reasons. The self-attribution of reasons in this case, according to
Levy (2012), has an integrating effect on behavior, and thus can be
thought of as underwriting the sort of agency required for moral
responsibility. Crucially, it is when the agent becomes conscious of
her self-attributed reasons that they have this integrating effect.
This provides grounds for claiming that attitudes for which agents are
responsible are those that integrate behavior when the agent becomes
aware of the content of those attitudes. Implicit attitudes are not
like this, according to Levy. What’s morally important is
that
awareness of the content of our implicit attitudes fails to
integrate them into our person level concerns in the manner required
for direct moral responsibility. (Levy 2012: 9).
The fact that implicit processes are often defined in contrast to
“controlled” cognitive processes (§2.2) implies that
they may affect behavior in a way that bypasses a person’s
agential capacities. The fact that implicit biases seem to
“rebound” in response to intentional efforts to suppress
them supports this interpretation (Huebner 2009; Follenfant & Ric
2010). Early research suggesting that implicit biases reflect mere
awareness of stereotypes, rather than personal attitudes, also implies
that these states reflect processes that “happen to”
agents. More recently, however, philosophers have questioned the
ramifications of these and other data for the notion of control
relevant to moral responsibility.
Perhaps the most familiar way of understanding control in the
responsibility literature is in terms of a psychological mechanism that
would allow an agent to act differently than she otherwise would act
when there is sufficient reason to do so (Fischer & Ravizza 2000).
The question facing this sort of reasons-responsiveness view of control
is whether automatized behaviors—which unfold in the absence of
explicit reasoning—should be thought of as under an agent’s
control. Some have argued that automaticity and control are not
mutually exclusive. Holroyd & Kelly (2016) advance a notion of
“ecological control”, and Suhler and Churchland (2009)
offer an account of nonconscious control that underwrites automaticity
itself, yet is ostensibly sufficient for underwriting responsibility.
Others have distinguished between automaticity and automatisms (e.g.,
sleepwalking); in this sense, the relevant moral distinction might be
drawn in terms of agents’ ability to “pre-program”
their automatic actions (but not automatistic actions) via previous
controlled choices (e.g., Wigley 2007); it might be drawn in terms of
agents’ ability to consciously monitor their automatic actions
(e.g., Levy & Bayne, 2004); or it might simply be the case that
putative implicit attitudes are not automatic because they are readily
changeable (e.g., Buckwalter
 forthcoming).[19]
Others still have distinguished between
“indirect” and “direct” control over
one’s attitudes or behavior (e.g., Holroyd 2012; Levy &
Mandelbaum 2014; Sie & Voorst Vader-Bours 2016). Holroyd (2012)
argues that there are many things over which we do not hold direct and
immediate control, yet for which we are commonly held responsible, such
as learning a skill, speaking a foreign language, and even holding
certain beliefs. None of these abilities or states can be had by fiat
of will; rather, they take time and effort to obtain. This suggests
that we can be held responsible for attitudes or behaviors over which
we only have indirect long-range control. The question, then, of
course, is whether agents can exercise indirect long-range control over
their implicit biases. Mounting evidence suggests that we can
 (§4.2).
“Attributionist” and Deep Self theories of moral
responsibility represent an alternative to arguments from awareness and
control. According to these theories, for an agent to be responsible
for an action is for that action to “reflect upon” the
agent “herself”. A common way of speaking is to say that
responsibility-bearing actions are attributable to agents in virtue of
reflecting upon the agent’s “deep self”, where the
deep self represents the person’s fundamental evaluative stance
(Sripada 2016). Although there is much disagreement in the literature
about what the deep self really is, as well as what it means for an
attitude or action to reflect upon it, attributionists agree that
people can be morally responsible for actions that are non-conscious
(e.g., “failure to notice” cases), non-voluntary (e.g.,
actions stemming from strong emotional reactions), or otherwise
divergent from an agent’s will (Frankfurt 1971; Watson 1975,
1996; Scanlon 1998; A. Smith 2005, 2008, 2012; Hieronymi 2008; Sher
2009; and H. Smith 2011).
One influential view developed in recent years is that agents are
responsible for just those actions or attitudes that stem from, or are
susceptible to modification by, the agent’s
“evaluative” or “rational” judgments, which are
judgments for which it is appropriate (in principle) to ask the agent
her reasons (in a justifying sense) for holding (Scanlon 1998; A. Smith
2005, 2008, 2012). A. Smith suggests that implicit biases stem from
rational judgments, because
a person’s explicitly avowed beliefs do not settle the
question of what she regards as a justifying consideration. (2012:
581–582, fn 10)
An alternative approach sees the source of the “deep
self” in an agent’s “cares” rather than in her
rational judgments (Shoemaker 2003, 2011; Jaworska 2007; Sripada 2016).
Cares have been described in different ways, but in this context are
thought of as psychological states with motivational, affective, and
evaluative dispositional properties. It is an open question whether
implicit biases are reflective of an agent’s cares (Brownstein
2016a, 2018). It is also possible that even in cases in which an
implicit bias is not attributable to an agent’s deep self, it may
still be appropriate to hold the agent responsible for
violating some duty or obligation she holds due to her implicit biases
(Zheng 2016). Glasgow (2016) similarly argues for responsibility for
implicit biases that may not be attributable to agents. His view
unfolds in terms of responsibility for actions from which agents are
nevertheless alienated. Glasgow defends this view on the basis of
“Content-Sensitive Variantism” and “Harm-Sensitive
Variantism”, a pair of views according to which alienation
exculpates depending on extra-agential features of an action, such as
the content of the action or the kind of harm it creates. These
variantist views are fairly strongly revisionist with respect to
traditional conceptions of responsibility in the 20th
century philosophical literature. Some have argued that research on
implicit bias calls for revisionism of this sort (Vargas 2005; Faucher
2016).
Researchers working in applied ethics may be less concerned with
questions about in-principle culpability and more concerned with
investigating how to change or control our implicit biases. Of course,
anyone committed to fighting against prejudice and discrimination will
likely share this interest. Policymakers and workplace managers may
also be concerned with finding effective interventions, given that they
are already directing tremendous public and private resources toward
anti-discrimination programs in workplaces, universities, and other
domains affected by intergroup conflict. Yet as Paluck and Green (2009)
suggest, the effectiveness of many of the strategies commonly used
remains unclear. Most studies on prejudice reduction are
non-experimental (lacking random assignment), are performed without
control groups, focus on self-report surveys, and gather primarily
qualitative (rather than quantitative) data.
An emerging body of laboratory-based research suggests that
strategies are available for regulating implicit biases, however. One
way to class these strategies is in terms of those that purport to
change the apparent associations underlying agents’
implicit biases, compared with those that purport to leave implicit
associations intact but enable agents to control the effects
of their biases on their judgment and behavior (Stewart & Payne
2008; Mendoza et al. 2010; Lai et al. 2013). For example, a
“change-based” strategy might reduce individuals’
automatic associations of “white” with “good”
while a “control-based” strategy might enable individuals
to prevent that association from affecting their behavior. Below, I
briefly describe some of these interventions. For comparison of the
data on their effectiveness, see Lai and colleagues (2014, 2016), and
for discussion of their significance for theories of the metaphysics of
implicit bias, including a helpful appendix listing
“debiasing” experiments, see Byrd (forthcoming).
Intergroup contact (Aberson et al. 2008; Dasgupta &
Rivera 2008; Anderson 2010 for discussion): long studied for its
effects on explicit prejudice (e.g., Allport 1954; Pettigrew &
Tropp 2006), interaction between members of different social groups
appears to diminish implicit bias as well, albeit under some
moderating conditions (e.g., equal status interaction) and not under
others.
Approach training (Kawakami et al. 2007, 2008; Phills et
al. 2011): participants repeatedly “negate” stereotypes
and “affirm” counter-stereotypes by pressing a button
labelled “NO!” when they see stereotype-consistent images
(e.g., of a black face paired with the word “athletic”) or
“YES!” when they see stereotype-inconsistent images (e.g.,
of a white face paired with the word “athletic”). Other
experimental scenarios have had participants push a joystick away from
themselves to “negate” stereotypes and pull the joystick
toward themselves to “affirm” counter-stereotypes.
Evaluative conditioning (Olson & Fazio 2006; De Houwer
2011): a widely used technique whereby an attitude object (e.g., a picture
of a black face) is paired with another valenced attitude object (e.g.,
the word “genius”), which shifts the valence of the first
object in the direction of the second.
Counter-stereotype exposure (Blair et al. 2001; Dasgupta
& Greenwald 2001): increasing individuals’ exposure to
images, film clips, or even mental imagery depicting members of
stigmatized groups acting in stereotype-discordant ways (e.g., images
of female scientists).
Implementation intentions (Gollwitzer & Sheeran 2006;
Stewart & Payne 2008; Mendoza et al. 2010; Webb et al. 2012):
“if-then” plans that specify a goal-directed response
that an individual plans to perform on encountering an anticipated cue.
For example, in a “Shooter Bias” test, where participants
are given the goal to “shoot” all and only those
individuals shown holding guns in a computer simulation, participants
may be asked to adopt the plan, “if I see a black face, I will
think
 ‘safe!’”[20]
“Cues for control” (Monteith 1993; Monteith et
al. 2002): techniques for noticing prejudiced responses, in particular the
affective discomfort caused by the inconsistency of those responses
with participants’ egalitarian goals.
Priming goals, moods, and motivations (Huntsinger et al.
2010; Moskowitz & Li 2011; Mann & Kawakami 2012): 
priming egalitarian goals, multicultural ideologies, or particular
moods can lower scores of prejudice on implicit measures.
There is some doubt about this way of categorizing interventions, as
some control-based interventions may also change agents’
underlying associations and some association-based interventions may
also promote control (Stewart & Payne 2008; Mendoza et al. 2010).
More significant though are concerns about the efficacy of these
interventions over time (Lai et al. 2016), their practical feasibility
(Bargh 1999; Schneider 2004), and the possibility that they may
distract from broader problems of economic and institutional forms of
injustice (Anderson 2010; Dixon et al. 2012; see
 §5).
Of course, most of the research on
interventions like these is recent, so it is simply not clear yet which
strategies, or combination of strategies (Devine et al. 2012), will or
won’t be effective. Some have voiced optimism about the role
lab-based interventions like these can play as elements of broader
efforts to combat prejudice and discrimination (e.g., Kelly et al.
2010a; Madva 2017).
Research on implicit bias has been criticized in several ways.
Below are brief descriptions of, and discussion about, prominent lines
of
 critique.[21]
I leave aside
critical assessments of specific implicit measures.
Research on implicit bias has received a lot of attention, not only
in philosophy and psychology, but in politics, journalism,
jurisprudence, business, and medicine as well. Some have worried that
this attention is excessive, such that the explanatory power of
research on implicit bias has been overstated (e.g., Singal 2017;
Jussim 2018 (Other Internet Resources); Blanton & Ikizer 2019).
While the difficulty of public science communication is pervasive
(i.e., not limited to implicit bias research), and the most egregious
cases are found in the popular press, it is true that some researchers
have overhyped the importance of implicit bias for explaining social
phenomena. Hype can have disastrous consequences, such as creating
public distrust in science. One important point to bear in mind,
however, is that the challenges facing science communication and the
challenges facing a body of research are distinct. That is, one
question is whether the science is strong, and it is a separate
question whether the strength of the science, such as it is, is
accurately communicated to the public. Overhyped research may create
incentives for scientists to do flashy but weak work—and this is
a problem—but problems with hype are nevertheless distinct from
problems with the science itself. 
Some have argued that explicit bias can explain much of what
implicit bias purports to explain (e.g., Hermanson 2017a,b, 2018
(Other Internet Resources); Singal 2017; Buckwalter 2018). Jesse
Singal (2017), for example, denies that implicit bias is more
important than explicit bias, pointing to the United States Department
of Justice’s findings about intentional race-based
discrimination in Ferguson, MO and to the fact that the United States
elected a relatively explicitly racist President in 2016.
Singal and others are surely right that explicit bias and outright
prejudice are persistent and, in some places, pervasive. It is,
however, unclear who, if anyone, thinks that implicit bias is more
important than explicit bias. Philosophers in particular have been
interested in implicit bias because, despite the persistence and
pervasiveness of explicit bias, there are many people—presumably
many of those reading this article—who aim to think and act in
unprejudiced ways, and yet are susceptible to the kinds of biased
behavior implicit bias researchers have studied. This is not only an
important phenomenon in its own right, but also may contribute causally
to the mainstream complacence toward the very outrageous instances of
bigotry Singal discusses. Implicit bias may also contribute causally to
explicit bias, particularly in environments suffused with prejudiced
norms (Madva 2019).
A related worry is that there is not agreement in the literature
about what “implicit” means. Arguably the most common
understanding is that “implicit” means
“unconscious.” But whatever is assessed by implicit
measures is arguably not unconscious (§3.1).
It is true that there is no widespread agreement about the meaning
of “implicit,” and it is also true that no theory of
implicit social cognition is consistent with all the current data. To
what extent this is a problem depends on background theories about how
science progresses. It is also crucial to recognize that implicit
measures are not high-fidelity assessments of any one distinct
“part” of the mind. They are not process pure (§1.2).
This means that they capture a mix of various cognitive and affective
processes. Included in this mix are people’s beliefs and explicit
attitudes. Indeed, researchers have known for some time that the best
way to predict a person’s scores on an implicit measure like the
IAT is to ask them their opinions about the IAT’s targets. This
does not mean that implicit measures lack “discriminant
validity,” however (i.e., that they are redundant with existing
measures). By analogy, you are likely to find that people who say that
cilantro is disgusting are likely to have aversive reactions to it, but
this doesn’t mean that their aversive reactions are an invalid
construct. Indeed, one of the leading theories of the dynamics and
processes of implicit social cognition since 2006—APE
(§2.2)—is based on a set of predictions about this process
impurity (i.e., about the interactions of implicit and explicit
evaluative processes).
Several meta-analyses have found that, according to standard
conventions, the correlation between implicit measures and behavior is
small to medium. Average correlations have ranged from approximately
.14 to .37
(Cameron et al. 2012; Greenwald et al. 2009; Oswald et al. 2013; Kurdi et al. 2019).
This variety is due
to several factors, including the type of measures, type of attitudes
measured (e.g., attitudes in general vs. intergroup attitudes in
particular), inclusion criteria for meta-analyses, and statistical
meta-analytic techniques. From these data, critics have concluded that
implicit measures are poor predictors of behavior. Oswald and
colleagues write, “the IAT provides little insight into who will
discriminate against whom, and provides no more insight than explicit
measures of bias” (2013, 18). Focusing on implicit bias research
more broadly, Buckwalter suggests that a review of the evidence
“casts doubt on the claim that implicit attitudes will be found
to be significant causes of behavior” (2018, 11).
Several background questions must be considered in order to assess
these claims. Should implicit measures be expected to have small,
medium, or large unconditional (or “zero-order”)
correlations with behavior? Zero-order correlations are those that
obtain between two variables when no additional variable has been
controlled for. Since the 1970s, research on self-reported attitudes
has largely focused on when—under what
conditions—attitudes predict behavior, not whether
attitudes predict behavior just as such. For example, attitudes better
predict behavior when there is clear correspondence between the
attitude object and the behavior in question
(Ajzen
& Fishbein 1977). While generic attitudes
toward the environment do not predict recycling behavior very well, for
instance, specific attitudes toward recycling do
(Oskamp
et al. 1991). In the 1970s and 1980s, a
consensus emerged that attitude-behavior relations depend in general on
the particular behavior being measured (e.g., political judgments vs.
racial judgments), the conditions under which the behavior is performed
(e.g., under time pressure or not), and the person who is performing
the behavior (e.g., personality;
Zanna
&
Fazio 1982). A wealth of theoretical models of attitude-behavior
relations take these facts into account to make principled predictions
about when attitudes do and do not predict behavior (e.g.,
Fazio 1990).
Similar work is underway focusing on
implicit social cognition (for review see Gawronski & Hahn 2019 and
Brownstein et al. ms).
In a related vein, it is also important to keep in mind that large
zero-order correlations are rarely found in social science, let alone
in attitude research. Large zero-order correlations should not be
expected to be found in implicit bias research, either
(Gawronski, forthcoming).
Indeed, the zero-order
correlations between other familiar constructs and outcome measures is
comparable to what has been found in meta-analyses of implicit
measures: beliefs and stereotypes about outgroups and behavior
(r = .12;
Talaska
et al. 2008); IQ
and income (r = .2–.3; Strenze 2007); SAT scores and freshman
grades in college (r = .24; Wolfe and Johnson 1995);
parents’ and their children’s socioeconomic status
(r = .2–.3; Strenze 2007). The fact that no meta-analysis of
implicit measures has reported nonsignificant correlations close to
zero or negative correlations with behavior further supports the
conclusion that the relationship between implicit bias and behavior
falls within the “zone” of the relationship between these
more familiar constructs and relevant kinds of behavior. Whether this
common pattern of findings in social science—of weak to moderate
unconditional relations with behavior—is succor for supporters of
implicit bias research or cause for concern about the social sciences
in general is an important and open question (see, e.g., Greenwald et
al. 2015; Oswald et al. 2015; Jost 2019;
Gawronski
 forthcoming).[22]
But note that
the consistent finding of meta-analyses of implicit measures
distinguishes this body of research from those that have been
swept up in the social sciences’ ongoing “replication
crisis.” That people, on average, display biases on implicit
measures is one of the most stable and replicated findings in recent
psychological
 science.[23]
The
debate described in this section pertains to interpreting the
significance of this finding.
So-called “structuralist” critics (e.g.,
Banks & Ford 2009; Anderson 2010;
Haslanger 2015;
Ayala 2016, 2018; Mallon ms) have argued that researchers ought to pay
more attention to systemic and institutional causes of
injustice—such as poverty, housing segregation, economic
inequality, etc.—rather than focusing on the biases inside the
minds of individuals. One way to express the structuralist idea is that
what happens in the minds of individuals, including their biases, is
the product of social inequities rather than an
explanation for them. Structuralists then tend to argue that
our efforts to combat discrimination and inequity ought to focus on
changing social structures themselves, rather than trying to change
individual’s biases directly. For example, Ayala argues that
“agents’ mental states [are] … not necessary to
understand and explain” when considering social injustice (2016,
9).
Likewise, in her call to combat segregation in the contemporary United States, Anderson (2010)
is critical of
what she sees as a distracting focus on the psychology of bias.
A strong version of the structuralist critique—that research
on the psychology of prejudice is entirely useless, distracting, or
even dangerous—is hard to defend. Large-scale demographic
research makes clear that psychological prejudice is a key driver of
(for example) economic inequality (e.g.,
Chetty et al. 2018)
and inequities in the criminal justice
system
(Center for Policing Equity 2016).
More
broadly, no matter how autonomously certain social structures operate,
people must choose to accept or reject those structures, to vote for
politicians who speak for or against them, and so on. How people assess
these options is at least in part a psychological question.
A weaker version of the structuralist critique calls for needed
attention to the ways in which psychological and structural phenomena
interact to produce and entrench discrimination and inequity. This
“interactionism” seeks to understand how bias operates
differently in different contexts. If you wanted to combat housing
segregation, for example, you would want to consider not only
problematic institutional practices, such as “redlining”
certain neighborhoods within which banks will not give mortgage loans,
and not only psychological factors, such as the propensity to perceive
low-income people as untrustworthy, but the interaction of the
two. A low-income person from a redlined neighborhood might not
be perceived as untrustworthy when they are interviewing for a job as a
nanny, but might be perceived as untrustworthy when they are
interviewing for a loan. Adopting the view that bias and structure
interact to produce unequal outcomes does not mean that researchers
must always account for both. Sometimes it makes sense to
emphasize one kind of cause or the other. 
An interactionist version of structuralism can incorporate research
on prejudice into a wider understanding of inequity, rather than eschew
it. One way to do so is to identify ways in which psychological biases
(whether implicit or explicit) might be key contributors to
social-structural phenomena. For example, structuralists sometimes
point to the drug laws and sentencing guidelines that contribute to the
mass incarceration of black men in the USA as examples of systemic
biases. Sometimes, however, when these laws and policies change,
discrimination persists. While arrests have declined for all racial
groups in states that have decriminalized marijuana, black people
continue to be arrested for marijuana-related offenses at a rate of
about 10 times that of white people
(Drug Policy Alliance 2018).
This suggests that psychological biases (belonging
to officers, policy makers, or voters) are an ineliminable part of
systemic inequity. Such interactionism is just one approach for
blending individual and institutional approaches to intergroup
discrimination (see, e.g.,
Madva
2016a, 2017;
Davidson & Kelly forthcoming). Another idea is to incorporate research
specifically on implicit bias into a wider understanding of the
structural sources of inequity by using implicit measures to assess
broad social patterns (rather than to assess the differences between
individuals). The “Bias of Crowds” model (§2.5) argues
that implicit bias is a feature of cultures and communities. For
example, average scores on implicit measures of prejudice and
stereotypes, when aggregated at the level of cities within the United
States, predict racial disparities of shootings of citizens by police
in those cities
(Hehman
et al. 2017). Thus,
while it is certainly true that most of the relevant literature and
discussion conceptualizes implicit bias as way of differentiating
between individuals, structuralists might utilize the data for
differentiating regions, cultures, and so on.
Nosek and colleagues (2011) suggest that the second generation of
research on implicit social cognition will come to be known as the
“Age of Mechanism”. Several metaphysical questions fall
under this label. One question crucial to the metaphysics of implicit
bias is whether the relevant psychological constructs should be thought
of as stable, trait-like features of a person’s identity or as
momentary, state-like features of their current mindset or situation
(§2.4). While current data suggest that implicit biases are more
state-like than trait-like, methodological improvements may generate
more stable, dispositional results on implicit measures. Ongoing
research on additional psychometric properties of implicit
measures—such as their discriminant validity and capacity to
predict behavior—will also strengthen support for some theories
of the metaphysics of implicit bias and weaken support for others.
Another open metaphysical question is whether the mechanisms underlying
different forms of implicit bias (e.g., implicit racial biases vs.
implicit gender biases) are heterogeneous. Some have already begun to
carve implicit social attitudes into kinds (Amodio & Devine 2006;
Holroyd & Sweetman 2016; Del Pinal et al. 2017; Del Pinal &
Spaulding 2018; Madva & Brownstein 2018). Future research on
implicit bias in particular domains of social life may also help to
illuminate this issue, such as research on implicit bias in legal
practices (e.g., Lane et al. 2007; Kang 2009) and in medicine (e.g.,
Green et al. 2007; Penner et al. 2010), on the development of implicit
bias in children (e.g., Dunham et al. 2013b), on implicit intergroup
bias toward non-black racial minorities, such as Asians and Latinos
(Dasgupta 2004), and cross-cultural research on implicit bias in
non-Western countries (e.g., Dunham et al. 2013a).
Future research on epistemology and implicit bias may tackle a
number of questions, for example: does the testimony of social and
personality psychologists about statistical regularities justify
believing that you are biased? What can developments in vision
science tell us about illicit belief formation due to implicit bias? In
what ways is implicit bias depicted and discussed outside academia
(e.g., in stand-up comedy focusing on social attitudes)? Also germane
are future methodological questions, such as how research on implicit
social cognition may interface with large-scale correlational
sociological studies on social attitudes and discrimination (Lee 2016).
Another crucial methodological question is whether and how theories of
implicit bias—and more generally psychological approaches to
understanding social phenomena—can come to be integrated with
broader social theories focusing on race, gender, class, disability,
etc. Important discussions have begun (e.g., Valian 2005; Kelly &
Roedder 2008; Faucher & Machery 2009; Anderson 2010; Machery et al.
2010; Madva 2017), but there is no doubt that more connections must be
drawn to relevant work on identity (e.g., Appiah 2005), critical theory
(e.g., Delgado & Stefancic 2012), feminist epistemology (Grasswick
2013), and race and political theory (e.g., Mills 1999).
As with all of the above, questions in theoretical ethics about
moral responsibility for implicit bias will certainly be influenced by
future empirical research. One noteworthy intersection of theoretical
ethics with forthcoming empirical research will focus on the
interpersonal effects of blaming and judgments about blameworthiness
for implicit
 bias.[24]
This
research aims to have practical ramifications for mitigating intergroup
conflict as well, of course. On this front, arguably the most pressing
question, however, is about the durability of psychological
interventions once agents leave the lab. How long will shifts in biased
responding last? Will individuals inevitably “relearn”
their biases (cf. Madva 2017)? Is it possible to leverage the lessons
of “situationism” in reverse, such that shifts in
individuals’ attitudes create environments that provoke more
egalitarian behaviors in others (Sarkissian 2010; Brownstein 2016b)?
Moreover, what has (or has not) changed in people’s feelings,
judgments, and actions now that research on implicit bias has received
considerable public attention (e.g., Charlesworth & Banaji
2019)?