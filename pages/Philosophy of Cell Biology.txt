It is sometimes assumed that scientific disciplines are
sharply-delineated and can be defined by their subject matter. In
fact, however, they are dynamically changing, shaped in large part by
scientists, their research tools, and academic and research
institutions. This is clearly true in the case of cell biology.
Scientists began investigating cells in the seventeenth century after
Hooke and van Leeuwenhoek reported what they saw with newly invented
microscopes. Hooke (1665) gave cells their name. But the cell did not
become a focal unit for studying biological processes until Schleiden
(1838) and Schwann (1839) advanced the cell theory according to which
cells are the basic living units. The term cytology was
widely used for the studies that ensued, which focused mainly on
describing cells as seen under the light microscope. Although Wilson
used the phrase cellular biology in his introduction to
Cowdry’s General Cytology (Cowdry 1924), and many of
its contributors aspired to integrate disciplines studying cells, the
term cell biology was only introduced in the years
immediately after World War II. Those adopting the term cell
biology advocated complementing new higher-resolution images
created with the electron microscope with results from biochemical,
biophysical, and molecular approaches to cells. This transition is
exemplified in the naming of what would become the flagship journal in
the field. The working title used in discussions about founding a new
journal was Journal of Cytology. By the time of the first
issue in 1955, it was called the Journal of Biophysical and
Biochemical Cytology, a title chosen to emphasize the integration
of multiple approaches to the study of cells. In 1962 it was
re-christened The Journal of Cell Biology, by which time the
International Society for Cell Biology and the American Society for
Cell Biology had also been established and the term cell
biology had acquired general currency for an integrated approach
to cells. (Bechtel 2006: chapter 7, examines the establishment and
renaming of the journal as well as the creation of the American
Society for Cell Biology.)
The introduction of new research techniques, especially cell
fractionation and electron microscopy, figured centrally in the
efforts to establish cell biology as a distinct discipline in the
post-World War II period as they enabled constructing what Palade
(1987) termed a bridge between morphological observations and
biochemistry. Cell fractionation employs the ultracentrifuge to
separate cell contents into fractions that contain the enzymes
responsible for distinct cell activities while electron microscopy
enables these fractions to be linked to organelles identifiable in
cells. Matlin (2018) describes how Claude employed centrifugation to
identify microsomes which exhibited high concentrations of RNA.
Porter’s (1953) complementary micrographs of these particles in
fractions and tissue slices (Figure 1) linked microsomes to the
intracellular structure he had named the endoplasmic
reticulum. As Matlin describes, this cycling between cell
fractionation to establish function and electron microscopy to
localize it to a structure was pursued repeatedly in cell biology.
Figure 1: Micrographs of a fraction
containing microsome and of a thin-sliced preparation from a cell
showing the endoplasmic reticulum (marked as er). From Porter
(1953), plate 48.
In terms of content, however, the break between cytology and cell
biology was nowhere near as sharp as the founders of cell biology
portrayed. Cowdry’s General Cytology (1924) clearly had
similar aspirations of linking studies of morphology and studies of
cellular chemistry (see discussions by historians, philosophers, and
biologists of Cowdry’s project in Matlin, Maienschein, &
Laubichler 2018). But in terms of the institutions in which scientists
conducted their research, the break was quite significant. With the
creation of professional societies using the name “cell
biology”, biologists began to self-identify as cell biologists.
Laboratories and academic departments adopted the name cell
biology and funding agencies recognized cell biology as a
supported field of research.
Although cell biologists actively courted biochemists and to some
degree biophysicists to join their new discipline, these efforts
largely failed. The most enduring connections were with molecular
biology, which was established as a new discipline in the same period
(see the entry on
 molecular biology).
 While molecular biology began with a focus on bacterial phage
(viruses that infect bacteria) and cell biology with eukaryotic cells
from mammals and plants, by the 1970s each discipline had broadened
its focus and soon many cell biologists were adopting a molecular
approach to cell structure and function. (A continuing point of
difference between cell biology and molecular biology is that cell
biology placed high value on linking chemical and molecular processes
to cell structure; see Matlin 2016). Academic departments began to
include both cell and molecular in their names. In
November 1989 the American Society for Cell Biology created a new
journal, Cell Regulation, but with its third volume in
January 1992, the name was changed to Molecular Biology of the
Cell. Although there are textbooks that reference just cell
biology in their title, several of the most prominent textbooks
feature both cell and molecular. These include
Alberts’ Molecular Biology of the Cell (Alberts,
Johnson, Lewis et al. 2015) , Cooper and Hausman’s The Cell:
A Molecular Approach (2007), and Iwasa and Marshall’s
Karp’s Cell and Molecular Biology (2016). The first
edition of Alberts’ Molecular Biology of the Cell
(Alberts, Bray, Lewis et al. 1983) was modeled in part on
Watson’s Molecular Biology of the Gene (1976) and
included Watson as a co-editor. Alberts attributes the vision of
integrating cell and molecular biology to Watson (see interview with
Alberts in the Oral History Collection at Cold Springs Harbor
Laboratory linked in
 Other Internet Resources below)
Not only have the disciplines studying cells undergone historical
transformations, so has the understanding of what cells are. As Hesse
(1966) argued, in many fields the objects of study are characterized
through metaphors to more familiar objects. In examining the different
metaphors that have been invoked for cells, Reynolds argues that the
choice of metaphor has consequences for how cells are understood: 
Metaphorical language … has been essential not only to the
activity of describing cells but also to seeing and
understanding them, and has played no less a fundamental role
than the literal and material lenses of the microscope. (2018: 4) 
Reynolds distinguishes two fundamental classes of metaphors: cells
as human artifacts and cells as organisms. Each has
taken on a variety of more specific forms. Among artifacts, cells have
been characterized as spaces enclosed by solid walls, building blocks,
factories, various types of machines, and electronic computers. The
organismal metaphors have suggested a conceptualization of cells as
elementary organisms (like unicellular amoebae) or citizens in a state
or society in which there is a division of labor and in which cells
make decisions that determine their own developmental
‘fates’, including the ultimate decision to initiate
programmed cell death (Reynolds
 2014).[1]
 These metaphors have given rise to distinctive research agendas that
focus on specific aspects of cells and their relationship to other
cells and their environment.
When Hooke introduced the term cell in Micrographia
in 1665 to describe the microscopic structure of cork that he observed
using a compound microscope
 (Figure 2A),
 he was comparing cells to the polygonal cells of beeswax and the
small rooms occupied by monks in a monastery. (Hooke 1665: 116, also
commented on the fluid content of cells, labeling them
“succus nutritus, or appropriate juices of
vegetables”. As discussed below, the fluid content of cells took
on new importance with the advent of the protoplasm theories in the
mid-nineteenth century.) Although researchers who focused on plants
tended to find Hooke’s metaphor appropriate, since plant cells
have clearly observable walls, others, beginning with van Leeuwenhoek,
who examined spermatozoa
 (Figure 2B),
 animal tissues, and bacteria, tended to adopt other terms such as
corpuscle or globule. 
A. Hooke’s (1665: Schem: XI, fig.
1 facing page 115) drawing of his observations of cells in cork.
B.Van Leeuwenhoek’s drawing of
spermatozoa from his Letter to Nehemiah Grew, 18 March 1678. 
Figure 2: Hooke’s drawing clearly
shows cells as areas enclosed by walls whereas van Leeuwenhoek’s
presents what we call cells as simple organisms.
Most of these early investigators focused on describing what they saw
in the microscope. But some, such as Buffon (1749), began to view
cells as basic living units. Nicholson characterizes this as the
beginning of an atomist tradition in biology that identified 
a basic indivisible unit of life and [sought] to explain the
morphological constitution and physiological operation of all living
beings in terms of these fundamental units. (2010: 203) 
This more theoretically committed conception of cells is exemplified
in the publications of Schleiden (1838) and Schwann (1839), who are
generally credited with establishing the cell theory, the fundamental
tenet of which is that cells are the basic units of living things.
Focusing exclusively on plants, Schleiden asserted that
“… every plant … is an aggregate of fully
individualized, independent, separate beings, the cells
themselves” (1838 [1847: 231–2]). In addition to their
walls, Schleiden appealed to the nucleus, which had been identified
with improved microscopes by Brown (1833), to identify plant
cells. The nucleus figured centrally in Schleiden’s account of
cell formation according to which a new cell formed through a process
like crystal formation—one type of material is deposited around
the nucleolus to form the nucleus and then other material was
deposited to form the cell body. The metaphor comparing cell formation
to crystal formation played a yet larger role when Schwann confronted
the challenge that animal cells differ vastly in their
appearance. Schwann argued that they were nonetheless all cells since
they all formed through a process analogous to crystal formation: 
The elementary parts of all tissues are formed of cells in an
analogous, though very diversified manner, so that it may be asserted
that there is one universal principle of development for the
elementary parts of organisms, however different, and that this
principle is the formation of cells. (Schwann 1839 [1847: 165])
(For discussion of this and other uses of the crystal metaphor in
biology, see Haraway 1976.)
Given that Schwann and Schleiden were masters of microscopy (as
witnessed by their drawings based on their microscopic observations),
and that other equally competent microscopists reported cells
dividing, their claim that cells form like crystals seems anomalous.
Bechtel (1984) argues that Schwann’s strong commitment to
developing mechanistic accounts of vital processes may have played a
major role in how he interpreted what he saw—the crystal
formation metaphor provided a mechanistic model whereas at the time
there were no mechanical models for cell division. In any case,
Schleiden’s and Schwann’s accounts of cell formation were
soon set aside as more researchers reported on cell division and
Schwann’s concerns with mechanism were supplanted by other
concerns. Virchow (1855), a pathologist, offered a theoretic argument
that only a process like cell division could explain the transmission
of disease and coined the oft-cited dictum “omnis cellula e
cellula” (all cells come from cells).
Schwann’s “theory of the cell”, however, was much
richer than his soon rejected view of cell formation. He argued that
cells were the basic units in which the processes of life (he coined
the term metabolism) occurred. He viewed metabolic processes
as catalyzed by the specific materials that were deposited in cells as
they formed. Schwann (1836) had himself recently discovered pepsin, a
catalyst that breaks down egg albumin. (Pepsin operates, however, not
within cells but after being secreted into digestive fluid.) When a
year later he argued that fermentation could not occur without the
involvement of a whole living yeast cell (Schwann 1837), some critics
took Schwann to be claiming that living systems exhibited mysterious
vital properties. But for Schwann this was consistent with his
mechanist approach—the metabolic processes only occurred when
the responsible catalysts were brought together in cells.
In the mid- to late-nineteenth century theorizing about cells went in
a number of directions. One direction focused even more than Schwann
on the material stuff constituting the cell. In the same period as
Schwann was advancing his theory of the cell, von Mohl (1835), a plant
researcher, introduced the name protoplasm for the fluid
material in cells. Animal researchers such as Dujardin (1835) called
the fluid seen in animal cells sarcode. Remak (1852) proposed
the protoplasm theory according to which the basic material of plants
and animal was identical and T. H. Huxley (1869) took this a step
further, arguing that protoplasm was the “physical basis of
life”. As discussed by Reynolds, protoplasm theory offered a
different fundamental biological theory—whereas Schleiden and
Schwann 
sought to unify all the various forms of life through a common
morphological type and developmental principle, the protoplasm theory
attempted to achieve this through the identification of a common
substance or material. (2018: 32) 
Accordingly, some supporters of protoplasm sought to dislodge the cell
as a basic living unit. The botanist Julius Sachs (1892), for example,
asserted “to call the protoplasm unit a cell was about as
appropriate as calling a live bee in a honeycomb a cell” (as
translated by Welch & Clegg 2010: C1281). Since protoplasm appears
as a viscous substance that is only artificially constrained by cell
boundaries, some protoplasm theorists viewed it as supplanting the
cell as an organizing unit. Where Schwann understood the cell to be
the fundamental unit of biological development, protoplasm theorists
understood protoplasm to drive ontogeny and cells to be merely
secondary structures deposited in the course of this primary
substance’s development. Recently a few biologists (Welch &
Clegg 2010, 2012) and philosophers (Nicholson, 2010) have argued for
reviving protoplasm theory. Their argument is that protoplasm theory
presents a more holistic systems or organismal perspective. (We
discuss holism and organicism in
 Section 3.)
The characterization of protoplasm as just a viscous substance
existing on its own was complicated by the identification of the cell
membrane as a structure distinct from the cell wall and, as discussed
below, of membrane-bound organelles. In the end, most biologists
acquiesced in some version of Schultze’s (1861: 11; as
translated by Hall, 1951: 451) compromise position that raised
protoplasm to be one defining feature of cells when he defined a cell
as “a lump of protoplasm inside of which lies a
nucleus”. Those who adopted such a compromise position often
invoked a metaphor, originating with Raspail, according to which the
cell is “a kind of laboratory within which all tissues organize
and grow” (1843: 28; as translated by Harris, 1999: 32). The
laboratory metaphor encouraged attempts to develop chemical
explanations for all the reactions occurring in cells. There were a
number of variants on this theme. Unger analogized the plant cell with
a “mächtige chemische Werkstätte” (powerful
chemical workshop) (1851: 23) and Virchow stated that “starch is
transformed into sugar in the plant and animal just as it is in a
factory” (1858: 107). 
Embracing this chemical perspective, several chemists sought to
identify the chemical catalysts needed for the reactions to occur.
Fermentation was a common focus and Kühne (1877) coined the term
enzyme (Greek for “in yeast”) for the putative
catalyst operative in yeast. Enthusiasm for this effort was
temporarily dampened by Pasteur (1860). A noted chemist, he
nonetheless maintained that fermentation could only be carried out in
whole living cells. This reflects a conception of the cell as an
explanatorily irreducible unit of life—a conception associated
with vitalism (discussed in more detail below). Enthusiasm for the
factory conception, though, was rekindled by Buchner (1897). Guided by
Pasteur’s contention that fermentation could not occur in the
absence of cells, he added sugar to an extract he prepared by
destroying all whole cells, thinking it would serve as a preservative.
When he observed the emission of bubbles, indicating fermentation in
the absence of living cells, he changed course and galvanized the
pursuit of chemical investigations of metabolism that resulted in the
establishment of modern biochemistry in the first decades of the
twentieth century (Kohler 1971; Cornish-Bowden 1997). One of its early
successes was the characterization in the 1930s of a pathway of enzyme
catalyzed reactions responsible for fermentation (Bechtel 1986).
At first the factory metaphor simply identified the cell as the place
in which chemical reactions occur, but over time researchers began to
focus on the different machines within the factory. The focus on
separate machines in the factory was promoted in the late nineteenth
and early twentieth century when, using improved microscopes and
applying stains to enhance contrast, investigators began to identify
structures within cells and theorize about their functional
significance. Researchers were particularly successful in
characterizing the nuclear events in cell division and fertilization.
Flemming (1879, 1882) described in detail how the threads that he
called chromatin (due to their absorption of dye), later
named chromosomes, divided longitudinally, with the two
halves moving apart so that one of each would end up in each daughter
cell
 (Figure 3).
 Soon after researchers such as Weismann and Correns pointed to links
between chromosome transmission and heredity, but it was Boveri (1902)
and Sutton (1903) who provided the compelling evidence that
Mendel’s factors (what would soon be called genes) are
in or on chromosomes. Darden and Maull (1977) analyze the linking of
genes to chromosomes as a major example of what they termed
interfield theories, theories that do not try to reduce one
account to another but integrate the findings of different fields in a
theory that bridges them.
Figure 3: Flemming’s (1882)
drawings of the stages of mitosis that highlights the formation of
spindles and their role in segregating chromosomes. Images 1–3 
are from Tafel IIIa; 4–7 from Tafel IIIb. (Figure from
 wiki commons.)
Late in the nineteenth century cytologists also succeeded in
identifying membrane-enclosed structures in the cytoplasm that came to
be known as organelles. Altmann (1890), using new stains he
developed, observed filaments within cells that he took to be
elementary organisms (a view he explicitly set in opposition to the
protoplasm theory). Although many researchers challenged
Altmann’s observations, Benda (1899) confirmed the existence of
filaments using a different stain and gave them the name
mitochondria (Greek for “thread” and
“granule”). Because of their reactivity with oxidative
stains, Michaelis (1899) proposed that they figured in oxidative
reactions in cells. Yet other researchers identified other organelles
such as the Golgi apparatus (Golgi 1898) and ergastoplasm (Garnier
1897), which was ultimately identified as the endoplasmic
reticulum.
In the early decades in the twentieth century biochemists and
cytologists developed their own research techniques and pursued their
investigations independently of the other. Most biochemists implicitly
embraced the assumption that the cell was a bag of chemicals that
could be studied in the extracts remaining after cell structure was
destroyed whereas those pursuing cytological inquiries tended to
embrace the factory metaphor in which organelles were distinct
machines. As witnessed by Cowdry’s (1924) General
Cytology and Bourne’s (1942) Cytology and Cell
Physiology, there were researchers who desired to build bridges
between biochemistry and cytology. There were some techniques, albeit
limited, for determining the chemical composition of organelles
(essentially, those of cytochemistry and histochemistry, which relied
on determining to which chemicals various stains bound). The nearly
simultaneous introduction of cell fractionation and electron
microscopy in the late 1940s provided the needed research techniques
and, as noted in
 Section 1,
 helped establish modern cell biology (Bechtel 2006; Matlin 2018). In
addition to the linkage of microsomes to the endoplasmic reticulum,
described above, researchers linked oxidative metabolism to the inner
membrane of the mitochondrion. New structures were also discovered and
connected to functions, such as the lysosome which was linked to the
breakdown and recycling of disrupted cell components (de Duve
1958).
The process of advancing new conceptions of cells continues. We note
just one example here. The pioneers of cell biology in the 1940s and
1950s embraced the machines in a factory metaphor, treating cell
organelles as compartments in which different chemical reactions were
catalyzed by the enzymes housed there. Once formed, the compartments,
on this view, did not change—the crucial activities occurred
within them. Over time, however, some researchers within cell biology
began to identify cell structures that executed mechanical movements,
as anticipated by Flemming’s (1882) characterization of the cell
spindle as pulling chromosomes apart in mitosis. Of particular
importance was research on the cytoskeleton (consisting of actin
fibres and microtubules). The term cytoskeleton was invoked
in the 1930s and 1940s to denote a rigid structure, and indeed it does
help give cells their shape. But it was soon found to be the locus of
movement. H. E. Huxley (1969) advanced an account of muscle
contraction as resulting from myosin molecules forming bridges that
pull on actin filaments. Both actin filaments and microtubules were
found to continually extend themselves at one end by incorporating new
proteins while removing them from the other, a process characterized
as treadmilling (Cleveland 1982). Video observations of
organelles moving along microtubules (R. Allen et al. 1982) led to the
discovery of kinesins (Vale, Reese, & Sheetz 1985), molecular
motors that walk along microtubules carrying cargo. It also
focused new attention on dyneins (Paschal & Vallee 1987),
previously only known for their roles in cilia and flagella, as
carrying cargo in the opposite direction. Recognition of this
continual movement of material within cells gave rise to a new
metaphor of the cell as a city with bustling traffic (Vale &
Milligan 2000).
Almost all of those pursuing the various metaphors discussed in the
previous section embraced a view that had its origins in Descartes,
who maintained both non-living and living systems (with the exception
of the human mind) operated like machines. But a significant number of
researchers rejected this perspective and maintained that living
organisms, including cells, are fundamentally different from ordinary
physical, mechanical systems. They argued that in one way or another
composition from material components is insufficient to account for
the phenomena associated with cells. We begin with these opponents of
mechanistic conceptions of cells and then examine how those advocating
mechanistic approaches responded.
In the eighteenth and nineteenth century the opponents of mechanism
were typically referred to as vitalists. The various
vitalists all rejected mechanism, taken as the view that organisms are
composed of physical parts that operate in accord with the same
principles as processes in the non-living world. Their positive views
varied. Some vitalists adopted a position much like that of substance
dualists with respect to the mind, arguing that some non-material
component—a vital force (vis vitalis)—operates in
living beings and accounts for their distinctive activities. Others
avoided positing an extra component but maintained that different laws
apply in living organisms than in non-living systems (for discussions
of these different versions of vitalism, see contributions in
Normandin & Wolfe 2013). Regardless of how they expressed their
positive views, vitalists commonly pointed to activities of living
organisms that they claimed could not be accounted for in the same
manner as physical processes. This is well illustrated in Bichat
(1805), who focused first on the apparent lack of determinism in the
behavior of biological organisms and second on the fact that organisms
seemed to oppose physical processes that threatened to destroy them
(in his words, they resist death). As noted above, Schwann
and Pasteur both claimed that living yeast are needed to produce
fermentation. While for Schwann this only entailed that fermentation
depended on the specific combination of materials found in cells, for
vitalists it entailed that living cells perform activities that could
not be performed by the collection of component molecules. Bichat and
Pasteur, as well as other prominent vitalists such as Müller
(1837–1840), embraced empirical and experimental research, but
drew limits with respect to what could be explained by appeal to the
material components of an organism alone.
In many cases, mechanists did not explicitly respond to vitalists but
simply moved forward with their research. Bernard (1865), was an
exception. He addressed Bichat’s challenges by introducing a
distinction between an organism’s internal and external
environment. Mechanistic operations within organisms are carried out
in the internal environment and jointly serve to maintain that
environment in a constant state. Because mechanistic operations
respond to conditions in the internal environment, they appear
indeterminate when considered only in relation to external stimuli.
Moreover, because these mechanisms work to maintain a constant
internal environment, one could explain the ability of organisms to
resist physical processes that might otherwise destroy them (for
discussions of Bernard see Holmes 1974 and LaFollette & Shanks
1994). Bernard’s approach was the foundation for Cannon’s
later well-known work on homeostasis (1929).
By the beginning of the twentieth century few biologists investigating
cells (Driesch, 1914,
being a notable exception) espoused vital forces and vitalism per
se ceased to be regarded as a tenable position. But many
biologists were still concerned to account for differences between
living cells and ordinary material systems. A prominent position
adopted by many investigators (Haldane 1929, 1931; Lillie 1934;
Needham 1936; Russell 1945, 1930; Von Bertalanffy 1952; Weiss 1963;
Woodger 1929) was holism, sometimes referred to as
organicism. (See Nicholson and Gawne 2015, for a discussion
of how the organicists differentiated their position from both
vitalism and mechanism.) Holists accepted that living systems were
built out of material parts. They insisted, however, that the
activities performed by components of living systems depended not just
on those components and their composition but on their organization.
Accordingly, one cannot just add together the activities of the
components to account for the whole (“the whole is not just the
sum of its parts”). The organized whole in part determines how
the parts behave. This attitude is manifest, for example, in J. S.
Haldane’s opposition to investigating the origins of life from
non-living matter: “There is and can be no origin of life out of
mechanical conditions. Such an origin is inconceivable” (Haldane
1930: 12).
The organization of cellular systems plays a central role in the
dialectic between mechanistic biologists and holists. Holists often
construe mechanistic biologists as downplaying the importance of
organization, but many mechanistic biologists deny this. Bernard had
emphasized the role of organization in allowing components to maintain
the constancy of the internal environment and those seeking to explain
homeostasis appealed to feedback loops. Recognizing that mechanistic
accounts are not limited to a simple, additive view of organization,
J. S. Haldane’s son, J. B. S. Haldane, abandoned his
father’s commitment to holism and embraced a mechanistic
framework that emphasized how biological components are affected by
being incorporated within organized systems. In making this break with
his father he was heavily influenced by working with the biochemist
Hopkins (1913), who likewise made organization central to his accounts
of biochemical processes. Accordingly, J. B. S. Haldane became one of
the pioneers in formulating biological inquiry into the origins of
life while still insisting that complex organization figured centrally
in activities of living organisms (Martin 2010).
Philosophers and scientists often invoke the word emergent
for phenomena that are different from the phenomena generated by their
components (see the entry on
 emergent properties).
 Sometimes emergent phenomena are viewed as incapable of being
explained in terms of their constituents. In the context of
characterizing systems biological accounts of cell phenomena, Boogerd
et al. (2005) develop an account according to which emergent phenomena
are ones that are fully explicable in terms of how their constituents
behave in the organized system, but not in terms of how they behave in
simpler (less complexly organized) systems. This recognizes that many
cellular constituents behave differently when incorporated into
particular systems in which they receive distinctive inputs. For other
recent treatments of emergence and its applications to cell biology,
see (Hooker 2011a; S.
Mitchell 2012; Mossio, Bich, & Moreno 2013; Winning & Bechtel
2019).
The desire to understand how the organization in cells and
multicellular organisms differs from that found in most naturally
occurring systems or human-made artifacts motivated a body of research
in theoretical biology in the 1970s and 1980s. Among the most
prominent contributors to theoretical biology were Pattee (many of his
most important papers have been collected in Pattee 2012), Rosen
(1985, 1991), Polanyi (1968), and Waddington (1961). Philosophy of
biology, as it developed as a specialty in philosophy of science in
the 1970s and 1980s, largely ignored this tradition. Today however a
number of philosophers concerned with cell biology are drawing upon
its insights. Here we focus on one key conceptual tool theoretical
biologists provide for understanding the distinctive activities of
living systems such as cells: constraints. The notion of constraint is
drawn from classical mechanics, where constraints serve to constitute
macro-scale objects from their micro-scale particles. Constraints thus
account for why macro-scale objects exhibit different properties than
their constituents. Constraints are not explained by laws but rather
serve as boundary conditions that must be ascertained empirically.
Accordingly, to the degree constraints explain biological activities
of cells, these activities cannot be reduced, in the sense of being
derived from the principles of chemistry or physics. Instead,
researchers must, on the basis of empirical inquiry, identify the
constraints actually realized in living cells.
The importance of constraints for understanding cellular and other
biological phenomena has been developed recently by Hooker (2011b, 2013) and Moreno and Mossio
(2015). Hooker makes clear that, although the term constraint
suggests limitations, constraints also extend possibilities—to
offer a cellular example, microtubules that run from the cell center
to the periphery restrict the movement of the kinesin and dynein
motors that move on them but also provide a possibility for transport
of organelles to distant locations. Moreno and Mossio in particular
develop a perspective that links the focus on constraints to the idea
that living cells are autopoietic. Drawing upon Maturana and
Varela’s (1980) conception of living systems as autopoietic
machines—machines that provide a network of production that
enables the construction of the living system—Moreno and Mossio
add a focus on the thermodynamic requirements of cells. Cells, as
highly organized systems, are far from equilibrium and require a
continual source of free energy to carry out the operations required
to synthesize new components and resist the tendency towards
equilibrium. What constraints do in organisms, on their account, is
direct flows of free energy to perform the work of building,
repairing, and reproducing the organism.
Some constraints are flexible, and these make possible an important
feature of living systems—the ability of organisms to control
production mechanisms (such as those involve in fermentation or muscle
contraction) through the actions of 
control mechanisms. On Pattee’s account, control mechanisms
change the flexible constraints in production mechanisms in
light of information that is procured by making measurements. Negative
feedback control mechanisms, such as thermostats, are simple examples:
the thermostat makes a measurement of a variable (temperature in the
room) that is affected by the operation of the production
mechanisms—the furnace—and based on the measurement
executes action on the constraints in the furnace mechanism. Examples
of feedback control mechanisms are widespread in cells. Some of the
best-known examples are the lac operon (Jacob & Monod
1961) and feedback control of glycolysis by ATP (Ghosh & Chance
1964). The measurements used by control systems need not be restricted
to states affected by the activity of the production mechanism; they
can also measure states in the organism or states in its environment.
Using such measurements, production mechanisms can be controlled so as
to operate only in particular circumstances, enabling the organism,
for example, to navigate to a food source or avoid a predator as
needed.
Moreno and Mossio (2015) offer an account in which appropriately
organized productive and control mechanisms enable cells, and by
extension multi-cellular organisms, to achieve what they refer to as
biological autonomy: 
a distinctive regime of causation, able not only of producing and
maintaining the parts that contribute to the functioning of the system
as an integrated, operational, and topologically distinct whole but
also able to promote the conditions of its own existence through its
interaction with the environment. (2015: xvi–xvii) 
Note that mechanisms, on this view, are contained within cells and
that it is cells, not mechanisms that are autonomous. What becomes
crucial for understanding the autonomy of cells is the organization of
control mechanisms that orchestrate the activities of various
productive mechanisms so as to maintain the cell (or the multicellular
organism). One notable feature of this focus on the organization
needed to maintain autonomy is that it is compatible with the more
traditional philosophical accounts of mechanism (discussed in
 section 5)
 but emphasizes a feature not prominent in them—that production
mechanisms are subject to control mechanisms. To understand the
behavior of cellular mechanisms, researchers must not only look inside
mechanisms to their organized parts and operations but outside to how
they are situated in cells and organisms in such a way that they can
be controlled by other mechanisms (Winning & Bechtel 2018; Bechtel
in press). Such an ontological framework for understanding cells (as
well as multicellular organisms) integrates insights from traditional
mechanists in biology and their vitalist/organicist/holist critics,
capturing what is distinctive of living organisms within a framework
that accepts them as consisting of mechanisms, but mechanisms
organized in appropriate ways.
We turn now from questions about the ontological status of cells to
epistemic questions about how scientists study them. We begin with how
scientists represent cells and information about them. Traditionally,
philosophy of science has focused on linguistic representations of
scientific knowledge. But in many fields of biology and especially in
cell biology, information is often presented in images.
Scientists’ very familiarity with cells results from visual
representations generated using microscopes. Developing microscopes
and techniques for using them to produce interpretable images
presented a number of challenges. We begin in section 4.1 with the
challenges in generating images (micrographs) at all and in section
4.2 consider challenges in evaluating the reliability of the resulting
images. Beyond these replete (highly detailed) representations, cell
biologists rely on a variety of less replete diagrams. (The
characterization of images as “replete” is due to Perini,
2013. Drawing on Goodman, she uses relative repleteness to
differentiate diagrams from pictures.) In particular, as discussed in
 section 4.3,
 when they are developing mechanistic hypotheses about cells, cell
biologists rely on cell diagrams that represent types of cell
components and mechanism diagrams that represent select components
within cells that are hypothesized to constitute parts of the
mechanism responsible for a given phenomenon (Downes 1992).
As discussed above, in the seventeenth century both Hooke and van
Leeuwenhoek pioneered the use of light microscopes to observe cells.
Subsequent investigators often designed their own microscopes. The
variations in these designs contributed to variability in the
resulting images. (Before photography, microscopists drew what they
saw using the microscope, introducing another source of variation.)
The variability of the images different researchers produced was one
factor that led biologists in the eighteenth and nineteenth centuries
to examine more carefully the processes through which microscopes
generate images (Schickore 2001, 2007). In his theoretical studies of
lenses, Newton (1704) characterized two types of aberrations created
by lenses: spherical aberrations, resulting from light rays coming
into focus at different points, and chromatic aberrations, resulting
from light of different wavelengths being refracted at different
angles. Schickore (2007) describes the many efforts by microscope
developers to correct for these aberrations and the creation of test
objects for evaluating the reliability of particular microscopes.
During the same time enthusiasts were advancing many claims about what
they saw, some of which were shown later to be artifacts. For example,
both Milne-Edwards (1823) and Dutrochet (1824) reported round
structures of a constant size similar to that reported for cells,
which they termed globules. (See Schickore 2009 for a
detailed examination of the reports of globules and an argument that
it was the variability in these reports that contributed to the
growing sense that something was amiss in the practices of the
microscopists.) Globules were, however, soon shown to be the products
of spherical aberrations. In the early nineteenth century several lens
makers developed strategies for eliminating spherical aberrations and
greatly limiting chromatic aberration (chromatic aberrations were not
fully eliminated until the introduction of the apochromatic lenses in
mid-century). As a result, the observations by Schleiden, Schwann, and
others discussed above were largely free of these distortions.
Light microscopes faced another limit, that on magnification (a
limitation imposed by the wave length of light). For cell researchers
to obtain higher-resolution images that could reveal constituents of
cells, a microscope relying on different physical principles was
required. The most important alternative to the light microscope for
studying cells was the (transmission) electron microscope, which
employs beams of electrons to create images in a manner comparable to
photography: locations on the photographic plate hit by many electrons
are black in the negative and white in the positive image. When
structures in the cell scatter electrons, the location in the negative
remains white and appears dark in the positive image. Although
electron microscopes were available in the early 1930s, only in the
years just before and during World War II did biologists begin to
explore their potential. One difficulty they confronted is that most
eukaryotic cells are too thick to be penetrated by the 50kV electron
beam available in the first electron microscopes. Microtomes had been
developed for cutting slices of cells for light microscopy, but new
approaches to microtome design were required to slice cells
sufficiently thinly without creating distortions. This problem was not
solved until the early 1950s (Porter & Blum 1953). Accordingly,
some of the first electron micrograph studies focused on fibrous
material such as collagen (Schmitt, Hall, & Jakus 1942) or on
bacteria (Stanley & Anderson 1941). Porter, Claude, and Fullam
(1945) created the first electron micrograph of a eukaryotic cell by
culturing it under conditions where the periphery spread very thinly,
allowing electrons to penetrate. They generated an image
 (Figure 4)
 that showed at the periphery 
filamentous mitochondria of various lengths and fairly constant width;
scattered, small elements of high density especially abundant around
the nucleus and presumably representing Golgi bodies; and a delicate
lace-work extending throughout the cytoplasm. (1945: 246)
Figure 4: Comparison of images of
fibroblast from a tissue cultured chick embryo as seen with electron
microscope (left) and light microscope (right). From Porter, Claude,
and Fullam (1945: plate 10).
Besides the challenge of creating preparations sufficiently thin to be
penetrated by the electron beam, researchers confronted a number of
other challenges in preparing biological material for electron
microscopy. For example, specimens must be placed in vacuum, and this
requires first removing all water, the primary constituent of cells,
without inducing to many distortions. Several historians and
philosophers (Rheinberger 1995, Rasmussen 1997, and Bechtel 2006) have
examined how biologists confronted these challenges and evaluated the
reliability of the resulting micrographs. How they addressed one
challenge—that of creating sufficient contrast in the
images—greatly affected the images that were produced. This was
already a challenge with light microscopy: cell material is mostly
translucent so that the light transmitted is mostly of the same
wavelength, making it hard to differentiate the various structures in
the image. To address the challenge, light microscopists in the
mid-nineteenth century began experimenting with dyes used for fabrics.
As noted in
 Section 2,
 Flemming named the nuclear structures he observed chromatin since
they bound the aniline dye he was using. The problem for electron
microscopy was similar—cell components differ little in their
ability to block electron transmission. Electron microscopists found
that several of the fixatives used in light microscopy, especially
those involving heavy metals, enhanced the ability to block electrons,
and accordingly the contrast in the resulting images. Given that there
was little knowledge about what given chemicals would bind to within
the cell, the investigation of these stains was mostly pursued by
researchers trying out different compounds and procedures for applying
them (exemplified in Palade’s 1952, study of osmium tetroxide)
to see what images they could generate. Indeed, different stains
(osmium tetroxide, glutaraldehyde, etc.) did yield different
images.
Since there was little understanding about what stains bound to,
skeptics often raised doubts that they were revealing actual
structures in cells. Bechtel (2000, 2006) highlights three
considerations that often figured in scientists’ evaluation of
whether micrographs were informative about cells or only artifacts of
the methods of preparation: (1) the quality of the micrographs
themselves—do they exhibit distinct patterns? (2) the robustness
of the results—can comparable results be generated with
different techniques (e.g., with light and electron microscopy or with
multiple stains)? and (3) the theoretical plausibility of the
results—do they fit into a coherent theoretical account? While
the first is seldom commented on in philosophical accounts, it is
notable that scientists are inclined to assume that if an image
reveals a distinct, replicable pattern, it reflects something in the
source (although this assessment may be retracted if, for example, a
researcher shows how the pattern could be generated by other means).
There has been extensive philosophical discussion of the
second—the inference that when the same result is generated by
independent means, it reflects a preexisting entity in the world
(Hacking 1983; Culp 1995; Stegenga & Menon 2017). However, this
criterion proves insufficient at just the point at which results with
new techniques, such as the electron microscope, are most
controversial—when the images contain structures beyond those
that can be detected with other existing techniques. In these cases,
the distinctness of the patterns together with considerations as to
the theoretical plausibility of the findings are the criteria
researchers can use. Appeal to theoretical plausibility, however,
would seem to be circular since in traditional philosophical accounts,
theories are tested by the evidence generated by the technique. But
developing a plausible theory that fits with an experimental finding
and other evidence is not easy and when they are able to do so,
scientists view it as buttressing their judgments that the images
reflect real structures. To illustrate how these considerations have
been invoked in in cell biology, we present two examples philosophers
have examined of conflicts over whether the structure shown in
microscopic images was real or an artifact. The cases ended
differently, one with the acceptance of the structure, one with its
rejection.
The first case involves the Golgi apparatus, first described by Golgi
(1898) in light microscope studies using the silver nitrate stain he
had introduced. Palade and Claude (1949a,b), two of the pioneers of
modern cell biology, who would ultimately share the Nobel Prize,
argued that it was an artifact of staining, including with osmium
tetroxide (which Palade helped establish as a primary stain for
electron microscopy). They appealed not only to the variable
appearance of the Golgi bodies, as had some earlier skeptics
(Strangeways & Canti 1927; Parat 1928; Baker 1944), but also to
their own ability to create myelin figures similar in appearance to
the Golgi bodies by adding osmium to egg white. As discussed by
Bechtel (2006), this is compelling evidence for the claim that the
Golgi apparatus is an artifact, but in this case the evidence was
eventually set aside without being refuted. Although Palade remained a
skeptic about the Golgi apparatus for 15 years (researchers in his lab
reported that they were not allowed to mention it during that
interval), he finally accepted its reality when researchers in his
laboratory demonstrated that many newly synthesized proteins pass
through the region of the cell where the Golgi apparatus appeared on
their way to being secreted. He did not, however, explain why he
changed his mind. This later research on the Golgi apparatus, but not
his earlier skepticism about the existence of the Golgi apparatus, was
noted in Palade’s Nobel Prize citation in 1974. Palade himself
later contributed to two reviews (Farquhar & Palade 1981, 1998)
that discuss earlier researchers who cast doubts on the reality of the
Golgi apparatus, but he never mentions his own claims that it was an
artifact. What seems particularly salient is that, as a result of
research in his laboratory, the Golgi apparatus was associated with a
function in the cell in a manner that it had not been previously. It
now fit into a plausible theory in which it figured in packaging
proteins for export from the cell. Bechtel argues that this is often a
major factor in researchers’ acceptance of the reality of
entities identified through new techniques.
The second example stems from early electron microscopy of bacterial
cells. Chapman and Hillier (1953) observed invaginations of the plasma
membrane in gram-negative bacteria, which they called peripheral
bodies. Robertson (1958) renamed them mesosomes and they
were implicated by many researchers in a variety of cell functions
before being rejected as artifacts in the 1970s (Silva et al. 1976).
Analyzing this example, Rasmussen (1993) argues that philosophical
criteria for distinguishing real entities from artifacts, such as
robustness, are insufficient to explain scientists’ changing
judgments about the mesosome. When Chapman and Hillier first made
their case for the existence of mesosomes, they had to explain away
differences between their micrographs and the observations of light
microscopists by arguing that a membrane that only appeared in images
with the light microscope was in fact due to the mesosomes being
imaged under low resolution. Rasmussen contends they offered this
convoluted argument rather than treating the mesosomes as an artifact
because they were promoting the new electron microscopes. He further
describes how their claims motivated research programs aimed at
purifying and biochemically characterizing the mesosome so as to
evaluate proposals regarding their function. This initially supported
claims to the mesosome’s reality, but other biochemists offered
conflicting evidence. In addition, another new technique for preparing
specimens for electron microscopy by freezing them, which had its own
passionate advocates, generated micrographs that did not show
mesosomes. According to Rasmussen, these competing findings, not
robustness, determined scientists’ judgments about
mesosomes.
The case of the mesosome has attracted substantial interest from other
philosophers of science. Culp (1994) challenged Rasmussen’s
interpretation of the history of mesosomes, contending that the
rejection of mesosomes as an artifact is in fact best explained on
grounds of robustness. In particular, she points to the combined data
from biochemists that revealed few differences between cytoplasmic
membranes and specimens supposedly from mesosomes and from a new
generation of electron microscopists that suggested that mesosomes
resulted from the fixative, glutaraldehyde, used to generate the
micrographs purporting to show them. These robustness considerations,
she claims, sufficed to lead the community to reject mesosomes. In a
later paper, Rasmussen responds to Culp’s analysis, continuing
to maintain that local details, not principles like robustness,
determine scientists’ judgments about artifacts: 
general principles like robustness are too vague to warrant anything
whatsoever, because when described in sufficient detail it emerges
that the way such principles are instantiated is in flux—and the
devil is in the details. (2001: 643) 
Several other philosophers have also taken up the case of the
mesosome. Allchin (2000) characterizes the initial evidence for the
mesosome as robust and describes how subsequent research showing how
particles were generated by degeneration of the membrane in
preparation for electron microscopy led to a reassessment, culminating
in mesosomes being recognized as artifacts. Weber (2005) argues that
the process of evaluating claims about artifacts employs causal
reasoning of the same sort as used in testing theoretical hypotheses.
According to him, mesosomes were judged to be artifacts when evidence
supported the claim that they were produced by chemical fixation.
Hudson (2014: chapter 2) advances yet another alternative: according
to which what mattered most to researchers was whether they regarded
the process for generating evidence for or against mesosomes as a
reliable process.
Microscopic images, whether hand-drawn, as they were in the nineteenth
century, or captured in photographs, are highly detailed. However, the
knowledge cytologists and cell biologists seek to develop is more
abstract and general—they seek to identify the types of
structures found in cells, not all their instances. Accordingly, cell
biologists frequently generate diagrams that leave out details.
Maienschein (1991) examines the origin of this practice in
Wilson’s (1896) classic text, The Cell in Development and
Inheritance. Early in the book Wilson provided a diagram
 (Figure 5A)
 which abstracts from the much more replete photographs he had
presented just a year earlier (Wilson 1895). Instead of showing all
the instances of different organelles, he presents just a few
instances of each type. Organelles are shown using icons that are
suggestive of their shapes. Such diagrams serve to convey the types of
organelles found in cells and their typical location, but falsely
suggest that most of the space in the cell is empty. Maienschein
contends that this transition from photographs to abstract diagrams
reflects Wilson’s growing confidence in the correctness of his
interpretation of what he was seeing through the microscope and a
transition “from presenting data to representing
theory” (Maienschein 1991: 252). A
sign of their theoretical status is that such figures, unlike
micrographs or data plots, often undergo numerous revisions as
scientists develop their account. Revising these diagrams is a means,
in fact, of developing theory. With the development of biochemical and
molecular accounts of cell phenomena, researchers often make what
Serpente (2011) characterizes as a transition from the iconic to the
symbolic. He presents protein-protein interaction maps and gene
regulatory diagrams as examples of symbolic representations.
A. Wilson’s (1896: 14, figure 5)
cell diagram that leaves out much of the detail that would be seen
through the microscope to focus on 
B. A mechanism diagram of the processes
of heterophagy and autophagy from de Duve and Wattiaux (1966: 468,
figure 6).
Figure 5 
Similar points also apply to another type of diagram that appears
frequently in cell biology—diagrams of hypothesized mechanisms
that are taken to be responsible for a particular phenomenon (Sheredos
et al. 2013; Abrahamsen, Sheredos, & Bechtel 2018). Such diagrams
do not try to show all the organelle types but, as in
 Figure 5B,
 only those thought to be involved in generating a particular
phenomenon—in this case, the breakdown of materials taken into
the cell (heterophagy) or of cell components themselves (autophagy).
One challenge with mechanism diagrams is that they are static whereas
mechanisms are engaged in change (the digestion of organelles and the
expulsion of the remaining material). One common strategy is to use
arrows to represent activities, although often within the same diagram
arrows may have multiple different meanings. Ultimately, however, it
is up to the viewer to animate a mechanism diagram (Hegarty
1992)—to rehearse mentally the different activities that are
represented and to imagine the changes that are being produced by
different parts.
One might think that diagrams are only important as a means of
illustrating results that are presented textually. But examination of
the practices of scientists reveals that they are far more central to
their reasoning. Early in the development of a mechanistic hypothesis
scientists sketch their ideas. Often mechanism diagrams (as well as
the other figures that show data or the workflow of the research) are
crafted long before text is drafted. Diagrams are commonly presented
in lab meetings and revised multiple times as scientists are refining
their claims. Researchers often generate the text of the paper only at
the end of this process. Taking advantage of access to the draft
figures and text for two research projects, Sheredos and Bechtel
(2017, in press) examined the process of interactive engagement in
which the investigators modified diagrams, changed text, and then
further modified the diagram. What this process suggests is that
sketching and resketching mechanisms plays a central role as
researchers seek to identify what they can conclude from their
experimental studies. In one of the cases they examined, an early
version of a diagram serves to pose a question that was addressed
through the experimental studies, resulting in a final diagram that
offers an answer to the question posed. Beyond supporting the
empirical claims of a research project, Jones and Wolkenhauer (2012)
provide an illuminating discussion of how diagrams serve to locate
information required for a computational model in a representation of
the cellular mechanism that is being modeled,
Historically prominent philosophers of science such as Popper and
Reichenbach rejected inquiries into how scientific theories are
discovered as not philosophical (see the entry on
 scientific discovery).
 Beginning with Hanson (1958), though, discovery has attracted the
interest of many philosophers of science. Investigations into
discovery in cell biology (and related fields such as biochemistry,
molecular biology, and neuroscience) inspired Bechtel and Richardson
(1993 [2010]) to argue that the goal in these fields was not to
construct nomological explanation (Hempel 1965) but rather to identify
the mechanism responsible for a phenomenon and determine how it
worked. Examples from cell biology have also figured prominently in
accounts of mechanistic explanation by Machamer, Darden, and Craver
(2000), Craver and Darden (2013), Bechtel (2006), and others (see
Glennan & Illari, 2018, for a compendium of contemporary accounts
of mechanisms and mechanistic explanation, and the entry
 mechanisms in science).
 The concept mechanism figures both in discussion of
ontological issues and epistemic issues, but the two uses can be
distinguished in the manner proposed by G. Allen (2005). In this
section we are concerned with what he called “operative or
explanatory mechanism” (2005: 261; the thesis that for
explanatory purposes components of cells should be conceived to
function as if they were machines);
 Section 3
 concerned “philosophical mechanism” (2005: 261; the
ontological thesis that organisms are or are constituted by
machines).
One of the central objectives of many philosophers of science focused
on mechanistic explanations is to characterize reasoning strategies or
heuristics that scientists use to develop mechanistic explanations
(Bechtel & Richardson 1993 [2010]; Craver & Darden 2013; Gross
2018). We discuss reasoning strategies that figure in different phases
of developing mechanistic explanations in cell biology: delineating
phenomena and situating them in responsible mechanisms, identifying
and characterizing the components of mechanisms (a reductionistic
phase), and determining the organization within mechanisms and between
mechanisms and their contexts (a more holistic phase).
Challenging the common characterization of explanations as explaining
data, Bogen and Woodward advanced the claim that scientific
explanations are targeted at phenomena. Instead of defining what
phenomena are, they give examples: “weak neutral currents, the
decay of the proton, and chunking and recency effects in human
memory” (1988: 306). From these examples, it is clear that Bogen
and Woodward understand phenomena to be repeatable processes that can
be observationally or experimentally detected in multiple ways. On
this characterization, cell activities such as protein synthesis or
cell division count as cellular phenomena. Although phenomena are
often construed as the starting point of research, Craver and Darden
(2013: Chapter 4) identify some of the experimental tasks involved in
characterizing phenomena such as identifying precipitating conditions,
manifestations, inhibiting conditions, modulating conditions, and
nonstandard conditions that alter the manifestation of the phenomenon.
They also emphasize the role of multiple experimental techniques in
specifying features of phenomena. A good deal of research in cell
biology is devoted to determining conditions under which cell
phenomena such as programmed cell death are initiated or inhibited and
cell biologists have been inventive in developing experimental
techniques needed to produce these phenomena. Bechtel (forthcoming)
points out that phenomena that are the target of explanation range
from highly specific (protein synthesis in liver cells under low
oxygen conditions) to much more general (protein synthesis
generically). Individual research projects often address highly
specific phenomena whereas textbooks or review articles discuss more
general phenomena.
While some account of the phenomenon under study is generally adopted
before researchers set out to identify the responsible mechanism, the
characterization often changes radically as research on the
responsible mechanism proceeds (Bechtel & Richardson 1993 [2010],
refer to researchers reconstituting the phenomenon). This is
illustrated with an example from the study of cells. A starting point
for early inquiries into how animals store energy for their activities
focused on the heat generated by metabolizing foodstuffs. This heat
was assumed to power other activities and approximately a hundred
years of research was devoted to explaining animal heat (see
Mendelsohn 1964). However, after Lohmann (1929) and Fiske and Subbarow
(1929) had identified adenosine triphosphate (ATP) and demonstrated
that when hydrolyzed it would release considerable energy, heat was
determined to be just a waste product of metabolism and researchers
instead focused on the synthesis and hydrolysis of ATP. This became
the reconstituted phenomenon to which much of the older research was
now applied.
What is distinctive of mechanistic explanations is that they decompose
mechanisms into parts or entities and the operations or activities
that contribute to realizing the phenomenon. Although sometimes
researchers can proceed by reasoning a priori about what
component activities would be required to produce a given phenomenon,
most often the decomposition is developed on the basis of experiments.
While the goal is to identify both parts and operations and to link
them, a given group of researchers may only have research techniques
that allow them to identify one or the other. This was the case in
cell biology. As discussed in
 Section 1,
 microscopic research enabled researchers to identify component parts
(organelles) of cells, but except for indirect clues such as shape or
the ability of an organelle to take up a stain, these researchers were
unable to procure information about the functions they performed.
Biochemists on the other hand were able to identify reactions involved
in many cell activities, but since they studied these in chemical
soups made by grinding up the cell, they were not able to link these
to cell structures. Cell biology developed with the introduction of
new techniques such as cell fractionation and electron microscopy that
opened up the possibility of localizing biochemical reactions in
organelles.
It is worth emphasizing that the sense in which decomposition is
reductionistic is very different from that involved in more
traditional philosophical accounts of theory reduction, which involves
the derivation of one theory from another (Nagel 1961; see the entries
on
 scientific reduction
 and
 reductionism in biology).
 Researchers pursue decomposition without assuming the existence of
full theories at either level or that a theory at one level can be
derived from one at a lower level. Moreover, in pursuing decomposition
researchers typically do not assume that knowledge of the lower-level
components is sufficient for explaining the phenomena—at a
minimum, they recognize that how the components are organized is also
important. The association of a cell process with a specific enzyme is
not the end of the explanatory process—the activity of enzymes
is often affected by the context in which the enzyme exists, for
example, in a membrane. For this reason, researchers value in
vitro reconstitution experiments in establishing that they have
correctly accounted for a phenomenon.
Allchin (1996, 2007) and Weber (2002), analyze an important
reconstitution experiment at the interface of cell biology and
biochemistry that played a major role in resolving a conflict between
two opposing accounts of the synthesis of ATP in oxidative metabolism.
Biochemists spent a couple decades trying to identify a purely
chemical pathway that used the energy released in oxidative metabolism
to synthesize ATP (as happens in glycolysis). P. Mitchell (1961)
advanced an alternative, chemiosmotic hypothesis according to which
energy was transferred via the creation of a proton gradient over a
membrane. Kagawa and Racker (1966) had already linked ATP synthase to
small knobs on the inner mitochondrial membrane but it was an
in vitro chimeric system combining fragments from bacteria,
plants, and animals created by Racker and Stoeckenius (1974) that
demonstrated that energy could first be captured in a proton gradient
and then used in mitochondria to synthesize ATP. Unlike purely
biochemical accounts that discounted cell structure, this research
showed the importance of not just enzymes but how they are situated
with respect to cell structures in explaining cellular phenomena. 
Even in chemical soups individual molecules are arranged in a
particular pattern. This pattern affects, for example, which molecule
encounters another. When analyzing such soups, however, chemists do
not try to decipher the arrangement but instead rely on statistical
measures about the likelihood of encounters. But cells are
different—different molecules are segregated in different
locations and how they are organized affects the resulting behavior
(this is true even in prokaryotic cells that lack internal membranes,
with molecules segregated in different parts of the cell, sometimes
changing location over the course of a day; see Cohen, Erb,
Selimkhanov, et al., 2014). Organization is also important in
manufactured products. If the parts of your computer were distributed
on your desk, and each part was provided with some input so that it
was performing an operation, the parts would still not carry out the
same activities as they do when they are in their proper arrangement.
Organization ensures that the outputs of one component are passed to
the appropriate others as inputs. The importance of organization is
further recognized when one considers that what human designers do is
impose new organization on existing components to achieve novel
desired effects. Evolution often does the same in biology.
Organization is especially important for attempts to understand the
activities of cells but, until recently, cell biologists have had
limited tools for determining organization. At a coarse-grained level,
the combination of cell fractionation and electron microscopy provided
information about organization and this provided insight into how
cells perform their activities. For example, recognizing that enzymes
that can break apart cell components are segregated from the rest of
the cell within the lysosome explains why they only carry out their
activities on structures that are first transported into the lysosome.
And knowing that a collection of enzymes is localized in an adjacent
membrane explains how products of one reaction are readily taken up in
another. In these cases the organization realized in cells and its
consequences can be characterized qualitatively and presented in a
mechanism diagram, as discussed in
 section 4.3.
 In other cases, however, the organization is more complex than can be
described in such qualitative terms. Especially when components are
organized into feedback loops, and the individual operations are
characterized in nonlinear functions, cell biologists turn to
computational models to understand their behavior. Bechtel and
Abrahamsen (2010) and Brigandt (2013) describe a number of examples
from recent research on cell phenomena that require computational
models and refer to the resulting explanations that apply dynamical
analysis to mechanistic accounts as “dynamical mechanistic
explanations”. Using computational modeling of spindle formation
in cell division as an example, Gross (2018) shows how computational
models can go beyond what has been established experimentally and
serve as heuristics guiding further research.
In recent years cell biologists have acquired new tools for studying
organization, many of these advanced in systems biology (Green 2017;
see the entry on
 philosophy of systems and synthetic biology).
 One approach in systems biology seeks to develop comprehensive
detailed models of the numerous components identified as involved in
specific phenomena (Gross & Green 2017). High-throughput data
about, for example, which proteins in a cell can bind to each other or
which pairs of genes, when mutated together, are lethal, has massively
increased the number of cell components associated with any given
phenomenon. To make sense of this cell researchers often seek abstract
models by constructing networks in which nodes stand for entities and
edges for interactions between entities (Green et al. 2018). The
challenge researchers face is to make sense of these
networks—given the large number of entities involved and their
many interactions, these networks can often appear as hairballs. We
briefly consider two strategies for analyzing networks in cell biology
that philosophers have analyzed.
Alon and his collaborators, focusing on gene transcription networks and metabolic
networks in bacteria and yeast, identified “recurring,
significant patterns of interconnections” involving two, three, or four
nodes, which they called motifs (Milo et al., 2002: 824). One example, occurring in
“hundreds of non-homologous gene systems” (Mangan,
Zaslaver, & Alon 2003: 197) in the transcription network of E.
coli, is the feedforward loop. In a feedforward loop
 (Figure 6),
 one unit sends inputs to two other units, the first of which also
sends an input to the other, which serves as the final output unit.
Using Boolean models, Alon and his colleagues demonstrated that
depending on whether the connections are excitatory or inhibitory,
such a motif can perform a number of different functions. For example,
if all the connections are excitatory and the final output unit is
only active when it receives input from both of the other units (the
connections to it constitute an AND-gate, as shown on the left in
 Figure 6),
 then the feedforward loop functions as a persistence
detector—the output unit only becomes active when the input to
the first unit endures at least until the second unit becomes active.
This, as Mangan et al. explain, protects the output (which might serve
to start the transcription of a gene) from being generated in response
to random noise presented to the input unit. The feedforward loop and
Alon’s other motifs are characterized abstractly without
specific reference to what entities and interactions correspond to the
nodes and edges. Philosophers who have examined this research refer to
these organizational patterns as design principles (Green
2015; Green, Levy, & Bechtel 2015; Levy & Bechtel 2013).
Figure 6: Three examples of feedforward
loops studied by Alon and his colleagues
The second strategy employs computational tools such as cluster
analysis to find collections of nodes that are especially highly
interconnected. To interpret these clusters in terms of mechanisms
operative in the cell, researchers often align them with Gene Ontology
(GO), a resource that was developed to represent published information
about cell components, molecular functions, and biological processes
in directed acyclic graphs so as to facilitate communication between
researchers working on different species (Ashburner et al. 2000).
Leonelli (2016) examines many of the epistemic issues the developers
of GO confronted in developing such a resource. Beyond its original
function, GO is now widely used to interpret network graphs
mechanistically. By annotating nodes in cellular networks with
information from GO when it is available researchers interpret these
clusters as corresponding to known mechanisms or sometimes as
constituting previously unknown mechanisms. Researchers make
inferences about nodes for which such information is lacking using
principles such as guilt by association—the entity is
inferred to occur in the same location or to contribute to the same
process as those with which it clusters (Bechtel 2017 analyzes several examples).
As a result of computational models and analyses of large networks,
cell biology in the twenty-first century is much more focused on
organization and appears much less reductionistic than it did in the
mid twentieth century. For some philosophers, this reliance on network
analyses represents a move away from mechanistic explanations towards
a process view (Théry 2015; Nicholson 2018; Dupré 2012).
Huneman (2010, 2018) construes the explanations resulting from network
analysis as a distinct type of explanation he labels topological
explanation. Others, however, argue that since these analyses
still draw upon the components constituting the system and the
biologists developing them continue to think of them as mechanistic,
we should extend the conception of mechanistic explanation to include
the holistic focus on organization developed in computational and
network analyses (Baetu 2015; Bechtel 2015; Levy 2014).
We have focused on core domains of cell biology, concerned with
explaining the basic activities of cells. There are, though, many
specialized fields concerned with cells. Some of these have become
active areas for philosophical inquiry in their own right. Here we
merely identify some of these and point readers to relevant work by
philosophers, including in several cases entries in the SEP. One such
domain is microbiology, concerned with single-cell organisms, whether
prokaryotic (lacking a nucleus and other organelles) or eukaryotic.
O’Malley (2014) has identified distinctive features of
microorganisms (e.g., lateral gene transfer) and explores how features
of microorganismal life challenge major assumptions about living
organisms that have resulted from focusing predominantly on
multi-cellular organisms. In other work (O’Malley 2010), she has
examined from a philosophical perspective the competing hypotheses
about the origin of eukaryotic cells from prokaryotic cells. Much of
cell biology has focused on mature cells of specific types and has
attended less to the processes by which cells in multi-cellular
organisms differentiate from a common cell, known as a stem
cell. Fagan (2013) has pioneered the philosophical examination of
stem-cell research, including the advent of techniques to revert
mature cells to stem cells. Cells are not only transformed in
developmental processes in multi-cell organisms, but also in diseases
such as cancer in which cells not only proliferate in uncontrolled
ways but also defeat many normally operating cellular mechanisms that
normally prevent proliferation. Plutynski (2018) has identified many
of the philosophical challenges arising in the efforts to explain
cancer (see the entry on cancer). An important
capability of many multicell organisms is the ability to detect
pathogens, viruses, and other threats and defend against them. Such
immune responses require, among other things, the capacity to
distinguish cells that belong to the organism itself from others (see
the entry on
 philosophy of immunology).
 A fundamental issue that arises in many of these contexts as well as
topics raised above is whether and how one should conceptualize cells
as individuals or agents (see the entry on
 the biological notion of individual).
In addition to approaching cells as objects of scientific study,
contemporary researchers often adopt an engineering approach to cells.
One context is in the domain of synthetic biology in which researchers
engineer cells, sometimes for research ends but other times to
generate products society finds useful (see the entry on
 philosophy of systems and synthetic biology).
 The recent development of gene editing tools such as CRISPR opens up
both epistemic and ethical issues (for the ethical issues, see the
entry on
 neuroethics).
 One context in which attempts to synthesize cells has played a
central role is in the attempt to understand the origins of life. Much
of the research on origins of life involves the development of
protocells—self-organized, spherical systems composed of lipids
(Rasmussen et al. 2009). This has become a prominent topic of inquiry
for both theoretically minded biologists and philosophers of biology
(see the entry on
 life;
 Bedau 2012; Moreno 2016; Dunér, Malaterre, and Geppert
2016).