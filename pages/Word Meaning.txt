The notions of word and word meaning are problematic
to pin down, and this is reflected in the difficulties one encounters
in defining the basic terminology of lexical semantics. In part, this
depends on the fact that the term ‘word’ itself is highly
polysemous (see, e.g., Matthews 1991; Booij 2007; Lieber 2010). For
example, in ordinary parlance ‘word’ is ambiguous between
a type-level reading (as in “Color and colour
are spellings of the same word”), an occurrence-level reading
(as in “there are thirteen words in the tongue-twister How
much wood would a woodchuck chuck if a woodchuck could chuck
wood?”), and a token-level reading (as in “John
erased the last two words on the blackboard”). Before proceeding
further, let us then elucidate the notion of word in more detail
 (Section 1.1),
 and lay out the key questions that will guide our discussion of word
meaning in the rest of the entry
 (Section 1.2).
We can distinguish two fundamental approaches to the notion of word.
On one side, we have linguistic approaches, which
characterize the notion of word by reflecting on its explanatory role
in linguistic research (for a survey on explanation in linguistics,
see Egré 2015). These approaches often end up splitting the
notion of word into a number of more fine-grained and theoretically
manageable notions, but still tend to regard ‘word’ as a
term that zeroes in on a scientifically respectable concept (e.g., Di
Sciullo & Williams 1987). For example, words are the primary locus
of stress and tone assignment, the basic domain of morphological
conditions on affixation, clitization, compounding, and the theme of
phonological and morphological processes of assimilation, vowel shift,
metathesis, and reduplication (Bromberger 2011).
On the other side, we have metaphysical approaches, which
attempt to pin down the notion of word by inquiring into the
metaphysical nature of words. These approaches typically deal with
such questions as “what are words?”, “how should
words be individuated?”, and “on what conditions two
utterances count as utterances of the same word?”. For example,
Kaplan (1990, 2011) has proposed to replace the orthodox type-token
account of the relation between words and word tokens with a
“common currency” view on which words relate to their
tokens as continuants relate to stages in four-dimensionalist
metaphysics (see the entries on
 types and tokens
 and
 identity over time).
 Other contributions to this debate can be found, a.o., in McCulloch
(1991), Cappelen (1999), Alward (2005), Hawthorne & Lepore (2011),
Sainsbury & Tye (2012), Gasparri (2016), and Irmak
(forthcoming).
For the purposes of this entry, we can rely on the following
stipulation. Every natural language has a lexicon organized
into lexical entries, which contain information about word
types or lexemes. These are the smallest linguistic
expressions that are conventionally associated with a
non-compositional meaning and can be articulated in isolation to
convey semantic content. Word types relate to word tokens and
occurrences just like phonemes relate to phones in phonological
theory. To understand the parallelism, think of the variations in the
place of articulation of the phoneme /n/, which is pronounced as the
voiced bilabial nasal [m] in “ten bags” and as the voiced
velar nasal [ŋ] in “ten gates”. Just as phonemes are
abstract representations of sets of phones (each defining one way the
phoneme can be instantiated in speech), lexemes can be defined as
abstract representations of sets of words (each defining one way the
lexeme can be instantiated in sentences). Thus, ‘do’,
‘does’, ‘done’ and ‘doing’ are
morphologically and graphically marked realizations of the same
abstract word type do. To wrap everything into a single
formula, we can say that the lexical entries listed in a
lexicon set the parameters defining the instantiation
potential of word types in sentences, utterances and inscriptions (cf.
Murphy 2010). In what follows, unless otherwise indicated, our talk of
“word meaning” should be understood as talk of “word
type meaning” or “lexeme meaning”, in the sense we
just illustrated.
As with general theories of meaning (see the entry on
 theories of meaning),
 two kinds of theory of word meaning can be distinguished. The first
kind, which we can label a semantic theory of word meaning,
is a theory interested in clarifying what meaning-determining
information is encoded by the words of a natural language. A framework
establishing that the word ‘bachelor’ encodes the lexical
concept adult unmarried
male would be an example of a semantic theory of word meaning.
The second kind, which we can label a foundational theory of
word meaning, is a theory interested in elucidating the facts in
virtue of which words come to have the semantic properties they have
for their users. A framework investigating the dynamics of semantic
change and social coordination in virtue of which the word
‘bachelor’ is assigned the function of expressing the
lexical concept adult
unmarried male would be an example of a foundational theory of
word meaning. Likewise, it would be the job of a foundational theory
of word meaning to determine whether words have the semantic
properties they have in virtue of social conventions, or whether
social conventions do not provide explanatory purchase on the facts
that ground word meaning (see the entry on
 convention).
Obviously, the endorsement of a given semantic theory is bound to
place important constraints on the claims one might propose about the
foundational attributes of word meaning, and vice versa.
Semantic and foundational concerns are often interdependent, and it is
difficult to find theories of word meaning which are either purely
semantic or purely foundational. According to Ludlow (2014), for
example, the fact that word meaning is systematically underdetermined
(a semantic matter) can be explained in part by looking at the
processes of linguistic negotiation whereby discourse partners
converge on the assignment of shared meanings to the words of their
language (a foundational matter). However, semantic and foundational
theories remain in principle different and designed to answer partly
non-overlapping sets of questions.
Our focus in this entry will be on semantic theories of word
meaning, i.e., on theories that try to provide an answer to such
questions as “what is the nature of word meaning?”,
“what do we know when we know the meaning of a word?”, and
“what (kind of) information must a speaker associate to the
words of a language in order to be a competent user of its
lexicon?”. However, we will engage in foundational
considerations whenever necessary to clarify how a given framework
addresses issues in the domain of a semantic theory of word
meaning.
The study of word meaning became a mature academic enterprise in the
19th century, with the birth of historical-philological
semantics
 (Section 2.2).
 Yet, matters related to word meaning had been the subject of much
debate in earlier times. We can distinguish three major classical
approaches to word meaning: speculative etymology, rhetoric, and
classical lexicography (Meier-Oeser 2011; Geeraerts 2013). We describe
them briefly in
 Section 2.1.
The prototypical example of speculative etymology is perhaps the
Cratylus (383a-d), where Plato presents his well-known
naturalist thesis about word meaning. According to Plato, natural kind
terms express the essence of the objects they denote and words are
appropriate to their referents insofar as they implicitly describe the
properties of their referents (see the entry on
 Plato’s Cratylus).
 For example, the Greek word ‘anthrôpos’
can be broken down into anathrôn ha opôpe, which
translates as “one who reflects on what he has seen”: the
word used to denote humans reflects their being the only animal
species which possesses the combination of vision and intelligence.
For speculative etymology, there is a natural or non-arbitrary
relation between words and their meaning, and the task of the theorist
is to make this relation explicit through an analysis of the
descriptive, often phonoiconic mechanisms underlying the genesis of
words. More on speculative etymology in Malkiel (1993), Fumaroli
(1999), and Del Bello (2007).
The primary aim of the rhetorical tradition was the study of
figures of speech. Some of these concern sentence-level variables such
as the linear order of the words occurring in a sentence (e.g.,
parallelism, climax, anastrophe); others are lexical in nature and
depend on using words in a way not intended by their normal or literal
meaning (e.g., metaphor, metonymy, synecdoche). Although originated
for stylistic and literary purposes, the identification of regular
patterns in the figurative use of words initiated by the rhetorical
tradition provided a first organized framework to investigate the
semantic flexibility of words, and laid the groundwork for further
inquiry into our ability to use lexical expressions beyond the
boundaries of their literal meaning. More on the rhetorical tradition
in Kennedy (1994), Herrick (2004), and Toye (2013).
Finally, classical lexicography and the practice of writing
dictionaries played an important role in systematizing the descriptive
data on which later inquiry would rely to illuminate the relationship
between words and their meaning. Putnam’s (1970) claim that it
was the phenomenon of writing (and needing) dictionaries that gave
rise to the idea of a semantic theory is probably an overstatement.
But the inception of lexicography certainly had an impact on the
development of modern theories of word meaning. The practice of
separating dictionary entries via lemmatization and defining them
through a combination of semantically simpler elements provided a
stylistic and methodological paradigm for much subsequent research on
lexical phenomena, such as decompositional theories of word meaning.
More on classical lexicography in Béjoint (2000), Jackson
(2002), and Hanks (2013).
Historical-philological semantics incorporated elements from all the
above classical traditions and dominated the linguistic scene roughly
from 1870 to 1930, with the work of scholars such as Michel
Bréal, Hermann Paul, and Arsène Darmesteter (Gordon
1982). In particular, it absorbed from speculative etymology an
interest in the conceptual mechanisms underlying the formation of word
meaning, it acquired from rhetorical analysis a taxonomic toolkit for
the classification of lexical phenomena, and it assimilated from
lexicography and textual philology the empirical basis of descriptive
data that subsequent theories of word meaning would have to account
for (Geeraerts 2013).
On the methodological side, the key features of the approach to word
meaning introduced by historical-philological semantics can be
summarized as follows. First, it had a diachronic and pragmatic
orientation. That is, it was primarily concerned with the historical
evolution of word meaning rather than with word meaning statically
understood, and attributed great importance to the contextual
flexibility of word meaning. Witness Paul’s (1920 [1880])
distinction between usuelle Bedeutung and okkasionelle
Bedeutung, or Bréal’s (1924 [1897]) account of
polysemy as a byproduct of semantic change. Second, it looked at word
meaning primarily as a psychological phenomenon. It assumed that the
semantic properties of words should be defined in mentalistic terms
(i.e., words signify “concepts” or “ideas” in
a broad sense), and that the dynamics of sense modulation, extension,
and contraction that underlie lexical change correspond to broader
patterns of conceptual activity in the human mind. Interestingly,
while the classical rhetorical tradition had conceived of tropes as
marginal linguistic phenomena whose investigation, albeit important,
was primarily motivated by stylistic concerns, for
historical-philological semantics the psychological mechanisms
underlying the production and the comprehension of figures of speech
were part of the ordinary life of languages, and engines of the
evolution of all aspects of lexical systems (Nerlich 1992).
The contribution made by historical-philological semantics to the
study of word meaning had a long-lasting influence. First, with its
emphasis on the principles of semantic change, historical-philological
semantics was the first systematic framework to focus on the dynamic
nature of word meaning, and established contextual flexibility as the
primary explanandum for a theory of word meaning (Nerlich & Clarke
1996, 2007). This feature of historical-philological semantics is a
clear precursor of the emphasis placed on context-sensitivity by many
subsequent approaches to word meaning, both in philosophy (see
 Section 3)
 and in linguistics (see
 Section 4).
 Second, the psychologistic approach to word meaning fostered by
historical philological-semantics added to the agenda of linguistic
research the question of how word meaning relates to cognition at
large. If word meaning is essentially a psychological phenomenon, what
psychological categories should be used to characterize it? What is
the dividing line separating the aspects of our mental life that
constitute knowledge of word meaning from those that do not? As we
shall see, this question will constitute a central concern for
cognitive theories of word meaning (see
 Section 5).
In this section we shall review some semantic and metasemantic
theories in analytic philosophy that bear on how lexical meaning
should be conceived and described. We shall follow a roughly
chronological order. Some of these theories, such as Carnap’s
theory of meaning postulates and Putnam’s theory of stereotypes,
have a strong focus on lexical meaning, whereas others, such as
Montague semantics, regard it as a side issue. However, such negative
views form an equally integral part of the philosophical debate on
word meaning.
By taking the connection of thoughts and truth as the basic issue of
semantics and regarding sentences as “the proper means of
expression for a thought” (Frege 1979a [1897]), Frege paved the
way for the 20th century priority of sentential meaning
over lexical meaning: the semantic properties of subsentential
expressions such as individual words were regarded as derivative, and
identified with their contribution to sentential meaning. Sentential
meaning was in turn identified with truth conditions, most explicitly
in Wittgenstein’s Tractatus logico-philosophicus
(1922). However, Frege never lost interest in the “building
blocks of thoughts” (Frege 1979b [1914]), i.e., in the semantic
properties of subsentential expressions. Indeed, his theory of sense
and reference for names and predicates may be counted as the inaugural
contribution to lexical semantics within the analytic tradition (see
the entry on
 Gottlob Frege).
 It should be noted that Frege did not attribute semantic properties
to lexical units as such, but to what he regarded as a
sentence’s logical constituents: e.g., not to the word
‘dog’ but to the predicate ‘is a dog’. In
later work this distinction was obliterated and Frege’s semantic
notions came to be applied to lexical units.
Possibly because of lack of clarity affecting the notion of sense, and
surely because of Russell’s (1905) authoritative criticism of
Fregean semantics, word meaning disappeared from the philosophical
scene during the 1920s and 1930s. In Wittgenstein’s
Tractatus the “real” lexical units, i.e., the
constituents of a completely analyzed sentence, are just names, whose
semantic properties are exhausted by their reference. In
Tarski’s (1933) work on formal languages, which was taken as
definitional of the very field of semantics for some time, lexical
units are semantically categorized into different classes (individual
constants, predicative constants, functional constants) depending on
the logical type of their reference, i.e., according to whether they
designate individuals in a domain of interpretation, classes of
individuals (or of n-tuples of individuals), or functions
defined over the domain. However, Tarski made no attempt nor felt any
need to represent semantic differences among expressions belonging to
the same logical type (e.g., between one-place predicates such as
‘dog’ and ‘run’, or between two-place
predicates such as ‘love’ and ‘left of’). See
the entry on
 Alfred Tarski.
Quine (1943) and Church (1951) rehabilitated Frege’s distinction
of sense and reference. Non-designating words such as
‘Pegasus’ cannot be meaningless: it is precisely the
meaning of ‘Pegasus’ that allows speakers to establish
that the word lacks reference. Moreover, as Frege (1892) had argued,
true factual identities such as “Morning Star = Evening
Star” do not state synonymies; if they did, any competent
speaker of the language would be aware of their truth. Along these
lines, Carnap (1947) proposed a new formulation of the sense/reference
dichotomy, which was translated into the distinction between
intension and extension. The notion of intension was
intended to be an explicatum of Frege’s
“obscure” notion of sense: two expressions have the same
intension if and only if they have the same extension in every
possible world or, in Carnap’s terminology, in every state
description (i.e., in every maximal consistent set of atomic
sentences and negations of atomic sentences). Thus,
‘round’ and ‘spherical’ have the same
intension (i.e., they express the same function from possible worlds
to extensions) because they apply to the same objects in every
possible world. Carnap later suggested that intensions could be
regarded as the content of lexical semantic competence: to know the
meaning of a word is to know its intension, “the general
conditions which an object must fulfill in order to be denoted by
[that] word” (Carnap 1955). However, such general conditions
were not spelled out by Carnap (1947). Consequently, his system did
not account, any more than Tarski’s, for semantic differences
and relations among words belonging to the same semantic category:
there were possible worlds in which the same individual a
could be both a married man and a bachelor, as no constraints were
placed on either word’s intension. One consequence, as Quine
(1951) pointed out, was that Carnap’s system, which was supposed
to single out analytic truths as true in every possible world,
“Bachelors are unmarried”—intuitively, a
paradigmatic analytic truth—turned out to be synthetic rather
than analytic.
To remedy what he agreed was an unsatisfactory feature of his system,
Carnap (1952) introduced meaning postulates, i.e.,
stipulations on the relations among the extensions of lexical items.
For example, the meaning postulate
stipulates that any individual that is in the extension of
‘bachelor’ is not in the extension of
‘married’. Meaning postulates can be seen either as
restrictions on possible worlds or as relativizing analyticity to
possible worlds. On the former option we shall say that “If Paul
is a bachelor then Paul is unmarried” holds in every
admissible possible world, while on the latter we shall say
that it holds in every possible world in which (MP) holds.
Carnap regarded the two options as equivalent; nowadays, the former is
usually preferred. Carnap (1952) also thought that meaning postulates
expressed the semanticist’s “intentions” with
respect to the meanings of the descriptive constants, which may or may
not reflect linguistic usage; again, today postulates are usually
understood as expressing semantic relations (synonymy, analytic
entailment, etc.) among lexical items as currently used by competent
speakers.
In the late 1960s and early 1970s, Montague (1974) and other
philosophers and linguists (Kaplan, Kamp, Partee, and D. Lewis among
others) set out to apply to the analysis of natural language the
notions and techniques that had been introduced by Tarski and Carnap
and further developed in Kripke’s possible worlds semantics (see
the entry on
 Montague semantics).
 Montague semantics can be represented as aiming to capture the
inferential structure of a natural language: every inference that a
competent speaker would regard as valid should be derivable in the
theory. Some such inferences depend for their validity on syntactic
structure and on the logical properties of logical words, like the
inference from “Every man is mortal and Socrates is a man”
to “Socrates is mortal”. Other inferences depend on
properties of non-logical words that are usually regarded as semantic,
like the inference from “Kim is pregnant” to “Kim is
not a man”. In Montague semantics, such inferences are taken
care of by supplementing the theory with suitable Carnapian meaning
postulates. Yet, some followers of Montague regarded such additions as
spurious: the aims of semantics, they said, should be distinguished
from those of lexicography. The description of the meaning of
non-logical words requires considerable world knowledge: for example,
the inference from “Kim is pregnant” to “Kim is not
a man” is based on a “biological” rather than on a
“logical” generalization. Hence, we should not expect a
semantic theory to furnish an account of how any two expressions
belonging to the same syntactic category differ in meaning (Thomason
1974). From such a viewpoint, Montague semantics would not differ
significantly from Tarskian semantics in its account of lexical
meaning. But not all later work within Montague’s program shared
such a skepticism about representing aspects of lexical meaning within
a semantic theory, using either componential analysis (Dowty 1979) or
meaning postulates (Chierchia & McConnell-Ginet 2000).
For those who believe that meaning postulates can exhaust lexical
meaning, the issue arises of how to choose them, i.e., of
how—and whether—to delimit the set of meaning-relevant
truths with respect to the set of all true statements in which a given
word occurs. As we just saw, Carnap himself thought that the choice
could only be the expression of the semanticist’s intentions.
However, we seem to share intuitions of analyticity, i.e., we
seem to regard some, but not all sentences of a natural language as
true by virtue of the meaning of the occurring words. Such intuitions
are taken to reflect objective semantic properties of the language,
that the semanticist should describe rather than impose at will. Quine
(1951) did not challenge the existence of such intuitions, but he
argued that they could not be cashed out in the form of a
scientifically respectable criterion separating analytic truths
(“Bachelors are unmarried”) from synthetic truths
(“Aldo’s uncle is a bachelor”), whose truth does not
depend on meaning alone. Though Quine’s arguments were often
criticized (for recent criticisms, see Williamson 2007), and in spite
of Chomsky’s constant endorsement of analyticity (see e.g. 2000:
47, 61–2), within philosophy the analytic/synthetic distinction
was never fully vindicated (for an exception, see Russell 2008).
Hence, it was widely believed that lexical meaning could not be
adequately described by meaning postulates. Fodor and Lepore (1992)
argued that this left semantics with two options: lexical meanings
were either atomic (i.e., they could not be specified by
descriptions involving other meanings) or they were holistic,
i.e., only the set of all true sentences of the language could count
as fixing them.
Neither alternative looked promising. Holism incurred in objections
connected with the acquisition and the understanding of language: how
could individual words be acquired by children, if grasping their
meaning involved, somehow, semantic competence on the whole language?
And how could individual sentences be understood if the information
required to understand them exceeded the capacity of human working
memory? (For an influential criticism of several varieties of holism,
see Dummett 1991; for a review, Pagin 2006). Atomism, in turn, ran
against strong intuitions of (at least some) relations among words
being part of a language’s semantics: it is because of what
‘bachelor’ means that it doesn’t make sense to
suppose we could discover that some bachelors are married. Fodor
(1998) countered this objection by reinterpreting allegedly semantic
relations as metaphysically necessary connections among extensions of
words. However, sentences that are usually regarded as analytic, such
as “Bachelors are unmarried”, are not easily seen as just
metaphysically necessary truths like “Water is
H2O”. If water is H2O, then its
metaphysical essence consists in being H2O (whether we know
it or not); but there is no such thing as a metaphysical essence that
all bachelors share—an essence that could be hidden to us, even
though we use the word ‘bachelor’ competently. On the
contrary, on acquiring the word ‘bachelor’ we acquire the
belief that bachelors are unmarried (Quine 1986); by contrast, many
speakers that have ‘water’ in their lexical repertoire do
not know that water is H2O. The difficulties of atomism and
holism opened the way to vindications of molecularism (e.g., Perry
1994; Marconi 1997), the view on which only some relations among words
matter for acquisition and understanding (see the entry on
 meaning holism).
While mainstream formal semantics went with Carnap and Montague,
supplementing the Tarskian apparatus with the possible worlds
machinery and defining meanings as intensions, Davidson (1967, 1984)
put forth an alternative suggestion. Tarski had shown how to provide a
definition of the truth predicate for a (formal) language L:
such a definition is materially adequate (i.e., it is a definition of
truth, rather than of some other property of sentences of
L) if and only if it entails every biconditional of the
form
where S is a sentence of L and p is its
translation into the metalanguage of L in which the definition
is formulated. Thus, Tarski’s account of truth presupposes that
the semantics of both L and its metalanguage is fixed
(otherwise it would be undetermined whether S translates into
p). On Tarski’s view, each biconditional of form (T)
counts as a “partial definition” of the truth predicate
for sentences of L (see the entry on
 Tarski’s truth definitions).
 By contrast, Davidson suggested that if one took the notion of truth
for granted, then T-biconditionals could be read as collectively
constituting a theory of meaning for L, i.e., as stating truth
conditions for the sentences of L. For example,
states the truth conditions of the English sentence “If the
weather is bad then Sharon is sad”. Of course, (W) is
intelligible only if one understands the language in which it is
phrased, including the predicate ‘true in English’.
Davidson thought that the recursive machinery of Tarski’s
definition of truth could be transferred to the suggested semantic
reading, with extensions to take care of the forms of natural language
composition that Tarski had neglected because they had no analogue in
the formal languages he was dealing with. Unfortunately, few of such
extensions were ever spelled out by Davidson or his followers.
Moreover, it is difficult to see how, giving up possible worlds and
intensions in favor of a purely extensional theory, the Davidsonian
program could account for the semantics of propositional attitude
ascriptions of the form “A believes (hopes, imagines, etc.) that
p”.
Construed as theorems of a semantic theory, T-biconditionals were
often accused of being uninformative (Putnam 1975; Dummett 1976): to
understand them, one has to already possess the information they are
supposed to provide. This is particularly striking in the case of
lexical axioms such as the following:
(To be read, respectively, as “the predicate ‘man’
applies to x if and only if x is a man” and
“the predicate ‘know’ applies to the pair \(\langle
x, y\rangle\) if and only if x knows y”). Here
it is apparent that in order to understand (V1) one must know what
‘man’ means, which is just the information that (V1) is
supposed to convey (as the theory, being purely extensional,
identifies meaning with reference). Some Davidsonians, though
admitting that statements such as (V1) and (V2) are in a sense
“uninformative”, insist that what (V1) and (V2) state is
no less “substantive” (Larson & Segal 1995). To prove
their point, they appeal to non-homophonic versions of lexical axioms,
i.e., to the axioms of a semantic theory for a language that does not
coincide with the (meta)language in which the theory itself is
phrased. Such would be, e.g.,
(V3), they argue, is clearly substantive, yet what it says is exactly
what (V1) says, namely, that the word ‘man’ applies to a
certain category of objects. Therefore, if (V3) is substantive, so is
(V1). But this is beside the point. The issue is not whether (V1)
expresses a proposition; it clearly does, and it is, in this sense,
“substantive”. But what is relevant here is informative
power: to one who understands the metalanguage of (V3), i.e., French,
(V3) may communicate new information, whereas there is no circumstance
in which (V1) would communicate new information to one who understands
English.
In the mid-1970s, Dummett raised the issue of the proper place of
lexical meaning in a semantic theory. If the job of a theory of
meaning is to make the content of semantic competence
explicit—so that one could acquire semantic competence in a
language L by learning an adequate theory of meaning for
L—then the theory ought to reflect a competent
speaker’s knowledge of circumstances in which she would assert a
sentence of L, such as “The horse is in the barn”,
as distinct from circumstances in which she would assert “The
cat is on the mat”. This, in turn, appears to require that the
theory yields explicit information about the use of
‘horse’, ‘barn’, etc., or, in other words,
that it includes information which goes beyond the logical type of
lexical units. Dummett identified such information with a word’s
Fregean sense. However, he did not specify the format in which word
senses should be expressed in a semantic theory, except for words that
could be defined (e.g., ‘aunt’ = “sister of a
parent”): in such cases, the definiens specifies what a
speaker must understand in order to understand the word (Dummett
1991). But of course, not all words are of this kind. For other words,
the theory should specify what it is for a speaker to know them,
though we are not told how exactly this should be done. Similarly,
Grandy (1974) pointed out that by identifying the meaning of a word
such as ‘wise’ as a function from possible worlds to the
sets of wise people in those worlds, Montague semantics only specifies
a formal structure and eludes the question of whether there is some
possible description for the functions which are claimed to be the
meanings of words. Lacking such descriptions, possible worlds
semantics is not really a theory of meaning but a theory of logical
form or logical validity. Again, aside from suggesting that “one
would like the functions to be given in terms of computation
procedures, in some sense”, Grandy had little to say about the
form of lexical descriptions.
In a similar vein, Partee (1981) argued that Montague semantics, like
every compositional or structural semantics, does not
uniquely fix the intensional interpretation of words. The addition of
meaning postulates does rule out some interpretations (e.g.,
interpretations on which the extension of ‘bachelor’ and
the extension of ‘married’ may intersect in some possible
world). However, it does not reduce them to the unique,
“intended” or, in Montague’s words,
“actual” interpretation (Montague 1974). Hence, standard
model-theoretic semantics does not capture the whole content of a
speaker’s semantic competence, but only its structural aspects.
Fixing “the actual interpretation function” requires more
than language-to-language connections as encoded by, e.g., meaning
postulates: it requires some “language-to-world
grounding”. Arguments to the same effect were developed
by Bonomi (1983) and Harnad (1990). In particular, Harnad had in mind
the simulation of human semantic competence in artificial systems: he
suggested that symbol grounding could be implemented, in part, by
“feature detectors” picking out “invariant features
of objects and event categories from their sensory projections”
(for recent developments see, e.g., Steels & Hild 2012). Such a
cognitively oriented conception of grounding differs from
Partee’s Putnam-inspired view, on which the semantic grounding
of lexical items depends on the speakers’ objective interactions
with the external world in addition to their narrow psychological
properties.
A resolutely cognitive approach characterizes Marconi’s (1997)
account of lexical semantic competence. In his view, lexical
competence has two aspects: an inferential aspect, underlying
performances such as semantically based inference and the command of
synonymy, hyponymy and other semantic relations; and a
referential aspect, which is in charge of performances such
as naming (e.g., calling a horse ‘horse’) and application
(e.g., answering the question “Are there any spoons in the
drawer?”). Language users typically possess both aspects of
lexical competence, though in different degrees for different words: a
zoologist’s inferential competence on ‘manatee’ is
usually richer than a layman’s, though a layman who spent her
life among manatees may be more competent, referentially, than a
“bookish” scientist. However, the two aspects are
independent of each another, and neuropsychological evidence appears
to show that they can be dissociated: there are patients whose
referential competence is impaired or lost while their inferential
competence is intact, and vice versa (see
 Section 5.3).
 Being a theory of individual competence, Marconi’s account does
not deal directly with lexical meanings in a public language:
communication depends both on the uniformity of cognitive interactions
with the external world and on communal norms concerning the use of
language, together with speakers’ deferential attitude toward
semantic authorities.
Since the early 1970s, views on lexical meaning were revolutionized by
semantic externalism. Initially, externalism was limited to proper
names and natural kind words such as ‘gold’ or
‘lemon’. In slightly different ways, both Kripke (1972)
and Putnam (1970, 1975) argued that the reference of such words was
not determined by any description that a competent speaker associated
with the word; more generally, and contrary to what Frege may have
thought, it was not determined by any cognitive content associated
with it in a speaker’s mind (for arguments to that effect, see
the entry on
 names).
 Instead, reference is determined, at least in part, by objective
(“causal”) relations between a speaker and the external
world. For example, a speaker refers to Aristotle when she utters the
sentence “Aristotle was a great warrior”—so that her
assertion expresses a false proposition about Aristotle, not a true
proposition about some great warrior she may “have in
mind”—thanks to her connection with Aristotle himself. In
this case, the connection is constituted by a historical chain of
speakers going back to the initial users of the name
‘Aristotle’, or its Greek equivalent, in baptism-like
circumstances. To belong to the chain, speakers (including present-day
speakers) are not required to possess any precise knowledge of
Aristotle’s life and deeds; they are, however, required to
intend to use the name as it is used by the speakers they are picking
up the name from, i.e., to refer to the individual those speakers
intend to refer to.
In the case of most natural kind names, it may be argued, baptisms are
hard to identify or even conjecture. In Putnam’s view, for such
words reference is determined by speakers’ causal interaction
with portions of matter or biological individuals in their
environment: ‘water’, for example, refers to this
liquid stuff, stuff that is normally found in our rivers,
lakes, etc. The indexical component (this liquid,
our rivers) is crucial to reference determination: it
wouldn’t do to identify the referent of ‘water’ by
way of some description (“liquid, transparent, quenches thirst,
boils at 100°C, etc.”), for something might fit the
description yet fail to be water, as in Putnam’s (1973, 1975) famous Twin
Earth thought experiment (see the entry on
 reference).
 It might be remarked that, thanks to modern chemistry, we now possess
a description that is sure to apply to water and only to water:
“being H2O” (Millikan 2005). However, even if
our chemistry were badly mistaken (as in principle it could turn out
to be) and water were not, in fact, H2O,
‘water’ would still refer to whatever has the same nature
as this liquid. Something belongs to the extension of
‘water’ if and only if it is the same substance as this
liquid, which we identify—correctly, as we believe—as
being H2O.
Let it be noted that in Putnam’s original proposal, reference
determination is utterly independent of speakers’ cognition:
‘water’ on Twin Earth refers to XYZ (not to
H2O) even though the difference between the two substances
is cognitively inert, so that before chemistry was created nobody on
either Earth or Twin Earth could have told them apart. However, the
label ‘externalism’ has been occasionally used for weaker
views: a semantic account may be regarded as externalist if it takes
semantic content to depend in one way or another on relations a
computational system bears to things outside itself (Rey 2005; Borg
2012), irrespective of whether such relations affect the
system’s cognitive state. Weak externalism is hard to
distinguish from forms of internalism on which a word’s
reference is determined by information stored in a speaker’s
cognitive system—information of which the speaker may or may not
be aware (Evans 1982). Be that as it may, in what follows
‘externalism’ will be used to mean strong, or Putnamian,
externalism.
Does externalism apply to other lexical categories besides proper
names and natural kind words? Putnam (1975) extended it to artifactual
words, claiming that ‘pencil’ would refer to
pencils—those objects—even if they turned out not
to fit the description by which we normally identify them (e.g., if
they were discovered to be organisms, not artifacts). Schwartz (1978,
1980) pointed out, among many objections, that even in such a case we
could make objects fitting the original description; we would
then regard the pencil-like organisms as impostors, not as
“genuine” pencils. Others sided with Putnam and the
externalist account: for example, Kornblith (1980) pointed out that
artifactual kinds from an ancient civilization could be re-baptized in
total ignorance of their function. The new artifactual word would then
refer to the kind those objects belong to independently of
any beliefs about them, true or false. Against such externalist
accounts, Thomasson (2007) argued that artifactual terms cannot refer
to artifactual kinds independently of all beliefs and concepts about
the nature of the kind, for the concept of the kind’s creator(s)
is constitutive of the nature of the kind. Whether artifactual words
are liable to an externalist account is still an open issue (for
recent discussions see Marconi 2013; Bahr, Carrara & Jansen 2019;
see also the entry on
 artifacts),
 as is, more generally, the scope of application of externalist
semantics.
There is another form of externalism that does apply to all or most
words of a language: social externalism (Burge 1979), the
view on which the meaning of a word as used by an individual speaker
depends on the semantic standards of the linguistic community the
speaker belongs to. In our community the word ‘arthritis’
refers to arthritis—an affliction of the joints—even when
used by a speaker who believes that it can afflict the muscles as well
and uses the word accordingly. If the community the speaker belongs to
applied ‘arthritis’ to rheumatoids ailments in general,
whether or not they afflict the joints, the same word form would not
mean arthritis and
would not refer to arthritis. Hence, a speaker’s mental
contents, such as the meanings associated with the words she uses,
depend on something external to her, namely the uses and the standards
of use of the linguistic community she belongs to. Thus, social
externalism eliminates the notion of idiolect: words only have the
meanings conferred upon them by the linguistic community
(“public” meanings); discounting radical incompetence,
there is no such thing as individual semantic deviance, there are only
false beliefs (for criticisms, see Bilgrami 1992, Marconi 1997; see
also the entry on
 idiolects).
Though both forms of externalism focus on reference, neither is a
complete reduction of lexical meaning to reference. Both Putnam and
Burge make it a necessary condition of semantic competence on a word
that a speaker commands information that other semantic views would
regard as part of the word’s sense. For example, if a speaker
believes that manatees are a kind of household appliance, she would
not count as competent on the word ‘manatee’, nor would
she refer to manatees by using it (Putnam 1975; Burge 1993). Beyond
that, it is not easy for externalists to provide a satisfactory
account of lexical semantic competence, as they are committed to
regarding speakers’ beliefs and abilities (e.g., recognitional
abilities) as essentially irrelevant to reference determination, hence
to meaning. Two main solutions have been proposed. Putnam (1970, 1975)
suggested that a speaker’s semantic competence consists in her
knowledge of stereotypes associated with words. A stereotype
is an oversimplified theory of a word’s extension: the
stereotype associated with ‘tiger’ describes tigers as
cat-like, striped, carnivorous, fierce, living in the jungle, etc.
Stereotypes are not meanings, as they do not determine reference in
the right way: there are albino tigers and tigers that live in zoos.
What the ‘tiger’-stereotype describes is (what the
community takes to be) the typical tiger. Knowledge of
stereotypes is necessary to be regarded as a competent speaker,
and—one surmises—it can also be considered sufficient for
the purposes of ordinary communication. Thus, Putnam’s account
does provide some content for semantic competence, though it
dissociates it from knowledge of meaning.
On an alternative view (Devitt 1983), competence on
‘tiger’ does not consist in entertaining propositional
beliefs such as “tigers are striped”, but rather in being
appropriately linked to a network of causal chains for
‘tiger’ involving other people’s abilities,
groundings, and reference borrowings. In order to understand the
English word ‘tiger’ and use it in a competent fashion, a
subject must be able to combine ‘tiger’ appropriately with
other words to form sentences, to have thoughts which those sentences
express, and to ground these thoughts in tigers. Devitt’s
account appears to make some room for a speaker’s ability to,
e.g., recognize a tiger when she sees one; however, the respective
weights of individual abilities (and beliefs) and objective grounding
are not clearly specified. Suppose a speaker A belongs to a
community C that is familiar with tigers; unfortunately,
A has no knowledge of the typical appearance of a tiger and
is unable to tell a tiger from a leopard. Should A be
regarded as a competent user ‘tiger’ on account of her
being “part of C” and therefore linked to a
network of causal chains for ‘tiger’?
Some philosophers (e.g., Loar 1981; McGinn 1982; Block 1986) objected
to the reduction of lexical meaning to reference, or to
non-psychological factors that are alleged to determine reference. In
their view, there are two aspects of meaning (more generally, of
content): the narrow aspect, that captures the intuition that
‘water’ has the same meaning in both Earthian and
Twin-Earthian English, and the wide aspect, that captures the
externalist intuition that ‘water’ picks out different
substances in the two worlds. The wide notion is required to account
for the difference in reference between English and Twin-English
‘water’; the narrow notion is needed, first and foremost,
to account for the relation between a subject’s beliefs and her
behavior. The idea is that how an object of reference is
described (not just which object one refers to) can make a difference
in determining behavior. Oedipus married Jocasta because he thought he
was marrying the queen of Thebes, not his mother, though as a matter
of fact Jocasta was his mother. This applies to words of all
categories: someone may believe that water quenches thirst without
believing that H2O does; Lois Lane believed that Superman
was a superhero but she definitely did not believe the same of her
colleague Clark Kent, so she behaved one way to the man she identified
as Superman and another way to the man she identified as Clark Kent
(though they were the same man). Theorists that countenance these two
components of meaning and content usually identify the narrow aspect
with the inferential or conceptual role of an
expression e, i.e., with the aspect of e that
contributes to determine the inferential relations between sentences
containing an occurrence of e and other sentences. Crucially,
the two aspects are independent: neither determines the other. The
stress on the independence of the two factors also characterizes more
recent versions of so-called “dual aspect” theories, such
as Chalmers (1996, 2002).
While dual theorists agree with Putnam’s claim that some aspects
of meaning are not “in the head”, others have opted for
plain internalism. For example, Segal (2000) rejected the intuitions
that are usually associated with the Twin-Earth cases by arguing that
meaning (and content in general) “locally supervenes” on a
subject’s intrinsic physical properties. But the most
influential critic of externalism has undoubtedly been Chomsky (2000).
First, he argued that much of the alleged support for externalism
comes in fact from “intuitions” about words’
reference in this or that circumstance. But ‘reference’
(and the verb ‘refer’ as used by philosophers) is a
technical term, not an ordinary word, hence we have no more intuitions
about reference than we have about tensors or c-command. Second, if we
look at how words such as ‘water’ are applied in ordinary
circumstances, we find that speakers may call ‘water’
liquids that contain a smaller proportion of H2O than other
liquids they do not call ‘water’ (e.g., tea): our use of
‘water’ does not appear to be governed by hypotheses about
microstructure. According to Chomsky, it may well be that progress in
the scientific study of the language faculty will allow us to
understand in what respects one’s picture of the world is framed
in terms of things selected and individuated by properties of the
lexicon, or involves entities and relationships describable by the
resources of the language faculty. Some semantic properties
do appear to be integrated with other aspects of language. However,
so-called “natural kind words” (which in fact have little
to do with kinds in nature, Chomsky claims) may do little more than
indicating “positions in belief systems”: studying them
may be of some interest for “ethnoscience”, surely not for
a science of language. Along similar lines, others have maintained
that the genuine semantic properties of linguistic expressions should
be regarded as part of syntax, and that they constrain but do not
determine truth conditions (e.g., Pietroski 2005, 2010). Hence, the
connection between meaning and truth conditions (and reference) may be
significantly looser than assumed by many philosophers.
“Ordinary language” philosophers of the 1950s and 1960s
regarded work in formal semantics as essentially irrelevant to issues
of meaning in natural language. Following Austin and the later
Wittgenstein, they identified meaning with use and were prone to
consider the different patterns of use of individual expressions as
originating different meanings of the word. Grice (1975) argued that
such a proliferation of meanings could be avoided by distinguishing
between what is asserted by a sentence (to be identified with its
truth conditions) and what is communicated by it in a given context
(or in every “normal” context). For example, consider the
following exchange:
Although B does not literally assert that Kim had breakfast on that
particular day (see, however, Partee 1973), she does communicate as
much. More precisely, A could infer the communicated content by
noticing that the asserted sentence, taken literally (“Kim had
breakfast at least once in her life”), would be less informative
than required in the context: thus, it would violate one or more
principles of conversation (“maxims”) whereas there is no
reason to suppose that the speaker intended to opt out of
conversational cooperation (see the entries on
 Paul Grice
 and
 pragmatics).
 If the interlocutor assumes that the speaker intended him to infer
the communicated content—i.e., that Kim had breakfast that
morning, so presumably she would not be hungry at
11—cooperation is preserved. Such non-asserted content, called
‘implicature’, need not be an addition to the overtly
asserted content: e.g., in irony asserted content is negated rather
than expanded by the implicature (think of a speaker uttering
“Paul is a fine friend” to implicate that Paul has
wickedly betrayed her).
Grice’s theory of conversation and implicatures was interpreted
by many (including Grice himself) as a convincing way of accounting
for the variety of contextually specific communicative contents while
preserving the uniqueness of a sentence’s “literal”
meaning, which was identified with truth conditions and regarded as
determined by syntax and the conventional meanings of the occurring
words, as in formal semantics. The only semantic role context was
allowed to play was in determining the content of indexical words
(such as ‘I’, ‘now’, ‘here’, etc.)
and the effect of context-sensitive structures (such as tense) on a
sentence’s truth conditions. However, in about the same years
Travis (1975) and Searle (1979, 1980) pointed out that the semantic
relevance of context might be much more pervasive, if not universal:
intuitively, the same sentence type could have very different truth
conditions in different contexts, though no indexical expression or
structure appeared to be involved. Take the sentence “There is
milk in the fridge”: in the context of morning breakfast it will
be considered true if there is a carton of milk in the fridge and
false if there is a patch of milk on a tray in the fridge, whereas in
the context of cleaning up the kitchen truth conditions are reversed.
Examples can be multiplied indefinitely, as indefinitely many factors
can turn out to be relevant to the truth or falsity of a sentence as
uttered in a particular context. Such variety cannot be plausibly
reduced to traditional polysemy such as the polysemy of
‘property’ (meaning quality or real estate), nor can it be
described in terms of Gricean implicatures: implicatures are supposed
not to affect a sentence’s truth conditions, whereas here it is
precisely the sentence’s truth conditions that are seen as
varying with context.
The traditionalist could object by challenging the
contextualist’s intuitions about truth conditions. “There
is milk in the fridge”, she could argue, is true if and only if
there is a certain amount (a few molecules will do) of a certain
organic substance in the relevant fridge (for versions of this
objection, Cappelen & Lepore 2005). So the sentence is true both
in the carton case and in the patch case; it would be false only if
the fridge did not contain any amount of any kind of milk (whether cow
milk or goat milk or elephant milk). The contextualist’s reply
is that, in fact, neither the speaker nor the interpreter is aware of
such alleged literal content (the point is challenged by Fodor 1983,
Carston 2002); but “what is said” must be intuitively
accessible to the conversational participants (Availability
Principle, Recanati 1989). If truth conditions are associated
with what is said—as the traditionalist would agree they
are—then in many cases a sentence’s literal content, if
there is such a thing, does not determine a complete, evaluable
proposition. For a genuine proposition to arise, a sentence
type’s literal content (as determined by syntax and conventional
word meaning) must be enriched or otherwise modified by primary
pragmatic processes based on the speakers’ background
knowledge relative to each particular context of use of the sentence.
Such processes differ from Gricean implicature-generating processes in
that they come into play at the sub-propositional level; moreover,
they are not limited to saturation of indexicals but may
include the replacement of a constituent with another. These tenets
define contextualism (Recanati 1993; Bezuidenhout 2002; Carston 2002;
relevance theory (Sperber & Wilson 1986) is in some respects a
precursor of such views). Contextualists take different stands on
nature of the semantic contribution made by words to sentences, though
they typically agree that it is insufficient to fix truth conditions
(Stojanovic 2008). See Del Pinal (2018) for an argument that radical
contextualism (in particular, truth-conditional pragmatics) should
instead commit to rich lexical items which, in certain conditions, do
suffice to fix truth conditions.
Even if sentence types have no definite truth conditions, it does not
follow that lexical types do not make definite or predictable
contributions to the truth conditions of sentences (think of indexical
words). It does follow, however, that conventional word meanings are
not the final constituents of complete propositions (see Allot &
Textor 2012). Does this imply that there are no such things as lexical
meanings understood as features of a language? If so, how should we
account for word acquisition and lexical competence in general?
Recanati (2004) does not think that contextualism as such is committed
to meaning eliminativism, the view on which words as types have no
meaning; nevertheless, he regards it as defensible. Words could be
said to have, rather than “meaning”, a semantic
potential, defined as the collection of past uses of a word
w on the basis of which similarities can be established
between source situations (i.e., the circumstances in which a speaker
has used w) and target situations (i.e., candidate occasions
of application of w). It is natural to object that even
admitting that long-term memory could encompass such an immense amount
of information (think of the number of times ‘table’ or
‘woman’ are used by an average speaker in the course of
her life), surely working memory could not review such information to
make sense of new uses. On the other hand, if words were associated
with “more abstract schemata corresponding to types of
situations”, as Recanati suggests as a less radical alternative
to meaning eliminativism, one wonders what the difference would be
with respect to traditional accounts in terms of polysemy.
Other conceptions of “what is said” make more room for the
semantic contribution of conventional word meanings. Bach (1994)
agrees with contextualists that the linguistic meaning of words (plus
syntax and after saturation) does not always determine complete,
truth-evaluable propositions; however, he maintains that they do
provide some minimal semantic information, a so-called
‘propositional radical’, that allows pragmatic processes
to issue in one or more propositions. Bach identifies “what is
said” with this minimal information. However, many have objected
that minimal content is extremely hard to isolate (Recanati 2004;
Stanley 2007). Suppose it is identified with the content that all the
utterances of a sentence type share; unfortunately, no such content
can be attributed to a sentence such as “Every bottle is in the
fridge”, for there is no proposition that is stably asserted by
every utterance of it (surely not the proposition that every bottle in
the universe is in the fridge, which is never asserted).
Stanley’s (2007) indexicalism rejects the notion of
minimal proposition and any distinction between semantic content and
communicated content: communicated content can be entirely captured by
means of consciously accessible, linguistically controlled content
(content that results from semantic value together with the provision
of values to free variables in syntax, or semantic value together with
the provision of arguments to functions from semantic types to
propositions) together with general conversational norms. Accordingly,
Stanley generalizes contextual saturation processes that are usually
regarded as characteristic of indexicals, tense, and a few other
structures; moreover, he requires that the relevant variables be
linguistically encoded, either syntactically or lexically. It remains
to be seen whether such solutions apply (in a non-ad hoc way)
to all the examples of content modulation that have been presented in
the literature.
Finally, minimalism (Borg 2004, 2012; Cappelen & Lepore
2005) is the view that appears (and intends) to be closest to the
Frege-Montague tradition. The task of a semantic theory is said to be
minimal in that it is supposed to account only for the literal meaning
of sentences: context does not affect literal semantic content but
“what the speaker says” as opposed to “what the
sentence means” (Borg 2012). In this sense, semantics is not
another name for the theory of meaning, because not all
meaning-related properties are semantic properties (Borg 2004).
Contrary to contextualism and Bach’s theory, minimalism holds
that lexicon and syntax together determine complete truth-evaluable
propositions. Indeed, this is definitional for lexical meaning: word
meanings are the kind of things which, if one puts enough of them
together in the right sort of way, then what one gets is propositional
content (Borg 2012). Borg believes that, in order to be
truth-evaluable, propositional contents must be “about the
world”, and that this entails some form of semantic externalism.
However, the identification of lexical meaning with reference makes it
hard to account for semantic relations such as synonymy, analytic
entailment or the difference between ambiguity and polysemy, and
syntactically relevant properties: the difference between “John
is easy to please” and “John is eager to please”
cannot be explained by the fact that ‘easy’ means the
property easy (see the
entry on
 ambiguity).
 To account for semantically based syntactic properties, words may
come with “instructions” that are not, however,
constitutive of a word’s meaning like meaning postulates (which
Borg rejects), though awareness of them is part of a speaker’s
competence. Once more, lexical semantic competence is divorced from
grasp of word meaning. In conclusion, some information counts as
lexical if it is either perceived as such in “firm, type-level
lexical intuitions” or capable of affecting the word’s
syntactic behavior. Borg concedes that even such an extended
conception of lexical content will not capture, e.g., analytic
entailments such as the relation between ‘bachelor’ and
‘unmarried’.
The emergence of modern linguistic theories of word meaning is usually
placed at the transition from historical-philological semantics
 (Section 2.2)
 to structuralist semantics, the linguistics movement started at the
break of the 20th century by Ferdinand de Saussure with his
Cours de Linguistique Générale (1995
[1916]).
The advances introduced by the structuralist conception of word
meaning are best appreciated by contrasting its basic assumptions with
those of historical-philological semantics. Let us recall the three
most important differences (Lepschy 1970; Matthews 2001).
The account of lexical phenomena popularized by structuralism gave
rise to a variety of descriptive approaches to word meaning. We can
group them in three categories (Lipka 1992; Murphy 2003; Geeraerts
2006).
The componential current of structuralism was the first to produce an
important innovation in theories of word meaning: Katzian semantics
(Katz & Fodor 1963; Katz 1972, 1987). Katzian semantics combined
componential analysis with a mentalistic conception of word meaning
and developed a method for the description of lexical phenomena in the
context of a formal grammar. The mentalistic component of Katzian
semantics is twofold. First, word meanings are defined as aggregates
of simpler conceptual features inherited from our general
categorization abilities. Second, the proper subject matter of the
theory is no longer identified with the “structure of the
language” but, following Chomsky (1957, 1965), with speakers’
ability to competently interpret the words and sentences of their
language. In Katzian semantics, word meanings are structured entities
whose representations are called semantic markers. A semantic
marker is a hierarchical tree with labeled nodes whose structure
reproduces the structure of the represented meaning, and whose labels
identify the word’s conceptual components. For example, the
figure below illustrates the sense of ‘chase’ (simplified
from Katz 1987).
Katz (1987) claimed that this approach was superior in both
transparency and richness to the analysis of word meaning that could
be provided via meaning postulates. For example, in Katzian semantics
the validation of conditionals such as \(\forall x\forall y
(\textrm{chase}(x, y) \to \textrm{follow}(x,y))\) could be reduced to
a matter of inspection: one had simply to check whether the semantic
marker of ‘follow’ was a subtree of the semantic marker of
‘chase’. Furthermore, the method incorporated syntagmatic
relations in the representation of word meanings (witness the
grammatical tags ‘NP’, ‘VP’ and
‘S’ attached to the conceptual components above). Katzian
semantics was favorably received by the Generative Semantics movement
(Fodor 1977; Newmeyer 1980) and boosted an interest in the formal
representation of word meaning that would dominate the linguistic
scene for decades to come (Harris 1993). Nonetheless, it was
eventually abandoned. As subsequent commentators noted, Katzian
semantics suffered from three important drawbacks. First, the theory
did not provide any clear model of how the complex conceptual
information represented by semantic markers contributed to the truth
conditions of sentences (Lewis 1972). Second, some aspects of word
meaning that could be easily represented with meaning postulates could
not be expressed through semantic markers, such as the symmetry and
the transitivity of predicates



(e.g., \(\forall x\forall y (\textrm{sibling}(x, y) \to
\textrm{sibling}(y, x))\) or \(\forall x\forall y\forall z
(\textrm{louder}(x, y) \mathbin{\&} \textrm{louder}(y, z) \to
\textrm{louder}(x, z))\); see Dowty 1979).


Third, Katz’s arguments for the view that word meanings are
intrinsically structured turned out to be vulnerable to objections
from proponents of atomistic views of word meaning (see, most notably,
Fodor & Lepore 1992).
After Katzian semantics, the landscape of linguistic theories of word
meaning bifurcated. On one side, we find a group of theories advancing
the decompositional agenda established by Katz. On the other
side, we find a group of theories fostering the relational
approach originated by Lexical Field Theory and relational semantics.
Following Geeraerts (2010), we will briefly characterize the following
ones.
The basic idea of the Natural Semantic Metalanguage approach
(henceforth, NSM; Wierzbicka 1972, 1996; Goddard & Wierzbicka
2002) is that word meaning is best described through the combination
of a small set of elementary conceptual particles, known as
semantic primes. Semantic primes are primitive (i.e., not
decomposable into further conceptual parts), innate (i.e., not
learned), and universal (i.e., explicitly lexicalized in all natural
languages, whether in the form of a word, a morpheme, a phraseme, and
so forth). According to NSM, the meaning of any word in any natural
language can be defined by appropriately combining these fundamental
conceptual particles. Wierzbicka (1996) proposed a catalogue of about
60 semantic primes, designed to analyze word meanings within so-called
reductive paraphrases. For example, the reductive paraphrase for
‘top’ is a part of
something; this part is above all the other parts of this
something. NSM has produced interesting applications in
comparative linguistics (Peeters 2006), language teaching (Goddard
& Wierzbicka 2007), and lexical typology (Goddard 2012). However,
the approach has been criticized on various grounds. First, it has
been argued that the method followed by NSM in the identification of
semantic primes is insufficiently clear (e.g., Matthewson 2003).
Second, some have observed that reductive paraphrases are too vague to
be considered adequate representations of word meanings, since they
fail to account for fine-grained differences between semantically
neighboring words. For example, the reductive paraphrase provided by
Wierzbicka for ‘sad’ (i.e., x feels
something; sometimes a person
thinks something like this: something bad happened; if i didn’t
know that it happened i would say: i don’t want it to happen; i
don’t say this now because i know: i can’t do anything;
because of this, this person feels something bad; x
feels something like
this) seems to apply equally well to ‘unhappy’,
‘distressed’, ‘frustrated’,
‘upset’, and ‘annoyed’ (e.g., Aitchison 2012).
Third, there is no consensus on what items should ultimately feature
in the list of semantic primes available to reductive paraphrases: the
content of the list is debated and varies considerably between
versions of NSM. Fourth, some purported semantic primes appear to fail
to comply with the universality requirement and are not explicitly
lexicalized in all known languages (Bohnemeyer 2003; Von Fintel &
Matthewson 2008). See Goddard (1998) for some replies and Riemer
(2006) for further objections.
For NSM, word meanings can be exhaustively represented with a
metalanguage appealing exclusively to the combination of primitive
linguistic particles. Conceptual Semantics (Jackendoff 1983, 1990,
2002) proposes a more open-ended approach. According to Conceptual
Semantics, word meanings are essentially an interface phenomenon
between a specialized body of linguistic knowledge (e.g.,
morphosyntactic knowledge) and core non-linguistic cognition. Word
meanings are thus modeled as hybrid semantic representations combining
linguistic features (e.g., syntactic tags) and conceptual elements
grounded in perceptual knowledge and motor schemas. For example, here
is the semantic representation of ‘drink’ according to
Jackendoff.
Syntactic tags represent the grammatical properties of the word under
analysis, while the items in subscript are picked from a core set of
perceptually grounded primitives (e.g., event,
state, thing, path, place, property,
amount) which are assumed to be innate, cross-modal and
universal categories of the human mind. The decompositional machinery
of Conceptual Semantics has a number of attractive features. Most
notably, its representations take into account grammatical class and
word-level syntax, which are plausibly an integral aspect of our
knowledge of the meaning of words. However, some of its claims about
the interplay between language and conceptual structure appear more
problematic. To begin with, it has been observed that speakers tend to
use causative predicates (e.g., ‘drink’) and the
paraphrases expressing their decompositional structure (e.g.,
“cause a liquid to go into someone or something’s
mouth”) in different and sometimes non-interchangeable ways
(e.g., Wolff 2003), which raises concerns about the hypothesis that
decompositional analyses à la Jackendoff may be regarded as faithful
representations of word meanings. In addition, Conceptual Semantics is
somewhat unclear as to what exact method should be followed in the
identification of the motor-perceptual primitives that can feed
descriptions of word meanings (Pulman 2005). Finally, the restriction
placed by Conceptual Semantics on the type of conceptual material that
can inform definitions of word meaning (low-level primitives grounded
in perceptual knowledge and motor schemas) appears to affect the
explanatory power of the framework. For example, how can one account
for the difference in meaning between ‘jog’ and
‘run’ without ut taking into account higher-level,
arguably non-perceptual knowledge about the social characteristics of
jogging, which typically implies a certain leisure setting, the
intention to contribute to physical wellbeing, and so on? See Taylor
(1996), Deane (1996).
The neat dividing line drawn between word meanings and general world
knowledge by Conceptual Semantics does not tell us much about the
dynamic interaction of the two in language use. The Two-Level
Semantics of Bierwisch (1983a,b) and Lang (Bierwisch & Lang 1989;
Lang 1993) aims to provide such a dynamic account. Two-Level Semantics
views word meaning as the result of the interaction between two
systems: semantic form (SF) and conceptual structure
(CS). SF is a formalized representation of the basic features of a
word. It contains grammatical information that specifies, e.g., the
admissible syntactic distribution of the word, plus a set of variables
and semantic parameters whose value is determined by the interaction
with CS. By contrast, CS consists of language-independent systems of
knowledge (including general world knowledge) that mediate between
language and the world (Lang & Maienborn 2011). According to
Two-Level Semantics, for example, polysemous words can express
variable meanings by virtue of having a stable underspecified SF which
can be flexibly manipulated by CS. By way of example, consider the
word ‘university’, which can be read as referring either
to an institution (as in “the university selected John’s
application”) or to a building (as in “the university is
located on the North side of the river”). Simplifying a bit,
Two-Level Semantics explains the dynamics governing the selection of
these readings as follows.
Two-Level Semantics shares Jackendoff’s and Wierzbicka’s
commitment to a descriptive paradigm that anchors word meaning to a
stable decompositional template, all the while avoiding the immediate
complications arising from a restrictive characterization of the type
of conceptual factors that can modulate such stable decompositional
templates in contexts. But there are, once again, a few significant
issues. A first problem is definitional accuracy: defining the SF of
‘university’ as \(\lambda x [\textrm{purpose} [x w]
\mathbin{\&} \textit{advanced study and teaching} [w]]\) seems too
loose to reflect the subtle differences in meaning among
‘university’ and related terms designating institutions
for higher education, such as ‘college’ or
‘academy’. Furthermore, the apparatus of Two-Level
Semantics relies heavily on lambda expressions, which, as some
commentators have noted (e.g., Taylor 1994, 1995), appears ill-suited
to represent the complex forms of world knowledge we often rely on to
fix the meaning of highly polysemous words. See also Wunderlich (1991,
1993).
The Generative Lexicon theory (GL; Pustejovsky 1995) takes a different
approach. Instead of explaining the contextual flexibility of word
meaning by appealing to rich conceptual operations applied on
semantically thin lexical entries, this approach postulates lexical
entries rich in conceptual information and knowledge of worldly facts.
According to classical GL, the informational resources encoded in the
lexical entry for a typical word w consist of the following
four levels.
In particular, qualia structure specifies the conceptual relations
that speakers associate to the real-world referents of a word and
impact on the way the word is used in the language (Pustejovsky 1998).
For example, our knowledge that bread is something that is brought
about through baking is considered a Quale of the word
‘bread’, and this knowledge is responsible for our
understanding that, e.g., “fresh bread” means “bread
which has been baked recently”. GL distinguishes four types of
qualia:
Take together, these qualia form the “qualia structure” of
a word. For example, the qualia structure of the noun
‘sandwich’ will feature information about the composition
of sandwiches, their nature of physical artifacts, their being
intended to be eaten, and our knowledge about the operations typically
involved in the preparation of sandwiches. The notation is as
follows.
sandwich(x)

const = {bread, …}

form = physobj(x)

tel = eat(P, g, x)

agent = artifact(x)
Qualia structure is the primary explanatory device by which GL
accounts for polysemy. The sentence “Mary finished the
sandwich” receives the default interpretation “Mary
finished eating the sandwich” because the argument
structure of ‘finish’ requires an action as direct object,
and the qualia structure of ‘sandwich’ allows the
generation of the appropriate sense via type coercion (Pustejovsky
2006). GL is an ongoing research program (Pustejovsky et al. 2012)
that has led to significant applications in computational linguistics
(e.g., Pustejovsky & Jezek 2008; Pustejovsky & Rumshisky
2008). But like the theories mentioned so far, it has been subject to
criticisms. A first general criticism is that the decompositional
assumptions underlying GL are unwarranted and should be replaced by an
atomist view of word meaning (Fodor & Lepore 1998; see Pustejovsky
1998 for a reply). A second criticism is that GL’s focus on
variations in word meaning which depend on sentential context and
qualia structure is too narrow, since since contextual variations in
word meaning often depend on more complex factors, such as the ability
to keep track of coherence relations in a discourse (e.g., Asher &
Lascarides 1995; Lascarides & Copestake 1998; Kehler 2002; Asher
2011). Finally, the empirical adequacy of the framework has been
called into question. It has been argued that the formal apparatus of
GL leads to incorrect predictions, that qualia structure sometimes
overgenerates or undergenerates interpretations, and that the rich
lexical entries postulated by GL are psychologically implausible
(e.g., Jayez 2001; Blutner 2002).
To conclude this section, we will briefly mention some contemporary
approaches to word meaning that, in different ways, pursue the
theoretical agenda of the relational current of the structuralist
paradigm. For pedagogical convenience, we can group them into two
categories. On the one hand, we have network approaches,
which formalize knowledge of word meaning within models where the
lexicon is seen as a structured system of entries interconnected by
sense relations such as synonymy, antonymy, and meronymy. On the
other, we have statistical approaches, whose primary aim is
to investigate the patterns of co-occurrence among words in linguistic
corpora.
The main example of network approaches is perhaps Collins and
Quillian’s (1969) hierarchical network model, in which words are
represented as entries in a network of nodes, each comprising a set of
conceptual features defining the conventional meaning of the word in
question, and connected to other nodes in the network through semantic
relations (more in Lehman 1992). Subsequent developments of the
hierarchical network model include the Semantic Feature Model (Smith,
Shoben & Rips 1974), the Spreading Activation Model (Collins &
Loftus 1975; Bock & Levelt 1994), the WordNet database (Fellbaum
1998), as well as the connectionist models of Seidenberg &
McClelland (1989), Hinton & Shallice (1991), and Plaut &
Shallice (1993). More on this in the entry on
 connectionism.
Finally, statistical analysis investigates word meaning by examining
through computational means the distribution of words in linguistic
corpora. The main idea is to use quantitative data about the frequency
of co-occurrence of sets of lexical items to identify their semantic
properties and differentiate their different senses (for overviews,
see Atkins & Zampolli 1994; Manning & Schütze 1999;
Stubbs 2002; Sinclair 2004). Notice that while symbolic networks are
models of the architecture of the lexicon that seek to be
psychologically adequate (i.e., to reveal how knowledge of word
meaning is stored and organized in the mind/brain of human speakers),
statistical approaches to word meaning are not necessarily interested
in psychological adequacy, and may have completely different goals,
such as building a machine translation service able to mimic human
performance (a goal that can obviously be achieved without reproducing
the cognitive mechanisms underlying translation in humans). More on
this in the entry on
 computational linguistics.
As we have seen, most theories of word meaning in linguistics face, at
some point, the difficulties involved in drawing a plausible dividing
line between word knowledge and world knowledge, and the various ways
they attempt to meet this challenge display some recurrent features.
For example, they assume that the lexicon, though richly interfaced
with world knowledge and non-linguistic cognition, remains an
autonomous representational system encoding a specialized body of
linguistic knowledge. In this section, we survey a group of empirical
approaches that adopt a different stance on word meaning. The focus is
once again psychological, which means that the overall goal of these
approaches is to provide a cognitively realistic account of the
representational repertoire underlying knowledge of word meaning.
Unlike the approaches surveyed in
 Section 4,
 however, these theories tend to encourage a view on which the
distinction between the semantic and pragmatic aspects of word meaning
is highly unstable (or even impossible to draw), where lexical
knowledge and knowledge of worldly facts are aspects of a continuum,
and where the lexicon is permeated by our general inferential
abilities (Evans 2010).
 Section 5.1
 will briefly illustrate the central assumptions underlying the study
of word meaning in cognitive linguistics.
 Section 5.2
 will turn to the study of word meaning in psycholinguistics.
 Section 5.3
 will conclude with some references to neurolinguistics.
At the beginning of the 1970s, Eleanor Rosch put forth a new theory of
the mental representation of categories. Concepts such as furniture
or bird,
she claimed, are not
represented just as sets of criterial features with clear-cut
boundaries, so that an item can be conceived as falling or not falling
under the concept based on whether or not it meets the relevant
criteria. Rather, items within categories can be considered more or
less representative of the category itself (Rosch 1975; Rosch &
Mervis 1975; Mervis & Rosch 1981). Several experiments seemed to
show that the application of concepts is no simple yes-or-no business:
some items (the “good examples”) are more easily
identified as falling under a concept than others (the “poor
examples”). An automobile is perceived as a better example of
vehicle than a rowboat,
and much better than an elevator; a carrot is more readily identified
as an example of the concept vegetable
than a pumpkin. If the concepts speakers
associate to category words (such as ‘vehicle’ and
‘vegetable’) were mere bundles of criterial features,
these preferences would be inexplicable, since they rank items that
meet the criteria equally well. It is thus plausible to assume that
the concepts associated to category words are have a center-periphery
architecture centered on the most representative examples of the
category: a robin is perceived as a more “birdish” bird
than an ostrich or, as people would say, closer to the
prototype of a bird or to the prototypical bird (see
the entry on
 concepts).
Although nothing in Rosch’s experiments licensed the conclusion
that prototypical rankings should be reified and treated as the
content of concepts (what her experiments did support was merely that
a theory of the mental representation of categories should be
consistent with the existence of prototype effects), the
study of prototypes revolutionized the existing approaches to category
concepts (Murphy 2002) and was a leading force behind the birth of
cognitive linguistics. Prototypes were central to the development of
the Radial Network Theory of Brugman (1988 [1981]) and Lakoff (Brugman
& Lakoff 1988), which proposed to model the sense network of words
by introducing in the architecture of word meanings the
center-periphery relation at the heart of Rosch’s seminal work.
According to Brugman, word meanings can typically be modeled as radial
complexes where a dominant sense is related to less typical senses by
means of semantic relations such as metaphor and metonymy. For
example, the sense network of ‘fruit’ features product
of plant growth at
its center and a more abstract outcome
at its periphery, and the two are
connected by a metaphorical relation). On a similar note, the
Conceptual Metaphor Theory of Lakoff & Johnson (1980; Lakoff 1987)
and the Mental Spaces Approach of Fauconnier (1994; Fauconnier &
Turner 1998) combined the assumption that word meanings typically have
an internal structure arranging multiple related senses in a radial
fashion, with the further claim that our use of words is governed by
hard-wired mapping mechanisms that catalyze the integration of word
meanings across conceptual domains. For example, it is in virtue of
these mechanisms that the expressions “love is war”,
“life is a journey”) are so widespread across cultures and
sound so natural to our ears. On the proposed view, these associations
are creative, perceptually grounded, systematic, cross-culturally
uniform, and grounded on pre-linguistic patterns of conceptual
activity which correlate with core elements of human embodied
experience (see the entries on
 metaphor
 and
 embodied cognition).
 More in Kövecses (2002), Gibbs (2008), and Dancygier &
Sweetser (2014).
Another major innovation introduced by cognitive linguistics is the
development of a resolutely “encyclopedic” approach to
word meaning, best exemplified by Frame Semantics (Fillmore 1975,
1982) and by the Theory of Domains (Langacker 1987). Approximating a
bit, an approach to word meaning can be defined
“encyclopedic” insofar as it characterizes knowledge of
worldly facts as the primary constitutive force of word meaning. While
the Mental Spaces Approach and Conceptual Metaphor Theory regarded
word meaning mainly as the product of associative patterns between
concepts, Fillmore and Langacker turned their attention to the
relation between word meaning and the body of encyclopedic knowledge
possessed by typical speakers. Our ability to use and interpret the
verb ‘buy’, for example, is closely intertwined with our
background knowledge of the social nature of commercial transfer,
which involves a seller, a buyer, goods, money, the relation between
the money and the goods, and so forth. However, knowledge structures
of this kind cannot be modeled as standard concept-like
representations. Here is how Frame Semantics attempts to meet the
challenge. First, words are construed as pairs of phonographic forms
with highly schematic concepts which are internally organized as
radial categories and function as access sites to encyclopedic
knowledge. Second, an account of the representational organization of
encyclopedic knowledge is provided. According to Fillmore,
encyclopedic knowledge is represented in long-term memory in the form
of frames, i.e., schematic conceptual scenarios that specify
the prototypical features and functions of a denotatum, along with its
interactions with the objects and the events typically associated with
it. Frames provide thus a schematic representation of the elements and
entities associated with a particular domain of experience and convey
the information required to use and interpret the words employed to
talk about it. For example, according to Fillmore & Atkins (1992)
the use of the verb ‘bet’ is governed by the risk
frame, which is as
follows:
In the same vein as Frame Semantics (more on the parallels in Clausner
& Croft 1999), Langacker’s Theory of Domains argues that our
understanding of word meaning depends on our access to larger
knowledge structures called domains. To illustrate the notion
of a domain, consider the word ‘diameter’. The meaning of
this word cannot be grasped independently of a prior understanding of
the notion of a circle. According to Langacker, word meaning is
precisely a matter of “profile-domain” organization: the
profile corresponds to a substructural element designated within a
relevant macrostructure, whereas the domain corresponds to the
macrostructure providing the background information against which the
profile can be interpreted (Taylor 2002). In the diameter/circle
example, ‘diameter’ designates a profile in the circle
domain. Similarly,
expressions like ‘hot’, ‘cold’, and
‘warm’ designate properties in the temperature
domain. Langacker
argues that domains are typically structured into hierarchies that
reflect meronymic relations and provide a basic conceptual ontology
for language use. For example, the meaning of ‘elbow’ is
understood with respect to the arm
domain, while the meaning of ‘arm’
is situated within the body
domain. Importantly, individual profiles
typically inhere to different domains, and this is one of the factors
responsible for the ubiquity of polysemy in natural language. For
example, the profile associated to the word ‘love’ inheres
both to the domains of embodied experience and to the abstract domains
of social activities such as marriage ceremonies.
Developments of the approach to word meaning fostered by cognitive
linguistics include Construction Grammar (Goldberg 1995), Embodied
Construction Grammar (Bergen & Chang 2005), Invited Inferencing
Theory (Traugott & Dasher 2001), and LCCM Theory (Evans 2009). The
notion of a frame has become popular in cognitive psychology to model
the dynamics of ad hoc categorization (e.g., Barsalou 1983,
1992, 1999; more in
 Section 5.2).
 General information about the study of word meaning in cognitive
linguistics can be found in Talmy (2000a,b), Croft & Cruse (2004),
and Evans & Green (2006).
In psycholinguistics, the study of word meaning is understood as the
investigation of the mental lexicon, the cognitive system
that underlies the capacity for conscious and unconscious lexical
activity (Jarema & Libben 2007). Simply put, the mental lexicon is
the long-term representational inventory storing the body of
linguistic knowledge speakers are required to master in order to make
competent use of the lexical elements of a language; as such, it can
be equated with the lexical component of an individual’s
language capacity. Research on the mental lexicon is concerned with a
variety of problems (for surveys, see, e.g., Traxler & Gernsbacher
2006, Spivey, McRae & Joanisse 2012, Harley 2014), that center
around the following tasks:
From a functional point of view, the mental lexicon is usually
understood as a system of lexical entries, each containing
the information related to a word mastered by a speaker (Rapp 2001). A
lexical entry for a word w is typically modeled as a complex
representation made up of the following components (Levelt 1989,
2001):
From this standpoint, a theory of word meaning translates into an
account of the information stored in the semantic form of lexical
entries. A crucial part of the task consists in determining exactly
what kind of information is stored in lexical semantic forms as
opposed to, e.g., bits of information that fall under the scope of
episodic memory or general factual knowledge. Recall the example we
made in
 Section 3.3:
 how much of the information that a competent zoologist can associate
to tigers is part of her knowledge of the meaning of the word
‘tiger’? Not surprisingly, even in psycholinguistics
tracing a neat functional separation between word processing and
general-purpose cognition has proven a problematic task. The general
consensus among psycholinguists seems to be that lexical
representations and conceptual representations are richly interfaced,
though functionally distinct (e.g., Gleitman & Papafragou 2013).
For example, in clinical research it is standard practice to
distinguish between amodal deficits involving an inability to
process information at both the conceptual and the lexical level, and
modal deficits specifically restricted to one of the two
spheres (Saffran & Schwartz 1994; Rapp & Goldrick 2006;
Jefferies & Lambon Ralph 2006; more in more in
 Section 5.3).
 On the resulting view, lexical activity in humans is the output of
the interaction between two functionally neighboring systems, one
broadly in charge of the storage and processing of
conceptual-encyclopedic knowledge, the other coinciding with the
mental lexicon. The role of lexical entries is essentially to make
these two systems communicate with one another through semantic forms
(see Denes 2009). Contrary to the folk notion of a mental lexicon
where words are associated to fully specified meanings or senses which
are simply retrieved from the lexicon for the purpose of language
processing, in these models lexical semantic forms are seen as highly
schematic representations whose primary function is to supervise the
recruitment of the extra-linguistic information required to interpret
word occurrences in language use. In recent years, appeals to
“ultra-thin” lexical entries have taken an eliminativist
turn. It has been suggested that psycholinguistic accounts of the
representational underpinnigs of lexical competence should dispose of
the largely metaphorical notion of an “internal word
store”, and there is no such thing as a mental lexicon in the
human mind (e.g., Elman 2004, 2009; Dilkina, McClelland & Plaut
2010).
In addition to these approaches, in a number of prominent
psychological accounts emerged over the last two decades, the study of
word meaning is essentially considered a chapter of theories of the
mental realization of concepts (see the entry on
 concepts).
 Lexical units are seen either as ingredients of conceptual networks
or as (auditory or visual) stimuli providing access to conceptual
networks. A flow of neuroscientific results has shown that
understanding of (certain categories of) words correlates with neural
activations corresponding to the semantic content of the processed
words. For example, it has been shown that listening to sentences that
describe actions performed with the mouth, hand, or leg activates the
visuomotor circuits which subserve execution and observation of such
actions (Tettamanti et al. 2005); that reading words denoting specific
actions of the tongue (‘lick’), fingers
(‘pick’), and leg (‘kick’) differentially
activate areas of the premotor cortex that are active when the
corresponding movements are actually performed (Hauk et al. 2004);
that reading odor-related words (‘jasmine’,
‘garlic’, ‘cinnamon’) differentially activates
the primary olfactory cortex (Gonzales et al. 2006); and that color
words (such as ‘red’) activate areas in the fusiform gyrus
that have been associated with color perception (Chao et al. 1999,
Simmons et al. 2007; for a survey of results on visual activations in
language processing, see Martin 2007).
This body of research originated so-called simulationist (or
enactivist) accounts of conceptual competence, on which
“understanding is imagination” and “imagining is a
form of simulation” (Gallese & Lakoff 2005). In these
accounts, conceptual (often called “semantic”) competence
is seen as the ability to simulate or re-enact perceptual (including
proprioceptive and introspective) experiences of the states of affairs
that language describes, by manipulating memory traces of such
experiences or fragments of them. In Barsalou’s theory of
perceptual symbol systems (1999), language understanding (and
cognition in general) is based on perceptual experience and memory of
it. The central claim is that “sensory-motor systems represent
not only perceived entities but also conceptualizations of them in
their absence”. Perception generates mostly unconscious
“neural representations in sensory-motor areas of the
brain”, which represent schematic components of perceptual
experience. Such perceptual symbols are not holistic copies of
experiences but selections of information isolated by attention.
Related perceptual symbols are integrated into a simulator
that produces limitless simulations of a perceptual component, such as
red or lift. Simulators are located in long-term
memory and play the roles traditionally attributed to concepts: they
generate inferences and can be combined recursively to implement
productivity. A concept is not “a static amodal structure”
as in traditional, computationally-oriented cognitive science, but
“the ability to simulate a kind of thing perceptually”.
Linguistic symbols (i.e., auditory or visual memories of words) get to
be associated with simulators; perceptual recognition of a word
activates the relevant simulator, which simulates a referent for the
word; syntax provides instructions for building integrated perceptual
simulations, which “constitute semantic
interpretations”.
Though popular among researchers interested in the conceptual
underpinnings of semantic competence, the simulationist paradigm faces
important challenges. Three are worth mentioning. First, it appears
that imulations do not always capture the intuitive truth conditions
of sentences: listeners may enact the same simulation upon exposure to
sentences that have different truth conditions (e.g., “The man
stood on the corner” vs. “The man waited on the
corner”; see Weiskopf 2010). Moreover, simulations may
overconstrain truth conditions. For example, even though in the
simulations listeners typically associate to the sentence “There
are three pencils and four pens in Anna’s mug”, the pens
and the pencils are in vertical position, the sentence would be true
even if they were lying horizontally in the mug. Second, the framework
does not sit well with pathological data. For example, no general
impairment with auditory-related words is reported in patients with
lesions in the auditory association cortex (e.g., auditory agnosia
patients); analogously, patients with damage to the motor cortex seem
to have no difficulties in linguistic performance, and specifically in
inferential processing with motor-related words (for a survey of these
results, see Calzavarini, to appear; for a defense of the embodied
paradigm, Pulvermüller 2013). Finally, the theory has
difficulties accounting for the meaning of abstract words (e.g.,
‘beauty’, ‘pride’, ‘kindness’),
which does not appear to hinge on sensory-motor simulation (see Dove
2016 for a discussion).
Beginning in the mid-1970s, neuropsychological research on cognitive
deficits related to brain lesions has produced a considerable amount
of findings related to the neural correlates of lexical semantic
information and processing. More recently, the development of
neuroimaging techniques such as PET, fMRI and ERP has provided further
means to adjudicate hypotheses about lexical semantic processes in the
brain (Vigneau et al. 2006). Here we do not intend to provide a
complete overview of such results (for a survey, see Faust 2012). We
shall just mention three topics of neurolinguistic research that
appear to bear on issues in the study of word meaning: the partition
of the lexicon into categories, the representation of common nouns vs.
proper names, and the distinction between the inferential and the
referential aspects of lexical competence.
Two preliminary considerations should be kept in mind. First, a
distinction must be drawn between the neural realization of word
forms, i.e., traces of acoustic, articulatory, graphic, and motor
configurations (‘peripheral lexicons’), and the neural
correlates of lexical meanings (‘concepts’). A patient can
understand what is the object represented by a picture shown to her
(and give evidence of her understanding, e.g., by miming the
object’s function) while being unable to retrieve the relevant
phonological form from her output lexicon (Warrington 1985; Shallice
1988). Second, there appears to be wide consensus about the
irrelevance to brain processing of any distinction between strictly
semantic and factual or encyclopedic information (e.g., Tulving 1972;
Sartori et al. 1994). Whatever information is relevant to such
processes as object recognition or confrontation naming is standardly
characterized as ‘semantic’. This may be taken as a
stipulation—it is just how neuroscientists use the word
‘semantic’—or as deriving from lack of evidence for
any segregation between the domains of semantic and encyclopedic
information (see Binder et al. 2009). Be that as it may, in
present-day neuroscience there seems to be no room for a correlate of
the analytic/synthetic distinction. Moreover, in the literature
‘semantic’ and ‘conceptual’ are often used
synonymously; hence, no distinction is drawn between lexical semantic
and conceptual knowledge. Finally, the focus of neuroscientific
research on “semantics” is on information structures
roughly corresponding to word-level meanings, not to sentence-level
meanings: hence, so far neuroscientific research has had little to say
about the compositional mechanisms that have been the focus (and,
often, the entire content) of theories of meaning as pursued within
formal semantics and philosophy of language.
Let us start with the partition of the semantic lexicon into
categories. Neuropsychological research indicates that the ability to
name objects or to answer simple questions involving such nouns can be
selectively lost or preserved: subjects can perform much better in
naming living entities than in naming artifacts, or in naming animate
living entities than in naming fruits and vegetables (Shallice 1988).
Different patterns of brain activation may correspond to such
dissociations between performances: e.g., Damasio et al. (1996) found
that retrieval of names of animals and of tools activate different
regions in the left temporal lobe. However, the details of this
partition have been interpreted in different ways. Warrington &
McCarthy (1983) and Warrington & Shallice (1984) explained the
living vs. artifactual dissociation by taking the category distinction
to be an effect of the difference among features that are crucial in
the identification of living entities and artifacts: while living
entities are identified mainly on the basis of perceptual features,
artifacts are identified by their function. A later theory (Caramazza
& Shelton 1998) claimed that animate and inanimate objects are
treated by different knowledge systems separated by evolutionary
pressure: domains of features pertaining to the recognition of living
things, human faces, and perhaps tools may have been singled out as
recognition of such entities had survival value for humans. Finally,
Devlin et al. (1998) proposed to view the partition as the consequence
of a difference in how recognition-relevant features are connected
with one another: in the case of artifactual kinds, an object is
recognized thanks to a characteristic coupling of form and function,
whereas no such coupling individuates kinds of living things (e.g.,
eyes go with seeing in many animal species). For non-neutral surveys,
see Caramazza & Mahon (2006) and Shallice & Cooper (2011).
On the other hand, it is also known that “semantic” (i.e.,
conceptual) competence may be lost in its entirety (though often
gradually). This is what typically happens in semantic dementia.
Empirical evidence has motivated theories of the neural realization of
conceptual competence that are meant to account for both
modality-specific deficits and pathologies that involve impairment
across all modalities. The former may involve a difficulty or
impossibility to categorize a visually exhibited object which,
however, can be correctly categorized in other modalities (e.g., if
the object is touched) or verbally described on the basis of the
object’s name (i.e., on the basis of the lexical item supposedly
associated with the category). The original “hub and
spokes” model of the brain representation of concepts (Rogers et
al. 2004, Patterson et al. 2007) accounted for both sets of findings
by postulating that the semantic network is composed of a series of
“spokes”, i.e., cortical areas distributed across the
brain processing modality-specific (visual, auditory, motor, as well
as verbal) sources of information, and that the spokes are two-ways
connected to a transmodal “hub”. While damage to the
spokes accounts for modality-specific deficits, damage to the hub and
its connections explains the overall impairment of semantic
competence. On this model, the hub is supposed to be located in the
anterior temporal lobe (ATL), since semantic dementia had been found
to be associated with degeneration of the anterior ventral and polar
regions of both temporal poles (Guo et al. 2013). According to more
recent, “graded” versions of the model (Lambon Ralph et
al. 2017), the contribution of the hub units may vary depending on
different patterns of connectivity to the spokes, to account for
evidence of graded variation of function across subregions of ATL. It
should be noted that while many researchers converge on a distributed
view of semantic representation and on the role of domain-specific
parts of the neural network (depending on differential patterns of
functional connectivity), not everybody agrees on the need to
postulate a transmodal hub (see, e.g., Mahon & Caramazza
2011).
Let us now turn to common nouns and proper names. As we have seen, in
the philosophy of language of the last decades, proper names (of
people, landmarks, countries, etc.) have being regarded as
semantically different from common nouns. Neuroscientific research on
the processing of proper names and common nouns concurs, to some
extent. To begin with, the retrieval of proper names is doubly
dissociated from the retrieval of common nouns. Some patients proved
competent with common nouns but unable to associate names to pictures
of famous people, or buildings, or brands (Ellis, Young &
Critchley 1989); in other cases, people’s names were
specifically affected (McKenna & Warrington 1980). Other patients
had the complementary deficit. The patient described in Semenza &
Sgaramella (1993) could name no objects at all (with or without
phonemic cues) but he was able to name 10 out of 10 familiar people,
and 18 out of 22 famous people with a phonemic cue. Martins &
Farrayota‘s (2007) patient ACB also presented impaired object
naming but spared retrieval of proper names. Such findings suggest
distinct neural pathways for the retrieval of proper names and common
nouns (Semenza 2006). The study of lesions and neuroimaging research
both initially converged in identifying the left temporal pole as
playing a crucial role in the retrieval of proper names, from both
visual stimuli (Damasio et al. 1996) and the presentation of speaker
voices (Waldron et al. 2014) (though in at least one case damage to
the left temporal pole was associated with selective sparing of proper
names; see Martins & Farrajota 2007). In addition, recent research
has found a role for the uncinate fasciculus (UF). In patients
undergoing surgical removal of UF, retrieval of common nouns was
recovered while retrieval of proper names remained impaired (Papagno
et al. 2016). The present consensus appears to be that “the
production of proper names recruits a network that involves at least
the left anterior temporal lobe and the left orbitofrontal cortex
connected together by the UF” (Brédart 2017).
Furthermore, a few neuropsychological studies have described patients
whose competence on geographical names was preserved while names of
people were lost: one patient had preserved country names, though he
had lost virtually every other linguistic ability (McKenna &
Warrington 1978; see Semenza 2006 for other cases of selective
preservation of geographical names). Other behavioral experiments seem
to show that country names are closer to common nouns than to other
proper names such as people and landmark names in that the
connectivity between the word and the conceptual system is likely to
require diffuse multiple connections, as with common nouns (Hollis
& Valentine 2001). If these results were confirmed, it would turn
out that the linguistic category of proper names is not homogeneous in
terms of neural processing. Studies have also demonstrated that the
retrieval of proper names from memory is typically a more difficult
cognitive task than the retrieval of common nouns. For example, it is
harder to name faces (of famous people) than to name objects;
moreover, it is easier to remember a person’s occupation than
her or his name. Interestingly, the same difference does not
materialize in definition naming, i.e., in tasks where names and
common nouns are to be retrieved from definitions (Hanley 2011).
Though several hypotheses about the source of this difference have
been proposed (see Brédart 2017 for a survey), no consensus has
been reached on how to explain this phenomenon.
Finally, a few words on the distinction between the inferential and
the referential component of lexical competence. As we have seen in
 Section 3.2,
 Marconi (1997) suggested that processing of lexical meaning might be
distributed between two subsystems, an inferential and a referential
one. Beginning with Warrington (1975), many patients had been
described that were more or less severely impaired in referential
tasks such as naming from vision (and other perceptual modalities as
well), while their inferential competence was more or less intact. The
complementary pattern (i.e., the preservation of referential abilities
with loss of inferential competence) is definitely less common. Still,
a number of cases have been reported, beginning with a stroke patient
of Heilman et al. (1976), who, while unable to perform any task
requiring inferential processing, performed well in referential naming
tasks with visually presented objects (he could name 23 of 25 common
objects). In subsequent years, further cases were described. For
example, in a study of 61 patients with lesions affecting linguistic
abilities, Kemmerer et al. (2012) found 14 cases in which referential
abilities were better preserved than inferential abilities. More
recently, Pandey & Heilman (2014), while describing one more case
of preserved (referential) naming from vision with severely impaired
(inferential) naming from definition, hypothesized that “these
two naming tasks may, at least in part, be mediated by two independent
neuronal networks”. Thus, while double dissociation between
inferential processes and naming from vision is well attested, it is
not equally clear that it involves referential processes in general.
On the other hand, evidence from neuroimaging is, so far, limited and
overall inconclusive. Some neuroimaging studies (e.g.,
Tomaszewski-Farias et al. 2005, Marconi et al. 2013), as well as TMS
mapping experiments (Hamberger et al. 2001, Hamberger & Seidel
2009) did find different patterns of activation for inferential vs.
referential performances. However, the results are not entirely
consistent and are liable to different interpretations. For example,
the selective activation of the anterior left temporal lobe in
inferential performances may well reflect additional syntactic demands
involved in definition naming, rather than be due to inferential
processing as such (see Calzavarini 2017 for a discussion).