Genetics is devoted to the study and manipulation of heredity and
variation in living organisms. Genetics is so pervasive in
twenty-first century science—in reproductive screening
technologies like preimplantation genetic diagnosis, in assessments of
what species are endangered, in public health programs that track
antibiotic-resistant bacteria, to name a few—that it is easy to
forget what these disparate practices all have in common with one
another: a focus on the patterns and mechanisms of trait transmission
from one generation to the next in order to understand and potentially
control that process. This contemporary focus can be traced back to
the first years of the twentieth century, when genetics took shape as
a unique field of study. 
Gregor Mendel, the Austrian monk now commonly referred to as the
“father of genetics”, never uttered the terms
“gene” or “genetics” because those terms
weren’t introduced until decades after his death. In 1865, he
reported on the results of breeding experiments he performed
hybridizing pea plants during which he tracked how a series of traits
(e.g., round vs. wrinkled peas, white vs. purple flowers) passed down
through generations. Mendel noted certain patterns of inheritance; for
example, traits seemed to be transmitted independently of one another
(Mendel 1866). Mendel died in 1884, at which point there was little
indication that he would end up in biology textbooks (Olby 1985). 
It wasn’t until 1900 that the full force of Mendel’s
observations became absorbed by the scientific community. That year,
three different European botanists reported on the results of their
own breeding experiments and linked their results back to
Mendel’s work decades earlier. Mendel’s research, at this
time, was characterized as revealing heredity to involve the
transmission of discrete hereditary factors that obeyed fundamental
principles—that organisms get one copy of each factor from each
parent and, in turn, pass one copy on to their own offspring (the law
of segregation), that the factors segregate independently of one
another (the law of independent assortment), and that certain factors
dominate other factors when it comes to expressing the trait
associated with that factor (the law of dominance) (entry:
 gene).
 William Bateson, a biologist at the University of Cambridge at the
time, was excited about the implications of Mendel’s work for
theories of evolution; he recruited a number of young
scientists—women in particular—to Cambridge to perform
experiments showing that Mendel’s principles extended across the
plant and animal kingdoms (Richmond 2001). He called this new
discipline “genetics”, and “gene” became the
term for the factor that was inherited (entry:
 the genotype/phenotype distinction).
With genetics carved out as a unique field of study, two threads of
genetic research ensued. One thread focused on identifying the
physical unit of heredity that was passed from generation to
generation—to figure out what genes were, where they were
located, how they operated, and how that operation produced the
hereditary patterns consistent with Mendel’s principles (entry:
 epigenesis and preformationism).
 Thomas Hunt Morgan’s research at Columbia University most
contributed to this work. Morgan studied fruit flies, in part because
they bred quickly and were easy to maintain. With a team of young
scientists, he crossed thousands upon thousands of fruit flies,
tracking how traits like eye color and wing shape transmitted across
generations. They also induced mutations using a variety of chemical
and radiation interventions (see the entry on
 experiment in biology).
 This research proved that genes were located on chromosomes, and it
showed how the physics of chromosomal action during meiosis impacted
the hereditary process; for example, genes closer together on
chromosomes were more often inherited together, while genes that were
further apart more often separated from one another during chromosomal
recombination. This realization allowed Morgan and his students to
produce the first gene maps—charts of genes’ relative
locations to one another—and also determine that certain traits
were sex-linked due to their location on sex chromosomes (Darden 1991;
Kohler 1994). 
The other thread of genetic research that emerged attended to the
evolutionary implications of Mendelian inheritance. Charles Darwin
published On the Origin of Species in 1859, and the fact of
evolution was widely recognized by 1900; however, the nature of the
evolutionary process remained contested (Darwin 1859). According to
Darwin, evolution was a very slow and gradual process, with natural
selection favoring subtle differences between organisms (slightly
longer legs, for example) that inclined one to be more successful at
reproduction in some particular environment (entry:
 Darwinism).
 Mendel’s principles of inheritance, when they were hailed at
the turn of the twentieth century, were thought by many geneticists to
be incompatible with Darwinian evolution because Mendelian inheritance
seemed more discrete (e.g., round or wrinkled peas, purple
or white flowers), and so it was favored by scientists who
advocated for a faster and more discontinuous process of evolution.
Indeed, one of the reasons that Bateson so eagerly championed
Mendel’s work was because he was a proponent of this less
gradual interpretation (entries:
 evolution;
 fitness).
The apparent incompatibility between Darwinian evolution and Mendelian
inheritance persisted until 1918 when British biologist R.A. Fisher
first indicated how the two sciences could be related; if traits were
assumed to be the product of many Mendelian factors, then Darwinian
natural selection could favor subtle variations in the traits, and the
populations undergoing that selection pressure would gradually evolve
all while the organisms in that population obeyed the Mendelian
principles of inheritance (Fisher 1918). Fisher’s contribution
was the first in a series of works—most notably by he, J.B.S.
Haldane, and Sewall Wright—to reconceive of evolution as changes
in population gene frequencies. The union of Mendel and Darwin came to
be called “the modern synthesis”, and the mathematical
models that Fisher, Haldane, and Wright developed launched the field
of population genetics (Provine 1971; entry:
 population genetics).
 Evolutionary biology, which had long been made up of qualitative
arguments about, for example, the similarity between selection on
domesticated crops/livestock and selection in nature, suddenly became
concerned with mathematical arguments about the quantifiable
influences of migration, genetic drift, mutation, and natural
selection on populations (entries:
 ecological genetics;
 heritability;
 genetic drift).
 
Francis Galton, the younger cousin of Darwin, coined the term
“eugenics” in 1883, meaning “good birth”
(Galton 1883). In nature, Darwin’s theory of evolution by
natural selection suggested that the fit members of a population would
outbreed the unfit members; Galton and other eugenicists, however,
worried that this natural process was not playing out in human
populations—that, in humans, the unfit were outbreeding the fit.
A variety of social, economic, and political forces, the eugenicists
warned, inclined people with undesirable traits (like criminality and
“feeblemindedness”—a catchall term for intellectual
disability) to have more children, while people with desirable traits
(like high intelligence and creativity) were incentivized to have
fewer children (Paul 1995).
When genetics came on the scientific scene in 1900, many eugenicists
embraced the new theory of inheritance, believing that it offered a
scientific foundation for their social vision. The eugenicists assumed
that human traits like criminality and intelligence played by the same
hereditary rules as the traits that Mendel and Morgan made famous,
where single genes were associated with single traits. The eugenic
task, then, was to decrease the transmission of undesirable genes to
the next generation, and increase the transmission of desirable genes.
Eugenicists advocated for a variety of social and political
interventions to produce more fit and less unfit
humans—sterilization of the unfit, immigration restriction acts
that prohibited the influx of people from entire nations deemed unfit,
anti-miscegenation laws that restricted “race mixing”; as
well as “fitter family contests” that praised the
fecundity of fit couples, eugenic sermon contests that encouraged
clergy to preach the religious justifications for eugenics, and
eugenic guidance to couples considering reproduction (Rosen 2004;
Lombardo 2008).
By the 1940s and 1950s, a number of social and scientific trends
emerged that undermined the eugenicists’ agenda. At the
conclusion of World War II, it became clear that Nazi atrocities
targeting certain populations for eradication were inspired by eugenic
ideas cultivated in the very countries that fought against Germany.
Social commentators pointed out that “fit humans” and
“unfit humans” were not scientific terms; rather, they
were racist, sexist, classist, and nativist concepts designed to
privilege wealthy, educated White people and devalue socioeconomically
disadvantaged minority populations. On the scientific side, social
sciences like anthropology and sociology drew increasing attention to
the social and economic forces at work in populations that contributed
to criminality, destitution, and mental illness. And genetics
too—the eugenicists’ chosen science—ultimately
invalidated eugenics. Complex human traits like intelligence and
antisocial behavior were not the result of any single gene; they were
the result of many genetic and environmental factors all working in
complicated and generally unpredictable ways over the course of human
development (Tabery 2014). That insight, alongside the social science
observations, ensured that no amount of sterilization or immigration
restriction would eliminate the traits eugenicists found undesirable
(Kevles 1985).
“Eugenics” as a term fell out of favor by mid-century.
Eugenic programs rebranded themselves as “medical
genetics” programs; eugenic journals and organizations dropped
the word in exchange for phrases like “social biology” and
“human genetics”. This terminological shift, however,
belied an ongoing interest among many scientists and non-scientists
alike in using the insights of genetics to control human heredity
(entry:
 eugenics).
Morgan’s fruit fly research proved that genes were on
chromosomes. But by the 1950s it was still unclear what genes were
made up of, what their molecular structure was, and how that structure
produced the observable hereditary patterns found in nature.
Biologists at the time debated whether the genetic material was
deoxyribonucleic acid (DNA) or proteins; Oswald Avery, Colin MacLeod,
and Maclyn McCarty had previously indicated in research on mice that
it was DNA, but this conclusion was by no means universally accepted
(Avery, MacLeod, and McCarty 1944). Alfred Hershey and Martha Chase at
Cold Spring Harbor Laboratory ingeniously used viruses—made up
of a protein body (or “coat”) and DNA inside—that
infected bacteria to settle the matter (Hershey and Chase 1952). They
first radioactively labeled the viruses’ protein coat and
allowed them to infect the bacteria, and then they radioactively
labeled the viruses’ DNA and allowed them to infect the
bacteria. The question was: When the viruses replicated in the
bacteria, would the radioactive marker show up in the bacteria
infected by viruses with radioactively-labeled proteins or in the
bacteria infected by viruses with radioactively-labeled DNA? It was
the latter, and that convinced molecular biologists that deciphering
the molecular structure of DNA was the next major challenge (entry:
 molecular biology).
A number of geneticists, structural chemists, and physicists descended
on the structure of DNA in the 1950s (Olby 1994). It was James Watson
and Francis Crick at the University of Cambridge who first determined
that DNA was a double-helix (Watson and Crick 1953). Utilizing
Watson’s genetic expertise, Crick’s work in theoretical
physics, and Rosalind Franklin’s x-ray crystallographic images
of DNA (appallingly, without her consent or even knowledge), Watson
and Crick built a model of DNA showing two polynucleotide strands
spiraling around one another (de Chadarevian 2002; Maddox 2002). The
strands were made up of a sequence of nucleic acids—combinations
of adenine (A), thymine (T), cytosine (C), and guanine (G), such that
an adenine on one strand hydrogen-bonded with a thymine on the other
and a cytosine on one strand hydrogen-bonded with a guanine on the
other (entry:
 models in science).
Molecular biologists spent the remainder of the 1950s and 1960s
determining how the double-helical structure of DNA helped to
elucidate the mechanisms of genetic replication and function. This
research was guided by the notion that the gene was an informational
molecule (Kay 2000). The nucleic acid bases were “letters”
which, in sets of three, made up “words” that
“coded” for one amino acid. Each chromosome was a
“chapter” in an organism’s entire genome—the
“book of life”. These weren’t just catchy phrases
for newspaper headlines. The technical language of molecular biology
employed the information metaphor. DNA was “transcribed”
into RNA, which was then “translated” into
proteins—the central dogma of molecular biology (entry:
 biological information).
Evolutionary biology also followed this reductionistic trend down to
molecules. Molecular evolution emerged as a field of study where
evolutionary changes were tracked at the level of DNA sequences, and
DNA became thought of as a ledger of evolutionary history. Most
genetic mutations, it became clear, had little to no impact on an
organism’s fitness—either because the mutation occurred in
such a way that the same amino acid was produced anyway (a product of
redundancy in the genetic code) or because the mutation occurred in a
region of the genome where no proteins were coded; this, in turn,
forced biologists to rethink the extent to which DNA sequences could
be understood as the product of natural selection alone (Kimura 1968;
Dietrich 1994). Mutation rates in homologous DNA sequences shared by
different species (say, humans and chimpanzees) also allowed for
inferring how far back those two species diverged from a common
ancestor (entries:
 ecological genetics;
 genetic drift). 
In the 1940s and 1950s, as the science of genetics was just beginning
to go molecular, a new batch of programs started showing up at medical
schools with names like “heredity clinic” and
“medical genetics”. The faculty and staff at these units
eschewed direct reference to eugenics, but their methods and clinical
advice were quite similar. They advertised reproductive counseling in
local newspapers. When couples showed up at the clinics, the
geneticists asked questions about what medical conditions ran in their
family, and then the scientists constructed pedigrees to track the
conditions. This facilitated hereditary risk assessments of what could
be transmitted to children born from that genetic union, and couples
at risk of bearing a child with some disability were advised not to
procreate (Comfort 2012).
With the onset of the molecular revolution in biology in the 1950s and
1960s, the suite of services that medical geneticists could offer
patients expanded. Carrier screening became available, which allowed
for informing a patient that they carried a recessive allele of a
disease-causing gene and so may pass that trait onto a child even if
they did not show symptoms of the disease themselves. Amniocentesis
was also developed, which allowed for extracting fetal cells from the
amniotic sac, culturing those cells, and then testing for a range of
medical conditions in the fetus (Harper 2008).
The proliferation of genetic technologies in the mid-twentieth century
brought with it a demand for healthcare workers who could process that
information and help patients understand it. The result was genetic
counseling programs. The professional emergence of genetic counseling
occurred at the very same time that abortion was decriminalized in a
number of countries, the disability rights movement and second-wave
feminism gained momentum, and bioethical attention to patient autonomy
became a serious consideration in clinical care. This created a
profound tension for the young community of genetic counselors. On the
one hand, genetic counseling had a disciplinary ancestry that ran
directly through heredity clinics and straightforward eugenics before
that; many of the genetic counselor’s tools (e.g., constructing
a family pedigree) and many of the traits that received genetic
counseling attention (e.g., Down syndrome) were the very same tools
and traits at the center of the earlier practices. On the other hand,
the genetic counselors—mostly women—were much more attuned
than the heredity clinic and medical genetics pioneers—mostly
men—to the moral and social problems with paternalistic medicine
and eugenics highlighted by the critics from feminism, disability
rights, and bioethics. The genetic counselors navigated this tension
by embracing a principle of nondirectiveness; rather than telling
patients how to act on the genetic information they received, genetic
counselors aspired to simply convey that genetic information in a
non-biased way and help the patients decide for themselves how to act
on that information (Stern 2012). 
Throughout the 1980s and 1990s, teams of researchers in the
ever-expanding pool of medical genetics and human genetics programs
competed to be the first to find the precise genomic location of
disease-causing genes. The challenge was to determine which
chromosome, then which stretch of DNA, and finally which specific
nucleic acid mutation was responsible for conditions like cystic
fibrosis and breast cancer; the leaders of the teams that crossed the
finish line first became scientific celebrities. The excitement
surrounding the genetic discoveries was partly about the thrill of a
scientific race; but more than that, it was also about the promise of
genetic tests that could tell patients more accurately whether or not
they were likely to develop some devastating disease and, beyond that,
lead to the development of gene-based treatments or even cures for
those diseases.
As more and more diseases were being linked up to locations in the
human genome throughout the 1990s, geneticists shifted their attention
to the grand prize—sequencing the entire human genome
(Cook-Deegan 1994). Ever since the discovery of the double-helical
structure of DNA, geneticists knew what genes were made of and what
structure they took. But they did not know how many genes there were.
And with the exception of the handful of genes that had already been
discovered, they did not know where the vast majority of genes were
located. The challenge was to spell out the ordered sequence of the
human genome in its entirety—no small task when you consider
that all three billion base pairs of the human genome reside in cells
no bigger than the period at the end of this sentence. The Human
Genome Project met the challenge with the largest and most expensive
biological collaboration in history. An international consortium of
scientists at twenty different sequencing centers spread out across
the United States, the United Kingdom, France, Germany, Japan, and
China took charge of different portions, breaking up the entire genome
into smaller and smaller overlapping segments until stretches several
thousand base pairs at a time could be sequenced. Then the whole thing
was pasted back together using the areas of overlap as guides. The
Human Genome Project was officially completed in 2003 (entry:
 the Human Genome Project).
 
The successful completion of the Human Genome Project was met with
lofty language. The media hailed the effort as uncovering the
“Holy Grail of biology”, and project leaders compared it
to splitting the atom and putting a human on the moon. At a
celebration of the Human Genome Project in 2003 (for video see
 Other Internet Resources),
 Francis Collins—one of the directors of the effort—boldly
predicted that the major genes for diabetes, mental illness, asthma,
and many other diseases would all be discovered in the next few years.
These genetic findings, he claimed, would in turn completely transform
the way those very common diseases were diagnosed and treated. By
2010, Collins foresaw a world of individualized medicine where genetic
testing was common and physicians tailored treatment plans and
lifestyle changes all to a patient’s unique DNA. And by 2020, he
hoped “we will have a gene-based designer drug available for
almost any disease that you can name.” A genomic revolution was
promised, where traditional “one-size-fits-all medicine”
was going to be replaced with “personalized medicine”
(entry:
 philosophy of medicine).
Interestingly, while the geneticists and news reporters were looking
ahead to the bright future of personalized medicine, the actual
results from the Human Genome Project pointed to a much more
complicated and daunting path ahead. Going into the sequencing,
geneticists assumed humans carried over 100,000 genes around with
them. Humans are more complex than mice or rice, the thought went, so
they should have more genes. Instead, the Human Genome Project
revealed that humans bore only 20,000 genes, while mice carried 25,000
and rice more than 30,000. This gene-count surprise was a strong
indication that there would be much more to personalized medicine than
simply assigning different stretches of DNA to different diseases
(entry:
 genomics and postgenomics).
We live now in the postgenomic era. Just what
“postgenomic” means, though, is a point of contention. On
the one hand, many life sciences have embraced genomics, bringing DNA
sequencing methods and an attention to genes into their
domain—conservation genomics, cancer genomics, behavioral
genomics, immunological genomics, bacterial genomics, and marine
genomics, to name just a few (entries:
 biodiversity;
 philosophy of immunology;
 conservation biology).
 On the other hand, it’s become abundantly clear that there is a
great deal of complexity between a sequence of DNA and a trait, and
that there are far more variables affecting that trait than just the
order of As, Cs, Ts, and Gs. This has spawned the proliferation of
other “omic” disciplines that attempt to catalogue and
functionally understand the great variety of molecular and cellular
entities and processes that contribute to the structure and function
of organisms—proteomics, transcriptomics, metabolomics,
lipidomics, and glycomics, to name just a few. What’s more,
genome-wide association studies, which scour entire genomes for
regions that are associated with traits, have revealed that in most
case, tens, hundreds, or thousands of genomic regions are implicated
in even fairly simple human traits like height. This
tension—between an urge to trace things down to genes and a
realization once you get there that there is far more to the story
than genes—is at the heart of many philosophical reflections on
genetics (Richardson and Stevens 2015; Reardon 2017; entries:
 genomics and postgenomics;
 philosophy of systems and synthetic biology;
 heritability).
 
The prominence of genetics—its impressive rise throughout the
twentieth century, its infiltration of other life and health sciences,
and its practical impact on human lives—made it a natural object
of scrutiny for philosophers interested in science and the
relationship between science and society. Philosophers have attended
to a great many conceptual, theoretical, metaphysical,
epistemological, methodological, ethical, legal, political, and social
questions pertaining to genetics. A sample of those questions and
direction to the entries where they are discussed in greater detail
follows.
Philosophy of science, throughout the early- and mid-twentieth
century, was predominated by attention to examples, problems, and
concepts from physics. Not surprisingly, the philosophical insights
produced a vision of how science worked that was modeled on physics.
Scientific explanations, as one example, arose from the derivation of
phenomena from physical laws of nature; rainbows are explained by
reference to the laws of reflection and refraction, alongside the
position of the sun, the position of raindrops, and the position of
the person seeing the rainbow. Scientific progress, as another
example, proceeded by way of higher-level sciences reducing to
lower-level sciences; thermodynamics (with its concepts of temperature
and pressure), the thought went, was reduced to statistical mechanics
(with its concepts of mean kinetic energy and force) (entries:
 scientific reduction;
 intertheory relations in physics;
 scientific progress;
 laws of nature). 
In the 1960s, philosophers of science started turning their attention
to biology in order to see how the philosophical insights applied
there. The earliest, influential example of this was Kenneth
Schaffner’s proposal for classical, Mendelian genetics reducing
to molecular genetics (Schaffner 1969). The great successes of
molecular biology following Watson and Crick’s discovery of the
double-helical structure of DNA can be best understood, Schaffner
argued, by recognizing how classical genetics was in the process of
being reduced to the biochemical processes investigated by molecular
biologists. On this reading, the “gene” from classical
genetics was reduced to a sequence of amino acids in DNA, and other
concepts from classical genetics like “dominance” were
likewise being reconfigured in the language of biochemistry (entry:
 reductionism in biology).
Schaffner’s case for reduction in genetics invited philosophers
to take a hard look at the theories and practices of biologists. A
variety of challenges to Schaffner’s thesis ensued. David Hull
(1974) claimed that there was no neat connection between classical
genetics’ gene concept and molecular genetics’ DNA
sequence because there was no neat, one-to-one mapping between
stretches of DNA and the traits that the classical geneticists
investigated; instead, Hull thought it better to characterize
molecular genetics as having replaced classical genetics. Philip
Kitcher (1984) agreed with Hull that reduction was the wrong way to
understand the relationship; however, he offered a different
formulation of the correct way. Kitcher said it was best to understand
classical genetics and molecular genetics as operating on different,
autonomous levels; classical geneticists investigated the transmission
of traits by studying the cytological mechanisms of chromosomal
action, while molecular geneticists investigated things like gene
replication and mutation by studying the molecular mechanisms of
protein synthesis (entries:
 molecular genetics;
 the unity of science). 
The debate over the relationship between classical genetics and
molecular genetics didn’t so much resolve itself as it did
evolve into a series of new philosophical questions. That is, as
philosophers were working through the details of genetics to assess
the relationship between the older version and the newer one, they
encountered a series of questions that demanded consideration: What
exactly is a “gene”? And what do genes actually do?
Philosophers continued to consider the reduction question (for
examples of subsequent defenders of reduction, see Waters 1990;
Schaffner 1993). But these other questions took on a life of their own
and became legitimate targets of inquiry independent of the reduction
debate. Philosophy of biology professionally emerged as a unique
sub-discipline of philosophy of science in the 1970s and 1980s as this
unfolded (entry:
 philosophy of biology).
 
No concept is more central to genetics than the “gene”.
And yet, the earliest philosophical examinations of genetics quickly
uncovered the fact that it was by no means clear that the gene from
classical genetics was the same thing as the gene from molecular
genetics. Subsequent attention to genetics only compounded the puzzle.
In the late 1970s, a series of discoveries complicated the simple
relationship between a single, uninterrupted sequence of DNA and its
polypeptide, protein product. Overlapping genes were discovered
(Barrell et al. 1976); such genes were considered overlapping because
two different amino acid chains might be read from the same stretch of
nucleic acids by starting from different points on the DNA sequence.
And split genes were found (Berget et al. 1977; Chow et al. 1977). In
contrast to the hypothesis that a continuous nucleic acid sequence
generated an amino acid chain, it became apparent that stretches of
DNA were often split between coding regions (exons) and non-coding
regions (introns). The distinction between exons and introns became
even more complicated when alternative splicing was discovered the
following year (Berk and Sharp 1978). A series of exons could be
spliced together in a variety of ways, thus generating a variety of
molecular products. Discoveries such as overlapping genes, split
genes, and alternative splicing made it clear that what counted as a
gene was by no means straightforward even if focus was confined to
molecular genetics (Griffiths and Stotz 2013; Rheinberger and
Müller-Wille 2017; entry:
 molecular biology).
Philosophers responded to these genetic discoveries in different
ways—by proposing multiple gene concepts, or by trying to unify
the disparate phenomena under a single gene concept. Lenny
Moss’s distinction between Gene-P and Gene-D is a classic
example of the multiple gene concepts approach (Moss 2002). Gene-P
embraced an instrumental preformationism; it was defined by its
relationship to a phenotype. In contrast, Gene-D referred to a
developmental resource; it was defined by its molecular sequence. An
example will help to distinguish the two: When one talked about
“the gene for cystic fibrosis”, the Gene-P concept was
being utilized; the concept referred to the ability to track the
transmission of this gene from generation to generation as an
instrumental predictor of cystic fibrosis, without being contingent on
knowing the causal pathway between the particular sequence of DNA and
the ultimate phenotypic disease. The Gene-D concept, in contrast,
referred instead to just one developmental resource (i.e., the
molecular sequence) involved in the complex development of the
disease, which interacted with a host of other such resources
(proteins, RNA, a variety of enzymes, etc.) (entry:
 gene).
 (For other examples of multiple gene concepts, see Fox Keller 2002; Baetu 2011;
Griffiths and Stotz 2013).
A second philosophical approach for conceptualizing the gene involved
rethinking a single, unified gene concept that captured the molecular
complexities. For example, Eva Neumann-Held claimed that a
“process molecular gene concept” embraced the
complications; on her unified view, the term “gene”
referred to “the recurring process that leads to the temporally
and spatially regulated expression of a particular polypeptide
product” (Neumann-Held 1999). Returning to the case of cystic
fibrosis, a process molecular gene for an individual without the
disease referred to one of a variety of transmembrane ion-channel
templates along with all the non-genetic influences on gene
expression, involved in the generation of the normal polypeptide
product. And so cystic fibrosis arose when a particular stretch of the
DNA sequence was missing from this process (entry:
 molecular genetics).
 (For additional examples of gene-concept unification, see Falk 2001;
Portin and Wilkins 2017.)
At a celebration of the Human Genome Project completion in 2003 (see
 section 1.5 above),
 genome enthusiasts promised that the major genes for heart disease,
mental health disorders, and diabetes would all be discovered shortly,
that treatments would soon follow, and that genetic cures would be
available by 2020. To call those claims “hype” would be an
understatement. Genome-wide association studies following the Human
Genome Project revealed that the most common human diseases were
impacted by many, many places in the genome, each of which made a very
small contribution to the risk of having or not having the ailment.
That, in turn, meant that there were no simple targets for genetic
interventions because there were simply too many genetic points and
pathways from those points. Most strikingly, even the traits for which
single genes were discovered—cystic fibrosis for
example—remained stubbornly resilient to cures, even though
those genes were identified 30 years prior. Philosophical critics of
genetics pointed to this mismatch between genetic promises and genetic
deliveries to argue against misleading and harmful notions of genetic
essentialism (Nelkin an Lindee 2004). “Misleading” because
studies about complex relationships between some portion of DNA and
some human trait often were simplified down to claims about
discoveries of “the gene for” that trait. And
“harmful” because the language encouraged continued
investment of public resources in gene-hunting research, when it was
possible that society would gain more from focusing biomedical
research agendas elsewhere (entries:
 feminist philosophy of biology;
 the human genome project;
 epigenesis and preformationism;
 philosophy of medicine;
 sociobiology).
 
One source of confusion, according to the critics, were
the metaphors that got embedded in molecular genetics in the
mid-twentieth century. Computing and architectural metaphors
suggesting that the genome was a “program” or
“blueprint” for development conveyed misleading ideas
about how cells operated (Kay 2000). Informational metaphors may capture some features of
biology, but the error was in thinking that only DNA carried that
biological information (Griffiths 2001; Jablonka 2002; entry:
 biological information).
 
An alternative way of making sense of what genes do without the
baggage of metaphors is to characterize it in causal language. C.
Kenneth Waters drew on a manipulationist theory of causation that
treated causes as manipulable difference-makers, where Waters’
insight was to add a distinction between “potential
difference-makers” and “actual difference-makers”
(Waters 2007). In carefully controlled genetic experiments such as
Morgan’s classic work on fruit flies (see
 section 1.1 above),
 Waters pointed out that there were many potential difference-makers
for a trait under investigation like eye color, but it was the actual
genetic difference that was responsible for the actual difference in
eye color. Waters claimed further that the spread of genetics
throughout the life sciences was partly attributable to this feature
of scientists being able to manipulate phenotypes by manipulating
genes (entries:
 molecular genetics;
 causation and manipulability). As with the information metaphor, though, philosophical
critics have been willing to accept Waters’ distinction between
potential and actual difference-makers, but then challenged the idea
that only genes acted as actual difference-makers (Griffiths and Stotz
2013; entry:
 developmental biology).
Research agendas for investigating the living world now commonly
reference genes as one unit, element, or level to be considered by
biologists (entry:
 levels of organization in biology).
 The Research Domain Criteria of psychiatry lists genes as one unit of
analysis alongside cells, circuits, and physiology (entry:
 philosophy of psychiatry).
 Conservation biologists attend to different targets of
conservation—genes, but also species, sub-species, populations,
and biomes (entries:
 conservation biology;
 biodiversity). The ubiquity of these references to genes in scientific
practices that range from understanding major depressive disorder to
saving the Great Barrier Reef serves as an affirmation of
Waters’ point about the power of genetics to act as a point of
intervention in the world. At the same time, the fact that genes are
but one item in these lists of possible points of intervention reminds
us that they are not unique. 
On Darwin’s original formulation, natural selection acted on
individual organisms (entries:
 Darwinism;
 natural selection). As a population of organisms struggled to survive in
some particular environment, some of the organisms had traits that
made them slightly fitter in that environment, and this meant they
were more likely to survive and reproduce; the offspring of those
organisms in turn inherited the traits of their parents, and so the
character of the population changed over time, eventually forming a
new or different species if the process went on long enough (entries:
 evolution;
 fitness). For example, in an environment that is growing
increasingly cold, animals with thicker fur might have an advantage
over the animals with less fur because of their ability to tolerate
the temperature; these thicker-furred animals would be more likely to
survive in that cold environment and also more likely to reproduce
offspring that inherit their parents’ thicker coats. Eventually,
the entire population of those animals may display this thicker fur
(entry:
 adaptationism).
 
Even Darwin recognized, however, that nature presented some cases that
did not fit this picture. In particular, seemingly altruistic
behaviors in the animal kingdom posed a puzzle (Wilson 2015). Take
social mammals like ground squirrels that whistle alarm calls to
notify other members of the group of a nearby threat. On the original
Darwinian formulation of natural selection, alarm calling should not
arise because it is not beneficial to the ground squirrels that
display the behavior. A ground squirrel that is more prone to
notifying its counterparts of a looming threat is presumably also more
likely to get killed by that threat, and so the proneness to warn
shouldn’t get propagated in the population if all the altruistic
ground squirrels are dying by very virtue of their altruistic behavior
(entry:
 biological altruism).
W. D. Hamilton proposed kin selection as a resolution to the altruism
puzzle (Hamilton 1964). If the population of ground squirrels above
were related, then the genes associated with alarm calling could
propagate in the group even if the altruistic ground squirrels were
killed more often because their kin who shared the genes survived. G.
C. Williams and Richard Dawkins subsequently generalized
Hamilton’s observation, arguing that all of natural selection
could be understood as operating at the genetic level (Williams 1966;
Dawkins 1976). On this “gene’s-eye view of
evolution”, natural selection targets genes (the
“replicators” in Dawkins’ language), while organisms
are the “vehicles” (or “interactors”) that
carry around the genes and interact with the environment (entry:
 replication and reproduction).
The gene’s-eye formulation of natural selection sparked an
explosion of philosophical attention to evolutionary biology (which
happened to roughly coincide with the attention to the relationship
between classic and molecular genetics discussed in
 section 2.1).
 This “levels” or “units of selection” debate
revolved around a number of questions (Keller 1999; Okasha 2006;
entry:
 units and levels of selection).
 For example, at what level does natural selection act? Genes,
individuals, groups, entire species (entry:
 biological notion of individual)?
 Group selectionists argued altruistic behavior could arise if one
group was fitter than another group because it had more altruistic
members in that group (Sober and Wilson 1998). Stephen Jay Gould made the
case for selection acting on entire species when, for example,
ecological specialists were more common than generalists (Gould 2002).
Other philosophers questioned the entire replicator/interactor
distinction; Developmental Systems Theorists advocated instead for
seeing the developmental system as a whole (genes and the environment)
as the evolving unit (Oyama, Griffiths, and Gray eds. 2001; see the entry
 on developmental biology).
Eugenics had many things wrong with it (see
 section 1.2 above).
 In part it was based on a poor understanding of human genetics.
Beyond that, though, it was also morally abhorrent. Eugenics was
grounded on biased ideas about what made lives worth living. And it
put decision-making power over what lives were worth living into the
hands of government bodies and institutions. Dozens of states in
America passed some form of sterilization law that targeted people
deemed undesirable, so that society wouldn’t be burdened by
people with physical and intellectual disabilities (Largent 2011).
States also enacted anti-miscegenation laws to prevent interracial
marriages that could contribute to “race suicide” (Pascoe
2009; entry:
 eugenics).
 
Architects of the pivot to medical genetics and genetic counseling
programs in the mid-twentieth century tried to curtail some of the
abhorrent features of this history by shifting control over
reproductive decisions from governments/institutions to
individuals/families (see
 section 1.4 above).
 A technology like amniocentesis allowed a woman to decide whether or
not she wanted to carry a fetus that was likely to have Down syndrome
to term or alternatively terminate the pregnancy. Carrier screening
for cystic fibrosis let prospective parents know what the chances were
that they could conceive a child with that condition. For families
with Huntington’s disease, preimplantation genetic diagnosis
after in vitro fertilization opened the door to only implanting
embryos that did not carry the Huntington’s disease allele.
Genetic counselors saw their role as facilitating these
patients’ reproductive decision-making in a non-biased way.
Medical geneticists hailed these genetic technologies, claiming they
empowered families (and women in particular) with greater autonomous
control over their reproductive decisions (entry:
 pregnancy, birth, and medicine).
Critics from disability studies, however, argued that medical
genetics’ break from eugenics was not so clean-cut (Barnes 2016;
Parens and Asch 1999; Silvers 2016). When hospitals offered pregnant
women the opportunity to screen for traits like Down syndrome and
cystic fibrosis but not other traits like sex or eye-color, it sent a
message to prospective parents that some traits—like Down
syndrome and cystic fibrosis—should be considered for
termination while other traits—like sex and
eye-color—should not (Parens and Asch 2000). Genetic counselors
aspired to offer genetic information in a non-directive fashion, but
disability-oriented critics argued that information shared with
prospective parents about living with a disability was often skewed
against disability (Asch 1989). The general message disability studies
scholars highlighted was that medical genetics reinforced the idea
that disability was something to be avoided, not accommodated (Scully
2008). This was seen as just the modern instantiation of the
straightforwardly eugenic idea that some lives are not worth living
(Saxton 1997; Wendell 1996). What’s more, the critics pointed
out that standardizing these genetic technologies in the healthcare
system actually put pressure on prospective parents to use them, which
meant the reproductive decisions were not as autonomous as genetic
defenders proclaimed (entries:
 disability: health, well-being, and personal relationships;
 feminist perspectives on disability). 
Medical genetics faced other criticisms as well. The standardization
of genetic technologies contributed to the medicalization of
conception, pregnancy, and birth; this altered the relationship
between expecting parents and fetuses because a pregnancy became
thought of as “tentative” until screening was complete,
and it also impacted the way expecting parents prepared for and
communicated about a pregnancy (Duden 1993). Racial health disparity
scholars also criticized biomedical research investment in medical
genetics because it pulled resources away from known causes of health
disparities in the environment that had a bigger impact on the problem
(Roberts 2011). In some cases, the concerns about medicalization and
the concerns about racial health disparities converged; for example,
Dorothy Roberts warned that it takes social and economic resources to
access the sorts of genetic technologies that hospitals offer (at
least in the United States), and so medical genetics only exacerbated
health disparities (Roberts 2009; entries:
 feminist bioethics;
 the human genome project). 
A common distinction in the medical world is that between
“treatment” and “enhancement”, where the
general idea is that biomedical interventions that are designed to
restore or sustain health count as treatments while those that go
beyond restoring or sustaining health count as enhancements (entry:
 human enhancement).
 The discussion in the previous section about disability largely
pertained to the treatment end of that spectrum (entry:
 neuroethics).
 Philosophical debates about medical genetics have also revolved
around the enhancement end (Buchanan 2011).
To a certain extent, the ability to screen for genes associated with
certain traits meant some form of genetic enhancement was always
possible, even if it seemed impractical (think worries about
blue-eyed, blond-haired “designer babies”, which were
common in the 1990s when the Human Genome Project was launched). The
emergence of relatively fast and cheap gene-editing technologies like
CRISPR-Cas9 has made the debates over genetic enhancement feel much
more pressing (National Academies of Sciences, Engineering, and
Medicine 2017). The ethical stakes have not really changed, but the
sense that discussions have shifted from purely hypothetical to
potentially realistic has changed. 
Philosophical critics of enhancement offer a number of cautions
against the practice. Michael Sandel famously associated desires for
enhancement with hubris, where contentment with what nature (and
chance) present was the more virtuous attitude (Sandel 2002). Parental
attempts to genetically enhance a child were also criticized on the
grounds that they violated the autonomy of the child because parents
were seeking too much control over the child’s future (Habermas
2003). Philosophers also warned of an unjust future made up of genetic
“haves” (those who have benefited from enhancement) and
“have nots” (those who remained unenhanced)—a
scenario popularized in the film GATTACA (entry:
 theory and bioethics).
Defenders of enhancement were quick to point out that many of the
arguments against enhancing were not reasonably confined to just
genetic technologies. Parents control their child’s future when
they send her to space camp rather than soccer camp; likewise, wealthy
parents are able to enhance their children with all sorts of
educational resources, like tutors, that poor parents cannot provide,
and these surely contribute to disparities. But parents are given wide
latitude to decide what camp their child will attend and how much they
will (or won’t) invest in their child’s educational
development. Julian Savulescu (2001) even argued that parents are
obligated to take advantage of genetic technologies if they offer
opportunities to increase the chances of a child living the best life.
Of course, just what counts as the “best life” is an
inherently value-laden judgment, and critics pointed out that
Savulescu’s characterization of it looked frighteningly similar
to the vision promoted by eugenicists a century ago (entries:
 parenthood and procreation;
 eugenics). 
Eugenicists understood races to be biologically distinct human
populations with rank-able physical and behavioral profiles (see
 section 1.2 above).
 Black people were more prone to criminality and lower intelligence
than White people, they believed; and a child who was born of
mixed-race parents was likely to fall somewhere in between. The
anti-miscegenation laws promoted by eugenicists were explicitly
designed to prevent White people from breeding with Black people.
Black people were not the only targets though. Eugenicists also spoke
of the “Irish race”, the “Italian race”, and
the “Slavic race”. These racial categories were just as
real for the eugenicists as the categories of Black, White, and Asian,
and anti-immigration laws were passed to keep undesirable races away
(entry:
 race).
The simplistic ideas about race held by eugenicists did not hold up to
scientific scrutiny. Twentieth century developments in fields like
human evolution, anthropology, and sociology proved that races were
not so cleanly distinguishable—that there were no racial atomic
numbers that carved up Black and White in the same way that proton
number neatly distinguished gold from mercury. Still, interest in race
and genetics persisted. Insights from the study of human evolution
revealed how the earliest humans moved out of Africa roughly 70,000
years ago in a series of migrations, some moving east across the Asian
continent, some moving north up into what is now Europe, some
eventually crossing the Bering Strait and entering the Americas. As
these human populations moved about, they encountered very different
environments with unique selection pressures (Herrera and
Garcia-Bertrand 2018). What counted as adaptive in the fjords of
Northern Europe looked quite different from what was adaptive in the
deserts of Northern Africa, and so the human populations over time
adapted to the distinct environments—some developing darker skin
while others grew lighter, some developing a resistance to cholera
while others developed a resistance to malaria.
The revised conceptions of race and genetics worked off this
evolutionary story, where the idea was that racial categories
represented human groups with unique genetic profiles tied to those
different selection histories (Hardimon 2017). A Black person, the
thought went, came from African ancestry, while a White person came
from European ancestry. In turn, the Black person was likely to have a
different genetic profile when it came to melanin production (which
controls skin shade) than the White person, and the Black person was
more likely to be a carrier for sickle-cell anemia (a blood disorder
that also offers some resistance to malaria) while the White person
was more likely to be a carrier for cystic fibrosis (a respiratory
disorder that also offers some resistance to cholera) (Spencer 2018).
Proponents of these newer ideas about race and genetics were eager to
say that they were not in the business of ranking races; they were
just tracing different racial histories and the modern phenotypic
results of them (entry:
 the social dimensions of scientific knowledge).
Even the revised understanding of race and genetics, however, faced
challenges. First, the groupings from human evolution did not map
neatly onto the traditional folk concepts of race; for example, if
“White” applied to people descended from the migration
that ultimately wound up in Northern Europe, then that would mean
children born today in Bangalore, Oslo, and Damascus all count as
“White”—a counterintuitive grouping of
“White” people (Smith 2011). Critics also pointed out that
the folk concepts of race evolved over time, largely in response to
political and economic pressures (like census counting and justifying
slavery), not biological insights (like paleogenetic discoveries)
(Roberts 2011). Some critics have encouraged distinguishing
“race” from “ancestry”, confining talk of
genetics (e.g., genetic ancestral testing, genetic medicine) to the
ancestry side of that distinction and keeping it out of discussions of
race (Yudell et al. 2016; entry:
 identity politics).
As the second century of genetics unfolds, there is little reason to
think the influential science will slow down. In New Zealand,
government leaders proposed using gene-editing technologies to
eradicate invasive species from the island nation (Yong 2017). In the
United States, the All of Us Research Program rolled out, with the aim
being to collect DNA from 1,000,000 Americans and link that
information up with medical records so as to usher in a new genomic
paradigm of precision medicine (Pear 2016). In China, scientists
inserted a human gene involved in brain development into monkeys in
order to see how the addition of that human gene altered the
monkey’s own development (CBC Radio in
 Other Internet Resources).
 These programs, practices, and proposals raised profound questions
about the dangers of intervening on complex ecosystems, the politics
of distributing scarce biomedical resources, and the ethics of
non-human animal research. As genetics progresses, so too will the
philosophical questions about it.