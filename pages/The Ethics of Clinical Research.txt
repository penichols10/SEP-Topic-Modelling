Human subjects research is research which studies humans, as opposed
to animals, atoms, or asteroids. Assessment of whether humans prefer
100 dollars or a 1% chance of 10,000 dollars constitutes human
subjects research. Clinical research refers to the subset of human
subjects research that evaluates the impact interventions have on
human beings with the goal of assessing whether they might help to
improve human health and well-being. The present analysis focuses on
research that is designed to improve human health and well-being by
identifying better methods to treat, cure or prevent illness or
disease. This focus is intended to bracket the question of whether
research on enhancements that might increase our well-being by making
us better than normal, allowing us to remember more or worry less,
without treating, curing, or preventing illness or disease
qualifies as clinical research.
We shall also bracket the question of whether quality improvement and
quality assurance projects qualify as clinical research. To briefly
consider the type of research at the heart of this debate, consider a
hospital which proposes to evaluate the impact of checklists on the
quality of patient care. Half the nurses in the hospital are told to
continue to provide care as usual; the other half are provided with a
checklist and instructed to mechanically check off each item as they
complete it when caring for their patients. The question of whether
this activity constitutes clinical research is of theoretical interest
as a means to clarifying the precise boundaries of the concept. Should
we say that this is not clinical research because the checklist is
used by the nurses, not administered to the patients? Or should we say
this is clinical research because it involves the systematic testing
of a hypothesis which is answered by collecting data on patient
outcomes? The answer has significant practical implications,
determining whether these activities must satisfy existing regulations
for clinical research, including whether the clinicians need to obtain
patients’ informed consent before the nurses taking care of them
can use the checklist.
While clinical medicine is enormously better than it was 100 or even
50 years ago, there remain many diseases against which current
clinical medicine offers an inadequate response. To name just a few,
malaria kills over a million people, mostly children, every year;
chronic diseases, chief among them heart disease and stroke, kill
millions each year, and there still are no effective treatments for
Alzheimer disease. The social value of clinical research lies in its
ability to collect information that might be useful for identifying
improved methods to address these conditions. Yet, it is the
rare  study which definitively establishes that a particular
method is effective and safe for treating, curing or preventing some
illness. The success of specific research studies more commonly lies
in the gathering of information that, together with the results of
many other studies, may yield these improvements. For example,
prior to establishing the efficacy of a new treatment for a given
condition, researchers typically need to identify the cause of the
condition, possible mechanisms for treating it, a safe and effective
dose, and ways of testing whether the drug alters the course of the
disease.
The process of testing potential new treatments can take 10–15 years,
and is standardly divided into phases. Formalized phase 0 studies are
a relatively recent phenomenon involving the testing of interventions
and methods which might be used in later phase studies. A phase 0
study might be designed to determine the mechanism of action of a
particular drug and evaluate different ways to administer it. Phase 1
studies are the earliest tests of a new intervention and are conducted
in small numbers of individuals. Phase 1 studies are designed to
evaluate the pharmacokinetics and pharmacodynamics of new treatments,
essentially evaluating how the drug influences the human body and how
the human body influences the drug. Phase 1 studies also evaluate the
risks of the treatment and attempt to identify an appropriate dose to
be used in subsequent phase 2 studies. Phase 1 studies pose risks and
frequently offer little if any potential for clinical benefit to
participants. As a result, a significant amount of the ethical concern
over clinical research focuses on phase 1 studies.
If phase 1 testing is successful, potential new treatments go on to
larger phase 2 studies which are designed to further assess risks and
also to evaluate whether there is any evidence that the treatment
might be beneficial. Successful phase 2 studies are followed by phase
3 studies which involve hundreds, sometimes thousands of patients.
Phase 3 studies are designed to provide a rigorous test of the
efficacy of a treatment and frequently involve randomization of
participants to the new treatment or a control, which might be the
current treatment or a placebo. Finally, post-marketing or phase 4
studies evaluate the use of interventions in clinical practice.
The first three phases of clinical trials typically include
purely research procedures, such as blood draws, imaging scans, or
biopsies, that are included in the study to collect data regarding the
treatment under study. Analysis of the central ethical challenge posed
by clinical research thus focuses on three related risk-benefit
profiles: (a) the risk-benefit profile of the interventions(s) under
study; (b) the risk-benefit profile of the included research
procedures; and (c) the risk-benefit profile of the study as a
whole.
In some cases, receipt of potential new treatments is in the ex ante
interests of research participants. For example, the risks posed by an
experimental cancer treatment might be justified by the possibility
that it will extend participants’ lives. Moreover, the
risk/benefit profile of the treatment might be as favorable for
participants as the risk/benefit profile of the available
alternatives. In these cases, receipt of the experimental intervention
ex ante promotes participants’ interests. In other cases,
participation in research poses ‘net’ risks, that is,
risks of harm which are not, or not entirely, justified by potential
clinical benefits to individual participants. A first in human trial
of an experimental treatment might involve a single dose to see
whether humans can tolerate it. And it might occur in healthy
individuals who have no need of treatment. These studies pose risks to
participants and offer essentially no chance for clinical benefit. The
qualifier to ‘essentially’ no chance of clinical benefit
is intended to capture the fact that the research procedures included
in clinical trials may inadvertently end up providing some clinical
benefit to some participants. For example, a biopsy that is used to
collect research data may disclose a previously unidentified and
treatable condition. The chance for such benefit, albeit real, is
typically so remote that it is not sufficient to compensate for the
risks of the procedure. Whether a study as a whole poses net risks
depends on whether the potential benefits of the experimental
intervention compensate for its risks plus the net risks of the
research procedures included in the study.
Clinical research which poses net risks raises important ethical
concern. Net-risk studies raise concern that participants are being
used as mere means to collect information to benefit future patients.
Research procedures that pose net risks may seem to raise less concern
when they are embedded within a study which offers a favorable
risk-benefit profile overall. Yet, since these procedures pose net
risks, and since the investigators could provide participants with the
new potential treatment alone, they require justification. An
investigator who is about to insert a needle into a research
participant to obtain some blood purely for laboratory purposes faces
the question of whether doing so is ethically justified, even when the
procedure is included in a study that offers participants the
potential for important medical benefit. The goal of ethical analyses
of clinical research is to provide an answer.
Clinical research poses three types of net risks: absolute, relative,
and indirect (Rid and Wendler 2011). Absolute net risks arise when the
risks of an intervention or procedure are not justified by its
potential clinical benefits. Most commentators focus on this
possibility with respect to research procedures which pose some risks
and offer (essentially) no chance of clinical benefit, such as blood
draws to obtain cells for laboratory studies. Research with healthy
volunteers is another example which frequently offers no chance for
clinical benefit. Clinical research also poses absolute net risks when
it offers a chance for clinical benefit which is not sufficient to
justify the risks participants face. A kidney biopsy to obtain tissue
from presumed healthy volunteers may offer some very low chance of
identifying an unrecognized and treatable pathology. This intervention
nonetheless poses net risks if the chance for clinical benefit for the
participants is not sufficient to justify the risks of their
undergoing the biopsy.
Relative net risks arise when the risks of a research intervention are
justified by its potential clinical benefits, but the
intervention’s risk-benefit profile is less favorable than the
risk-benefit profile of one or more available alternatives. Imagine
that investigators propose a randomized, controlled trial to compare
an inexpensive drug against an expensive and somewhat more effective
drug. Such trials make sense when, in the absence of a direct
comparison, it is unclear whether the increased effectiveness of the
more expensive drug justifies its costs. In this case, receipt of the
cheaper drug would be contrary to participants’ interest in
comparison to receiving the more expensive drug. The trial thus poses
relative net risks to participants.
Indirect net risks arise when a research intervention has a favorable
risk-benefit profile, but the intervention diminishes the risk-benefit
profile of other interventions provided as part of or in parallel to
the study. For example, an experimental drug for cancer might
undermine the effectiveness of other drugs individuals are taking for
their condition. The risks of research participation can be compounded
if the indicated response to the harm in question poses additional
risks. Kidney damage suffered as the result of research participation
might lead to the need for short-term dialysis which poses additional
risks to the individual; a participant who experiences a postlumbar
puncture headache might need a ‘blood patch’ which poses
some risk of blood entering the epidural space which would call for a
further response which carries additional risks. While commentators
tend to focus on the risks of physical harm, participation in clinical
research can pose other types of risks as well, including
psychological, economic, and social risks. Depending on the study and
the circumstances, individuals who are injured as the result of
participating in research might incur significant expenses. Most
guidelines and regulations stipulate that evaluation of the
acceptability of clinical research studies should take into account
all the different risks to which participants are exposed.
To assess the ethics of exposing research participants to risks, one
needs an account of why exposing others to risks raises ethical
concern in the first place. Being exposed to risks obviously raises
concern to the extent that the potential harm to which the risk refers
is realized: the chance of a headache turns into an actual headache.
Being exposed to risks also can lead to negative consequences as a
result of the recognition that one is at risk of harm. Individuals who
recognize that they face a risk may become frightened; they also may
take costly or burdensome measures to protect themselves. In contrast,
the literature on the ethics of clinical research implicitly assumes
that being exposed to risks is not per se harmful. The mere fact that
participants are exposed to risks is not regarded as necessarily
contrary to their interests. It depends on whether the risk is
realized in an actual harm.
It is one thing to expose a consenting adult to risks to save the
health or life of an identified and present other, particularly when
the two individuals are first degree relatives. It is another thing,
or seems to many to be another thing, to expose consenting individuals
to risks to help unknown and unidentified, and possibly future others.
Almost no one objects to operating on a healthy, consenting adult to
obtain a kidney that might save a present and ailing sibling,
even though the operation poses some risk of serious harm and offers
the donor no potential for clinical benefit. Attempts to obtain a
kidney from a healthy, consenting adult and give it to an unidentified
individual are met with greater ethical concern. The extent of the
concern increases as the path from risk exposure to benefit becomes
longer and more tenuous. Many clinical research studies expose
participants to risks in order to collect generalizable information
which, if combined with the results of other, as yet non-existent
studies, may eventually benefit future patients through the
identification of a new intervention, assuming the appropriate
regulatory authorities approve it, some company or group chooses to
manufacture it, and patients can afford to purchase it. The potential
benefits of clinical research may thus be realized someday, but the
risks and burdens are clear and present.
Increasingly, researchers obtain and store human biological
samples for use in future research studies. These studies raise
important questions regarding what might be called
‘contribution’ and ‘information’ risks. The
former question concerns the conditions under which it is acceptable
to ask individuals to contribute to answering the scientific question
posed by a given study (Jonas 1969). The fact that this question has
been ignored by many commentators and regulations may trace to an
overly narrow understanding of individuals’ interests.
Individuals undoubtedly have an interest in avoiding the kinds of
physical harms, pain, infection, loss of organ function,
that they face in clinical research. This leaves the question of
whether individuals’ interests are implicated, and whether they
can be set back, by contributing to particular projects,
activities and goals?
Consider the routine practice of storing leftover samples of
participants’ blood for use in future research projects.
For the purposes of protecting participants’ interests, does it
matter what goals these future studies attempt to advance? Can
individuals be harmed if their samples are used to promote
goals which conflict with their fundamental values? For
example, are the interests of individuals who fundamentally
oppose cloning set back if their samples are used in a
study that successfully identifies improved methods to clone human
beings?
The possibility of information risks garnered significant
attention when investigators used DNA samples obtained from members of
the Havasupai tribe in Arizona to study “theories of the
tribe’s geographical origins.” The study’s
conclusion that early members of the tribe had migrated from Asia
across the Bering Strait contradicted the tribe’s own views that
they originated in the Grand Canyon (Harmon 2010). Can learning the
truth, in this case the origins of one’s tribal group, harm
research participants?
Attempts to determine when it is acceptable to expose participant to
risks for the benefit of others, including future others who do not
yet exist, have been significantly influenced by its history, by how
it has been conducted and, in particular, by how it has been
misconducted (Lederer 1995; Beecher 1966). Thus, to understand the
current state of the ethics of clinical research, it is useful to know
something of its past.
Modern clinical research may have begun on the 20th of May,
1747, aboard the HMS Salisbury. James Lind, the ship’s surgeon,
was concerned with the costs scurvy was exacting on British sailors,
and was skeptical of some of the interventions, cider, elixir of
vitriol, vinegar, sea-water, being used at the time to treat it.
Unlike other clinicians of his day, Lind did not simply assume that he
was correct and treat his patients accordingly. He designed a study.
He chose 12 sailors from among the 30 or so Salisbury’s crew
members who were suffering from scurvy, and divided them into six
groups of 2 sailors each. Lind assigned a different intervention to
each of the groups, including two sailors turned research participants
who received 2 oranges and 1 lemon each day. Within a week these two
were sailors again; the others remained patients, and several were
dying.
The ethics of clinical research begins by asking how we should think
about the fate of these latter sailors. Did Lind treat them
appropriately? Do they have a moral claim against Lind? It is widely
assumed that physicians should do what they think is best for the
patient in front of them. Lind, despite being a physician, did not
follow this maxim. He felt strongly that giving sea water to
individuals with scurvy was a bad idea, but he gave sea water to 2 of
the sailors in his study to test whether he, or others, were right. To
put the fundamental concern raised by clinical research in its
simplest form: Did Lind sacrifice these two sailors, patients under
his care, for the benefit of others? 
Lind’s experiments represent perhaps the first modern clinical
trial because he attempted to address one of the primary challenges
facing those who set out to evaluate medical treatments. How does one
show that any differences in the outcomes of the treatments under
study are a result of the treatments themselves, and not a result of
the patients who received them, or other differences in the
patients’ environment or diet? How could Lind be confident that
the improvements in the two sailors were the result of the oranges and
lemons, and not a result of the fact that he happened to give them to
the two patients who occupied the most salutary rooms on the ship?
Lind tried to address the challenge of confounding variables by
beginning with patients who were as similar as possible. He carefully
chose the 12 subjects for his experiment from a much larger pool of
ailing sailors; he also tried to ensure that all 12 received the same
rations each day, apart from the treatments provided as part of his
study. It is also worth noting that Lind’s dramatic results were
largely ignored for decades, leading to uncounted and unnecessary
deaths, and highlighting the importance of combining clinical research
with effective promulgation and implementation. The Royal Navy did not
adopt citrus rations for another 50 years (Sutton 2003), at which
point scurvy essentially disappeared from the Royal Navy.
Lind’s experiments, despite controlling for a number of factors,
did not exclude the possibility that his own choices of which sailors
got which treatment influenced the results. More recent experiments,
including the first modern randomized, placebo controlled trial of
Streptomycin for TB in 1948 (D’Arcy Hart 1999), attempt to
address this concern by assigning treatments to patients using a
random selection process. By randomly assigning patients to treatment
groups these studies ushered in the modern era of controlled, clinical
trials. And, by taking the choice of which treatment a given patient
receives out of the hands of the treating clinician, these trials
underscore and, some argue, exacerbate the ethical concerns raised by
clinical research (Hellman and Hellman 1991). A foundational principle
of clinical medicine is the importance of individual judgment. A
physician who decides which treatments her patients receive by
flipping a coin is guilty of malpractice. A clinical investigator who
relies on the same methods receives awards and gets published in elite
journals. One might conclude that sacrifice of the interests of some,
often sick patients, for the benefit of future patients, is
essentially mandated by the scientific method (Miller & Weijer
2006; Rothman 2000). The history of clinical research seems to provide
tragic support for this view.
The history of clinical research is littered with abuses. Indeed, one
account maintains that the history of pediatric research is
“largely one of child abuse” (Lederer and Grodin 1994, 19;
also see Lederer 2003). This history has had a significant influence
on how research ethicists understand the concerns raised by clinical
research and on how policy makers attempt to address them. In
particular, policy makers have responded to previous abuses by
developing guidelines intended to prevent their recurrence.
The most influential abuses in this regard were the horrific
experiments conducted by Nazi physicians during WW II (abuses
perpetrated by Japanese physicians were also horrific, but have
received significantly less attention). Response to these abuses led
to the Nuremberg Code (Grodin & Annas 1996; Shuster 1997), which
is frequently regarded as the first set of formal guidelines for
clinical research, an ironic claim on two counts. First, there is some
debate over whether the Nuremberg Code was intended to apply generally
to clinical research or whether, as a legal ruling in a specific
trial, it was intended to address only the cases before the court
(Katz 1996). Second, the Germans themselves had developed systematic
research guidelines as early as 1931 (Vollmann & Winau 1996).
These guidelines were still legally in force at the time of the Nazi
atrocities and clearly prohibited a great deal of what the Nazi
doctors did.
Wide consensus developed by the end of the 1950s that the Nuremberg
Code was inadequate to the ethics of clinical research. Specifically,
the Nuremberg Code did not include a requirement that clinical
research receive independent ethics review and approval. In addition,
the first and longest principle in the Nuremberg Code states that
informed consent is “essential” to ethical clinical
research (Nuremberg Military Tribunal 1947). This requirement provides
a powerful safeguard against the abuse of research participants. It
also appears to preclude clinical research with individuals who cannot
consent.
One could simply insist that the informed consent of participants is
necessary to ethical clinical research and accept the opportunity
costs thus incurred. Representatives of the World Medical Association,
who hoped to avoid these costs, began meeting in the early 1960s to
develop guidelines, which would become the Declaration of
Helsinki, to address the perceived shortcomings of the Nuremberg Code
(Goodyear, Krleza-Jeric, and Lemmens 2007). They recognized that
insisting on informed consent as a necessary condition for clinical
research would preclude a good deal of research designed to find
better ways to treat dementia and conditions affecting children, as
well as research in emergency situations. Regarding consent as
necessary precludes such research even when it poses only minimal
risks or offers participants a compensating potential for important
clinical benefit. The challenge, still facing us today, is to identify
protections for research participants which are sufficient to protect
them without being so strict as to preclude appropriate research
designed to benefit the groups to which they belong.
The Declaration of Helsinki (World Medical Organization 1996) allows
individuals who cannot consent to be enrolled in clinical research
based on the permission of the participant’s representative. The
U.S. federal regulations governing clinical research take a similar
approach. These regulations are not laws in the strict sense of being
passed by Congress and applying to all research conducted on U.S.
soil. Instead, the regulations represent administrative laws which
effectively attach to clinical research at the beginning and the end.
Research conducted using U.S. federal monies, for instance, research
funded by the NIH, or research involving NIH researchers, must follow
the U.S. regulations (Department of Health and Human Services 2005).
Research that is included as part of an application for approval from
the U.S. FDA also must have been conducted according to FDA
regulations which, except for a few exceptions, are essentially the
same. Although many countries now have their own national regulations
(Brody 1998), the U.S. regulations continue to exert enormous
influence around the world because so much clinical research is
conducted using U.S. federal money and U.S. federal investigators, and
the developers of medical treatments often want to obtain approval for
the U.S. market.
The abuses perpetrated as part of the infamous Tuskegee syphilis study
were made public in 1972, 40 years after the study was initiated. The
resulting outcry led to the formation of the U.S. National Commission,
which was charged with evaluating the ethics of clinical research with
humans and developing recommendations for appropriate safeguards.
These deliberations resulted in a series of recommendations for the
conduct of clinical research, which became the framework for existing
U.S. regulations. The U.S. regulations, like many regulations, place
no clear limits on the risks to which competent and consenting adults
may be exposed. In contrast, strict limits are placed on the level of
research risks to which those unable to consent may be exposed,
particularly children. In the case of pediatric research, the standard
process for review and approval is limited to studies that offer a
‘prospect of direct’ benefit and research that poses
minimal risk or a minor increase over minimal risk. Studies that
cannot be approved in one of these categories must be reviewed by an
expert panel and approved by a high government official. While this
4th category offers important flexibility, it implies that, at least
in principle, U.S. regulations do not mandate a ceiling on the risks
to which pediatric research participants may be exposed for the
benefit of others. This reinforces the importance of considering how
we might justify exposing participants to research risks, both minimal
and greater than minimal, for the benefit of others.
Lind’s experiments on scurvy exemplify the fact that clinical
research is often conducted by clinicians and often is conducted on
patients. Many commentators have thus assumed that clinical
research should be governed by the ethics of clinical care, and the
methods of research should not diverge from the methods that are
acceptable in clinical care. On this approach, participants should not
be denied any beneficial treatments available in the clinical setting
and they should not be exposed to any risks not present in the
clinical setting.
Some proponents (Rothman 2000) argue that this approach is implied by
the kind of treatment that patients, understood as individuals who
have a condition or illness needing treatment, are owed. Such
individuals are owed treatment that promotes, or at least is
consistent with their medical interests. Others (Miller & Weijer
2006) argue that the norms of clinical research derive largely from
the obligations that bear on clinicians. These commentators argue that
it is unacceptable for a physician to participate in, or even support
the participation of her patients in a clinical trial unless that
trial is consistent with the patients’ medical interests. To do
less is to provide substandard medical treatment and to violate
one’s obligations as a clinician.
The claim that the treatment of research participants should be
consistent with the norms which govern clinical care has been applied
most prominently to the ethics of randomized clinical trials (Hellman
& Hellman 1991). Randomized trials determine which treatment a
given research participant receives based on a random process, not
based on clinical judgment of which treatment would be best for that
patient. Lind assigned the different existing treatments for scurvy to
the sailors in his study based not on what he thought was best for
them, but based on what he thought would yield an effective
comparative test. Lind did not give each intervention to the same
number of sailors because he thought that all the interventions had an
equal chance of being effective. To the contrary, he did this because
he was confident that several of the interventions were harmful and
this design was the best way to show it. Contemporary clinical
researchers go even further, assigning participants to treatments
randomly. Because this aspect of clinical research represents a clear
departure from the practice of clinical medicine it appears to
sacrifice the interests of participants in order to collect valid
data.
One of the most influential responses to this concern (Freedman 1987)
argues that randomization is acceptable when the study in question
satisfies what has come to be known as ‘clinical
equipoise.’ Clinical equipoise obtains when, for the population
of patients from which participants will be selected, the available
clinical evidence does not favor one of the treatments being used over
the others. In addition, it must be the case that there are no
treatments available outside the trial that are better than those used
in the trial. Satisfaction of these conditions seems to imply that the
interests of research participants will not be undermined in the
service of collecting scientific information. If the available data do
not favor any of the treatments being used, randomizing participants
seems as good a process as any other for choosing which treatment they
receive.
Proponents of clinical equipoise as an ethical requirement for
clinical research determine whether equipoise obtains not by appeal to
the belief states of individual clinicians, but based on whether there
is consensus among the community of experts regarding which treatment
is best. Lind believed that sea water was ineffective for the
treatment of scurvy. Yet, in the absence of agreement among the
community of experts, this view essentially constituted an individual
preference rather than a clinical norm. This suggests that it was
acceptable for Lind to randomly assign sailors under his care to the
prevailing treatments in order to test, in essence, whose preferred
treatment was the best. In this way, the existence of uncertainty
within the community of experts seems to offer a way to reconcile the
methods of clinical research with the norms of clinical medicine.
Critics respond that even when clinical equipoise obtains for the
population of patients, the specific circumstances of individual
patients within that population might imply that one of the treatments
under investigation is better for them (Gifford 2007). A specific
patient may have reduced liver function which places her at greater
risk of harm if she receives a treatment metabolized by the liver. And
some patients may have personal preferences which incline them toward
one treatment over another (e.g., they may prefer a one-time riskier
procedure to multiple, lower risk procedures which pose the same
collective risk). Current debate focuses on whether randomized
clinical trials can take these possibilities into account in a way
that is consistent with the norms of clinical medicine.
Even if the existence of clinical equipoise can justify some
randomized trials, a significant problem remains, namely, many studies
and procedures which are crucial to the identification and development
of improved methods for protecting and advancing health and well-being
are inconsistent with participants’ medical interests. This
concern arises for many phase 1 studies which offer essentially no
chance for medical benefit and pose at least some risks, and to that
extent are inconsistent with the participants’ medical
interests.
Phase 3 studies which randomize participants to a potential new
treatment or existing standard treatment, and satisfy clinical
equipoise, typically include purely research procedures, such as
additional blood draws, to evaluate the drugs being tested. Enrollment
in these studies may be consistent with participants’ medical
interests in the sense that the overall risk-benefit ratio is at
least as favorable as the available alternatives. Yet, evaluation of
the overall risk-benefit profile of the study masks the fact that it
includes individual procedures which are contrary to
participants’ medical interests, and contrary to the norms of
clinical medicine.
The attempt to protect research participants by appeal to the
obligations clinicians have to promote the medical interests of their
patients also seems to leave healthy volunteers unprotected.
Alternatively, proponents might characterize this position in terms of
clinicians’ obligations to others in general: clinicians should
not perform procedures on others unless doing so promotes the
individual’s clinical interests. This approach seems to preclude
essentially all research with healthy volunteers. For example, many
phase 1 studies are conducted in healthy volunteers to determine a
safe dose of the drug under study. These studies, vital to drug
development, are inconsistent with the principle that clinicians
should expose individuals to risks only when doing so is consistent
with their clinical interests. It follows that appeal to clinical
equipoise alone cannot render clinical research consistent with the
norms of clinical practice.
Commentators sometimes attempt to justify net-risk procedures that are
included within studies, and studies that overall pose net risks by
distinguishing between ‘therapeutic’ and
‘non-therapeutic’ research (Miller and Weijer 2006). The
claim here is that the demand of consistency with participants’
medical interests applies only to therapeutic research;
non-therapeutic research studies and procedures may diverge from these
norms to a certain extent, provided participants’ medical
interests are not significantly compromised. The distinction between
therapeutic and non-therapeutic research is sometimes based on the
design of the studies in question, and sometimes based on the
intentions of the investigators. Studies designed to benefit
participants, or investigators who intend to benefit participants are
conducting therapeutic studies. Those designed to collect
generalizable knowledge or in which the investigators intend to do so
constitute non-therapeutic research.
The problem with the distinction between therapeutic and
non-therapeutic research so defined is that research itself often is
defined as a practice designed to collect generalizable knowledge and
conducted by investigators who intend to achieve this end (Levine
1988). On this definition, all research qualifies as non-therapeutic.
Conversely, most investigators intend to benefit their participants in
some way or other. Perhaps they design the study in a way that
provides participants with clinically useful findings, or they provide
minor care not required for research purposes, or referrals to
colleagues. Even if proponents can make good on the distinction
between therapeutic and non-therapeutic research in theory, these
practices appear to render it irrelevant to the practice of clinical
research. More importantly, it is not clear why investigators’
responsibilities to patients, or patients’ claims on
investigators, should vary as a function of this distinction. Why
think that investigators are allowed to expose patients to some risks
for the benefit of others, but only in the context of research that is
not designed to benefit the participants? To apply this proposed
resolution to pediatric research, why might it be acceptable to expose
infants to risks for the benefit of others, but only in the context of
studies which offer the infants no chance for personal benefit?
To take one possibility, it is not clear that this view can be
defended by appeal to physicians’ role responsibilities. A prima
facie plausible view holds that physicians’ role
responsibilities apply to all encounters between physicians and
patients who need medical treatment. This view would imply that
physicians may not compromise patients’ medical interests when
conducting therapeutic studies, but also seems to prohibit
non-therapeutic research procedures with patients. Alternatively, one
might argue that physicians’ role responsibilities apply only in
the context of clinical care and so do not apply in the context of
clinical research at all. This articulation yields a more plausible
view, but does not support the use of the therapeutic/ non-therapeutic
distinction. It provides no reason to think that physicians’
obligations differ based on the type of research in question.
Critics argue that these problems highlight the fundamental confusion
that results when one attempts to evaluate clinical research based on
norms appropriate to clinical medicine. They instead distinguish
between the ethics of clinical research and the ethics of clinical
care, arguing that it is inappropriate to assume that investigators
are subject to the claims and obligations which apply to physicians,
despite the fact that the individuals who conduct clinical research
often are physicians (Miller and Brody 2007).
The claim that clinical research should satisfy the norms of clinical
medicine has this strong virtue: it provides a clear method to protect
individual research participants and reassure the public that they are
being so protected. If research participants are treated consistent
with their medical interests, we can be reasonably confident that
improvements in clinical medicine will not be won at the expense of
exploiting them. Most accounts of the ethics of clinical research now
recognize the limitations of this approach and search for ways to
ensure that research participants are not exposed to excessive risks
without assuming that the claims of clinical medicine apply to
clinical researchers (Emanuel, Wendler, and Grady 2000; CIOMS 2002).
Dismissal of the distinction between therapeutic and non-therapeutic
research thus yields an increase in both conceptual clarity and
concern regarding the potential for abuse of research
participants.
Clinicians, first trained as physicians taught to act in the best
interests of the patient in front of them, often struggle with the
process of exposing some patients to risky procedures for the benefit
of others. It is one thing for philosophers to insist, no matter how
accurately, that research participants are not patients and need not
be treated according to the norms of clinical medicine. It is another
thing for clinical researchers to regard research participants who are
suffering from disease and illness as anything other than patients.
These clinical instincts, while understandable and laudable, have the
potential to obscure the true nature of clinical research, as
investigators and participants alike try to convince themselves that
clinical research involves nothing more than the provision of clinical
care. One way to try to address this collective and often willful
confusion would be to identify a justification for exposing research
participants to net risks for the benefit of others.
It is often said that those working in bioethics are obsessed with the
principle of respect for individual autonomy. Advocates of this view
of bioethicists cite the high esteem accorded in the field to the
requirement of obtaining individual informed consent and the frequent
attempts to resolve bioethical challenges by citing its satisfaction.
One might assume that this view within bioethics traces to implicit
endorsement of a libertarian analysis according to which it is
permissible for competent and informed individuals to do whatever they
prefer, provided those with whom they interact are competent, informed
and in agreement. In the words of Mill, investigators should be
permitted to conduct research and expose participants to risks
provided they obtain their “free, voluntary, and undeceived
consent and participation” (On Liberty, page 11). Setting aside
the question of whether this view accurately characterizes bioethics
and bioethicists generally, it does not apply to the vast majority of
work done on the ethics of clinical research. Almost no one in the
field argues that it is permissible for investigators to conduct any
research they want provided they obtain the free and informed consent
of those they enroll.
Current research ethics does place significant weight on informed
consent and many regulations and guidelines devote much of their
length to articulating the requirement for informed consent. Yet, as
exemplified by the response to the Nuremberg Code, almost no one
regards informed consent as necessary and sufficient for ethical
research. Most regulations and guidelines, beginning with the
Declaration of Helsinki, first adopted in 1964 (World Medical
Organization 1996), allow investigators to conduct research on humans
only when it has been approved by an independent group charged with
ensuring that the study is ethically acceptable. Most regulations
further place limits on the types of research that independent ethics
committees may approve. They must find that the research has important
social value and the risks have been minimized, thereby restricting
the types of research to which even competent adults may consent. Are
these requirements justified, or are they inappropriate infringements
on the free actions of competent individuals? The importance of
answering this question goes beyond its relevance to debates over
Libertarianism. Presumably, the requirements placed on clinical
research have the effect of reducing to some extent the number of
research studies that get conducted. The fact that at least some of
the prohibited studies likely would have important social value,
helping to identify better ways to promote health and well-being,
provides a normative reason to eliminate the restrictions, unless
there is some compelling reason to retain them.
The libertarian claim that valid informed consent is necessary and
sufficient to justify exposing research participants to risks for the
benefit of others seems to imply, consistent with the first principle
of the Nuremberg Code, that research with individuals who cannot
consent is unethical. This plausible and tempting claim commits one to
the view that research with children, research in many emergency
situations, and research with the demented elderly all are ethically
impermissible. One could consistently maintain such a view but the
social costs of adopting it would be great. It is estimated, for
example, that approximately 70% of medications provided to children
have not been tested in children, even for basic safety and efficacy
(Roberts, Rodriquez, Murphy, Crescenzi 2003; Field & Behrman 2004;
Caldwell, Murphy, Butow, and Craig 2004). Absent clinical research
with children, pediatricians will be forced to continue to provide
frequently inappropriate treatment, leading to significant harms that
could have been avoided by pursuing clinical research to identify
better approaches.
One response would be to argue that the Libertarian analysis is not
intended as an analysis of the conditions under which clinical
research is acceptable. Instead, the claim might be that it provides
an analysis of the conditions under which it is acceptable to conduct
clinical research with competent adults. Informed consent is necessary
and sufficient for enrolling competent adults in research. While this
view does not imply that research with participants who cannot consent
is impermissible, it faces the not insignificant challenge of
providing an account for why such research might be acceptable.
Bracketing the question of individuals who cannot consent, many of the
limitations on clinical research apply to research with competent
adults. How might these limitations be justified? One approach would
be to essentially grant the Libertarian analysis on theoretical
grounds, but then argue that the conditions for its implementation are
rarely realized in practice. In particular, there are good reasons,
and significant empirical data, to question how often clinical
research actually involves participants who are sufficiently informed
to provide valid consent. Even otherwise competent adults often fail
to understand clinical research sufficiently to make their own
informed decisions regarding whether to enroll (Flory and Emanuel
2004).
To consider an example which is much discussed in the research ethics
literature, it is commonly assumed that valid consent for randomized
clinical trials requires individuals to understand randomization. It
requires individuals to understand that the treatment they will
receive, if they enroll in the study, will be determined by a process
which does not take into account which of the treatments is better for
them (Kupst 2003). There is an impressive wealth of data which
suggests that many, perhaps most individuals who participate in
clinical research do not understand this (Snowden 1997; Featherstone
and Donovan 2002; Appelbaum 2004). The data also suggest that these
failures of understanding often are resistant to educational
interventions.
With this in mind, one might regard the limitations on research with
competent adults as betraying the paternalism embedded in most
approaches to the ethics of clinical research (Miller and Wertheimer
2007). Although the charge of paternalism often carries with it some
degree of condemnation, there is a strong history of what is regarded
as appropriate paternalism in the context of clinical research. This
too may have evolved from clinical medicine. Clinicians are charged
with protecting and promoting the interests of the patient “in
front of them”. Clinician researchers, who frequently begin
their careers as clinicians, may regard themselves as similarly
charged. Paternalism involves interfering with the liberty of agents
for their own benefit (Feinberg 1986; see also entry on
 paternalism).
 As the terms are used in the present debate, ‘soft’
paternalism involves interfering with the liberty of an individual in
order to promote their interests on the grounds that the action being
interfered with is the result of impaired decision-making: “A
freedom-restricting intervention is based on soft paternalism only
when the target’s decision-making is substantially impaired,
when the agent lacks (or we have reason to suspect that he lacks) the
information or capacity to protect his own interests—as when
A prevents B from drinking the liquid in a glass because
A knows it contains poison but B does not” (Miller
& Wertheimer 2007). ‘Hard’ paternalism, in contrast,
involves interfering with the liberty of an individual in order to
promote their interests, despite the fact that the action being
interfered with is the result of an informed and voluntary choice by a
competent individual.
If the myriad restrictions on clinical research were justified on the
basis of hard paternalism they would represent restrictions on
individuals’ autonomous actions. However, the data on the extent
to which otherwise competent adults fail to understand what they need
to understand to provide valid consent suggests that the limitations
can instead be justified on the grounds of soft paternalism. This
suggests that while the restrictions may limit the liberty of adult
research participants, they do not limit their autonomy. In this way,
one may regard many of the regulations on clinical research not as
inconsistent with the libertarian ideal, but instead as starting from
that ideal and recognizing that otherwise competent adults often fail
to attain it.
Even if most research participants had sufficient understanding to
provide valid consent, it would not follow that there should be no
limitations on research with competent adults. The conditions on what
one individual may do to another are not exhausted by what the second
individual consents to. Perhaps some individuals may choose for
themselves to be treated with a lack of respect, even tortured. It
does not follow that it is acceptable for me or you to treat them
accordingly. As independent moral agents we need sufficient reason to
believe that our actions, especially the ways in which we treat
others, are appropriate, and this evaluation concerns, in typical
cases, more than just the fact that the affected individuals consented
to them.
Understood in this way, many of the limitations on the kinds of
research to which competent adults may consent are not justified, or
at least not solely justified, on paternalistic grounds. Instead,
these limitations point to a crucial and often overlooked concern in
research ethics. The regulations for clinical research often are
characterized as protecting the participants of research from harm.
Although this undoubtedly is an important and perhaps primary function
of the regulations, they also have an important role in limiting the
extent to which investigators harm research participants, and limiting
the extent to which society supports and benefits from a process which
inappropriately harms others. It is not just that research
participants should not be exposed to risk of harm without compelling
reason. Investigators should not expose them to such risks without
compelling reason, and society should not support and benefit from
their doing so.
This aspect of the ethics of clinical research has strong connections
with the view that the obligations of clinicians restrict what sort of
clinical research they may conduct. On that view, it is the fact that
one is a physician and is obligated to promote the best interests of
those with whom one interacts professionally which determines what one
is allowed to do to participants. This connection highlights the
pressing questions that arise once we attempt to move beyond the view
that clinical research is subject to the norms of clinical medicine.
There is a certain plausibility to the claim that a researcher is not
acting as a clinician and so may not be subject to the obligations
that bear on clinicians. Or perhaps we might say that the
researcher/subject dyad is distinct from the physician/patient dyad
and is not necessarily subject to the same norms. But, once we
conclude that we need an account of the ethics of clinical
research, distinct from the ethics of clinical care, one is left with
the question of which limitations apply to what researchers may do to
research participants.
It seems clear that researchers may not expose research participants
to risks without sufficient justification, and also clear that this
claim applies even to those who provide free and informed consent. The
current challenge then is to develop an analysis of the conditions
under which it is acceptable for investigators to expose participants
to risks and determine to what extent current regulations need to be
modified to reflect this analysis. To consider briefly the extent of
this challenge, and to underscore and clarify the claim that the
ethics of clinical research go beyond the protection of research
participants to include the independent consideration of what
constitutes appropriate behavior on the part of investigators,
consider an example.
Physical and emotional abuse cause enormous suffering, and a good deal
of research is designed to study various methods to reduce instances
of abuse and also to help victims recover from being abused. Imagine
that a team of investigators establishes a laboratory to promote the
latter line of research. The investigators will enroll consenting
adults and, to mimic the experience of extended periods of abuse in
real life, they will abuse their participants emotionally and
physically for a week. The abused participants will then be used in
studies to evaluate the efficacy of different methods for helping
victims to cope with the effects of abuse.
The proper response to this proposal is not to claim that the study is
acceptable because the participants are competent and they gave
informed consent. The proper response is to point out that, while
these considerations are undoubtedly important, they do not
establish that the study is ethically acceptable. One needs to
consider many other things. Is the experiment sufficiently similar to
real life abuse that its results will have external validity? Are
there less risky ways to obtain the same results? Finally, even if
these questions are answered in a way that supports the research, the
question remains whether investigators may ethically treat their
participants in this way. The fact that essentially everyone working
in research ethics would hold that this study is
unethical—investigators are not permitted to treat participants
in this way—suggests that research ethics, both in terms of how
it is practiced and possibly how it should be practiced, goes beyond
respect for individual autonomy to include independent standards on
investigator behavior. Defining those standards represents one of the
more important challenges for research ethics.
As exemplified by Lind’s experiments on treatments for scurvy,
clinical research studies were first conducted by clinicians wondering
whether the methods they were using were effective. To answer this
question, the clinicians altered the ways in which they treated their
patients in order to yield information that would allow them to assess
their methods. In this way, clinical research studies initially were
part of, but an exception to standard clinical practice. As a result,
clinical research came to be seen as an essentially unique activity.
And widespread recognition of clinical research’s scandals and
abuses led to the view that this activity needed its own extensive
regulations.
More recently, some commentators have come to question the view that
clinical research is a unique human activity, as well as the
regulations and guidelines which result from this view. In particular,
it has been argued that this view has led to overly restrictive
requirements on clinical research, requirements that hinder
scientists’ ability to improve medical care for future patients,
and also fail to respect the liberty of potential research
participants. This view is often described in terms of the claim that
many regulations and guidelines for clinical research are based on an
unjustified ‘research exceptionalism’ (Wertheimer
2010).
The central ethical concern raised by clinical research involves the
practice of exposing participants to risks for the benefit of others.
Yet, as noted previously, we are constantly exposing individuals to
risks for our benefit and the benefit of others. When you drive to the
store, you expose your neighbors to some increased risk of pollution
for the benefits you derive from shopping; speeding ambulances expose
pedestrians to risks for the benefit of the patients they carry;
factories expose their workers to risks for the benefit of their
customers; charities expose volunteers to risks for the benefit of
recipients. Despite this similarity, clinical research is widely
regarded as ethically problematic and is subject to significantly
greater regulation, review, and oversight (Wilson and Hunter 2010).
Almost no one regards driving, ambulances, charities, or factories as
inherently problematic. Even those who are not great supporters of a
given charity do not argue that it treats its volunteers as guinea
pigs, despite exposing them to risks for the benefit of others. And no
one argues that charitable activities should satisfy the requirements
that are routinely applied to clinical research, such as the
requirements for independent review and written consent based on an
exhaustive description of the risks and potential benefits of the
activity, its purpose, duration, scope, and procedures.
Given that many activities expose some to risks for the benefit of
others, yet are not subject to such extensive regulation, some
commentators conclude that many of the requirements for clinical
research are unjustified (Sachs 2010, Stewart et al. 2008, and
Sullivan 2008). This work is based on the assumption that, when it
comes to regulation and ethical analysis, we should treat clinical
research as we treat other activities in daily life which involve
exposing some to risks for the benefit of others. This assumption
leads to a straightforward solution to the central ethical problem
posed by clinical research.
Exposing factory workers to risks for the benefit of others is deemed
ethically acceptable when they agree to do the work and are paid a
fair wage. The solution suggested for the ethical concern of
non-beneficial research is to obtain consent and pay research
participants a fair wage for their efforts. This view is much less
restrictive than current regulations for clinical research, but seems
to be less permissive than a Libertarian analysis. The latter
difference is evident in claims that research studies should treat
participants fairly and not exploit them, even if individuals consent
to being so treated.
The gap between this approach and the traditional view of research
ethics is evident in the fact that advocates of the traditional view
tend to regard payment of research participants as exacerbating rather
than resolving its ethical concerns, raising, among others, worries of
undue inducement and commodification. Those who are concerned about
research exceptionalism, in contrast, tend to regard payment as it is
regarded in most other contexts in daily life: some is good and more
is better.
The claims of research exceptionalism have led to valuable discussion
of the extent to which clinical research differs from other activities
which pose risks to participants for the benefit of others and whether
any of the differences justify the extensive regulations and
guidelines standardly applied to clinical research. Proponents of
research exceptionalism who regard many of the existing regulations as
unjustified face the challenge of articulating an appropriate set of
regulations for clinical research. While comparisons to factory work
provide a useful lens for thinking about the ethics of clinical
research, it is not immediately obvious what positive recommendations
follow from this perspective. After all, it is not as if there is
general consensus regarding the regulations to which industry should
be subject. Some endorse minimum wage laws; others oppose them. There
are further arguments over whether workers should be able to unionize;
whether governments should set safety standards for industry; whether
there should be rules protecting workers against discrimination.
A few commentators (Caplan 1984; Harris 2005; Heyd 1996) try to
justify exposing research participants to risks for the benefit
of others by citing an obligation to participate in clinical research.
At least all individuals who have access to medical care have
benefited from the efforts of previous research participants in the
form of effective vaccines and better medical treatments. One might
try to argue that these benefits obligate us to participate in
clinical research when its our turn.
Current participation in clinical research typically benefits future
patients. However, if we incur an obligation for the benefits that are
due to previous research studies, we presumably are obligated to the
patients who participated in those studies, an obligation we cannot
discharge by participating in current studies. This approach also does
not provide a way to justify the very first clinical trials, such as
Lind’s, which of necessity enrolled participants who had never
benefited from previous clinical research.
Alternatively, one might argue that the obligation to participate does
not trace to the benefits we receive from the efforts of previous
research participants. Rather, the obligation is to the overall social
system of which clinical research is a part (Brock 1994). For example,
one might argue that individuals acquire this obligation as the result
of being raised in the context of a cooperative scheme or society. We
are obligated to do our part because of the many benefits we have
enjoyed as a result of living within such a scheme.
The first challenge for this view is to explain why the mere enjoyment
of benefits, without some prospective agreement to respond in kind,
obligates individuals to help others. Presumably, your doing a nice
thing for me yesterday, without my knowledge or invitation, does not
obligate me to do you a good turn today. This concern seems even
greater with respect to pediatric research. Children certainly benefit
from previous research studies, but typically do so unknowingly and
often with vigorous opposition. The example of pediatric research
makes the further point that justification of clinical research
on straightforward contractualist grounds will be difficult at best.
Contract theories have difficulties with those groups, such as
children, who do not accept in any meaningful way the benefits of the
social system under which they live (Gauthier 1990).
In a Rawlsian vein, one might try to establish an obligation to
participate in clinical research based on the choices individuals
would make regarding the structure of society from a position of
ignorance regarding their own place within that society, from behind a
veil of ignorance (Rawls 1999). To make this argument, one would have
to modify the Rawlsian argument in several respects. The knowledge
that one is currently living could well bias one’s decision
against the conduct of clinical research. Those who know they are
alive at the time the decision is being made have already reaped many
of the benefits they will receive from the conduct of clinical
research.
To avoid these biases, we might stretch the veil of ignorance to
obscure the generation to which one belongs—past, present or
future (Brock 1994). Under a veil of ignorance so stretched,
individuals might choose to participate in clinical research as long
as the benefits of the practice exceed its overall burdens. One could
then argue that justice as fairness gives all individuals an
obligation to participate in clinical research when their turn comes.
This approach seems to have the advantage of explaining why we can
expose even children to some risks for the benefit of others, and why
parents can give permission for their children to participate in such
research. This argument also seems to imply not simply that clinical
research is acceptable, but that, in a range of cases, individuals
have an obligation to participate in it. It implies that adults whose
turn has come are obligated to participate in clinical research,
although for practical reasons we might refrain from forcing them to
do so.
This justification for clinical research faces several challenges.
First, Rawlsian arguments typically are used to determine the basic
structure of society, that is, to determine a fair arrangement of the
basic institutions within the society (Rawls 1999). If the structure
of society meets these basic conditions, members of the society cannot
argue that the resulting distribution of benefits and burdens is
unfair. Yet, even when the structure of society meets the conditions
for fairness, it does not follow that individuals are obligated to
participate in the society so structured. Competent adults can decide
to leave a society that meets these conditions rather than enjoy its
benefits (whether they have any better places to go is another
question). The right of exit suggests that the fairness of the system
does not generate an obligation to participate, but rather defends the
system against those who would argue that it is unfair to some of the
participants over others. At most, then, the present argument can show
that it is not unfair to enroll a given individual in a research
study, that this is a reasonable thing for all individuals, including
those who are unable to consent.
Second, it is important to ask on what grounds individuals behind the
veil of ignorance make their decisions. In particular: are these
decisions constrained or guided by moral considerations? (Dworkin
1989; Stark 2000). It seems plausible to think that they would be.
After all, we are seeking the ethical approach or policy with respect
to clinical research. The problem, then, is that the answer we get in
this case may depend significantly on which ethical constraints are
built into the system, rendering the approach question begging. Most
importantly, we are considering whether it is ethical to expose
individuals who cannot consent to risks for the benefit of others. If
it isn’t, then it seems that this should be a limitation on the
choices individuals can make from behind the veil of ignorance, in
which case appeal to those choices will not be able to
justify pediatric research, nor  research with incompetent
adults. And if this research is ethical it is unclear why we need this
mechanism to justify it.
Proponents might avoid this dilemma by assuming that individuals
behind the veil of ignorance will make decisions based purely on
self-interest, unconstrained by moral limits or considerations.
Presumably, many different systems would satisfy this requirement. In
particular, the system that produces the greatest amount of benefits
overall may well be one that we regard as unethical. Many endorse the
view that clinical research studies which offer no potential benefit
to participants and pose a high chance of serious risk, such as death,
are unethical, independent of the magnitude of the social value to be
gained. For example, almost all research ethicists would regard as
unethical a study which intentionally infects a few participants
with the HIV virus, even when the study offers the potential to
identify a cure for AIDS. Yet, individuals behind the veil of
ignorance who make decisions based solely on self-interest might well
allow this study on the grounds that it offers a positive cost-benefit
ratio overall: the high risks to a few participants are clearly
outweighed by the potential to save the lives of millions.
The question here is not whether a reasonable person would choose to
make those who are badly off even worse off in order to elevate the
status of those more privileged. Rather, both options involve some
individuals being in unfortunate circumstances, namely, infected with
the HIV virus. The difference is that the one option (not conducting
the study) involves many more individuals becoming infected over time,
whereas the other option involves significantly fewer individuals
being infected, but some as the result of being injected in the
process of identifying a cure. Since the least desirable circumstances
(being infected with HIV) are the same in both cases, the reasonable
choice, even if one endorses the maximin strategy, seems to be
whichever option reduces the total number of individuals who are in
those circumstances, revealing that, in the present case at least, the
Rawlsian approach seems not to take into account the way in which
individuals end up in the positions they occupy.
Limits on risks are a central part of almost all current research
regulations and guidelines. With respect to those who can consent,
there is an essentially implicit agreement that the risks should not
be too high (as noted earlier, some argue that there should not
be any net risks to even competent adults in the context of so-called
therapeutic research). However, there is no consensus regarding how to
determine which risks are acceptable in this context. With respect to
those who cannot consent, many commentators argue that clinical
research is acceptable when the net risks are very low. The challenge,
currently faced by many in clinical research, is to identify a
standard, and find a reliable way to implement it, for what
constitutes a sufficiently low risk in this context. An interesting
and important question in this regard is whether the level of
acceptable risks varies depending on the particular class of
individuals who cannot consent. Is the level of acceptable risks the
same for individuals who were once competent, such as previously
competent adults with Alzheimer disease, individuals who are not now
but are expected to become competent, such as healthy children, and
individuals who are not now and likely never will be competent, such
as individuals born with severe cognitive disabilities?
Some argue that the risks of clinical research qualify as sufficiently
low when they are ‘negligible’, understood as risks that
do not pose any chance of serious harm (Nicholson 1986). Researchers
who ask children a few questions for research purposes may expose them
to risks no more worrisome than that of being mildly upset for a few
minutes. Exposing participants to a risk of minor harm for the benefit
of others does not seem to raise ethical concern. Or one might argue
that the ethical concerns it raises do not merit serious ethical
concern. Despite the plausibility of these views, very few studies
satisfy the negligible risk standard. Even routine procedures that are
widely accepted in pediatric research, such as single blood draws,
pose some, typically very low risk of more than negligible harm.
Others (Kopelman 2000; Resnik 2005) define risks as sufficiently low
or ‘minimal’ when they do not exceed the risks individuals
face during the performance of routine examinations. This standard
provides a clear and quantifiable threshold for acceptable risks. Yet,
the risks of routine medical procedures for healthy individuals are so
low that this standard seems to prohibit intuitively acceptable
research. This approach faces the additional problem that, as the
techniques of clinical medicine become safer and less invasive,
increasing numbers of procedures used in clinical research would
move from acceptable to unacceptable. And, at a theoretical level, one
might wonder why we should think that the risks we currently happen to
accept in the context of clinical care for healthy children should
define the level of risk that is acceptable in clinical research. Why
think that the ethical acceptability of a  blood draw in
pediatric research depends on whether clinicians still use blood draws
as part of clinical screening for healthy children?
Many guidelines (U.S. Department of Health and Human Services 2005;
Australian National Health and Medical Research Council 1999) and
commentators take the view that clinical research is ethically
acceptable as long as the net risks do not exceed the risks
individuals face in daily life. Many of those involved in clinical
research implicitly assume that this minimal risk standard is
essentially equivalent to the negligible risk standard. If the risks
of research are no greater than the risks individuals face in daily
life, then the research does not pose risk of any serious harm. As an
attitude toward many of the risks we face in daily life, this view
makes sense. We could not get through the day if we were conscious of
all the risks we face. Crossing the street poses more risks than one
can catalog, much less process readily. When these risks are
sufficiently low, psychologically healthy individuals place them in
the cognitive background, ignoring them unless the circumstances
provide reason for special concern (e.g. one hears a siren, or sees a
large gap in the sidewalk).
Paul Ramsey reports that members of the US National Commission often
used the terms ‘minimal’ and ‘negligible’ in a way
which seemed to imply that they were willing to allow minimal risk
research, even with children, on the grounds that it poses no chance
of serious harm (Ramsey 1978). The members then went on to argue that
an additional ethical requirement for such research is a guarantee of
compensation for any serious research injuries. This approach to
minimal risk pediatric research highlights nicely the somewhat
confused attitudes we often have toward risks, especially those of
daily life.
We go about our daily lives as though harms with very low probability
are not going to occur, effectively treating low probability harms as
zero probability events. To this extent, we are not Bayesians about
the risks of daily life. We treat some possible harms as impossible
for the purposes of getting through the day. This attitude, crucial to
living our lives, does not imply that there are no serious risks in
daily life. The fact that our attitude toward the risks of everyday
life is justified by its ability to help us to get through the day
undermines its ability to provide an ethical justification for
exposing research participants to the same risks in the context of
non-beneficial research (Ross & Nelson 2006).
First, the extent to which we ignore the risks of daily life is not a
fully rational process. In many cases, our attitude regarding risks is
a function of features of the situation that are not correlated
directly with the risk level, such as our perceived level of control
and our familiarity with the activity (Tversky, Kahneman 1974;
Tversky, Kahneman 1981; Slovic 1987; Weinstein 1989). Second, to the
extent that the process of ignoring some risks is rational, we are
involved in a process of determining which risks are worth paying
attention to. Some risks are so low that they are not worth paying
attention to. Consideration of them would be more harmful (would cost
us more) than the expected value of being aware of them in the first
place.
To some extent, then, our attitudes in this regard are based on a
rational cost/benefit analysis. To that extent, these attitudes do not
provide an ethical argument for exposing research participants to
risks for the benefit of others. The fact that the costs to an
individual of paying attention to a given risk in daily life are
greater than the benefits to that individual does not seem to have any
relevance for what risks we may expose them to for the benefit of
others. Finally, there is a chance of serious harm from many of the
activities of daily life. This reveals that the ‘risks of daily
life’ standard does not preclude the chance of some participants
experiencing serious harm. Indeed, one could put the point in a much
stronger way. Probabilities being what they are, the risks of daily
life standard implies that if we conduct enough minimal risk research
eventually a few participants will die and scores will suffer
permanent disability.
As suggested above, a more plausible line of argument would be to
defend clinical research that poses minimal risk on the grounds
that it does not increase the risks to which participants are
exposed. It seems plausible to assume that at any given time an
individual will either be participating in research or involved in the
activities of daily life. But, by assumption, the risks of the two
activities are essentially equivalent, implying that enrollment in the
study, as opposed to allowing the participant to continue to
participate in the activities of daily life does not increase the
risks to which he is exposed.
The problem with this argument is that the risks of research often are
additive rather than substitutive. For example, participation in a
study might require the participant to drive to the clinic for a
research visit. The present defense succeeds to the extent that this
trip replaces another trip in the car, or some similarly risky
activity in which the participant would have been otherwise involved.
In practice, this often is not the case. The participant instead may
simply put off the trip to the mall until after the research visit. In
that case, the participant’s risk of serious injury from a car
trip may be doubled as a result of her participation in research.
Moreover, we accept many risks in daily life because the relevant
activities offer those who pursue them a chance of personal benefit.
We allow children to take the bus because we assume that the benefits
of receiving an education justify the risks. The fact that we accept
these risks given the potential benefits provides no reason to think
that the same risks or even the same level of risk would be acceptable
in the context of an activity which offers no chance of medical
benefit. Finally, applied strictly, this justification seems to imply
that investigators should evaluate what risks individuals would face
if they did not enroll in the research, and enroll only those who
would otherwise face similar or greater levels of risk.
In one of the most influential papers in the history of research
ethics, Hans Jonas (1969) argues that the progress clinical research
offers is normatively optional, whereas the need to protect
individuals from the harms to which clinical research exposes them is
mandatory. He writes:
Jonas’s view does not imply that clinical research is
necessarily unethical, but the conditions on when it may be conducted
are very strict. This argument may seem plausible to the extent that
one regards, as Jonas does, the benefits of clinical research to be
ones that make an acceptable state in life even better. The example of
arthritis cited by Jonas characterizes this view. Curing arthritis,
like curing dyspepsia, baldness, and the minor aches and pains of
living and aging, may be nice, but may be thought to address no
profound problem in our lives. If this were all that clinical research
had to offer, we might be reluctant to accept many risks in order to
achieve its goals. We should not, in particular, take much chance of
wronging individuals, or exploiting them to realize these goals.
This argument makes sense to the extent that one regards the status
quo as acceptable. Yet, without further argument, it is not clear why
one should accept this view; it seems almost certain that those
suffering from serious illness that might be addressed by future
research will not accept it. Judgments regarding the present state of
society concern very general level considerations and a determination
that society overall is doing fairly well is consistent with many
individuals suffering terrible diseases. Presumably, the suffering of
these individuals provides some reason to conduct clinical research.
In response, one might understand Jonas to be arguing that the present
state of affairs involves sufficiently good medicine and adequately
flourishing lives such that the needs which could now be addressed by
additional clinical research are not of sufficient importance to
justify the risks raised by conducting it. It might have been the
case, at some point in the past, that life was sufficiently nasty,
brutish and short to justify running the risk of exploiting research
participants in the process of identifying ways to improve the human
lot. But, we have advanced, in part thanks to clinical research,
well beyond that point. This reading need not interpret Jonas as
ignoring the fact that there remain serious ills to be cured. Instead,
he might be arguing that these ills, while real and unfortunate, are
not of sufficient gravity, or perhaps prevalence to justify the risks
of conducting clinical research.
This view implicitly expands the ethical concerns raised by clinical
research. We have been focusing on the importance of protecting
individual research participants. However, Jonas assumes that clinical
research also threatens society in some sense. There are at least two
possibilities here. First, it might be thought that the conduct of
unethical research reaches beyond individual investigators to taint
society as a whole. This does not seem unreasonable given that
clinical research typically is conducted in the name of and often for
the benefit of society. Second, one might be concerned that allowing
investigators to expose research participants to some risks for the
benefit of others might put us on a slippery slope that ends with
serious abuses throughout society.
An alternative reading would be to interpret Jonas as arguing from a
version of the active-passive distinction. It is often claimed that
there is a profound moral difference between actively causing harm
versus merely allowing harm to occur, between killing someone versus
allowing them to die, for example. Jonas often seems to appeal to this
distinction when evaluating the ethics of clinical research. The idea
is that conducting clinical research involves investigators actively
exposing individuals to risks of harm and, when those harms are
realized, it involves investigators actively harming them. The
investigator who injects a participant with an experimental
medication actively exposes the individual to risks for the
benefit of others and actively harms, perhaps even kills those who
suffer harm as a result. And, to the extent that clinical research is
conducted in the name of and for the benefit of society in general,
one can say without too much difficulty that society is complicit in
these harms. Not conducting clinical research, in contrast, involves
our allowing individuals to be subject to diseases that we might
otherwise have been able to avoid or cure. And this situation, albeit
tragic and unfortunate, has the virtue of not involving clear moral
wrongdoing.
The problem with at least this version of the argument is that the
benefits of clinical research often involve finding safer ways to
treat disease. The benefits of this type of clinical research, to the
extent they are realized, involve clinicians being able to provide
less harmful, less toxic medications to patients. Put differently,
many types of clinical research offer the potential to identify
medical treatments which harm patients less than current ones. This
not an idle goal. One study found that the incidence of serious
adverse events from the appropriate use of clinical medications (i.e.
excluding such things as errors in drug administration, noncompliance,
overdose, and drug abuse) in hospitalized patients was 6.7%. The same
study, using data from 1994, concludes that the approved and properly
prescribed use of medications is likely the 5th leading
cause of death in the US (Lazarou, Pomeranz, and Corey 1998).
These data suggest that the normative calculus is significantly more
complicated than the present reading of Jonas suggests. The question
is not whether it is permissible to risk harming some individuals in
order to make other individuals slightly better off. Instead, we have
to decide how to trade off the possibility of clinicians exposing
patients to greater risks of harm (albeit with a still favorable
risk-benefit ratio) in the process of treating them versus clinical
researchers exposing participants to risk of harm in the process of
trying to identify improved methods to treat others. This is not to
say that there is no normative difference between these two
activities, only that that difference is not accurately described as
the difference between harming individuals versus improving their lot
beyond some already acceptable status quo. It is not even a difference
between harming some individuals versus allowing other individuals to
suffer harms. The argument that needs to be made is that harming
individuals in the process of conducting clinical research potentially
involves a significant moral wrong not present when clinicians harm
patients in the process of treating them.
The primary concern here is that, by exposing participants to risks of
harm, the process of conducting clinical research involves the threat
of exploitation of a particular kind. It runs the risk of
investigators treating persons as things, devoid of any interests of
their own. The worry here is not so much that investigators and
participants enter together into the shared activity of clinical
research with different, perhaps even conflicting goals. The concern
is rather that, in the process of conducting clinical research,
investigators treat participants as if they had no goals at all or,
perhaps, that any goals they might have are normatively
irrelevant.
Jonas argues that this concern can be addressed, and the process of
experimenting on some to benefit others made ethically acceptable,
only when the research participants share the goals of the research
study. Ethically appropriate research, on Jonas’s view, is
marked by: “appropriation of the research purpose into the
person’s own scheme of ends” (Jonas 1969, 236). And
assuming that it is in one’s interests to achieve one’s,
at least, proper goals, it follows that, by participating in research,
participants will be acting in their own interests, despite the fact
that they are thereby being exposed to risky procedures which are
performed to collect information to benefit others.
Jonas claims in some passages that research participants, at least
those with an illness, can share the goals of a clinical research
study only when they have the condition or illness under study (Jonas
1969). These passages reveal something of the account of human
interests on which Jonas’s arguments rely. On standard
preference satisfaction accounts of human interests, what is in a
given individual’s interests depends on what the individual
happens to want or prefer, or the goals the individual happens to
endorse, or the goals the individual would endorse in some idealized
state scrubbed clean of the delusions, misconceptions and confusion
which inform their actual preferences (Griffin 1986). On this view,
participation in clinical research would promote an individual’s
interests as long as she was well informed and wanted to participate.
This would be so whether or not she had the condition being studied.
Jonas’s view, in contrast, seems to be that there are objective
conditions under which individuals can share the goals of a given
research study. They can endorse the cause of curing or at least
finding treatments for Alzheimer disease only if they suffer from the
disease themselves.
One possible objection would be to argue that there are many reasons
why an individual might endorse the goals of a given study, apart
from having the disease themselves. One might have family members
with the disease, or co-religionists, or have adopted improved
treatment of the disease as an important personal goal. The larger
question is whether participants endorsing the goals of a clinical
research study is a necessary condition on its acceptability. Recent
commentators and guidelines rarely, if ever, endorse this condition,
although at least some of them might be assuming that the requirement
to obtain free and informed consent will ensure its satisfaction. It
might be assumed, that is, that competent, informed, and free
individuals will enroll in research only when they share the goals of
the study in question.
Jonas was cognizant of the extent to which the normative concerns
raised by clinical research are not exhausted by the risks to which
participants are exposed, but also include the extent to which
investigators and by implication society are the agents of their
exposure to risks. For this reason, he recognized that the libertarian
response is inadequate, even with respect to competent adults who
truly understand. Finally, to the extent Jonas’s claims rely on
an objective account of human interests, one may wonder whether he
adopts an overly restrictive one. Why should we think, on an objective
account, that individuals will have an interest in contributing to the
goals of a given study only when they have the disease it addresses?
Moreover, although we will not pursue the point here, appeal to an
objective account of human interests raises the possibility of
justifying the process of exposing research participants to risks for
the benefit of others on the grounds that contributing to valuable
projects, including presumably some clinical research
studies, promotes (most) individuals’ interests (Wendler
2010).
The fundamental ethical challenge posed by clinical research is
whether it is acceptable to expose some to research risks for the
benefit of others. In the standard formulation, the one we have been
considering to this point, the benefits that others enjoy as the
result of participants’ participation in clinical research are
medical and health benefits, better treatments for disease, better
methods to prevent disease.
Industry funded research introduces the potential for a very different
sort of benefit and thereby potentially alters, in a fundamental way,
the moral concerns raised by clinical research. Pharmaceutical
companies typically focus on generating profit and increasing stock
price and market share. Indeed, it is sometimes argued that
corporations have an obligation to their shareholders to pursue
increased market share and share price (Friedman 1970). This approach
may well lead companies to pursue new medical treatments which have
little or no potential to improve overall health and well-being
(Huskamp 2006; Croghan and Pittman 2004). “Me-too” drugs
are the classic example here. These are drugs identical in all
clinically relevant respects to approved drugs already in use. The
development of a me-too drug offers the potential to redistribute
market share without increasing overall health and well-being.
There is considerable debate regarding how many me-too drugs there
really are and what is required for a drug to qualify as effectively
identical (Garattini 1997). For example, if the existing treatment
needs to be taken with meals, but a new treatment need not, is that a
clinically relevant advance? Bracketing these questions, a drug
company may well be interested in a drug which clearly qualifies as a
me-too drug. The company may be able, by relying on a savvy marketing
department, to convince physicians to prescribe, and consumers to
request the new one, thus increasing profit for the company without
advancing health and well-being.
The majority of clinical research was once conducted by governmental
agencies. For example, the US NIH is likely the largest governmental
sponsor of clinical research in the world. However, its research
budget has declined over the past 20 years (Mervis 2004, 2008), and it
is estimated that a majority, perhaps a significant majority of
clinical research studies are now conducted by industry: “as
recently as 1991 eighty per cent of industry-sponsored trials were
conducted in academic health centers…Impatient with the slow
pace of academic bureaucracies, pharmaceutical companies have moved
trials to the private sector, where more than seventy per cent of them
are now conducted” (Elliott 2008, Angell 2008, Miller and Brody
2005).
In addition to transforming the fundamental ethical challenge posed by
clinical research, industry sponsored research has the potential to
transform the way that many of the specific ethical concerns are
addressed within that context. For example, the possibility that
investigators and funders may earn significant amounts of money from
their participation in clinical research might, it is thought, warp
their judgment in ways that conflict with appropriate protection of
research participants (Fontanarosa, Flanagin, and DeAngelis 2005).
When applied to investigators and funders this concern calls into
question the very significant percentage of research funded by and
often conducted by for-profit organizations. Skeptics might wonder
whether the goal of making money has any greater potential to
influence judgment inappropriately compared to many other motivations
that are widely accepted, even esteemed in the context of clinical
research, such as gaining tenure and fame, impressing one’s
colleagues, or winning the Nobel Prize.
Financial conflicts of interest in clinical research point to a
tension between relying on profits to motivate research versus
insulating drug development and testing from the profit motive as a
way of protecting research participants and future patients (Psaty and
Kronmal 2008). Finally, if a company can make billions of dollars a
year from a single drug, one wonders what constitutes an appropriate
response to the participants who were vital to its development. On a
standard definition, whether a given transaction is fair depends on
the risks and burdens that each party to the transaction bears and the
extent to which others benefit from the party’s participation in
the transaction (see entry on
 exploitation).
 A series of clinical research studies can result in a company earning
tens of billions of dollars in profits. Recognizing that a fair level
of benefit is a complex function of participants’ inputs
compared to the inputs of others, and the extent to which third
parties benefit from those inputs, it is difficult to see how one
might fill in the details of this scenario to show that the typically
minimal, or non-existent compensation offered to research participants
is fair.
At the same time, addressing this potential for exploitation by
offering substantial payments to research participants who contribute
to especially lucrative studies would introduce its own set of ethical
concerns: is payment an appropriate response to the kind of
contribution made by research participants; might payment constitute
an undue inducement to participate; will payment undermine other
participants’ altruistic motivations; to what extent does
payment encourage research participants to provide misleading or false
information to investigators in order to enroll and remain in research
studies?
Like most early clinical research, James Lind’s experiments on
treatments for scurvy took place in the clinical setting, with a
physician evaluating different possible treatments in his patients.
This practice eventually led to concern that it did not offer
sufficient protection for patients who were frequently unaware that
they were involved in research. And this concern led to separating
clinical research from clinical care and subjecting it to
significantly more extensive regulations, including independent review
and extensive consent. This approach offered greater protections for
research participants and likely led to more sophisticated clinical
trials as the result of being conducted by dedicated researchers
rather than clinicians in their spare time. 
This segregation of clinical research from clinical care has also had
significant drawbacks. It makes clinical research much more expensive
and much more difficult to conduct. Given that studies are conducted
in specialized environments, this approach has also undermined to some
extent the relevance that the findings of clinical trials have for
clinical care. For example, clinical trials assessing possible new
treatments for hypertension are conducted in individuals who are aware
that the trials exist and take the time to find out what is required
to enroll. These studies frequently have trouble enrolling sufficient
numbers of participants and take years to complete. At the same time,
on the other side of the clinical research/clinical care divide,
millions of patients with hypertension who might be eligible for a
clinical trial are obtaining care from their doctors. Moreover, the
countless data points generated by these encounters, what dose did the
patient receive, did they experience any side effects, how long did
they last, end up gathering dust in the patients’ medical
records rather than being systematically collected and used to inform
future practice.
In response, commentators have called for developing learning health
care systems. According to one definition, a learning health care
system is one in which “science, informatics, incentives, and
culture are aligned for continuous improvement and innovation, with
best practices seamlessly embedded in the care process, patients and
families active participants in all elements, and new knowledge
captured as an integral by-product of the care experience”
(Committee on Learning 2013). The important point for present purposes
is that these commentators are calling for the desegregation of
clinical research and clinical care, with clinical research once again
being conducted in the context of clinical care. This approach offers
what seems a potentially appealing response to the central challenge
of justifying the risks to which research participants are exposed.
Put simply, this practice would be justified by the fact that the
practice of exposing individuals to research risks is an essential
component of a learning health care system which continuously
evaluates methods of providing clinical care and passes the benefits
of the improvements on to its members (Faden et al 2013).
The segregated model of clinical research was designed to protect
research participants from exploitation. The primary drawbacks are
that it is inefficient and it raises concerns over free riders,
allowing patients to realize the benefits of improved clinical care
without having to accept any of the risks associated with its
generation. Learning health care systems are intended to address the
problem of inefficiency. Given that all the members of learning health
care systems are possible research participants and also beneficiaries
of research, they may, in the process, address concerns over free
riders. 
The ethical challenge learning health care systems face is whether it
is possible to do away with the segregated model, along with its
regulations and practices, without reintroducing the potential for
participant exploitation. For example, should individuals in a
learning health care system be told that their data might be used for
research purposes? Should they be notified when it is? To what extent
can patients in learning health care systems be exposed to added risks
for the purposes of research? Should they be permitted to decline
being so exposed? In the end, then, on-going attempts to address the
concerns raised by clinical research raise new ethical concerns and,
thereby, offer opportunities for philosophers and others looking for
interesting, not to mention practically important issues in need of
analysis and resolution. 