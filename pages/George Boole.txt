George Boole was born November 2, 1815 in Lincoln, Lincolnshire,
England, into a family of modest means, with a father who was
evidently more of a good companion than a good breadwinner. His father
was a shoemaker whose real passion was being a devoted dilettante in
the realm of science and technology, one who enjoyed participating in
the Lincoln Mechanics’ Institution; this was essentially a community
social club promoting reading, discussions, and lectures regarding
science. It was founded in 1833, and in 1834 Boole’s father became the
curator of its library. This love of learning was clearly inherited by
Boole. Without the benefit of an elite schooling, but with a
supportive family and access to excellent books, in particular from
Sir Edward Bromhead, FRS, who lived only a few miles from Lincoln,
Boole was able to essentially teach himself foreign languages and
advanced mathematics.
Starting at the age of 16 it was necessary for Boole to find gainful
employment, since his father was no longer capable of providing for
the family. After 3 years working as a teacher in private schools,
Boole decided, at the age of 19, to open his own small school in
Lincoln. He would be a schoolmaster for the next 15 years, until 1849
when he became a professor at the newly opened Queen’s University in
Cork, Ireland. With heavy responsibilities for his parents and
siblings, it is remarkable that he nonetheless found time during the
years as a schoolmaster to continue his own education and to start a
program of research, primarily on differential equations and the
calculus of variations connected with the works of Laplace and
Lagrange (which he studied in the original French).
There is a widespread belief that Boole was primarily a
logician—in reality he became a recognized mathematician well
before he had penned a single word about logic, all the while running
his private school to care for his parents and siblings. Boole’s
ability to read French, German and Italian put him in a good position
to start serious mathematical studies when, at the age of 16, he read
Lacroix’s Calcul Différentiel, a gift from his friend
Reverend G.S. Dickson of Lincoln. Seven years later, in 1838, he would
write his first mathematical paper (although not the first to be
published), “On certain theorems in the calculus of
variations,” focusing on improving results he had read in
Lagrange’s Méchanique Analytique.
In early 1839 Boole travelled to Cambridge to meet with the young
mathematician Duncan F. Gregory (1813–1844), the editor
of the Cambridge Mathematical Journal
(CMJ)—Gregory had co-founded this journal in 1837 and
edited it until his health failed in 1843 (he died in early 1844, at
the age of 30). Gregory, though only 2 years beyond his degree in
1839, became an important mentor to Boole. With Gregory’s support,
which included coaching Boole on how to write a mathematical paper,
Boole entered the public arena of mathematical publication in
1841.
Boole’s mathematical publications span the 24 years from 1841 to 1864,
the year he died from pneumonia. Breaking these 24 years into three
segments, the first 6 years (1841–1846), the 
second 8 years (1847–1854), and the last 10 years 
(1855–1864), we find that his published work on 
logic was entirely in the middle 8 years.
In his first 6 career years, Boole published 15 mathematical papers,
all but two in the CMJ and its 1846 successor, The
Cambridge and Dublin Mathematical Journal. He wrote on standard
mathematical topics, mainly differential equations, integration and the
calculus of variations. Boole enjoyed early success in using the new
symbolical method in analysis, a method which took a differential
equation, say:
and wrote it in the form Operator\((y) =\) cos\((x)\). 
This was (formally) achieved by letting:
leading to an expression of the differential equation as:
Now symbolical algebra came into play by simply treating the operator
\(D^2 - D - 2\) as though it were an ordinary polynomial in
algebra. Boole’s 1841 paper On the integration of linear
differential equations with constant coefficients gave a nice
improvement to Gregory’s method for solving such differential
equations, an improvement based on a standard tool in algebra, partial fractions, 
which he applied to the reciprocal of differential operators like the above.
In 1841 Boole also published his first paper on invariants, a paper
that would strongly influence Eisenstein, Cayley, and Sylvester to
develop the subject. Arthur Cayley (1821–1895), the future
Sadlerian Professor in Cambridge and one of the most prolific
mathematicians in history, wrote his first letter to Boole in 1844,
complimenting him on his excellent work on invariants. He became a
close personal friend, one who would go to Lincoln to visit and stay
with Boole in the years before Boole moved to Cork, Ireland. In 1842
Boole started a correspondence with Augustus De Morgan
(1806–1871) that initiated another lifetime friendship.
In 1843 the schoolmaster Boole finished a lengthy paper on
differential equations, combining an exponential substitution and
variation of parameters with the separation of symbols method. The
paper was too long for the CMJ—Gregory, and later De
Morgan, encouraged him to submit it to the Royal Society. The first
referee rejected Boole’s paper, but the second recommended it for the
Gold Medal for the best mathematical paper written in the years
1841–1844, and this recommendation was accepted. In 1844 the
Royal Society published Boole’s paper and awarded him the Gold 
Medal—the first Gold Medal awarded by the Society to a mathematician.
The next year Boole read a paper at the annual meeting of the British
Association for the Advancement of Science at Cambridge in June 1845.
This led to new contacts and friends, in particular William Thomson
(1824–1907), the future Lord Kelvin.
Not long after starting to publish papers, Boole was eager to
find a way to become affiliated with an institution of higher learning.
He considered attending Cambridge University to obtain a degree, but
was counselled that fulfilling the various requirements would likely
seriously interfere with his research program, not to mention the
problems of obtaining financing. Finally, in 1849, he obtained a
professorship in a new university opening in Cork, Ireland. In the
years he was a professor in Cork (1849–1864) he would
occasionally inquire about the possibility of a position back in
England.
The 8 year stretch from 1847 to 1854 starts and ends with Boole’s
two books on mathematical logic. In addition Boole published 24 more
papers on traditional mathematics during this period, while only one
paper was written on logic, that being in 1848. He was awarded an
honorary LL.D. degree by the University of Dublin in 1851, and this was
the title that he used beside his name in his 1854 book on logic.
Boole’s 1847 book, Mathematical Analysis of Logic, will be
referred to as MAL; the 1854 book, Laws of Thought,
as LT.
During the last 10 years of his career, from 1855 to 1864, Boole
published 17 papers on mathematics and two mathematics books, one on
differential equations and one on difference equations. Both books were
highly regarded, and used for instruction at Cambridge. Also
during this time significant honors came in:
Unfortunately his keen sense of duty led to his walking through a
rainstorm in late 1864, and then lecturing in wet clothes. Not long
afterwards, on December 8, 1864 in Ballintemple, County Cork, Ireland,
he died of pneumonia, at the age of 49. Another paper on mathematics
and a revised book on differential equations, giving considerable
attention to singular solutions, were published post mortem.
The reader interested in excellent accounts of
Boole’s personal life is referred to Desmond MacHale’s George
Boole, His Life and Work, 1985/2014,
and the more recent book 
New Light on George Boole, 2018, by Desmond MacHale and Yvonne Cohen,
sources to which this article is greatly
indebted.
To understand how Boole developed his
algebra of logic, it is useful to review the broad
outlines of the work on the foundations of algebra that had been
undertaken by mathematicians affiliated with Cambridge University in
the 1800s prior to the beginning of Boole’s mathematical publishing
career. An excellent reference for further reading connected to this
section is the annotated sourcebook From Kant to Hilbert,
1996, by William Ewald, which contains a complete copy of Boole’s
Mathematical Analysis of Logic.
The 19th century opened in England with mathematics in the doldrums.
The English mathematicians had feuded with the continental
mathematicians over the issues of priority in the development of the
calculus, resulting in the English following Newton’s notation, and
those on the continent following that of Leibniz. One of the obstacles
to overcome in updating English mathematics was the fact that the great
developments of algebra and analysis had been built on dubious
foundations, and there were English mathematicians who were quite vocal
about these shortcomings. In ordinary algebra, it was the use of
negative numbers and imaginary numbers that caused concern. 
The first major attempt among the English to clear up the foundation
problems of algebra was the Treatise on Algebra, 1830, by
George Peacock (1791–1858). A second edition appeared as two
volumes, 1842/1845. He divided the subject into two parts, the first
part being arithmetical algebra, the algebra of the positive
numbers (which did not permit operations like subtraction in cases
where the answer would not be a positive number). The second part was
symbolical algebra, which was governed not by a specific
interpretation, as was the case for arithmetical algebra, but solely
by laws. In symbolical algebra there were no restrictions on using
subtraction, etc.
Peacock believed that in order for symbolical algebra to be a useful
subject its laws had to be closely related to those of arithmetical
algebra. In this connection he introduced his principle of the
permanence of equivalent forms, a principle connecting results in
arithmetical algebra to those in symbolical algebra. This principle has
two parts:
A fascinating use of algebra was introduced in 1814 by
François-Joseph Servois (1776–1847) when he tackled
differential equations by separating the differential operator 
from the subject, as described in an example given
above. This application of algebra captured the interest of Gregory
who published a number of papers on the method of the separation
of symbols, that is, the separation into operators and objects,
in the CMJ. He also wrote on the foundation of algebra, and
it was Gregory’s foundation that Boole embraced almost verbatim prior to writing LT.
Gregory had abandoned Peacock’s principle of the permanence of
equivalent forms in favor of three simple laws, one of which Boole
regarded as merely a notation convention. Unfortunately these laws
fell far short of what is required to justify even some of the most
elementary results in algebra, like those involving subtraction.
In On the foundation of 
algebra, 1839, the first of four papers on this 
topic by De Morgan that appeared in the Transactions of the Cambridge
Philosophical Society, one finds a tribute to the separation of
symbols in algebra, and the claim that modern algebraists usually
regard the symbols as denoting operators (e.g., the derivative
operation) instead of objects like numbers. The footnote
credits Peacock with being the first to separate what are now called 
the syntactic [technical] and the semantic [logical] aspects of algebra. 
In the second
foundations paper (in 1841) De Morgan proposed what he considered to be
a complete set of eight rules for working with symbolical algebra.
Regarding the origin of the name Boolean algebra, Charles
Sanders Peirce (1839–1914) introduced, among several other
phrases, the name Boolian algebra for the algebra that
resulted from dispensing with the arithmetical scaffolding of
Boole’s equational algebra of logic.  With the spelling
Boolean algebra this was embraced by his close friend the
Harvard philosopher Josiah Royce (1855–1916) around 1900, and
then by Royce’s students (including Norbert Wiener, Henry
M. Sheffer and Clarence I. Lewis) and in due course by other Harvard
professors and the world. It essentially referred to the modern
version of the algebra of logic introduced in 1864 by William Stanley
Jevons (1835–1882), a version that Boole had rejected in their
correspondence—see Section 5.1. For this reason the word
Boolean will not be used in this article to describe the
algebra of logic that Boole actually created; instead the name
Boole’s algebra will be used.  (See the 2015 article
“George Boole and Boolean Algebra” by Burris.)
In MAL, and more so in LT, Boole was interested in the insights
that his algebra of logic gave to the inner workings of the mind. This pursuit
has met with little favor, and is not discussed in this article.
In New Light on George Boole (2018) by MacHale and Cohen one
finds, published for the first time, (an edited version of) the
biography by MaryAnn Boole (1818–1887) of her famous brother,
and on p. 41 there is the following passage:
Boole’s final path to logic fame occurred in a curious way. In
early 1847 he was stimulated to renew his investigations into logic by
a trivial but very public dispute between De Morgan and the Scottish
philosopher Sir William Hamilton (1788–1856)—not to be
confused with his contemporary the Irish mathematician Sir William
Rowan Hamilton (1805–1865). This dispute revolved around who
deserved credit for the idea of quantifying the predicate (e.g.,
All \(A\) is all \(B\), All \(A\) is some \(B\),
etc.). MaryAnn wrote that when the true method flashed upon him in
1847, “he was literally like a man dazzled with excess of
light”.  Within a few months Boole had written his 82 page
monograph, Mathematical Analysis of Logic, first presenting
an algebraic approach to Aristotelian logic, then looking briefly at
the general theory. (Some say that this monograph and De
Morgan’s book Formal Logic appeared on the same day in
November 1847.)
We are not told what the true method was that flashed upon Boole. One
possibility is the discovery of the Expansion Theorem and the
properties of constituents.
In pages 15–59, a little more than half of the 82 pages in
MAL, Boole focused on a slight generalization of Aristotelian
logic, namely augmenting its four types of categorical propositions by
permitting the subject and/or predicate to be of the form
not-\(X\). In the chapter on conversions, such as Conversion by
Limitation—All \(X\) is \(Y\), therefore Some \(Y\) is
\(X\)—Boole found the Aristotelian classification defective in
that it did not treat contraries, such as not-\(X\), on the same
footing as the named classes \(X, Y, Z\), etc. For example, he converted No \(X\) is \(Y\) into All
\(Y\) is not-\(X\), and All X is Y into All not-Y is not-X.
For his extended version of Aristotelian logic he stated (MAL, p. 30) a set
of three transformation rules which he claimed allowed one to construct all valid
two-line categorical arguments. These transformation rules did not appear in LT. 
It is somewhat
curious that when it came to analyzing categorical syllogisms, it was only
in the conclusion that he permitted his generalized categorical propositions
to appear.
Among the vast possibilities for hypothetical syllogisms, the ones 
 that he discussed were standard, with one new example added.
The Introduction chapter of MAL starts with Boole reviewing the
symbolical method:
 The second chapter, First
Principles, lets the symbol 1 represent the Universe 
“comprehending every conceivable class of objects, whether
actually existing or not.” Capital letters \(X, Y,
Z,\ldots\) denoted classes. Then, no doubt heavily
influenced by his very successful work using algebraic techniques on
differential operators, and consistent with De Morgan’s 1839 assertion
that algebraists preferred interpreting symbols as operators, Boole
introduced the elective symbol \(x\) corresponding to the class
\(X\), the elective symbol \(y\) corresponding to
\(Y\), etc. The elective symbols denoted elective
operators—for example the elective operator red
when applied to a class would elect (select) the red items in the
class. 
Then Boole said “When no subject is expressed, we shall 
suppose 1 (the Universe) to be the subject understood”. 
He goes on to explain that \(x(1)\) is just \(X\). Evidently this means, aside
from defining the elective symbol \(x\), when presented with the term \(x\) without
a subject one is actually dealing with \(X\).
The first operation that Boole introduced was multiplication
\(xy\). The standard juxtaposition notation \(xy\) for multiplication also 
had a standard meaning for operators (for example, differential operators), 
namely one applied \(y\) to an object and then \(x\) is applied to the
result.  As pointed out by 
Theodore Hailperin (1916–2014) 
(1981, p. 176; and 1986, pp. 67,68), 
this established notation
convention handed Boole his interpretation of the multiplication \(xy\) of
elective symbols as the composition of the two operators. Thus when 
encountering
the expression \(xy\) without a subject one was dealing with the class
\(x(y(1))\), “the result being the class whose members 
are both Xs and Ys”. We call this class the intersection
of \(X\) and \(Y\). In LT Boole dropped the (unnecessary) use of elective 
symbols and simply let \(x\), \(y\) denote classes, with \(xy\) being their 
intersection. 
The first law in MAL (p. 16) was the distributive law
where Boole said that \(u+v\) corresponded to dividing a class into two parts, 
evidently meaning \(U\) and \(V\) are disjoint classes. 
This was the first mention of addition in MAL. 
He added (MAL, p. 17) the commutative law \(xy =
yx\) and the index law \(x^n = x\)—in LT the
latter would be replaced by the law of duality \(x^2 = x\)
(called the idempotent law in 1870 by the Harvard
mathematician Benjamin Peirce (1809–1880), in another
context).
After stating the above distributive and commutative laws, Boole
believed he was entitled to fully employ the ordinary algebra of his
time, saying (MAL, p. 18) that
 Boole went beyond the foundations of symbolical algebra that
Gregory had used in 1840—he added De Morgan’s 1841 single
rule of inference, that equivalent operations performed upon
equivalent subjects produce equivalent results.
 It is likely more difficult for the modern reader to come to grips
with the idea that Boole’s algebra is based on 
Common Algebra, the algebra of numbers, than would have been the case with 
Boole’s contemporaries—the modern
reader has been exposed to modern Boolean algebra (and perhaps Boolean
rings). In the mid 1800s the word algebra meant, for most mathematicians, 
simply the algebra of numbers. 
Boole’s three laws for his algebra of logic are woefully inadequate
for what follows in MAL. The reader will, for the most part,
be well served by assuming that Boole is doing ordinary polynomial
algebra augmented by the assumption that any power
\(x^n\) of an elective symbol \(x\) can be replaced by \(x\). 
One can safely assume that any polynomial equation p = q that holds 
in Common Algebra is valid
in Boole’s algebra as is any equational argument
that holds in Common Algebra.
[A note of caution: the argument 
“\(x^2 = x \therefore x = 1\) or
\(x = 0\)” is valid in Common Algebra, but it is 
not an
equational argument since the conclusion is a disjunction of
equations, not a single equation.] 
Boole’s algebra
was mainly concerned with polynomials with integer coefficients, and
with their values when the variables were restricted to taking on only
the values 0 and 1.
Some of the key polynomials in Boole’s work, 
along with their values
on \(\{0,1\}\), are presented in the following table:
Note that all of the polynomials \(p\)(x,y) in the above
table, except for addition and subtraction, take values in \(\{0,1\}\) when
the variables take values in \(\{0,1\}\). Such polynomials are called
switching functions in computer science and electrical
engineering, and as functions on \(\{0,1\}\) they are idempotent, that is,
p\(^2 =\) p. The switching functions are exactly
the idempotent polynomials in Boole’s algebra.
In Boole’s algebra, any polynomial \(p(x)\)
in one variable can be reduced to a linear polynomial \(ax + b\)
since one has
Likewise any polynomial \(p(x, y)\) can be expressed as \(axy + bx +
cy + d\). Etc.
However Boole was much more interested in the fact that \(ax + b\)
can be written as a linear combination of \(x\) and \(1-x\), namely
This gives his Expansion Theorem in one variable:
The Expansion Theorem for polynomials in two variables is
For example,
The expressions \(xy, \ldots, (1 - x)(1 - y)\), were called
the constituents of \(p(x,y)\)—it would be better to
call them the constituents of the variables \(x, y\)—and the
coefficients \(p(1,1), \ldots, p(0,0)\) were the modulii of
\(p(x,y)\).
Similar results hold for polynomials in any number of variables
(MAL, pp. 62–64), and there are three
important facts about the constituents for a given list of
variables:
In the chapter Of Expression and Interpretation, Boole
said “the class not-X will be determined by 
the symbol \(1-x\)”.
 This is the first appearance of subtraction
in MAL. 
Boole’s initial equational expressions of the
Aristotelian categorical propositions 
(MAL, pp. 21,22) will be called his
primary expressions. Then in the next several pages he adds
supplementary expressions; of these the main ones will be called the
secondary expressions.
The first primary expression given was for All \(X\) is
\(Y\), an equation which he then converted into
\(x(1-y) = 0\). This was the first appearance of
0 in MAL. It was not introduced as the symbol for the empty
class—indeed the empty class does not appear in MAL.
Evidently “\(= 0\)” performed the role of a predicate in
MAL, with an equation \(E = 0\) asserting that the class
denoted by \(E\) simply did not exist. (In LT, what we call the
empty class was introduced and denoted by 0.)
 
Syllogistic reasoning is just an exercise in elimination,
namely the middle term is eliminated from the premises to give the
conclusion. Elimination is a standard topic in the theory of
equations, and Boole borrowed a simple elimination result regarding
two equations to use in his algebra of logic—if the premises of
a syllogism involved the classes \(X, Y\), and
\(Z\), and one wanted to eliminate the middle term \(Y\),
then Boole put the equations for the two premises in the form
where \(y\) does not appear in the coefficients a,b,c,d.
The result of eliminating \(y\) in ordinary algebra gives the
equation
and this is what Boole used in MAL. Unfortunately this is a
weak elimination result for Boole’s algebra. One finds, using the
improved reduction and elimination theorems of LT, that the
best possible result of elimination is
 
Applying weak elimination to the primary equational expressions was not 
sufficient to derive all of the valid syllogisms. For example, in the cases 
where the premises had primary expressions \(ay = 0\) and \(cy = 0\),
this elimination gave \(0 = 0\), even when there was a non-trivial 
conclusion.  Boole introduced the alternative equational
expressions (see MAL, p. 32) of categorical propositions to
be able to derive all of the valid syllogisms. 
Toward the end of the chapter on categorical syllogisms there is a
long footnote (MAL, pp. 42–45) claiming
(MAL, pp. 42, 43) that secondary expressions alone are
sufficient for the analysis of [his generalization of] Aristotelian
categorical logic. The footnote loses much of its force because the
results it presents depend heavily on the weak elimination theorem
being best possible, which is not the case. Regarding the secondary
expressions, in the Postscript to MAL he says:
His justification of this claim would appear in LT. 
Indeed Boole used only the secondary expressions of MAL to 
express propositions as equations in LT, but there the 
reader will no longer find a leisurely and detailed treatment of Aristotelian logic—the discussion of this subject is delayed until 
the last chapter on logic, namely Chapter XV (the only one in LT to 
analyze particular propositions). In this chapter the application of his
algebra of logic to Aristotelian logic is presented in such a compressed form 
(by omitting all details of 
the reduction, elimination and solution steps), concluding with such long 
equations, that the reader is not likely to want to check that Boole’s analysis is correct.
On p. 48 of MAL Boole said: 
Boole analyzed the seven hypothetical syllogisms that were
standard in Aristotelian logic, from the Constructive and Destructive
Conditionals to the Complex Destructive Dilemma. Letting capital
letters \(X, Y, \ldots\) represent categorical propositions, the
hypothetical propositions traditionally involved in
hypothetical syllogisms were in one of the forms \(X\) is
true, \(X\) is false, If \(X\) is true
then \(Y\) is true, \(X\) is true or \(Y\) is true or 
…, 
as well as 
\(X\) is true and \(Y\) is true and …  
At the end of the
chapter on hypothetical syllogisms he noted that it was easy to create
new ones, and one could enrich the collection by using mixed
hypothetical propositions such as If \(X\) is true, then
either \(Y\) is true, or \(Z\) is true. 
Most important in this chapter was Boole’s 
claim that his algebra of
logic for categorical propositions was equally suited to the study of
hypothetical syllogisms. This was based on adopting the standard
reduction of hypothetical propositions to propositions about classes
by letting the hypothetical universe, also denoted by 1, 
“comprehend all conceivable
cases and conjunctures of circumstances”.
Evidently his notion of a case 
was an assignment of truth values to the propositional variables.
For \(X\) a categorical proposition Boole let \(x\) denote the elective
operator that selects the cases for which \(X\) is true.  
 Boole said the universe of a categorical proposition has two
cases, true and false. To find an equational
expression for a hypothetical proposition Boole resorted to a near
relative of truth tables (MAL, p. 50). To each case, that is,
assignment of truth values to \(X\) and \(Y\), he associated an
elective expression as follows:
These elective expressions are, of course, the constituents
of \(x\), \(y\).
Boole expressed a propositional formula \(\Phi(X,Y, \ldots)\) by
an elective equation \(\phi(x,y, \ldots)\) = 1 by ascertaining all the
distinct cases (assignments of truth values) for which the formula
holds,
and summing their corresponding elective expressions to obtain \(\phi\).
For example, the elective expression for \(X\) is true or \(Y\) is true, with
or inclusive, is thus \(xy + x(1 - y) + (1 - x)y = 1\),
which simplifies to \(x + y - xy = 1\). 
Boole did not have the modern view that a propositional formula can be
considered as a function on the truth values \(\{\rT, \rF\}\), taking 
values in \(\{\rT,\rF\}\).
 
The function viewpoint gives us an algorithm to determine which
constituents are to be summed to give the desired elective expression,
namely those constituents associated with the cases for which the
propositional formula has the value \(\rT\).  
By not viewing propositional formulas as functions on \(\{\)T, F\(\}\) Boole
missed out on being the inventor of truth tables. His algebraic
method of analyzing hypothetical syllogisms was to transform each of
the hypothetical premises into an elective equation, and then apply
his algebra of logic (which was developed for categorical
propositions). For example, the premises \(X\) is true
or \(Y\) is true, with or inclusive, and \(X\) is false 
are expressed by the equations
\(x + y - xy = 1\) and
\(x = 0\). From these it immediately follows that
\(y = 1\), giving the conclusion
\(Y\) is true.
Boole only considered rather simple hypothetical propositions on the
grounds these were the only ones encountered in common usage (see
LT, p. 172). His algebraic approach to propositional logic
is easily extended to all propositional formulas as follows. For
\(\Phi\) a propositional formula the associated elective function
\(\Phi^*\) is defined recursively as follows:
Then one has:
This looks quite different from modern propositional logic where one
takes a few tautologies, such as \(X \rightarrow(Y \rightarrow X)\), as axioms, and inference rules such as modus ponens to
form a deductive system. 
This translation, from \(\Phi\) to \(\Phi^*\), viewed as mapping
expressions from modern Boolean algebra to polynomials, would be
presented in the 1933 paper Characteristic functions and the algebra of logic
 by Hassler Whitney (1907–1989), with
the objective of showing that one does not need to learn the algebra
of logic [modern Boolean algebra] to verify the equational laws and
equational arguments of Boolean algebra—they can be translated
into the ordinary algebra with which one is familiar. Howard Aiken
(1900–1973), Director of the Harvard Computation Laboratory,
would use such translations of logical functions into ordinary algebra
in his 1951 book Synthesis of Electronic Computing and Control
Circuits, specifically stating that he preferred Boole’s
numerical function approach to that of Boolean algebra or
propositional logic.
Beginning with the chapter Properties of Elective Functions, Boole 
developed general theorems for working with elective functions and equations
 in his algebra of logic—the Expansion (or Development) 
Theorem (described above in Section 3.5) and the properties of constituents 
are discussed in this chapter. 
He used the power series expansion of an elective function in his proof of the 
one-variable 
case of the Expansion Theorem (MAL, p. 60), perhaps intending to apply it 
to rational elective functions. 
The operation of division with polynomial 
functions was introduced in MAL but never successfully developed 
in his algebra of logic—there are no equational 
laws for how to deal with division. It was abandoned in LT 
except for being 
frequently used as a mnemonic device when solving a polynomial equation. 
 
From the Expansion Theorem and the properties of constituents he showed that the
modulii of the sum/difference/product of two elective functions are
the sums/differences/products of the corresponding modulii of the two
functions. 
The Expansion Theorem is used (MAL, p. 61) to prove an important result, 
that p(x) and q(x) are equivalent in Boole’s algebra if and only if corresponding 
modulii are the same, that is, \(p(1)=q(1)\) and \(p(0)=q(0)\). This result 
generalizes to functions of several variables. It will not be stated as such in 
LT, but will be absorbed in the much more general (if somewhat opaquely 
stated) result that will be called the Rule of 0 and 1.
Using the Expansion Theorem Boole showed (MAL, p. 64)
that every elective equation \(p = 0\) is equivalent to the collection
of constituent equations \(r = 0\) where the modulus (coefficient) of
\(r\) in the expansion of \(p\) is not zero, and thus every
elective equation is interpretable. Furthermore this led
(MAL, p. 65) to the fact that \(p = 0\) is equivalent to the
equation \(q = 0\) where \(q\) is the sum of the constituents in the
expansion of \(p\) whose modulus is non-zero. 
  As examples, consider
the equations \(x + y = 0\) and \(x - y = 0\). The following table
gives the constituents and modulii of the expansions:
Thus \(x + y = 0\) is equivalent to the collection of constituent
equations 
as well as to the single equation
and \(x - y = 0\) is equivalent to the collection of constituent
equations 
as well as to the single equation
The Solution Theorem described how to solve an elective equation 
for one of its symbols in terms of the others, often introducing constraint
equations on the independent variables. In his algebra of logic he could always 
solve an elective equation for any one of its elective symbols. For example, 
the equation \(q(x)y = p(x)\) was solved by using formal division 
 \(y = p(x)/q(x)\) and then using formal expansion to obtain \(y = ax + b(1-x)\)
where \(a = p(1)/q(1)\) and \(b = p(0)/q(0)\), and then decoding the
fractional coefficients. This theorem will be discussed in more detail in 
Step 7 of Section 6.2. 
Boole’s final example (MAL, p. 78) was solving three
equations in three unknowns for one of the unknowns in terms of the
other two.  This example used a well known technique for handling side
conditions in analysis called Lagrange Multipliers—this method
(which reduced the three equations in the example to a single equation
in five unknowns) reappeared in LT (p. 117), but was only
used once.  It was superseded by the sum of squares reduction
(LT, p. 121) which does not introduce new variables.  Using
the Reduction and Elimination Theorems in LT one discovers
that Boole’s constraint equations (3) (MAL, p. 80) for
his three equation example are much too weak—each of the
products should be 0, and there are additional constraint equations.
MAL shows more clearly than LT how closely Boole’s
algebra of logic is based on Common Algebra plus the idempotent law. 
The Elimination Theorem that he borrowed from common algebra
turned out to be weaker than what his algebra offered, and his method
of reducing equations to a single equation was clumsier than the main
one used in LT, but the Expansion Theorem and Solution
Theorem were the same. One sees that MAL contained not only
the basic outline for LT, but also some parts fully
developed. Power series were not completely abandoned in LT—they
appeared, but only in a footnote (LT, p. 72).
The logic portion of 
Boole’s second logic book, An Investigation 
of The Laws of Thought
on which are founded the Mathematical Theories of Logic and
Probabilities, published in 1854, 
would 
be devoted to trying to clarify and
correct what was said in MAL, and providing more
substantial applications, the main one being his considerable work in
probability theory.
 At the end of Chapter I Boole mentioned the theoretical
possibility of using probability theory, enhanced by his algebra of
logic, to uncover fundamental laws governing society by analyzing large
quantities of social data by large numbers of (human) computers. 
Boole used lower case Latin letters at the end of the alphabet, like \(x,y,z\), to represent classes. The universe was a class, denoted by 1; and there was
a class described as “Nothing”, 
denoted by 0,
which we call the empty class. The operation of
multiplication was defined to be what we call intersection, and this led
to his first law, \(xy = yx\), and then to the
idempotent law \(x^2 = x\). Addition was introduced as
aggregation when the classes were disjoint. He stated the commutative
law for addition, \(x + y = y + x\), and the distributive law \(z(x +
y) = zx + zy\). Then followed \(x - y = - y + x\) and \(z(x - y) = zx
- zy\). The associative laws for addition and multiplication were
conspicuously absent. 
The idempotent law \(x^2 = x\) was different from Boole’s 
laws for the common algebra—it only applied to the
individual class symbols, not in general to compound terms that one
could build from these symbols.  For example, one does not have 
\((x + y)^2 = x + y\) in Boole’s system, otherwise 
 by ordinary
algebra with idempotent class symbols, this would imply \(2xy = 0\),
and then \(xy = 0\), which would force \(x\) and \(y\) to represent
disjoint classes. But it is not the case that every pair of classes is
disjoint. 
It was this equational argument, that \((x + y)^2 = x + y\) implies \(xy = 0\), that  led Boole to view addition \(x + y\) as a partial operation, only defined 
when \(xy = 0\), that is, when \(x\) and \(y\) are disjoint classes. 
The only place where he wrote down this argument was in his unpublished
notes—see 
Boole: Selected Manuscripts …, 1997,
edited by Ivor Grattan-Guiness and Gérard Bornet, pp. 91,92.
A similar equational argument, that \((x - y)^2 = x - y\) implies \(y = xy\), led 
to \(x - y\) being only defined when \(y = xy\), that is, when \(y = x \cap y\), 
which is the same as \(y \subseteq x\).
It was not until p. 66 of LT that Boole clearly informed the reader that 
addition, which had been introduced on p. 33, was a partial operation on 
classes: 
A similar statement about subtraction being a partial operation did not appear
until p. 93:
Here the “latter function” means
\(x-y\). The dispersion of relevant facts about a topic, such as definitions of
the fundamental operations of addition and subtraction, is not helpful to the 
reader. 
Another example of the need for caution when working with partial algebras is 
how important it was that Boole chose the fundamental operation minus to be 
binary subtraction, and not the unary operation of additive inverse that is 
standard in ring theory. Note that the standard operations union, symmetric
difference, intersection 
and complement of the Boolean algebra of classes are definable in Boole’s partial-algebra by totally defined terms—he used these to be able to express propositions about classes by equations:
Subtraction was needed to find a totally defined term, namely \(1-x\), 
that expresses the complement of \(x\). The term \(1+(-x)\), like \(-x\),
 is only defined for \(x=0\) in Boole’s 
partial-algebra. 
The same three equations define the Boolean algebra of classes in the standard
Boolean ring of classes where addition is symmetric difference, except in this 
case subtraction is a derived operation, namely \(1-x\) is defined to be
 \(1 + (-x)\), the unary minus 
being a fundamental operation in the Boolean ring, indeed in any ring.
One might expect that Boole was building toward claiming an axiomatic
foundation for his algebra of logic that, as he had (erroneously)
claimed in MAL,
justified using all the processes of common algebra.
Indeed he did discuss the rules of inference, that adding or
subtracting equals from equals gives equals, and multiplying equals by
equals gives equals. But then the development of an axiomatic
approach came to an abrupt halt. There was no discussion as to whether
the stated axioms (which he called laws) and rules of inference (which he
called axioms) were sufficient for  his algebra of
logic. (They were not.) Instead he simply and briefly, with
remarkably little fanfare, presented a radically new foundation for
his algebra of logic (LT pp. 37,38):
Note that this algebra restricted the values of the variables to 0 and 1,
but placed no such restriction on the values of terms. There was no assertion
that this was to be a two-element algebra. Burris and Sankappanavar (2013) 
viewed the quote as saying that this algebra was just the ordinary 
algebra of numbers modified by restricting the variables to the values 0 and 1 
to determine the validity of an argument. They called this 
Boole’s Rule of 0 and 1, 
and said he used this Rule to justify three of his main theorems 
(Expansion, Reduction, Elimination). These main theorems 
along with the Solution Theorem yielded Boole’s
General Method for discovering the strongest possible consequences of
propositional premises under certain desired constraints (such as
eliminating some of the variables). Further comments on this 
Rule are below in Section 5.2. 
In Chapter V he defended the use of uninterpretables in his
work; as part of his justification for the use of uninterpretable
steps in symbolic algebra he pointed to the well known use of
\(\sqrt{-1}\) to obtain trigonometric identities. 
Unfortunately his Principles of Symbolical
Reasoning do not, in general, apply to partial algebras, that is,
where some of the operations are only partially defined, such as
addition and subtraction in Boole’s algebra. 
Nonetheless it turns out one can prove 
that they do apply to his algebra of logic. In succeeding chapters he
gave the Expansion Theorem, the new full-strength Elimination Theorem,
an improved Reduction Theorem, and the Solution Theorem where formal division 
and formal expansion were used to solve an
equation.
Boole turned to the topic of the interpretability of a
logical function in Chapter VI Section 13. 
He had already stated in MAL that every equation is
interpretable (by showing an equation was equivalent to a collection of
constituent equations). However algebraic terms need not be interpretable, e.g.,
\(1+1\) is not interpretable. Some terms are partially interpretable,
and equivalent terms can have distinct domains of interpretability.
  In Chapter VI 
Section 13, he
comes to the conclusion that the condition for a polynomial \(p\) to be
equivalent to a (totally) interpretable function is that it
satisfy \(p^2 = p\), in which case it is equivalent to a sum of
distinct constituents, namely those belonging to the non-vanishing
modulii of \(p\). A polynomial is idempotent if and only if
all of its modulii are idempotent, that is, they are in \(\{0, 1\}\),
in which case the expansion of the polynomial is a sum of distinct
constituents (or it is 0).
Boole’s Chapter XI Of Secondary Propositions is parallel to the treatment
in MAL except that he changed from using the cases
when \(X\) is true to the times when \(X\) is
true. In Chapter XIII Boole selected arguments
of Clarke and Spinoza, on the nature of an eternal being, to put under
the magnifying glass of his algebra of logic, starting with the
comment (LT, p. 185):
One conclusion was (LT, p. 216):
In the final chapter on logic, Chapter XV, Boole presented his
analysis of the conversions and syllogisms of Aristotelian logic. 

He now considered this ancient logic to be a weak, fragmented attempt at a
logical system. 

This neglected chapter is quite interesting
because it is the only chapter where he analyzed particular
propositions, making essential use of additional letters like
\(v\) to encode some. 

This is also the chapter where he stated (incompletely) the rules for working
with some.
Boole noted in Chapter XV of LT that when a premise about \(X\) and \(Y\)
is expressed as an equation involving \(x, y\) and
\(v\), the symbol \(v\) expressed some, but
only in the context in which it appeared in the premise. For example,
All \(X\) is \(Y\) has the 
expression \(x = vy\), which implies \(vx = vy\). 
This could be interpreted as Some \(X\) is \(Y\). 
A consequence of \(vx = vy\) is \(v(1-x) = v(1-y)\). 
However it was not permitted to read
this as Some not-\(X\) is not-\(Y\) since
\(v\) did not appear with \(1-x\) or
\(1-y\) in the premise. 
In Chapter XV Boole gave the reader a brief summary of traditional
Aristotelian categorical logic, and analyzed some simple examples
using ad hoc techniques with his algebra of logic. Then he launched
into proving a comprehensive result by applying his General Method to
the pair of equations:
This was the case of like middle terms.
He permitted some of the parameters \(v, v', w, w'\) to be replaced by 1,
but not both \(v, v'\) and not both \(w, w'\) can be replaced by 1.
One could also replace \(y\) by \(1-y\) in both equations, and independently 
replace \(x\) by \(1-x\) and \(z\) by \(1-z\).
The premises of many categorical syllogisms can be expressed in
this form. His goal was to eliminate \(y\) and find expressions for
\(x, 1-x\) and \(vx\) in terms of \(z, v, v', w, w'\). 
Boole omitted writing out the reduction of the pair of equations to a single
equation, as well as the elimination of the middle term \(y\) from this 
equation and the details of applying the Solution Theorem to obtain the
desired expressions for \(x, 1-x\) and \(vx\), 
three equations involving large algebraic expressions. 
His summary of the interpretation of this rather complicated algebraic 
analysis was simply that in the case of like middle terms with at least one
middle term universal, equate the extremes. For example the premises
All \(y\) is \(x\) and Some \(z\) is \(y\) are expressed by the 
pair of equations 
Thus the conclusion equation is \(vx = wz\), which has the interpretation
Some \(x\) is \(z\). 
Then he noted that the remaining categorical syllogisms are such that 
their premises can be put in the form:
This is the case of unlike middle terms.
This led to another triple of large equations, again with details of the
derivation omitted, but briefly summarized by Boole in two recipes.
First, in the case of unlike middle terms with at least one universal
extreme, change the quantity and quality of that extreme and equate it to 
the other extreme. For example the premises
All \(x\) is not-\(y\) and Some \(z\) is \(y\) gives the 
pair of equations 
Thus the conclusion equation is \(v(1-x) = wz\), which has the interpretation
Some not-\(x\) is \(z\). 
Secondly, in the case of unlike middle terms, both of which are universal,
change the quantity and quality of one extreme and equate it to 
the other extreme. For example the premises
All not-\(y\) is \(x\) and All \(y\) is \(z\) gives the 
pair of equations 
Thus one conclusion equation is \(1-x = wz\), which has the interpretation 
All not-\(x\) is \(z\).
The other is \(vx = 1-z\), which has the interpretation
All not-\(z\) is \(x\). Each of these two propositions is just the
conversion by negation of the other.
 
Boole noted (LT p. 237) that: 
Many objections to Boole’s system have been published over the years;
four among the most important concern:
For example, Boole’s use of \(v\) in the
equational expression of propositions has been a long-standing bone of
contention. 
Ernst Schröder (1841–1902) 
argued in Volume II of his
Algebra der Logik (1891, p. 91) that the particular 
 propositions about classes simply could not be expressed by equations in the 
algebra of logic. 
We look at a different objection, namely at the Boole/Jevons dispute
over adding \(x + x = x\) as a law. 
[The following details are from The development of the theories
of mathematical logic and the principles of mathematics, William
Stanley Jevons, by Philip Jourdain, 1914.]
In an 1863 letter to Boole regarding a draft of a commentary on
Boole’s system that Jevons was considering for his forthcoming book
(Pure Logic, 1864), Jevons said:
It is surely obvious, however, that \(x+x\) is equivalent only to
\(x,\ldots\);
Professor Boole’s notation [process of subtraction] is
inconsistent with a self-evident law.
If my view be right, his system will come to be regarded as a
most remarkable combination of truth and error.
Boole replied:
Jevons responded by asking if Boole could deny the truth of \(x + x = x\).
Boole, clearly exasperated, replies:
Jevons’s final effort to get Boole to understand the issue was:
Jevons’s new law, \(x + x = x\), resulted from
his conviction that + should denote what we now call
union, where the membership of \(x + y\) is given by an
inclusive or. Boole simply did not see any way to define
\(x + y\) as a class unless \(x\) and \(y\)
were disjoint, as already noted.
Various explanations have been given as to why Boole could not
comprehend the possibility of Jevons’s suggestion. Boole clearly had
the semantic concept of union—he expressed the union
of \(x\) and \(y\) as \(x + (1-x)y\), a sum of two disjoint classes, and
pointed out that the elements of this class are the ones that belong
to either \(x\) or \(y\) or both. So how could he so
completely fail to see the possibility of taking union for his
fundamental operation + instead of his curious partial union
operation?
The answer is simple: the law \(x + x = x\) would have destroyed his
ability to use ordinary algebra: from \(x + x = x\) one
has, by ordinary algebra, \(x = 0\). This would force every class
symbol to denote the empty class. Jevons’s proposed law \(x + x
= x\) was simply not true if one was committed to constructing the
algebra of logic on top of the laws and inference rules of ordinary
algebra. (Boolean rings satisfy all the laws of ordinary algebra, but not all
of the inferences, for example, \(2x = 0\) implies \(x = 0\) does
not hold in Boolean rings.) It seems quite possible that Boole found
the simplest way to construct a model—whose 
domain was classes contained
in the universe of discourse—for an algebra of logic that
allowed one to use all the equations and equational arguments
that were valid for numbers. 
A popular misconception is that Boole’s algebra of logic is the
Boolean algebra of classes with the usual operations of union,
intersection and complement. This error was forcefully pointed out by
Hailperin in his 1981 paper “Boole’s algebra isn’t
Boolean algebra,” a theme repeated in his path-breaking book
Boole’s Logic and Probability (1986).  Nonetheless the
goal of the two algebras, Boole’s algebra and Boolean algebra,
is the same, to provide an equational logic for the calculus of
classes and for propositional logic.  Thanks to Hailperin’s
writings, for the first time there was clarity as to why Boole’s
algebra gave correct results. 
In his 1959 JSL review article Michael Dummett said:
For example, one does not find a clear statement 
 of what Boole meant by equivalent or interpretable. 
For those familiar with partial algebras the latter word can easily 
be taken to mean is defined—the domain of
definition of an algebraic term has a recursive definition, just as  
algebraic term has a recursive definition. After verifying examples like those
in LT that show Boole’s algebraic methods 
give correct results for propositions about classes, the challenge for those
who want to make sense of Boole’s algebra of logic
 is to make 
enough of the foundations of Boole’s 
algebra sufficiently precise so as to be able to justify the algebraic 
procedures he used. 
In LT Boole gave detailed instructions on how to use his algebra to obtain valid propositional conclusions from propositional premises about classes when the propositions were universal. (He avoided particular propositions until Chapter XV, the last chapter of  LT on logic.) He showed how to express English language propositions as equations, the steps needed to obtain desired conclusion equations, and how they are to be interpreted as conclusion propositions to the premises. The algebraic steps can be lengthy and he gave some shortcuts in Chapter IX, but we now know that any method to carry out such deductions will confront a complexity of computation that grows rapidly with the number of propositional variables. 
As stated in Dummett’s comment above, Boole was anything but clear as to why his algebra worked as claimed, to give best possible conclusions to premises. This has inspired considerable commentary on what Boole meant to say, or should have said, and to what extent his justifications are valid. 
Jevons (1864) gave sharp criticisms on the shortcomings of 
Boole’s algebra of logic and abandoned it to create the first
version of modern Boolean algebra. (He did not have the unary complement operation
that is now standard, but instead used De Morgan’s convention that the complement of a class \(A\) is \(a\).)
The title of Jevon’s 1864 book
started out with the words Pure Logic, referring to the fact
that his version of the algebra of logic had been cleansed from
connections to the algebra of numbers. The same point would be made in
the introduction to Whitehead and Russell’s Principia
Mathematica, that they had adopted the notation of Peano in part
to free their work from such connections. 
According to Hailperin(1986), the proof-theoretic side of Boole’s algebra is simply that of non-trivial commutative rings with unit and distinguished idempotent elements, but without non-zero additively or multiplicatively nilpotent elements. His favorite models were rings of signed multi-sets and he used them to 
explain why Boole’s theorems are correct for the algebra of 
logic of universal propositions. (Hailperin’s analysis did not apply to particular propositions.)
 Frank W. Brown’s paper 
George Boole’s deductive system (2009) 
claims that one can avoid Hailperin’s signed multi-sets 
by working with the ring of polynomials Z[X] modulo a certain ideal. 
Burris and Sankappanavar (2013) use the fact that 
Boole’s model, a partial-algebra, is isomorphic to the 
restriction of the operations of addition, multiplication and subtraction in the
ring \(Z^U\) to the idempotent elements of the ring. Here \(Z\) is the ring of
integers, and \(U\) is the universe of discourse.

From this one can deduce that any Horn sentence which holds in \(Z\) when the variables are restricted to 0 and 1 will hold in \(Z^U\) when the variables are restricted to idempotent elements, and thus will hold in Boole’s partial-algebra.  This gives an expanded version of Boole’s Rule of 
0 and 1, and since his main results (Expansion, Reduction, Elimination and Solution) can be expressed by such Horn sentences, one has a quick
proof that they are indeed valid. 
While reading through this section, on the technical details of
Boole’s methods, the reader may find it useful to consult the
These examples have been augmented with comments explaining, in each
step of a derivation by Boole, which aspect of his methods is being
employed. 
Boole used three methods to analyze arguments in LT:
The theorems of LT combine to yield the master result,
When applying the ad hoc method, he used ordinary algebra
along with the idempotent law \(x^2 = x\) to
manipulate equations. There was no pre-established procedure to
follow—success with this method depended on intuitive skills
developed through experience.
The second method, the Rule of 0 and 1, is very powerful, but it
depends on being given a collection of premise equations and a
conclusion equation. It is a truth-table like method (but Boole never
drew a table when applying the method) to determine if the argument is
correct. He only used this method to establish the theorems that
justified his General Method, even though it is an excellent tool for
verifying simple arguments like syllogisms. Boole was mainly
interested in finding the most general conclusion from given premises,
modulo certain conditions, and aside from his general theorems, showed
no interest in simply verifying logical arguments. The Rule of 0 and
1 is a somewhat shadowy figure in LT—it has no name,
and is reformulated in Section 6 of Chapter V as a procedure to use when 
carrying out a derivation and encountering uninterpretable terms. 
The third method to analyze arguments was the highlight of Boole’s
work in logic, his General Method (discussed immediately after this).
This is the one he used for all but the simplest examples in
LT; for the simplest examples he resorted to the first method
of ad hoc algebraic techniques because, for one skilled in algebraic
manipulations, using them is usually far more efficient than going
through the General Method.
The final version (from LT) of his General Method for
analyzing arguments is, briefly stated, to:
 With this method Boole had replaced the art of reasoning from
premise propositions to conclusion propositions by a routine mechanical
algebraic procedure. On p. 240 of LT he said that it was always 
theoretically possible to carry out elimination by piecing together 
syllogisms, but there was no method (i.e., algorithm) given for doing this.
In LT Boole divided propositions into two kinds, primary
and secondary. These correspond to, but are not exactly the same as,
the Aristotelian division into categorical and hypothetical
propositions. First we discuss his General Method applied to primary
propositions. 
Boole recognized three “great leading types” of primary propositions (LT, p. 64):
These were his version of the Aristotelian categorical propositions,
where \(X\) is the subject term and \(Y\) the predicate term. The
terms \(X\) and \(Y\) could be complex, for example, \(X\) could
be “Either \(u\) and not-\(v\), or else \(w\)”. For Boole such terms were not very complicated, at most a disjunction  
of conjunctions of simple terms and their contraries, no doubt a reflection
of the fact that natural language terms are not very complex.
STEP 1: Propositional terms were expressed by algebraic terms as 
in the following; one can substitute more complex terms for \(x\), \(y\). Boole 
did not give a recursive definition, only some simple examples:
STEP 2: Having expressed the propositional terms as algebraic terms,
one then expressed the propositions as equations using the
following; again one can substitute more complex terms for \(x\), \(y\), but not for \(v\):
In LT, prior to chapter XV, the one on Aristotelian logic, Boole’s
examples only used universal propositions. (One can speculate that he
had encountered difficulties with particular propositions and avoided
them.) Those of the form All x is y were first expressed
as \(x = vy\), and then \(v\) was promptly eliminated, giving \(x =
xy\). (Similarly if \(x\) was replaced by not-\(x\), etc.) Boole said
the elimination of \(v\) was a convenient but unnecessary step. For the examples of 
All x is y in
the first fourteen chapters he could simply have used the expression
\(x = xy\), skipping the use of the parameter \(v\). 
To simplify the
notation he used the same letter, say \(v\), for some when there were
several universal premises, an incorrect step if one accepts
Boole’s claim that it is not necessary to eliminate the
\(v\)’s immediately. Distinct universal propositions require
different \(v\)’s in their translation; else one can run into
the following situation. Consider the two premises All \(x\) is
\(z\) and All \(y\) is \(z\). Using the same
\(v\) for their equational expressions gives \(x = vz\) and \(y =
vz\), leading to the equation \(x = y\), and then to the false
conclusion \(x\) equals \(y\). In chapter XV he was careful to use
distinct \(v\)’s for the expressions of distinct premises.
Boole used the four categorical propositions as his primary forms in
1847, but in 1854 he eliminated the negative propositional forms,
noting that one could change not \(y\) to
not-\(y\). Thus in 1854 he would express No \(x\)
is \(y\) by All \(x\) is not-\(y\), with the
translation \(x = v(1-y)\), and then eliminating \(v\) to obtain
which simplifies to \(xy = 0\).
STEP 3: After expressing the premises in algebraic form one has a
collection of equations, say
Write these as equations with 0 on the right side, that is, as
with
An alternative way of forming the \(r_i\) to preserve the idempotent property,
in case the \(p_i\) and \(q_i\) have this property, is given in Section 3 of
Chapter X. 
STEP 4: (REDUCTION) [LT (p. 121) ]
Reduce the system of equations 
to a single equation \(r = 0\). Boole had three different
methods for doing this—one of them was only for
the case that the \(r_i\) were idempotent. He had a strong preference for
summing the squares: 
An alternative way of forming \(r\) to preserve the idempotent property,
in case the \(r_i\) have this property, is also given in Section 3 of
Chapter X. 
Steps 1 through 4 are mandatory in Boole’s General Method. After
executing these steps there are various options for continuing,
depending on the goal. 
STEP 5: (ELIMINATION) [LT (p. 101)]
Suppose one wants the most general equational conclusion derived
from \(r = 0\) that involves some, but not all, of the class
symbols in \(r\). Then one wants to eliminate certain symbols. Suppose \(r\)
involves the class symbols
Then one can write \(r\) as \(r(x_1, \ldots, x_j, y_1, \ldots ,y_k)\).
Boole’s procedure to eliminate the symbols \(x_1, \ldots ,x_j\)
from
to obtain
was as follows: 
For example, eliminating \(x_1, x_2\) from
gives
where
STEP 6: (DEVELOPMENT, or EXPANSION)
[MAL (p. 60), LT (pp. 72, 73)].
Given a term, say \(r(x_1, \ldots, x_j, y_1, \ldots, y_k)\), one can
expand the term with respect to a subset of the class symbols. To
expand with respect to \(x_1, \ldots, x_j\) gives
where \(a_1 , \ldots ,a_j\) range over all sequences of 0s and 1s of
length \(j\), and where the \(C(a_i, x_i)\) are defined by:
The products 
are the constituents of \(x_1 , \ldots ,x_j\). There are \(2^j\)
different constituents for \(j\) symbols—the regions of a Venn
diagram give a popular way to visualize constituents. It will be convenient to
say an equation of the form 
is a constituent equation. 
STEP 7: (DIVISION: SOLVING FOR A CLASS SYMBOL)
 [MAL (p. 73), LT (pp. 86–92) ]
Given an equation \(r = 0\), suppose one wants to solve this equation
for one of the class symbols, say \(x\), in terms of the other class
symbols, say they are \(y_1 , \ldots ,y_k\). To solve:
for \(x\), first let:
Then:
where \(s(y_1 ,\ldots ,y_k)\) is:
the sum of all constituents
 \(C(a_1, y_1) \cdots C(a_k,  y_k)\)
 where \(a_1 , \ldots ,a_k\) range over all sequences of 0s and
 1s for which:
plus
the sum of all the terms of the form
 \(v_{a_1 \ldots a_k} \cdot C(a_1, y_1) \cdots C(a_k, y_k)\)
 for which:
The \(v_{a_1 \ldots a_k}\) are parameters, denoting arbitrary classes 
(the appearance of parameters is similar to what one sees in the solution of linear
differential equations, a subject in which Boole was an expert).
To the equation (*) for \(x\) adjoin the constraint conditions 
(these are constituent equations that Boole called 
“independent relations”)
whenever
Note that one is to evaluate the terms:
using ordinary arithmetic. Thus solving an equation \(r = 0\) for a
class symbol \(x\) gives an equation
perhaps with constraint constituent equations. On p. 92 Boole noted that the
solution plus constraint equations could be written simply as
where A, B and C are each sums of distinct constituents. 
This presentation gives exactly the same solution as that of Boole, but without 
the mysterious use of fractions like \(0/0\) and \(1/0\). Boole used formal division
to express \(x\) as \(N\) divided by \(D\), and then a formal expansion where 
the coefficient of the constituent \(C(a_1, y_1) \cdots C(a_k,  y_k)\) is the fraction 
He had the following rules for how to decode 
the effect of the coefficients on their constituents: (1) for the coefficient
\(m/m\) with \(m \neq 0\) the constituent is kept in the solution;
(2) for \(0/m\) with \(m \neq 0\) the constituent is deleted;
(3) the coefficient \(0/0\) is changed into an
 arbitrary parameter; and (4) any other coefficient indicated that the constituent 
was to be removed and set equal to 0. This use of formal division and formal
expansion is best regarded as a clever mnemonic device.
STEP 8: (INTERPRETATION) [MAL pp. 64–65, LT
(Chap. VI, esp. pp. 82–83)]
Any polynomial equation \(p(y_1 , \ldots ,y_k) = 0\) is equivalent to the collection of constituent equations
for which \(p(a_1 , \ldots ,a_k)\) is not 0. A constituent equation
merely asserts that a certain intersection of the original classes and
their complements is empty. For example,
expresses the proposition All \(y_1\) is \(y_2\) or
\(y_3\), or equivalently, All \(y_1\) and not \(y_2\) is
\(y_3\). It is routine to interpret constituent equations as
propositions.
Secondary propositions were Boole’s version of the propositions that
one encounters in the study of hypothetical syllogisms in Aristotelian
logic, statements like If \(X\) is true or \(Y\) is true then
\(Z\) is true. The symbols \(X, Y, Z\), etc. referred to primary propositions. In keeping with the
incomplete nature of the Aristotelian treatment of hypothetical
propositions, Boole did not give a precise description of possible
forms for his secondary propositions.
The key (but not original) observation that Boole used was simply that
one can convert secondary propositions into primary propositions. In
MAL he adopted the convention found in Whately (1826), that
given a propositional symbol \(X\), the symbol \(x\) will
denote the cases in which \(X\) is true, whereas in
LT Boole let \(x\) denote the times for which
\(X\) is true. With this the secondary proposition
If \(X\) is true or \(Y\) is true then \(Z\) is true is expressed
by All \(x\) or \(y\) is \(z\). The
equation \(x = 1\) is the equational translation of
\(X\) is true (in all cases, or for all times), and
\(x = 0\) says \(X\) is false (in
all cases, or for all times). The concepts of all cases and
all times depend on the choice of the universe of
discourse. 
With this translation scheme it is clear that Boole’s treatment of
secondary propositions can be analyzed by the methods he had developed
for primary propositions. This was Boole’s propositional logic.
Boole worked mainly with Aristotelian propositions in MAL,
using the traditional division into categoricals and hypotheticals.
 In LT this
division was replaced by the similar but more general primary versus
secondary classification, where the subject and predicate were allowed
to become complex names, and the number of propositions in an argument
became unrestricted. With this the parallels between the logic of
primary propositions and that of secondary propositions became clear,
with one notable difference, namely it seems that the secondary
propositions that Boole considered always translated into universal
primary propositions.