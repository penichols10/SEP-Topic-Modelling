In discussing the role of causation in physics or in our conception of
the world more generally we may be engaged in several different
philosophical projects (Woodward 2014): a metaphysical
project, a descriptive project, and what Woodward calls
a “functional project”. While there are obvious
points of contact among these projects and a philosophical account may
contribute to more than one project simultaneously, the three projects
have distinctly different core aims and are characterized by different
methodologies.
The aim of the metaphysical project is to uncover the
metaphysical grounds or truth-makers for causal claims. The main
division in the metaphysics of causation is between broadly Humean and
non-Humean accounts of causation. Humean accounts deny the existence
of fundamental modalities and maintain that fundamentally the universe
is composed of a distribution of categorical properties and relations
instantiated by localized entities—what David Lewis called the
“Humean mosaic” (Lewis 1973; Loewer 2012). On the Humean
view, all true modal claims, including causal claims, are grounded in
non-modal features of the mosaic. Even though Humeans deny that modal
properties, including causal properties, are part of the world’s
fundamental ontology, they may allow for the existence of
non-fundamental causal facts that are reducible to fundamental
physical properties. Thus, Humeans can be—and many of them
are—non-fundamentalists rather than
eliminativists about
 causation.[1]
Non-Humeans, by contrast, take fundamental properties to include modal
properties, such as nomic necessitation relations, dispositional
essences, or causal properties. For non-Humeans causal features can be
among the fundamental building blocks of the world. Some non-Humeans
hold that the dynamical laws of physics are fundamentally causal laws
in virtue of which earlier states of a system or of the world produce
later states (Maudlin 2007). Others maintain that objects possess
fundamental dispositions, capacities, or essences that are causal in
nature (Cartwright 1989; Bird 2007).
In contrast with the metaphysical project, the descriptive
project aims to describe our causal reasoning practices.
Traditionally philosophers have tended to conceive of this project as
having as its core aim to provide conceptual analyses of our everyday
concept or concepts of cause. A conceptual analysis offers necessary and
sufficient conditions for claims of the form “c causes
e”. Regularity accounts, Mackie’s INUS
condition account, or David Lewis’s counterfactual analysis are
all examples of the descriptive project. In principle, the project
could appeal to a broad range of data, including empirical work by
psychologists and cognitive scientists. Generally, however, the
descriptive project has focused almost exclusively on probing what
philosophers upon reflection take to be commonsense intuitions
concerning causal judgements (often involving Billy and Suzy throwing
rocks or assassins pouring poison in drinks). Those developing
conceptual analyses tend to focus their analyses on commonsense causal
claims rather than on the use of causal notions in physics or in the
sciences more generally.
A third project, the functional project, which Woodward
outlines and defends in (Woodward 2014), asks what kind of functional
role causal concepts play in our cognitive architecture and what
purposes and goals causal cognition can serve. An influential argument
for the indispensability of causal concepts is Nancy
Cartwright’s argument that causal notions play a crucial role in
distinguishing effective from ineffective strategies (Cartwright
1979).
The functional project has close affinities to what in recent years
has been discussed under the term conceptual engineering
(Cappelen 2018). Conceptual engineering aims to develop precise
philosophical concepts that fulfil certain cognitive goals, often
taking an existing concept as its starting point, offering a
philosophical precissification of this concept and then engaging in an
assessment of the precissified concept’s usefulness. In taking
existing concepts of causation as its starting point the functional
project concerning causation engages primarily in what David Chalmers
calls “re-engineering” rather than “de novo
engineering” (Chalmers 2018, see
 Other Internet Resources).
 Unlike the descriptive project, the functional project possesses a
methodological or normative dimension, evaluating the usefulness of
causal concepts and of types of causal reasoning.
Woodward (2014) distinguishes “how does causation fit with
physics” as a separate philosophical project on a par with the
metaphysical, descriptive, and functional projects. Yet the question
concerning the fit with physics is best thought of as a question to
be addressed within each of the three projects. Indeed, philosophical
discussions examining the fit of causal notions with physics can
benefit from distinguishing carefully—and perhaps more carefully
than it is often done—among the different projects within which
the discussions take
 place.[2]
The fit with physics question seems unavoidable for the
metaphysical project more generally: if a certain
metaphysical account could be shown to be incompatible with the
fundamental physical theories we accept, then this would constitute a
reason for rejecting the account, since compatibility with physics
arguably is a condition of adequacy for any metaphysical account of
causation.
For any metaphysical account compatible with physics the
question arises, what the truth-makers of causal claims are or what
grounds these claims. There are three options: (i) either the truth-makers of causal claims
are physical features of the world, (ii) or they supervene on physical
features of the world, (iii) or they are non-physical features of the
world that do not supervene on physical features of the world. If the
truth-makers of causal claims are either physical features or
supervene on such features, then the metaphysical account is
compatible with the thesis of the completeness of the physical. On the
third option, however, the account is incompatible with the
completeness of the physical.
Causal eliminativists argue that there is no metaphysical
account of causation compatible with physics or compatible with the
completeness of physics and, hence, that causal notions should, as
Bertrand Russell (1912) urged, be expunged from the philosophical
vocabulary. Causal non-eliminativists who are also causal
non-fundamentalists can come in various stripes. Some
non-fundamentalists allow that non-fundamental causal concepts can be
a legitimate part of at least some domains of physics. Other
non-fundamentalists deny the latter claim and reject that causal
notions and causal judgements can play a legitimate role within
physics. John Norton is a non-fundamentalist who appears to be
endorsing the former view, arguing that while causal fundamentalism is
false “in appropriately restricted circumstances our science
entails that nature will conform to one or other form of our causal
expectations” (Norton 2003: 13). Yet Norton also seems to have
some sympathies for causal eliminativism, since he likens causal
concepts to the concept of caloric—a concept that no longer is
accepted as playing a legitimate role in science.
Within the descriptive project we can distinguish two
different ways of engaging with the fit-with-physics question. Most
obviously (and what is more prominent in the philosophical literature)
we can propose a conceptual analysis of our intuitive, commonsense
notion of cause and then ask whether and to what extent the analysans
also plays a legitimate role in physics. For example, we might ask to
what extent Lewis’s notion of causal dependence analyzed in
terms of time-asymmetric counterfactual dependence plays a role in
reasoning in physics. As we will see below, prominent arguments
denying that causal notions fit with physics are most plausibly
understood as engaging in this version of a descriptive project.
Alternatively, a descriptive project may take physicists’ own
widespread use of causal notions, both in research articles and in
physics textbooks, as its starting point and proposes an analysis of
the underlying causal concepts. Fritz Rohrlich’s proposal that
causality has three different meanings in classical physics arguably
is an example of this project. According to Rohrlich the three
meanings are: 
(a) predictability or Newtonian causality, (b) restriction of signal
velocities to those not exceeding the velocity of light, and (c) the
absence of “advanced” effects of fields with finite
propagation velocity. (2007: 50) 
the last of which concerns the temporal asymmetry of causation. To
some extent Yemima Ben-Menahem (2018) also engages in this project,
when she argues that the concepts of determinism, stability, locality,
symmetry and conservation law are all causal concepts.
Similarly, we can distinguish two approaches to the fit-with-physics
question within the functional project. For one, we may
examine whether certain causal concepts that play useful cognitive
roles in everyday contexts or in the special sciences also afford a
legitimate role to causal reasoning in physics. Woodward
 (2007) takes this approach to the
functional project and explores to what extent interventionist causal
concepts that play an essential role in how we navigate the world fit
with theorizing in physics. Alternatively, we can take the practices
of physical theorizing and model-building as starting points and
examine whether we can “engineer” causal concepts that
fulfil certain cognitive functions within these contexts. Ben-Menahem
(2018) is one of very few philosophers who take this approach to the
functional project.
Several of the most widely-discussed arguments aimed at establishing
that there is no legitimate place for causal notions in physics can be
traced to the writings of Ernst Mach (1900, 1905) and to Bertrand
Russell’s extremely influential article “On the Notion of
Cause” (1912). Russell’s target is the notion of cause in
general, even though some of his arguments appeal to purported
features of physical theorizing. Mach’s arguments focus more
directly on physics, arguing that there is something distinct about
physics that makes it especially inhospitable to causal notions.
Contemporary defenders of neo-Machian and neo-Russellian arguments
include Huw Price (1997; Price & Weslake 2009), Hartry Field
(2003), John Earman (2011), and, to some extent, John Norton (2003;
2007;
 2009).[3]
Neo-Machian and neo-Russellian arguments have a common structure. They
point to some putative conceptual feature of causal relations and then
argue that suitably fundamental theories of physics are incapable of
grounding or incorporating these features (Ney 2016). The following
five arguments have been particularly influential:
The nineteenth-century physicists Kirchhoff and Mach objected that
causal notions are “infected by vagueness” (Kirchhoff
1876: v, my translation) and “lack the precision” (Mach
1905: 278, my translation) of the mathematical functional dependencies
associated with physical theories. This criticism is echoed in the
twenty-first century by John Norton (2003) and John Earman (2011),
among others.
Within the descriptive project the vagueness challenge can be
understood as arguing that our intuitive, commonsense notion or
notions of cause are too vague to be given philosophical analyses
precise enough to be able to play a legitimate role in the
mathematized sciences. To the extent that physicists themselves engage
in causal talk, this has to be understood as part of an informal
framework within which physicists talk about theories but not as part
of the formally rigorous and precise content of physical theories
themselves (Earman 2011). One could imagine an analogous challenge as
part of the functional project, even though here the criticism, if
successful, would point to limits of the project of conceptually
re-engineering causal notions and would show that causal concepts are
so vague as to resist sufficient philosophical precissification.
Within the metaphysical project the vagueness challenge arguably looms
especially large for defenders of metaphysically “rich”
causal notions, such as notions of causal production. Here the
challenge may be part of more general empiricist scruples about rich
causal notions of production or “bringing about” along the
lines of what has traditionally been taken to be Hume’s
criticism of causation. Yet the vagueness challenge can also be
presented as a challenge to broadly Humean accounts of causation, such
as David Lewis’s account (Lewis 1973).
On most accounts of causation causal claims closely track
counterfactual claims. But counterfactual claims are notoriously vague
and appear to be highly context-dependent. Consider the following
example of how the vagueness challenge for counterfactual claims may
arise in combination with the time-asymmetry challenge: imagine a
fully elastic collision between two billiard balls on a frictionless
plane. What, if anything, can single out putatively causal
counterfactuals of the form “If the balls’ initial state
prior to the collision had been different, their final state after the
collision would be different” from putatively anti-causal
backtracking counterfactuals of the form “If the balls’
final state were different, their initial state would have to have
been different”? Newton’s laws underwrite both kinds of
counterfactual equally: just as different initial states are
associated with different final states, differences in the
system’s final state have to be correlated with differences in
the initial state. Within the context of an initial value problem, in
which the initial state of the balls is given and Newton’s laws
are used to calculate the subsequent motion of the balls, the former
type of counterfactual is the appropriate one, while the context of a
final value problem (which asks us to calculate the balls’ prior
motion given their final state) suggests the latter type of
counterfactual.
Similarly, we might ask if hitting one of the balls with a hammer
exactly as they collide will result in changes to the balls’
motion after they collide or before they collide.
Intuitively it seems that the force exerted by the hammer will
causally influence the ball’s subsequent motion. But it is not
obvious how this verdict is borne out purely by considering
Newton’s equations: singling out the forward-looking
counterfactual as the correct one seems arbitrary. As John Earman
argues,
the exercise of trying to divine the truth value of such
counterfactual assertions, even when it is agreed at the outset what
the basic laws are, is an invitation to a contest of conflicting
intuitions about cotenability of conditions and the closeness of
possible worlds. (Earman 2011: 494)
David Lewis proposes an answer to this concern, arguing that the
time-asymmetry of counterfactuals is secured by an asymmetric
overdetermination of the present by the future (Lewis 1979), but
Lewis’s overdetermination thesis is false in the context of the
deterministic theories he considers (Frisch 2005: ch. 8; Loewer 2007).
A successful response to the vagueness challenge would have to show
that there exist causal notions that can be reconstructed in a manner
sufficiently precise to allow these notions to play a role in
mathematized sciences such as physics. Bayes-net or structural
accounts of causation (Pearl 2000; Spirtes, Glymour, and Scheines 1993
[2000]) take up this challenge (see
 section 3
 below).
Many paradigmatically causal claims relate one cause (or at most a
very small set of causal factors) to an effect. Often discussed
examples in the literature include “the short circuit caused the
fire” or “Suzy’s throwing the rock caused the window
to break”. If one takes it to be an essential characteristic of
causal claims that they involve only a small set of fairly localized
causally relevant factors—what Norton calls “the dominant
causes” (Norton 2003: 17)—acting along clearly
distinguishable causal routes, then this gives rise to the following
argument against the applicability of causal notions in physics (Field
2003).
If we assume that a system is governed by deterministic physical laws,
then the laws allow us to derive the occurrence of some event E
from appropriate initial and boundary conditions. What combination of
initial and boundary conditions are required depends on the type of
mathematical equation that express the laws. In the case of hyperbolic
equations, such as the wave equation, we can formulate a pure initial
value problem. In this case, the event E occurring at spatial
location x and time \(t_1\) is derivable from an earlier state
S specified on a complete initial value surface in a region
surrounding x at time \(t_0.\) What is important is that
nothing less than a complete specification of the state on the initial
value surface will allow E to be derived from the
laws—the laws are silent on how a system with incompletely
specified initial conditions may evolve. That is, (a) we need to
specify the initial state in the relevant region completely and in
whatever detail the laws at issue require; and (b) the relevant region
comprising the initial value surface may be quite large. In the case
of relativistic theories initial value surfaces consist of entire
cross sections of the backward lightcone of E. (The backward
lightcone consists of those events from which a signal traveling at
most at the speed of light could reach E.) In the case of
classical Newtonian theories, which allow for signals propagating at
arbitrarily high speeds, S would have to amount to the state of
the entire universe at some time \(t_0\) prior to E.
In his seminal article (1912), Russell uses similar considerations to
argue that the causal law “same cause, same effect” is
either trivial or false: if the cause of an event E is taken to
include less than a complete specification of all the putatively
causal factors relevant to the E’s occurrence, then the
law is false, since then the occurrence of E could then still
be disrupted by some external influence not captured within the
specification of the set of E’s causes. But once the
state S on an initial value surface is fully specified in
sufficient detail including the entire environment of E, this
state will generally be so complex that it is highly likely that a
state of exactly this kind will never again occur in the history of
the universe. That is, true causal regularities of the form
“whenever S is instantiated E will occur”
will be instantiated at most once.
What, then, is the cause of an event E? It is not enough for
defenders of causation simply to give up the principle “same
cause, same effect”. The challenge, according to the
dominant cause argument, is to find a criterion that somehow
allows us to distill a small set of causes from the complete goings-on
on an initial value surface S. And causal skeptics argue that
physical theories themselves cannot provide such a criterion. If
causes determine their effects, nothing short of the complete state on
S will classify as the cause or the set of causes of E.
This leaves three options, all of which may seem unpalatable for a
defender of causation in physics.
Either we take the entire state on S to be the single cause of
E. But then causes will in general be highly non-localized
events. Or we allow for a very large and potentially infinite set of
causes of E at all times t prior to E. But then
we have given up on the idea that it is part of the concept of cause
to single out a small set of factors as being responsible for an
event. Or, finally, we concede that whatever considerations allow us
to single out a small set of factors come from outside of physics. But
then, it seems, we have to concede that causal notions do not play a
role within physics proper. Isolating a small set of factors as
the causes or the dominant causes of an event presupposes a
distinction between causes and background conditions. For example, we
might take the short-circuit to be a cause of the fire, relegating the
presence of oxygen and of flammable materials and the prevailing
meteorological conditions to the background conditions. Yet, the
argument claims, such a distinction cannot be drawn on purely physical
grounds. Any reason for singling out the short circuit goes beyond a
purely physical description of the situation and must be driven by
adding context-dependent or pragmatic considerations.
The dominant cause challenge can be raised both as part of the
descriptive project and as part of the functional project. Within the
descriptive project the claim is that it is part of our commonsense
notion of causation that events only have a small number of causes.
Within the functional project the claim becomes that singling out a
small set of events as an event’s causes serves important
purposes in our cognitive architecture that would not be fulfilled by
any notion of cause allowing for an event to have an arbitrarily large
set of causes. This claim might be supported by pointing out that
causal claims are used to assign responsibility or blame, to single
out factors that are particularly amenable to interventions into a
system and for control, or to single out factors that we may find
particularly salient in a given context—functions that all
appear to require zeroing in on only a small number of dominant
factors as an event’s causes.
In response to the dominant cause challenge one can argue that (either
descriptively or functionally) we ought to distinguish between more
strongly pragmatic causal notions and an objective—or at least
less context-dependent—core concept of causation. The fact that
a small set of particularly salient or explanatorily relevant causal
factors, in a given context, are often singled out as the
causes of an event points to a pragmatic dimension of causal talk. Yet
this is compatible with allowing for much more complex and
fine-grained underlying structures, which are causal in a
non-pragmatic (or at least less pragmatic) sense of
“cause”. Our physical theories, according to this view,
describe an arbitrarily complex and appropriately objective causal
web, yet in any given explanatory context only an extremely small
subset of the web’s nodes is singled out as pragmatically
salient causes.
Within the descriptive project the reply would have to argue that our
commonsense concept of cause is multidimensional in this sense. Within
the functional project, the reply could concede that focusing on a
small set of causal factors fulfills certain pragmatic and context
dependent roles yet maintain that these are not the only functions of
causal concepts and causal judgements and that there are other
functions that are compatible with—or even require—a
broader notion of what counts as an event’s causes.
Causes are often taken to act deterministically in accord with the
principle “same cause—same effect”. Indeed, that
causes determine their effects is built into many philosophical
accounts of causation, such as Hume’s regularity account. We
have already seen that the demand that causes determine their effects
puts pressure on the idea that paradigmatic causal claims relate a
small number of localized events to one another. But even considered
on its own the association between determinism and causation can be
marshalled in support of an anti-causal argument.
One version of the determinism challenge proceeds from the following
two premises. First, according to the most promising accounts of
causation, causes act deterministically: a complete set of causes
determines its effects. Second, mature theories of physics are not
deterministic. From these premises the argument concludes that
cause-effect relations cannot be part of our mature theories of
physics.
Norton presents the determinism challenge as part of a more general
challenge that any defender of causal notions in physics faces, which
he puts in terms of the following dilemma:
EITHER conforming a science to cause and effect places a restriction
on the factual content of a science; OR it does not. […] In the
first horn, we must find some restriction on factual content that can
be properly applied to all sciences; but no appropriate restriction is
forthcoming. In the second horn, since the imposition of the causal
framework makes no difference to the factual content of the sciences,
it is revealed as an empty honorific. (2003: 3–4)
For causal notions to play a legitimate role in physics, Norton claims
they must do so as part of an acceptable “principle of
causality” that provides a universal constraint on all physical
theories. What form might such a principle take? The physicist Erwin
Schrödinger proposed a causal principle that combines some of the
options canvased by Norton—determinism, locality, and temporal
asymmetry:
the exact situation at any point P at a given moment is
unambiguously determined by the exact physical situation within a
certain surrounding of P at any previous time, say \(t -
\tau.\) (Schrödinger 1951: 28).
It follows from the existence of successful physical theories that
violate the principle either that we have yet to find the
correct principle of causality or that there is no such
principle that constitutes a universal causal constraint. This, Norton
argues, entails that “the notion of cause is dispensable”
(Norton 2003: 8).
That a principle of determinism is violated in physics receives
support from quantum mechanics—or at least from any
interpretation according to which the theory is indeterministic. Yet
if quantum mechanics were cited as reason for the failure of a
principle of causality, one can try to rescue the principle by
introducing a notion of probabilistic causation: causes do not
determine their effects but determine the probabilities of an
effect’s occurrence. Here, too, Schrödinger can serve as
example, who later maintained that
what do change [in the evolution of a quantum state and as a result of
quantum measurements are] the probabilities; these, moreover,
causally. (Schrödinger 1935: 809; quoted in Ben-Menahem 2018:
92)
Partly motivated by the probabilistic nature of quantum mechanics,
philosophers have developed probabilistic accounts of causation
(Suppes 1970), according to which the presence of a causes raises (or
at least changes) the probabilities of its effects.
Norton (2003) argues that this defense is unsuccessful, since
determinism comes under pressure even in what is often taken to be the
paradigm of a deterministic theory, Newtonian physics. Norton’s
example of an non-deterministic Newtonian system is that of a mass,
subject only to gravitational force, initially at rest sitting at the
apex of a dome whose height h depends on the its radius
r according to \(h=(2/3g)r^{3/2}.\) Norton shows that this
system has an infinity of solutions, each corresponding to the mass
sliding down the dome in some arbitrary radial direction \(\cdot\) at
some arbitrary time T. Hence the system is indeterministic and
for each particular solution there is nothing to which we can point as
the cause of the mass’s beginning to slide down the dome in
direction \(\cdot\) at time T.
It is unclear, however, whether examples such as this do indeed show
that Newtonian mechanics is indeterministic. The force acting on the
mass, which is given by \(F=r^{1/2},\) does not satisfy a continuity
condition for \(r=0\)—the so-called Lipschitz
condition. According to the Cauchy-Lipschitz theorem, the
Lipschitz continuity condition (which, intuitively, restricts how fast
a function can change) is a sufficient condition for the initial value
problem to have a unique solution—that is, for the system at
issue to behave deterministically (“Cauchy-Lipschitz
theorem” in
 Other Internet References).
 And one can argue that conditions constraining the allowable force
functions in Newton’s law, such as the Lipschitz condition, are
an integral part of the content of Newtonian physics. Thus, whether we
take Newtonian physics to be deterministic depends on what we take the
content of the theory to be. If what counts as a Newtonian system is
not given by Newton’s laws alone but depends on additional
conditions, including the Lipschitz condition, then the theory is
deterministic after all (Fletcher 2012). If the content of the theory
is given merely by the conjunction of Newton’s laws without
additional constraints on allowable force functions, then
Norton’s example shows that the theory is not deterministic.
The determinism challenge can be raised as part of each of the three
philosophical projects we distinguished: one might argue that our
intuitive concept of cause is deterministic or that only a
deterministic concept of cause could serve fulfil certain useful
cognitive functions. But Norton’s dilemma that a causal
constraint either has to place a constraint on all
sciences—that is, is a universal constraint—or
would amount to a mere honorific is perhaps most easily resisted
within the functional project. As Woodward (2014) emphasizes, it is
compatible with causal judgments playing an important cognitive role
in some domains that there are limits to the scope of causal thinking
and that causal concepts are not universally applicable. Thus, the
usefulness of deterministic causal reasoning might be restricted to
some contexts—contexts that may include some theoretical
contexts in physics—while there may also be domains in physics
in which deterministic causal notions are not applicable. Norton
assumes that a constraint that is not universal is no constraint at
all, but this assumption can be
 denied.
Norton’s demand that any causal principle needs to be a
universal principle may be on a stronger footing within the
metaphysical project and in fact Norton himself calls his argument
“the fundamentalist’s dilemma”: If a metaphysical
account of causation is committed to the principle “same
cause–same effect” or even just to a probabilistic version
of this principle, then the existence of genuine indeterminism in
physics of the kind discussed by Norton would pose a serious threat to
that account. 
According to many conceptions of causation, causes are local in
various senses: First, causes are synchronically local: they are
“smallish,” spatially localized events—or at least
their size is proportionate to the size of the effect under
consideration. If we demand locality in this sense, then this puts
into sharper focus one of the horns of the trilemma posed by the
dominant cause: we cannot identify the state on an entire
time-slice S as the cause of an event in the future of S
on pains of violating the locality constraint that causes be
spatio-temporally localized.
Causal relations are also often assumed to satisfy diachronic locality
constraints. Broadly such constraints come in two types: according to
the first type of constraint causes do not act where they are not.
That is, causes do not act across spatial or temporal gaps. According
to the second type of constraint causal influences do not propagate
infinitely fast. The two types of constraint are logically
independent. Newtonian gravitational theory violates both types of
constraint, but rigid body mechanics violates only the finite-speed
constraint while action-at-a-distance versions of classical
electrodynamics (that posit particles but not fields) satisfy the
finite speed constraint but posit propagation of electromagnetic
influences across spatio-temporal gaps. Classical electrodynamics as
classical particle-field theory is the paradigm of a local theory:
interactions among charged particles propagate at a finite
speed—the speed of light—and are mediated by the
electromagnetic field (Frisch 2002).
Locality constraints come under pressure in quantum mechanics (see
 section 7
 below). One can use this fact as a premise in an anti-causal argument
paralleling the argument appealing to failures of determinism.
In analogy to the case of the determinism challenge, one can resist
the conclusion of the argument by denying premise 1 and maintain that
causal constraints can play a legitimate and useful role in physical
theorizing even if they are not part of a universal principle of
causality. Thus, one could maintain that the relativistic constraint
that influences do not propagate faster than the speed of light is a
genuinely causal constraint, which functions as a desideratum on
physical theories but does not lose its legitimacy and importance
should it not be satisfied by all successful theories of physics.
Again, the challenge may be raised within each of the three
philosophical projects, with subtle but important different in each
case. Within the descriptive project the argument would aim to show
that a certain feature of our common-sense notion of cause does not
allow this notion to at least some theoretical frameworks in
physics.
Within the functional project one could argue in defense of premises 2
and 3 that local causal relations satisfy certain crucial desiderata
not satisfied by non-local, putatively causal relations. Some
well-known remarks by Albert Einstein concerning the role of locality
suggest one route such an argument could take. According to Einstein,
in a world that is not synchronically and diachronically local
“physical thought” and “the establishment of
empirically testable laws in the sense familiar to us” would be
impossible (1948: 322; quoted in Howard 1985:
187–8). Einstein’s remarks suggest an argument according
to which only causal relations satisfying various locality principles
can fulfil cognitively useful functions such as allowing for testable
predictions and guiding interventions. Yet this line of argument comes
under pressure to the extent that quantum systems violate certain
locality principles yet allow for testable predictions and
experimental interventions (see
 section 7).
Perhaps the most influential argument for the claim that causal
notions cannot play a legitimate role in physics appeals to the fact
that the causal relation is generally understood to be asymmetric.
This asymmetry is often assumed to coincide with a temporal asymmetry
according to which effects do not precede their
 causes.[4]
 This gives rise to the time asymmetry challenge that contrasts the
time-asymmetry of causal relations with the purported fact that
physical laws make no distinction between the past and future
direction. This contrast is offered as a reason for why causal
relations cannot be a part of physical theorizing. The time-asymmetry
challenge can be represented in premise-conclusion form as
follows:
Some authors respond to this argument by rejecting premise 1, positing
a symmetric notion of causal dependence, which for non-simultaneous
cause-effect pairs is also time-symmetric (Ney 2009). A potential
problem for this response is that it appears to collapse the
distinction between the notions of dependence and causation and may
have difficulties explaining how a symmetric notion of causal
dependence (as opposed to an asymmetric notion of causation) can play
a role in elucidating the apparently asymmetric notions of
intervention, responsibility, or explanation. These problems appear
especially pressing from the perspective of the functional project.
Thus, Ney’s proposal is most promisingly considered as part of a
metaphysical project of uncovering the metaphysical grounds of causal
claims.
What is it for laws to have the same character in the future and past
directions? Farr and Reutlinger (2013) point out that this can be made
precise in two logically independent ways and that we have to
distinguish the claim (i) that the laws are both future and past
deterministic from the claim (ii) that the laws are time-reversal
invariant. Russell is often interpreted as appealing to (ii), when he
says that
in the motion of mutually gravitating bodies, there is nothing that
can be called a cause and nothing that can be called an effect; there
is merely a formula. (Russell 1927 [2012: 141])
Maudlin (2007) points out that on reading (ii) premise 2 is false,
since not all the fundamental laws of physics are in fact
time-reversal invariant. According to the CPT-Theorem, any plausible
quantum theory will be invariant under the combination of parity
transformation plus charge conjugation plus time reversal. Since there
is experimental evidence for CP-violations, we should conclude that
quantum theories violate time-reversal invariance.
It has also been argued that even when considering time-reversal
invariance the argument applies only to deterministic theories, since
theories with non-trivial probabilistic state-transition laws are
inherently time-asymmetric. As Satosi Watanabe (1965) shows, there can
be no genuinely probabilistic theory with both non-trivial forward and
backward transition probabilities (see also Healey 1983; Callender
2000). Thus, if quantum mechanics is understood as a fundamentally
probabilistic theory, the argument’s scope is limited to what by
the argument defenders’ own lights are the less fundamental
deterministic theories of classical physics.
The time-asymmetry challenge is sometimes raised in the context of
discussing the interpretation of a theory’s so-called
“Green’s function”, which specifies how a system
responds to a localized point-like disturbance. For example, the
Green’s function associated with the wave equation describes the
circular ripples on a pond’s surface after a small pebble was
dropped in. When a theory’s equations are linear, the overall
response of a system to multiple point-like disturbances can be
calculated by summing or integrating over all disturbances. The
important mathematical result is that any solution to the
theory’s equations can be represented as a sum of two
components: a sum or integral over the Green’s function for all
the point-like disturbances plus a solution to the source-free
equations. That is, the most general solution to the wave equation
applied to our pond will consist of sums of ripples associated with
any pebbles being dropped into the pond plus source-free waves on the
pond not associated with any pebble as their “source”.
Putting the same point somewhat more formally, any linear differential
operator L associated with an inhomogeneous differential equation
\(\mathrm{L}y=f(x)\) and with constant coefficients possesses a
fundamental solution or Green’s function G, which is a solution
to the inhomogeneous differential equation \(\mathrm{LG}=\delta (x),\)
where \(\delta(x)\) is the delta function, a generalized function that
is zero everywhere except at \(x=0.\) The Green’s function tells
us what the contribution of introducing a point-disturbance or
perturbation into a system at \((x', t')\) is to the state of the
system at some other point \((x, t).\) The overall response of a
system at \((x, t)\) to multiple perturbations is calculated by
summing or integrating over all point-like disturbances. The most
general solution to a theory is given by adding to this response to
disturbances a solution to the homogenous equation
\(\mathrm{L}y=0.\)
Green’s functions, which are broadly applicable in physics, are
quite naturally interpreted in causal terms, allowing us to represent
the state at \((x, t)\) as sum of different disturbances as its
causes. In fact, Green’s functions provide “a primary
locus for causal claims within physics texts” (Smith 2013: 667).
The causal significance of the Green function formalism has been
challenged, however, by invoking a version of the time-asymmetry
challenge. In the case of systems governed by the time-reversal
invariant hyperbolic equations (such as the wave equation
which can be derived from the Maxwell-Lorentz equations in classical
electrodynamics) any state of a system can be represented either as a
sum of “causal” or so-called retarded
Green’s functions and a solution to an initial value problem of
the homogenous, source-free equation \(\mathrm{LG}=0;\) or as a sum of
“anti-causal” or advanced Green’s functions
and a solution to a final value problem of the homogenous, source-free
equation. Both representations are mathematically equivalent
representations of one and the same state of the system. From this observation the
time asymmetry challenge concludes that nothing in the mathematical
formalism can legitimately distinguish between the different
representations to single out one representation as the correct causal
representation. Since interpreting both retarded and advanced
representations causally is incompatible with the causal asymmetry,
neither representation ought to be interpreted causally.
While the time asymmetry challenge can be raised for the Green’s
function associated with hyperbolic equations, it is worth pointing
out that there are also theories or theoretical frameworks in physics
with time-asymmetric Green’s functions, chiefly among them
linear response theory. There is no philosophical
consensus on the causal status of the Green’s function
formalism. Frisch (2009a; 2009b; 2014) argues for a causal
interpretation of the Green’s function formalism, both in the
case of the time-reversal invariant wave equation and in the case of
the explicitly time-asymmetric linear response theory, while Norton
(2009) and Smith (2013) are critical of causal interpretations of the
formalism.
Some authors have challenged premise 5 of the time-asymmetry
challenge, according to which only those features that can be grounded
in physical laws can play a proper role in physics. In effect, this
premise implies that the proper content of physics is restricted to
physical laws, thereby denying that initial, final, or boundary
conditions are an integral part of the content of physics. One
motivation for this restriction might be the thought that initial or
final conditions are contingent “one-off” states, and
hence should not count as part of the content of physics, which
consists of general claims about the physical world: that a billiard
ball was at rest on a specific spot on a billiard table when it was
struck by a second ball is not part of physics but the laws of elastic
collision are. But this line of thought ignores that initial
conditions can also have more general features that are shared across
a wide class of conditions. In fact, many physical situations are
characterized by an asymmetry between prevailing initial and final
conditions, according to which initial conditions are random while
final conditions are not. And this asymmetry arguably is closely
related to a causal asymmetry (Arntzenius 1992; Maudlin 2007).
Under certain plausible assumptions an initial independence or initial
randomness assumption allows us to derive a principle of the common
cause (first proposed by Hans Reichenbach (1956)), according to which
spatially distant correlated events A and B that are not
related as cause and effect possess a common cause in their past that
explains the correlation and screens off the correlation between
A and B (Arntzenius 1999 [2010]). That is, formally,
if
Then there is some event C in the past of A and B
that explains the correlation between A and B and which
satisfies:
A central feature of common cause reasoning is that it allows
inferences from local data without full knowledge of the state of the
world (or the state of a larger system) on a complete final value
surface—states to which we very often do not have full access.
Common-cause reasoning not only is a core function of causal
representations in commonsense contexts but also is a central and
ineliminable inference pattern in physics.
As a particularly vivid example illustrating common-cause inferences
from very limited knowledge of the state of the universe on a final
value surface consider the detection of gravitational waves in 2016,
which, physicists concluded, were emitted by two colliding black
holes. If we wanted to derive the black hole event within General
Relativity from knowledge of the data on a complete final value
surface, we would have to know the precise state of the universe in a
sphere with a diameter of many, many light-years—something that
is obviously impossible for us to know. Instead researchers inferred
the black-hole event from the two signals detected locally in the LIGO
detectors in Washington and Louisiana, arguing that the extremely
strong correlations between the signals detected at both detectors
simultaneously are evidence for the colliding black holes as the
signals’ common cause.
Implicit in this causal inference is the assumption that there was no
“carefully calibrated” gravitational wave coming in from
past infinity, converging on the location of the postulated black-hole
event, and then re-diverging—thereby mimicking a wave produced
by two colliding black holes. This alternative explanation of the
simultaneously detected signals is ruled out as utterly implausible,
because a source free gravitational field coming in from past infinity
that mimicked the field associated with the black hole event would
have required absurdly strongly correlated initial conditions in
remote parts of spacetime. By positing an initial randomness
assumption such seemingly miraculous correlations are effectively
excluded.
Notice, however, that, by contrast with correlations among initial
conditions, we do not find “absurdly strongly correlated”
final conditions implausible: indeed, correlated final
conditions are just what we expect as joint effects of a common cause
such as the collapsing of the black holes.
The principle of the common cause can be derived within the context of
deterministic laws. Yet deterministic laws also appear to undercut the
time-asymmetry of common cause inferences. Under determinism, if there
is an event C in the past of two events A and B
that screens A and B off from each other, then there
will also be an event C* taking place after A and
B that occurs exactly if C occurs and that renders the
two events conditionally independent (Arntzenius 1992). In reply to
this worry one can argue that future screening-off events, unlike
those in the past, will in general be highly non-natural and
non-localized (Woodward 2007). Demanding that appropriate physical
variables represent localized and not highly gerrymandered events
allows us to preserve the asymmetry induced by the initial randomness
assumption.
There is broad agreement in the literature that our universe is
characterized by an asymmetry between prevailing initial and final
conditions, yet there is an ongoing debate what precisely this
asymmetry entails for the status of the causal asymmetry and for the
time-asymmetry challenge (see also
 section 5).
While some authors suggest that time-asymmetric causal relations might
be strictly incompatible with time-reversal invariant dynamical laws,
it is more promising to try and argue that adding time-asymmetric
causal relations to physics cannot be justified within physics. It is
this latter claim that an appeal to initial conditions and their role
in underwriting common cause reasoning is meant to challenge. From the
perspective of the functional project, the central role of
common-cause reasoning both in commonsense and in physics contexts
provides perhaps the strongest argument for the claim that the very
same—or at least very closely related—causal concepts are
operative in common sense causal judgements and in causal reasoning in
physics.
Within the metaphysical project, granting that an asymmetry in initial
conditions can justify an appeal to causal judgments is compatible
with two distinct types of view on the metaphysics of causation. Some
authors take the asymmetry between prevailing initial and final
conditions to be metaphysically primary and maintain that it is this
asymmetry which grounds our ability to reason causally. Consequently,
on this view, causal relations are metaphysically not on a par with
nomic relations: while physical laws are nomologically necessary,
causal relations are contingent, since the asymmetry between initial
and final conditions, from which causal relations are derived, is
merely a de facto, nomologically contingent asymmetry. This
view suggests a weaker version of the time-asymmetry argument, which
points to the time-reversal invariance of the laws to conclude that
causal relations cannot be grounded in the laws but only in a de
facto asymmetry between prevailing initial and final conditions.
The argument does not deny that causal judgments play a legitimate
role in physics but denies that causal relations are metaphysically
fundamental and affords them a modally weaker status than that of
physical laws. That initial conditions are contingent is denied,
however, by Barry Loewer (2007), who argues that the initial
randomness assumption comes out as a law according to Lewis’s
best system account of laws.
Other authors, by contrast, take the causal asymmetry (rather than the
initial randomness assumption) to be primary, and argue that the
causal asymmetry can then explain, account for, or ground the
asymmetry between prevailing initial and final conditions. Thus,
Hausman and Woodward (1999) argue that the reason why the values of
initial variables characterizing initial states are uncorrelated is
that these variables do not have common causes in their past. The
causal asymmetry, on this view, is explanatorily prior to the
asymmetry between initial and final states. Similarly, Pearl (2009)
argues against the view that the initial randomness assumption allows
us to derive the causal asymmetry from a non-causal premise that the
initial randomness assumption should itself be thought of as a causal
assumption. Maudlin (2007: 133) argues that the asymmetry between
initial and final conditions is a manifestation of a more fundamental
nomic asymmetry, which he however characterizes in strongly causal
terms as an asymmetry of later states being nomically produced by or
generated from earlier states.
The approaches to causation most widely discussed in the philosophical
literature in recent decades have been the Bayes net or structural
model accounts of causation developed by Peter Spirtes and his
co-authors (Spirtes, Glymour, & Scheines 1993 [2000]) and by Judea
Pearl (2000; 2009). The account’s formal framework is also at
the heart of Woodward’s highly influential interventionist
account of causation and explanation (Woodward 2003a).
Structural model accounts propose mathematically rigorous and precise
representations of causal structures. On Pearl’s account, a
structural causal model (SCM) consists of a directed acyclic graph
(representable in terms of a “blobs-and-arrows” diagram)
over a set of variables \(V={X, Y, \ldots}\) consisting of
It is part of the definition of a structural model that the exogenous
variables are probabilistically independent of one another (e.g.,
Pearl 2000: 44). From this together with the assumption that a causal
model is complete one can derive the causal Markov condition,
which is a generalized common cause condition and states that for
every variable X in V, X is probabilistically
independent of the variables in the set \((V -
\textrm{Descendants}(X))\) conditional on the parents of X.
There is a debate in the literature to what extent structural model
accounts can be applied in physics and what implications these
accounts have for the role of causation in physics. Frisch
 (2014) proposes that we can
construct causal representations of physical systems by identifying
the variables characterizing a system’s initial state with the
exogenous variables in a causal model and the theory’s
Green’s functions with the model’s structural equations
(see also Lloyd 1996).
One can then use the machinery of structural models to derive a common
cause principle from the initial randomness assumption. In particular,
the initial randomness assumption can be used to break the symmetry
between the retarded and advanced Green’s functions for
hyperbolic equations: Causal model constructed with retarded
Green’s functions as structural equations satisfy the
probabilistic independence assumption required in the structural model
framework, while putatively causal models constructed with
“anti-causal” or advanced Green’s functions violate
the probabilistic independence assumption, since in such models the
highly correlated variables characterizing a system’s final
state functions as exogenous variables. Thus structural model
accounts may provide an appropriate framework to support the claim that
causal inferences and judgments play an important role in physics.
By contrast, Pearl (2000) and Woodward (2007) point to an aspect of
structural causal models that may make the framework fit less well
with at least some aspects of theorizing in physics. Structural causal
models make perspicuous the tight connections between the notions of
cause and intervention or manipulation. A structural causal model
provides us with information on how the values of variables change
under external interventions into a system, while causal discovery
algorithms allow us to construct causal models from information about
probability distributions over the values of variables characterizing
the system and information about the effects of interventions. On
Woodward’s account the notion of causation is spelled out in
terms of so-called “hard” or “arrow-breaking”
interventions. Arrow-breaking interventions allow us to investigate
how changes to a variable V percolate through a system, when we
place V under the full control of an intervention variable and
break all other causal arrows into V. Yet arrow-breaking
interventions may not be possible within the context of certain
physical theories. Newtonian gravitational forces, for example, cannot
be turned off.
 In reply to this worry one can argue that interventions into
physical systems may be more adequately modeled in term of
“soft”, non-arrow breaking interventions, as investigated
in Eberhardt and Scheines (2007). What the comparative merits of
characterizing causal structures in terms of “hard” or
“soft” interventions are, is the subject of an ongoing
debate.
Pearl (2000) also argues that any account of causation that closely
links causal notions and the notion of interventions cannot be applied
in the context of a truly fundamental physics that includes models of
the universe as a whole. The reason is that global models of the
universe as a whole do not have an “outside” that could be
represented by an intervention variable and from which an intervention
into the universe could take place. Yet Pearl’s own do-calculus
first introduces interventions formally in a way that does not posit
intervention variables external to the causal model of interest.
Nonetheless, interventions into the universe as a whole are not
physically possible. Here the question concerning interventions in
fundamental physics makes contact with another issue on which there is
an active debate: the question on appropriate constraints on allowable
interventions and the question to what extent interventions need to be
physically or conceptually possible.
Taking a somewhat broader view, the worry concerning global models
arises from a conception of physics that is widely held by
philosophers—a conception according to which one of the
fundamental aims of physics is to present us with global dynamical
models of the dynamical laws that adequately represent the universe as
a whole. The worry then is that causal models, in particular on an
interventionist conception of cause, cannot get a foothold within such
a globalist conception of physics. This globalist conception can be
contrasted with one according to which the laws of physics are
understood as rules governing localized subsystems of the universe
(Ismael 2016; see also Cartwright 1999). Defenders of the latter
conception would argue not only that it can more easily accommodate
causal reasoning than the globalist picture but also that it fits the
day-to-day practice of most of physics considerably better than the
globalist conception.
Conserved quantity accounts of causation are reductive accounts of
causation that are explicitly designed to locate causation within the
realm of physics avoiding the vagueness challenge. Most
prominent here is the causal process account first proposed by Wesley
Salmon (1984) and developed further in Phil Dowe’s conserved
quantity account (Dowe 2000; see also Kistler 1999 [2006]). Proponents
of conserved quantity accounts take their accounts to contribute to
the metaphysical project of determining objective causal structures
that serve as truth-makers of causal claims.
Dowe distinguishes causal processes and causal interactions, which he
defines as follows:
Conserved quantities are those quantities, such as energy, momentum,
mass, or charge, that are conserved according to our physical
theories. By deriving its inspiration from physics, where conservation
laws play a central role, conserved quantity accounts promise to be
able to meet the neo-Machian and neo-Russellian challenges. In fact,
since, according to Noether’s First Theorem, there is a
conservation law associated with each continuous symmetry property of
a system, there seems to be a clear formal route for locating causal
claims within physics.
It is unclear, however, how broadly applicable Dowe’s account
is. Classical electrodynamics satisfies energy-momentum conservation.
Yet while charged particles are associated with world lines, the
electromagnetic field, with which charged particles interacts, cannot
be associated with a world line along which energy is conserved. As
Marc Lange (2002) has argued, it is problematic to think of the
electromagnetic field energy as a kind of “stuff” that
flows in a uniquely identifiable manner among different field-regions.
Thus, Dowe’s conserved quantity account appears to be designed
for an ontology of discrete objects and it is unclear how the account
might be extended to cover field theories as well.
While conserved quantity accounts offer an analysis of the notion of
being causally related, they do not, on their own, provide a
distinction between cause and effect. To introduce the direction of
the causal relation, Dowe supplements his conserved quantity account
by appealing to Reichenbach’s fork asymmetry (1956).
Reichenbach distinguishes forks that are temporally open from forks
that are closed. If there is an event C occurring in past that
screens off A from B, but there is no screening-off
event in the future of A and B, then this constitutes an
open fork. If there is an event C in the past and in addition
an event \(C'\) in the future of A and B that screen off
A from B, we have a closed fork. Now,
Reichenbach’s fork asymmetry thesis consists in the claim that
all open forks are open toward the future. Dowe (2000: 204) defines
the direction of causal processes by the direction of the majority of
open forks (thereby, in principle, allowing for the possibility of
backward causation). 
Thus, in order to define the direction of causation, the conserved
quantity accounts need to be supplemented by probabilistic
information. It has also been argued that conserved quantity accounts
cannot adequately distinguish those features of a process that are
explanatorily relevant from those that are not without relying on
counterfactuals (Woodward 2003b [2019]). Earman (2014) argues that the
most appropriate way to characterize causal processes is in terms of
systems governed by equations that allow for a well-posed initial
value formulation, which are systems governed by hyperbolic equations
(see also Woodward 2016).
Several authors have argued that the causal asymmetry and the
direction of causation are closely related to the thermodynamic
asymmetry that the entropy of a closed system does not decrease. An
early discussion of possible connection between the two
“temporal arrows” occurs in Reichenbach (1956). More
recent discussions of connections between the two arrows argue more
perspicuously that the thermodynamic and the time-asymmetry of
causation have as their common origin the initial randomness
assumption or the assumption of initial microscopic chaos. The
assumption of initial microscopic chaos is a central assumption in
neo-Boltzmannian accounts of the thermodynamic asymmetry. These
accounts posit a cosmological hypothesis, according to which the
universe began its life in an extremely low entropy state \(M(0),\)
which Richard Feynman has dubbed the Past Hypothesis (1965),
together with an equiprobability distribution over all microstates
compatible with \(M(0).\) David Albert and Barry Loewer argue that the
package consisting of past hypothesis, probability postulate, and the
dynamical laws on the microlevel—a package they call the
Mentaculus after the Coen Brother’s movie A Serious
Man—not only can account for the thermodynamic asymmetry
but also provides the probabilities for every macroscopic
generalization as well as the machinery to ground the causal asymmetry
and epistemic asymmetries concerning our access to past and future
events (Albert 2000, 2015; Loewer 2007, 2012).
While on Albert and Loewer’s account the causal asymmetry is
ultimately grounded in the probability postulate, they attempt to
derive this asymmetry via a somewhat circuitous route. The first step
is to argue that the Mentaculus implies a branching tree structure
toward the future on the macrolevel, according to which the
universe’s macrostate at a time is compatible with many more
different macro-evolutions toward the future than macro-evolutions
toward the past. In a second step they argue that this branching tree
structure underwrites an asymmetry of counterfactual dependence on the
macro-level and thereby supports a broadly Lewisian counterfactual
analysis of the temporal arrow of causation. The claim that the
Mentaculus implies a temporally asymmetric branching tree structure of
the kind postulated by Loewer is criticized in Frisch (2010): since
future higher entropy macro states occupy vastly larger regions of
phase space than lower entropy past states, thermodynamically normal
evolutions, if anything, suggest an upside-down tree
 structure.
Other authors have proposed more direct arguments for deriving the
causal asymmetry from assumptions made in the foundations of
thermodynamics, than the one developed by Albert and Loewer, arguing
that we can derive the common cause principle and thereby the
direction of causation directly from the assumption of initial
probabilistic independence. For a large class of microscopic
conditions the probabilistic independence assumption follows from the
assumption of initial microscopic chaos (Horwich 1987; Papineau
1985). In particular, as Arntzenius argues, the probabilistic
independence assumption will be satisfied for spatially separated
microstates if we assume initial microscopic chaos (Arntzenius 1999
[2010]). There is also an active debate in the literature on how the
causal and thermodynamic asymmetries relate to various epistemic
asymmetries, such as an asymmetry of records or an asymmetry
concerning our epistemic access to the past and to the future. For
different accounts of the knowledge asymmetry and an asymmetry of
records see Horwich 1987; Albert 2000, 2015; Loewer 2007; Frisch 2007,
2014; and Ismael 2016.
Most authors exploring the connection between the thermodynamic and
causal asymmetries argue that the causal asymmetry is ultimately
grounded in some facts about the initial state of the universe. Some
authors, however, have argued that the explanatory direction is
reversed and that the causal asymmetry accounts for the asymmetry
between prevailing initial and final conditions. Maudlin has argued
that the difference between initial and final conditions is a
reflection of causal laws that, he maintains, underwrite the passage
of time (Maudlin 2007: 131). 
Electromagnetic radiation phenomena exhibit a temporal asymmetry: we
observe radiation coherently diverging from a radiating source, such
the light emitted by a star, but we do not observe radiation
coherently converging into a source, unless we delicately set up such
a system. What can explain this asymmetry? And how is the asymmetry
related to the causal asymmetry, on the one hand, and the
thermodynamic asymmetry, on the other? Debates on these questions have
a long history. On one side we find both physicists and philosophers
who maintain—in the case of physicists sometimes more, sometimes
less explicitly—that the “arrow of radiation” is a
manifestation of a causal asymmetry (Ritz 1908; Einstein 1909a;
Jackson 1999; Griffiths 2017; Rohrlich 2006; Frisch 2005; 2006;
2014). On the other side, there are physicists and philosophers who
maintain that the arrow of radiation has the same root as the
thermodynamic arrow, an asymmetry between prevailing initial and final
conditions (Einstein 1909b; Wheeler & Feynman 1945; Price 1997;
2006; North 2003; Zeh 2007; Earman 
2011).[5]
This debate is far from settled. The laws of classical
electrodynamics, the Maxwell-Lorentz equations, imply a wave equation,
which is a time-reversal invariant hyperbolic equation of motion and
is standardly solved using the Green’s functions formalism.
Thus, the disagreement is partly a disagreement over the question
whether the radiation field’s causal or retarded Green’s
function captures important features of how charged particles and
electromagnetic fields interact that are not properly captured by the
“anti-causal” or advanced Green’s functions.
Some authors argue that the asymmetry of radiation is, like the
thermodynamic asymmetry purely a macroscopic phenomenon (Price 1997;
Field 2003). But this claim is not borne out by how physicists treat
interactions between charges and fields. To the extent that
microscopic charged particles can be (and in fact are) modeled
classically, their interactions with fields are modeled as exhibiting
temporal asymmetries just as macroscopic field sources do (Jackson
1999). These asymmetries include the fact that accelerating
microscopic charged particles are damped (since they radiate off part
of the energy they receive), rather than being anti-damped (extracting
additional energy from the surrounding field) (Rohrlich 2007). Thus, whatever the correct account of the
asymmetry of radiation is, it has to apply to microscopic charged
particles as well as to macroscopic collections of charges.
Authors who try to derive the radiation arrow from probabilistic
considerations sometimes argue that coherently converging waves do not
occur because such waves would require a radically improbable
coordinated behavior of incoming waves (Earman 2011; see also Atkinson
2006). But this line of argument is in danger of committing what Price
has called “the temporal double-standard fallacy” (Price
1997). From an acausal perspective the coordinated behavior of
outgoing waves in the future of a radiating source should appear to be
no less improbable than coordinated behavior of incoming waves in the
past of the source. Thus, probabilistic accounts that deny a
fundamental causal asymmetry need to be careful to avoid probabilistic
arguments for the asymmetry ultimately driven by causal intuitions and
presumably have to be content with positing the initial randomness
assumption as a fundamental de facto asymmetry that cannot be
further justified.
It is important here, too, to be clear on what is at stake in the
debate. Within the metaphysical project, the debate concerns the
question what the fundamental grounds for the asymmetry of radiation
phenomena are: is the asymmetry an expression of a fundamental causal
asymmetry or is it due to an asymmetry between prevailing initial and
final conditions?
From the perspective of a purely functional project, by contrast,
there is less disagreement between these two views than it may
initially seem. From a functionalist perspective, defenders of a
causal picture and defenders of a probabilistic account can be
understood as emphasizing two different aspects that both are integral
features of causal models: an initial independence assumption and
directed causal relationships among variables. Thus, certain
criticisms of causal accounts of the asymmetry of radiation, such as
Earman (2011), are most charitably understood as attacking a
metaphysical account of the role of causation in accounting for
radiation phenomena. Earman’s criticism cannot undermine a
functional account of causation in physics, since it invokes the very
same probabilistic considerations that, on a functional account,
underwrite representing radiation phenomena in terms of causal
models.
It is often argued that quantum mechanics is particularly inhospitable
to causal notions. Early discussions of a putative tension between
causal notions and quantum mechanics focused mainly on the
indeterminism of quantum mechanics. More recent discussions by
contrast, focus on the problem that nonlocal quantum correlations
violate Bell inequalities as presenting a challenge to causal
analyses. In a standard setup of a Bell-type experiment one considers
two observers who perform experiments in two spatially separated
laboratories on two entangled subsystems. The two experiments are
performed independently but can have outcomes that can be correlated
in ways that are not readily accounted for by classical causal models.
There are different strategies for deriving Bell inequalities.
Particularly helpful analyses for the status of various causal
assumptions are developed in Wiseman and Cavalcanti (2017) and Wood
and Spekkens (2015), who examine how to apply Pearl’s structural
causal models to Bell experiments (Myrvold, Genovese, & Shimony
2019). Simplifying somewhat, Wiseman and Cavalcanti assume that Bell
experiments take place in Minkowski spacetime and have real outcomes
that are not relative to anything. They then show that the quantum
correlations predicted for Bell experiments conflict with the
conjunction of three postulates: relativistic causality,
according to which an event’s causal past is its past lightcone;
free choice, which states that measurement settings can be
freely chosen and, hence, have no causes within the system under
consideration; and Reichenbach’s principle of a common
cause, according to which correlations among events that are not
related as cause and effect are explained by a common cause in their
joint past that screens off the correlation.
If we want to accept the quantum mechanical predictions, which appear
to be empirically well-confirmed, we have to reject at least one of
the postulates. Rejecting free choice amounts to accepting
superdeterminism, according to which measurement settings cannot be
freely chosen. Alternatively, we can give up relativistic
causality, either by allowing for superluminal influences from
the outcomes at one wing on that at the other wing, or by positing
retro-causal relations, which allow measurement outcomes to influence
the earlier state of the source (see, e.g., Price 2012; and references
in Friederich & Evans 2019).
While these strategies allow us to retain Reichenbach’s
principle that correlations among non-causally related events are
explained by a common cause that screens off these correlations, they
come at a price and, as Wood and Spekkens (2015) show, violate a
condition that goes by the names of faithfulness (Spirtes,
Glymour, & Scheines 1993 [2000: 35]), stability (Pearl
2009: 49), or no-fine-tuning and which states that every
causal dependence implies a probabilistic dependence. Wood and
Spekkens show that faithfulness, the causal Markov condition, and the
assumptions that the quantum predictions are correct form an
inconsistent set. Thus, at least one of the two
conditions—faithfulness or the Markov condition—has to be
given up.
What speaks for retaining faithfulness is that it is a central
assumption in many causal discovery algorithms. Yet there are also
arguments suggesting that faithfulness cannot be a necessary condition
on causal models (Cartwright 2001). Paradigmatic cases of violations
of faithfulness involve cancelations among different causal paths, as
they occur in feedback-control structures. For example, ambient
temperature is causally relevant to human body temperature, even
though body temperature is probabilistically independent of ambient
temperature over a wide range of ambient temperatures, since the human
body responds to changes in ambient temperatures through various
mechanisms along different causal routes, which are fine-tuned in a
way that allow the body to maintain a constant core temperature. Thus,
faithfulness arguably is not a necessary condition for causal
models.
One might reply, however, that canceling path violations of
faithfulness result from a system’s specific causal structure:
the causal structures at issue appear to be designed precisely to
allow for what amounts to violations of faithfulness. By contrast,
Wood and Spekkens show that if we hold on to the Markov condition,
then violations of faithfulness have to be a generic feature of
quantum causal systems that violate the Bell inequalities. It is
unclear, whether a plausible account of such generic violations of
faithfulness in quantum systems involving cancelling paths can be
given. Näger (2016) explores several alternative ways in which
faithfulness might be violated in quantum causal systems. By contrast,
Glymour (2006) argues that instead of giving up faithfulness, we ought
to reject the Markov condition in quantum contexts.
Another response to the problem of embedding correlations among
outcomes in a causal structure has been to assume a type of holism
which prohibits treating the spatially separated parts of the
entangled system as distinct subsystems. The suggestion is instead to
think of the measurement results at the two wings of the experiment as
“a single indivisible non-local event” (Skyrms 1984).
Hausman and Woodward (1999) argue that it also follows from an
interventionist analysis that the measurement outcomes at the two
wings of the experiment ought to be treated as a single non-local
event. How to causally model entangled states remains a debated
question.
One might want to conclude from the fact that quantum correlations are
incompatible with the conjunction of faithfulness and the Markov
condition that causal notions are inapplicable in the quantum realm.
Yet we can experimentally interact with quantum systems and can
intervene in and control such systems in ways that appear to be causal
in similar ways to our interactions with classical physical systems.
Moreover, as Sally Shrapnel (2014) has argued, there are macroscopic
phenomena, such as the avian magneto-compass, that seem to require
multi-level explanations that include quantum causal effects, which
play an apparently causal, difference-making role. Efforts, such as
Wood and Spekkens (2015) to try to develop causal representations of
our interactions with quantum systems have led to the emergence of a
research field devoted to extending the framework of structural causal
models to quantum mechanics and to develop quantum causal models
(Costa & Shrapnel 2016; Allen et al. 2017; Shrapnel 2019). From
the perspective of the three philosophical projects we distinguished
above, these efforts are most naturally seen as engaging in the
functional project of developing (and de novo engineering)
causal concepts appropriate for the quantum realm and showing how such
concepts can play a useful role in explanations and in capturing the
ways in which we can manipulate and control quantum systems.
The philosophical literature on causal explanation in general and in
physics, more specifically, has developed largely independently of,
and without engaging with, philosophical discussions in the
neo-Russellian tradition questioning the legitimacy of causal concepts
in physics (with Woodward’s work being a notable exception). In
fact, while neo-Russellian arguments have attracted renewed attention
since the turn of the twenty-first century and continue to be widely
endorsed, causal or causal-mechanical theories of explanation, which
were developed in response to problems faced by the
deductive-nomological model of scientific explanation developed by
Carl Hempel (Hempel & Oppenheim 1948), arguably have become the
default view in the philosophical literature on scientific explanation
(Woodward 2003b [2019]). In fact, one central recent debate within
this literature takes for granted that there are causal explanations
and then discusses whether all scientific explanations are
causal (Lewis 1986; Skow 2014) or whether there is room for genuinely
non-causal scientific explanations as well (Lange 2016).
Earlier defenders of causal accounts of explanation took one
distinguishing feature of causal accounts to be their metaphysical,
or—as Alberto Coffa called it—ontic conception of
explanation. The goal of explanation, on this conception is to locate
a phenomenon within the objective “causal nexus” (Salmon
1984: 120). Yet as Woodward’s (2003a) work shows, is that it
also possible to investigate the relation between explanation and
causation within the functional project of examining the cognitive
role of explanatory and causal judgments and their connection to
prediction, manipulation, and control.
Causal imperialists, as we might call them, argue that all
scientific explanations are fundamentally causal. Neo-Russellians, by
contrast, deny that causal notions and causal explanation can play any
role in suitably fundamental theories of physics. Yet, despite their
stark disagreement, neo-Russellians and causal imperialists share a
commitment to what Woodward has called “the hidden structure
strategy” (Woodward 2003b [2019]). Both views are committed to
the existence of what Peter Railton has called an “ideal
explanatory text” (Railton 1981) that contains all the
information relevant to a complete explanation of some phenomenon.
While actual explanations may fall short of providing us with the
complete information contained in the ideal explanatory text, they are
explanatory, according to the hidden structure strategy, in virtue of
providing us with some information about the text.
For the neo-Russellian, the fundamental explanatory structures consist
of microphysically complete dynamical models of the backward lightcone
of a given explanandum. While the neo-Russellian view is
compatible with the claim that in some non-fundamental domains and for
pragmatic reasons information about the ideal explanatory text may
fruitfully be presented in causal terms, the view holds that ideal
physical explanations are not causal. Causal imperialism turns this
picture on its head: for Lewis and others the underlying ideal
explanatory structures are causal structures. Hence all explanations
are causal in virtue of the fact that they provide information about
this structure, even though the information provided in an actual
explanation may not be presented in causal terms.
As Woodward (2003b [2019]) has argued, a problem for the hidden
structure strategy is to explain how hidden structures that are
epistemically inaccessible to us can account for the explanatory
import of the explanatory accounts we give. For the neo-Russellian the
problem is that we seem to be able to provide successful causal
explanations of phenomena even when the complete initial data that are
part of the ideal explanatory text are in principle inaccessible to
us. The causal imperialist’s version of the hidden structure
strategy faces an analogous problem. There are apparently successful
explanations of phenomena that do not identify causes of the
phenomenon.
Consider for example an explanation of the heat capacity of metals
and, in particular, of the fact that the heat capacity is much lower
than predicted classically (Kittel 2005: 141ff). The explanation
appeals to the Pauli exclusion principle and shows how the heat
capacity depends on particle statistics. In order to get the correct
result for the heat capacity, we need to model free electrons in the
metal as satisfying the quantum-mechanical Fermi-Dirac statistics and
the exclusion principle. The explanation appeals to the structure of
the phase-space available to the electrons.
According to causal imperialists, such as Lewis, this explanation is
causal by virtue of the fact that it provides information about the
causal history of a sample of metal. Does this construal of the
explanation as pointing to a hidden causal structure allow us to make
perspicuous its explanatory import? As we have seen, Pearl’s and
Woodward’s accounts of causation emphasize two features as
characteristic functions of causal notions. First, knowledge of causal
structures allows us to identify relationships amenable to
manipulation and control; and second, common cause reasoning enables
us to draw inferences from one time to another even when we possess
only incomplete knowledge of the state of a system on an initial or
final value surface.
Now, the explanation of the heat capacity embeds its
explanandum into patterns of functional dependencies and
allows us to answer how the heat capacity would change if the
available phase space were different. That is, the explanation does
enable us to answer what Woodward calls
what-if-things-had-been-different questions (Woodward 1979),
which, according to Woodward’s account of explanation is an
important feature of causal explanations. Yet the counterfactuals at
issue cannot be interpreted in terms of interventions or manipulations
on the electron states. Indeed, the fact that the value of the heat
capacity of metals follows from structural features of the
electrons’ phase space and is not something that, even in
principle, is open to manipulation or control arguably is itself
explanatorily relevant. Thus, one might worry that by classifying
explanations such as this as causal the causal imperialist obliterates
what is an important distinction between different explanatory
functions and epistemic goals.