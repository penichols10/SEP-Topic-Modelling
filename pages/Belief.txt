It is common to think of believing as involving
entities—beliefs—that are in some sense contained in the
mind. When someone learns a particular fact, for example, when Kai
reads that astronomers no longer classify Pluto as a planet, he
acquires a new belief (in this case, the belief that astronomers no
longer classify Pluto as a planet). The fact in question—or more
accurately, a representation, symbol, or marker of that fact—may
be stored in memory and accessed or recalled when necessary. In one
way of speaking, the belief just is the fact or proposition
represented, or the particular stored token of that fact or
proposition; in another way of speaking, the more standard in
philosophical discussion, the belief is the state of having such a
fact or representation stored. (Despite the ease with which we slide
between these different ways of speaking, they are importantly
distinct: Contrast the state of having hot water in one’s water
heater—the state of being “hot-water ready”,
say—with the stuff actually contained in the heater, that
particular mass of water, or water in general.)
It is also common to suppose that beliefs play a causal role in the
production of behavior. Continuing the example, we might imagine that
after learning about the demotion of Pluto, Kai naturally turns his
attention elsewhere, not consciously considering the matter for
several days, until when reading an old science textbook he encounters
the sentence “our solar system contains nine planets”.
Involuntarily, his new knowledge about Pluto is called up from memory.
He finds himself doubting the truth of the textbook’s claim, and
he says, “actually, astronomers no longer accept that”. It
seems plausible to say that Kai’s belief about Pluto, or his
possession of that belief, caused, or figured in a causal explanation
of, his utterance.
Various elements of this intuitive characterization of belief have
been challenged by philosophers, but it is probably fair to say that
the majority of contemporary philosophers of mind accept the bulk of
this picture, which embodies the core ideas of the
representational approach to belief, according to which
central cases of belief involve someone’s having in her head or
mind a representation with the same propositional content as the
belief. (But see §2.2, below, for some caveats, and see the entry
on
 mental representation.)
 As discussed below, representationalists may diverge in their
accounts of the nature of representation, and they need not agree
about what further conditions, besides possessing such a
representation, are necessary if a being is to qualify as having a
belief. Among the more prominent advocates of a representational
approach to belief are Fodor (1975, 1981, 1987, 1990), Millikan (1984,
1993), Dretske (1988), Cummins (1996), Burge (2010), Mandelbaum (2016;
Quilty-Dunn and Mandelbaum 2018), and Zimmerman (2018).
One strand of representationalism, endorsed by Fodor, takes mental
representations to be sentences in an internal language of
thought. To get a sense of what this view amounts to, it is
helpful to start with an analogy. Computers are sometimes
characterized as operating by manipulating sentences in “machine
language” in accordance with certain rules. Consider a
simplified description of what happens as one enters numbers into a
spreadsheet. Inputs from the keyboard cause the computer, depending on
the programs it is running and its internal state, to instantiate or
“token” a sentence (in machine language) with the content
(translated into English) of, for example, “numerical value 4 in
cell A1”. In accordance with certain rules, the machine then
displays the shape “4” in a certain location on the
monitor, and perhaps, if it is implementing the rule “the values
of column B are to be twice the values of column A”, it tokens
the sentence “numerical value 8 in cell B1” and displays
the shape “8” in another location on the monitor. If we
someday construct a robot whose behavior resembles that of a human
being, we might imagine it to operate along broadly the lines
described above—that is, by manipulating machine-language
sentences in accordance with rules, in connection with various
potential inputs and outputs. Such a robot might somewhere store the
machine-language sentence whose English translation is “the
chemical formula for water is H2O”. We might suppose
this robot is able to act as does a human who possesses this belief
because it is disposed to access this sentence appropriately on
relevant occasions: When asked “of what chemical elements is
water compounded?”, the robot accesses the water sentence and
manipulates it and other relevant sentences in such a way that it
produces an appropriate response.
According to the language of thought hypothesis (see the entry on the
 language of thought hypothesis),
 our cognition proceeds rather like such a robot’s. The formulae
we manipulate are not in “machine language”, of course,
but rather in a species-wide “language of thought”. A
sentence in the language of thought with some particular propositional
content P is a “representation” of P. On
this view, a subject believes that P just in case she has a
representation of P that plays the right kind of role—a
“belief-like” role—in her cognition. That is, the
representation must not merely be instantiated somewhere in the mind
or brain, but it must be deployed, or apt to be deployed, in ways we
regard as characteristic of belief. For example, it must be apt to be
called up for use in theoretical inferences toward which it is
relevant. It must be ready for appropriate deployment in deliberation
about means to desired ends. It is sometimes said, in such a case,
that the subject has the proposition P, or a representation
of that proposition, tokened in her “belief box” (though
of course it is not assumed that there is any literal box-like
structure in the head).
Dretske’s view centers on the idea of representational systems
as systems with the function of tracking features of the world (for
similar views, see Millikan 1984, 2017; Neander 2017). Organisms,
especially mobile ones, generally need to keep track of features of
their environment to be evolutionarily successful. Consequently, they
generally possess internal systems whose function it is to covary in
certain ways with the environment. For example, certain marine
bacteria contain internal magnets that align with the Earth’s
magnetic field. In the northern hemisphere, these bacteria, guided by
the magnets, propel themselves toward magnetic north. Since in the
northern hemisphere magnetic north tends downward, they are thus
carried toward deeper water and sediment, and away from toxic,
oxygen-rich surface water. We might thus say that the magnetic system
of these bacteria is a representational system that functions to
indicate the direction of benign or oxygen-poor environments. In
general, on Dretske’s view, an organism can be said to represent
P just in case that organism contains a subsystem whose
function it is to enter state A only if P holds, and
that subsystem is in state A.
To have beliefs, Dretske suggests, is to have an integrated manifold
of such representational systems, acquired in part through associative
learning, poised to guide behavior. Given the lack of such a complex,
and the lack of associative learning, magnetosome bacteria cannot, on
Dretske’s view, rightly be regarded as literally possessing
full-fledged beliefs. But exactly how rich an organism’s
representational structure must be for it to have beliefs, and in what
ways, Dretske does not address, regarding it as a terminological
boundary dispute, rather than a matter of deep ontological
significance. (For more on belief in non-human animals see §4
below.)
If one accepts a representational view of belief, it’s plausible
to suppose that the relevant representations are structured
in some way—that the belief that P & Q,
for instance, shares something structurally in common with the belief
that P. To say this is not merely to say that the belief that
P & Q has the following property: It cannot be
true unless the belief that P is true. Consider the following
possible development of Dretske’s representational approach: An
organism has developed a system that functions to detect whether
P is or is not the case. It’s supposed to enter state
alpha when P is true; its being in alpha has the function of
indicating P. Also, the organism has developed a separate
system for detecting whether P & Q is the case.
It’s supposed to enter state beta when P &
Q is true; its being in beta has the function of indicating
P & Q. But alpha and beta have nothing important
in common other than what, in the outside world, they are supposed to
represent; they have no structural similarity; one is not compounded
in part from the other. Conceivably, all our beliefs could be set up
in this way, having as little in common as alpha and beta—one
internally unstructured representational state after another. To say
that mental representations are structured is in part to deny that our
minds work like that.
Among the reasons to suppose that our representations are structured,
Fodor argues, are the productivity and systematicity
of thought (Fodor 1987; Fodor and Pylyshyn 1988; Aizawa 2003). Thought
and belief are “productive” in the sense that we can
potentially think or believe an indefinitely large number of things:
that elephants despise bowling, that 245 + 382 = 627, that river
bottoms are usually not composed of blue beads. If representations are
unstructured, each of these different potential beliefs must, once
believed, be an entirely new state, not constructed from
representational elements previously available. Similarly, thought and
belief are “systematic” in the sense that an organism who
thinks or believes that Mengzi repudiated Gaozi will normally also
have the capacity (if not necessarily the inclination) to think or
believe that Gaozi repudiated Mengzi; an organism who thinks or
believes that dogs are insipid and cats are resplendent will normally
also have the capacity to think or believe that dogs are resplendent
and cats are insipid. If representations are structured, if they have
elements that can be shuffled and recombined, the productivity and
systematicity of thought and belief seem naturally to follow.
Conversely, someone who holds that representations are unstructured
has, at least, some explaining to do to account for these features of
thought. (So also, apparently, does someone who denies that belief is
underwritten or implemented by a representational system of any
sort.)
Supposing representations are structured, then, what kind of
structure do they have? Fodor notes that productivity and
systematicity are features not just of thought but also of language,
and concludes that representational structure must be linguistic. He
endorses the idea of an innate, species-wide language of thought (as
discussed briefly in §1.1 above); others tie the structure more
closely to the thinker’s own natural (learned) language (Harman
1973; Field 1978; Carruthers 1996). However, still others assert that
the representational structure underwriting belief isn’t
language-like at all.
A number of philosophers have argued that our cognitive
representations have, or can have, a map-like rather than a
linguistic structure (Lewis 1994; Braddon-Mitchell and Jackson 1996;
Camp 2007, 2018; Rescorla 2009; though see Blumson 2012 and Johnson
2015 for concerns about whether map-like and language-like structures
are importantly distinct). Map-like representational systems are both
productive and systematic: By recombination and repetition of its
elements, a map can represent indefinitely many potential states of
affairs; and a map-like system that has the capacity, for example, to
represent the river as north of the mountain will normally also have
the capacity to represent, by a re-arrangement of its parts, the
mountain as north of the river. Although maps may sometimes involve
words or symbols, nothing linguistic seems to be essential to the
nature of map-like representation: Some maps are purely pictorial or
combine pictorial elements with symbolic elements, like coloration to
represent altitude, that we don’t ordinarily think of as
linguistic.
The maps view makes nice sense of the fact that when a person changes
one belief, a multitude of other beliefs seem also to change
simultaneously and effortlessly: If you shift a mountain farther north
on a map, for example, you immediately and automatically change many
other aspects of the representational system (the distance between the
mountain and the north coast, the direction one must hike to go from
the mountain to the oasis, etc.). In contrast, if you change the
linguistic representation “the mountain peak is 15 km north of
the river” to “the mountain peak is 20 km north of the
river”, no other representation necessarily changes: It takes a
certain amount of inferential work to ramify the consequences
through the rest of the system. Since it doesn’t seem like
we’re constantly making such a plethora of inferences, the maps
view might have an advantage here. On the other hand, perhaps just
because the linguistic view requires inference for what appears to
happen automatically on the maps view, the linguistic view can more
easily account for failures of rationality, in which not all the
necessary changes are made and the subject ends up with an
inconsistent view. Indeed generally speaking it’s unclear how
the map view can accommodate inconsistent beliefs unless one allows a
proliferation of maps, with the complications that ensue (like
redundancy and mechanisms for relating the maps). Certain sorts of
indeterminacy may also be more difficult to accommodate in map-like
than in language-like structures. A linguistic representation like
“there are some lakes east of the mountain” can leave
completely unspecified how many lakes, of what shape, and where; a map
does not, it seems, as easily do this. One further point of apparent
difference between the two views will be discussed in §2.2 below.
Generally speaking, one might worry that the maps view
overgenerates and overspecifies beliefs, while the
linguistic view undergenerates and underspecifies
them.
A third and very different way of thinking about representational
structure arises from the perspective of connectionism, a
position in cognitive science and computational theory. According to
connectionism, cognition proceeds by activation streaming through a
series of “nodes” connected by adjustable
“connection weights”—somewhat as neural networks in
the brain can have different levels of activation and different
strengths of connection between each other. It is sometimes suggested
(e.g., by van Gelder 1990; Smolensky 1995; Shea 2007) that the
structure of connectionist networks is representational but
non-linguistic or non-“compositional”; and perhaps so also
is human representational structure. However, it would take us too far
afield to enter this technical issue here. (For more on this topic see
the entry on
 connectionism.)
Recent representational approaches sometimes especially emphasize the
normative dimension of belief. That is, they emphasize the idea that
it is central to a mental state’s being a belief as
opposed to some other mental state (e.g., a supposition, an imagining,
a desire) that it is necessarily defective in a certain way if it is
false. Shah and Velleman (2005) argue that conceiving of an attitude
as a belief that P entails conceiving of it as governed by a
norm of truth, that is, as an attitude that is correct if and
only if P is true. Engel (2018) argues that among
propositional attitudes, belief is the only one whose
“correctness condition” is truth, distinguishing it from
other closely related mental states, such as acceptances and epistemic
feelings. Burge (2010) argues that the “primary constitutive
function” of believing is the production of veridical
propositional representations. A related literature addresses whether
belief is essentially subject to a norm of truth (Wedgwood 2002;
McHugh and Whiting 2010; Gluer and Wikforss 2013).
Representationalist normativism has roots in the idea that
representational systems are functional systems of a certain sort
(Millikan 1984; Dretske 1988): Function appears to be a normative
concept, implying at least a contrast with malfunction. Similarly, but
with no commitment to representationalism specifically, belief has
often been described as having a “direction of fit” in the
sense that beliefs (unlike, for example, desires) ought to fit with,
or get it right about, or match up to, the states of affairs they
describe or represent (Anscombe 1957/1963; Searle 1983; Humberstone
1992; Frost 2014). If you believe that P and P is false,
you have erred or made a mistake, whereas if you desire that P
and P is false, you have not in the same way erred or made a
mistake.
While representationalists like Fodor, Dretske, and Mandelbaum contend
that having the right internal, representational structure is
essential to having beliefs, another group of philosophers treats the
internal structure of the mind as of only incidental relevance to the
question of whether a being is properly described as believing. One
way to highlight the difference between this view and
representationalism is this: Imagine that we discover an alien being,
of unknown constitution and origin, whose behavior and overall
behavioral dispositions are perfectly normal by human standards.
“Rudolfo”, say, emerges from a spacecraft and integrates
seamlessly into U.S. society, becoming a tax lawyer, football fan, and
Democratic Party activist. Even if we know next to nothing about what
is going on inside his head, it may seem natural to say that Rudolfo
has beliefs much like ours—for example, that the 1040 is
normally due April 15, that a field goal is worth 3 points, and that
labor unions tend to support Democratic candidates. Perhaps we can
coherently imagine that Rudolfo does not manipulate sentences in a
language of thought or possess internal representational structures of
the right sort. Perhaps it is conceptually, even if not physically,
possible that he has no complex, internal, cognitive organ, no real
brain. But even if it is granted that a creature must have human-like
representations in order to behave thoroughly like a human being, one
might still think that it is the pattern of actual and potential
behavior that is fundamental in belief—that representations
are essential to belief only because, and to the extent to, they
ground such a pattern. Dispositionalists and
interpretationists are drawn to this way of thinking.
Traditional dispositional views of belief assert that for someone to
believe some proposition P is for that person to possess one
or more particular behavioral dispositions pertaining to P.
Often cited is the disposition to assent to utterances of P
in the right sorts of circumstances (if one understands the language,
wishes to reveal one’s true opinion, is not physically
incapacitated, etc.). Other relevant dispositions might include the
disposition to exhibit surprise should the falsity of P make
itself evident, the disposition to assent to Q if one is
shown that P implies Q, and the disposition to
depend on P’s truth in executing one’s plans.
Perhaps all such dispositions can be brought under a single heading,
which is, most generally, being disposed to act as though P
is the case. Such actions are normally taken to be at least pretty
good prima facie evidence of belief in P;
the question is whether being disposed, overall, so to act is
tantamount to believing P, as the dispositionalist
thinks, or whether it is merely an outward sign of belief. Braithwaite
(1932–1933) and Marcus (1990) are prominent advocates of the
traditional dispositional approach to belief (though Braithwaite
emphasizes in his analysis another form of belief, rather like
“occurrent” belief as described in §2.1 below).
There are two standard objections to traditional dispositional
accounts of belief. The first, tracing back at least to Chisholm
(1957), assumes that the dispositionalist’s aim is to
reduce or analyze facts about belief entirely into
facts about outward behavior, facts specifiable without reference to
other beliefs, desires, inner feelings, and so forth (see the entry on
philosophical
 behaviorism).
 Such a reduction or analysis appears impossible for the following
reason: People with the same belief may behave very differently,
depending on their other beliefs, desires, and so forth. For example,
a person who believes that it will rain will only be disposed to take
an umbrella if she also believes that the umbrella will ward off the
water and if she doesn’t want to get wet. Change the surrounding
beliefs and desires and very different behavior may result. A
dispositionalist attempting to specify the particular behavioral
dispositions associated with, for example, the belief that it’s
raining will then either get it wrong about the dispositions of some
people (such as those who like to get wet) or will be forced to
incorporate into her dispositional analysis conditional antecedents
invoking the very ideas she is trying to analyze or reduce
away—saying, for example, that the person who believes that
P will behave in such-and-such a way if she also
believes X and desires Y—apparently dooming
the reductionist project. (It may be possible to avoid this objection
by invoking a “Ramsey”-like approach to the reduction [see
the section on Functional States and Ramsey Sentences in the entry on
 functionalism
 and Lewis 1972], but this type of analysis was not widely discussed
until after traditional dispositional approaches to belief had gone
largely out of fashion.)
The second standard objection to traditional dispositional accounts of
belief is to note the loose connection between belief and behavior in
some cases—for example, in a recently paralyzed person, or in
someone who wants to keep a private opinion (e.g., a Muscovite who
believes, in 1937, that Stalin’s purges are morally wrong), or
in matters of very little practical relevance (e.g., an American
homebody’s belief that there is at least one church in Nice).
Again, the traditional dispositionist seems faced with a choice
between oversimplifying (and thus mischaracterizing some
people’s dispositions) and loading the dispositions with
potentially problematic or unwieldy conditional antecedents (e.g.,
she’d get the umbrella if her paralysis healed;
he’d speak up if the political climate changed). On the
other hand, however, the demand for an absolutely precise
specification of the conditions under which a disposition will be
manifested, without exception, may be excessive. As Cartwright (1983)
has noted, even perfectly respectable claims in the physical sciences
often hold only ceteris paribus or “all else being
equal”.
In light of these concerns and others, most recent philosophers
sympathetic with the view described in the first paragraph of this
section have abandoned traditional dispositionalism. They divide into
roughly two classes, which we may call liberal
dispositionalists and interpretationists. Liberal
dispositionalists avoid the first objection by abandoning the
reductionist project associated with traditional dispositionalism.
They permit appeal to other mental states in specifying the
dispositions relevant to any particular belief—including other
beliefs and desires. They also broaden the range of dispositions
considered relevant to the possession of a belief so as to include at
least some dispositions to undergo private mental episodes that do not
manifest in outwardly observable behavior—dispositions, for
example, for the subject to feel (and not just exhibit) surprise
should she discover the falsity of P, for her privately to
draw conclusions from P, to feel confidence in the truth of
P, to utter P silently to herself in inner speech,
and so forth. This appears also to mitigate the second objection to
some extent: The Muscovite possesses his belief about Stalin’s
purges at least as much in virtue of the things he says silently to
himself and the disapproval he privately feels as in virtue of his
disposition to express that opinion were the political climate to
change. Advocates of views of this sort include Price (1969), Audi
(1972), Baker (1995), Schwitzgebel (2002, 2013), and arguably Ryle
(1949) and Ramsey (1926 [1990], 1927–1929 [1991]; see Wright
2017).
However, a philosopher approaching belief with the specific goal of
defending physicalism or materialism—the view that everything in
the world, including the mind, is wholly physical or material (see
 physicalism)—might
 have reason to be dissatisfied with liberal dispositionalism, for the
very reason that it abandons the reductionist project. Although
liberal dispositional accounts of belief are consistent with
physicalism, they do not substantially advance that thesis,
since they relate belief to other mental states that may or may not be
seen as physical. The defense of physicalism was one of the driving
forces in philosophy of mind in the period during which the most
influential approaches to belief in contemporary analytic philosophy
of mind were developed—the 1960s through the 1980s—and it
was one of the principal reasons philosophers were interested in
accounts of propositional attitudes such as belief. Consequently, the
failure of liberal dispositionalism to advance the physicalist thesis
might be seen as an important drawback.
Interpretationism shares with dispositionalism the emphasis on
patterns of action and reaction, rather than internal representational
structures, but retains the focus, abandoned by the liberal
dispositionalist, on observable behavior—behavior
interpretable by an outside observer. Since behavior is widely assumed
to be physical, interpretationism can thus more easily be seen as
advancing the physicalist project. The two most prominent
interpretationists have been Dennett (1978, 1987, 1991) and Davidson
(1984; see
 Donald Davidson;
 also see Lewis 1974).
To gain a sense of Dennett’s view, consider three different
methods we can use to predict the behavior of a human being. One
method, which involves taking what Dennett calls the “physical
stance”, is to apply our knowledge of physical law. We can
predict that a diver will trace a roughly parabolic trajectory to the
water because we know how objects of approximately that mass and size
behave in fall near the surface of the Earth. A second method, which
involves taking the “design stance”, is to attribute
functions to the system or its parts and to predict that the system
will function properly. We can predict that a jogger’s pulse
will increase as she heads up the hill because of what we know about
exercise and the proper function of the circulatory system. A third
method, which involves taking the “intentional stance”, is
to attribute beliefs and desires to the person, and then to predict
that they will behave rationally, given those beliefs and desires.
Much of our prediction of human behavior appears to involve such
attribution (though see Andrews 2012). Certainly, treating people as
mere physical bodies or as biological machines will not, as a
practical matter, get us very far in predicting what is important to
us.
On Dennett’s view, a system with beliefs is a system whose
behavior, while complex and difficult to predict when viewed from the
physical or the design stance, falls into patterns that may be
captured with relative simplicity and substantial if not perfect
accuracy by means of the intentional stance. The system has the
particular belief that P if its behavior conforms to a
pattern that may be effectively captured by taking the intentional
stance and attributing the belief that P. For example, we can
say that Heddy believes that a hurricane may be coming because
attributing her that belief (along with other related beliefs and
desires) helps reveal the pattern, invisible from the physical and
design stances, behind her boarding up her windows, making certain
phone calls, stocking up provisions, etc. All there is to having
beliefs, according to Dennett, is embodying patterns of this sort.
Dennett acknowledges that his view has the unintuitive consequence
that a sufficiently sophisticated chess-playing machine would have
beliefs if its behavior is very complicated from the design stance
(which would involve appeal to its programmed strategies) but
predictable with relative accuracy and simplicity from the intentional
stance (attributing the desire to defend its queen, the belief that
you won’t sacrifice a rook for a pawn, etc.).
Davidson also characterizes belief in terms of practices of belief
attribution. He invites us to imagine encountering a being with a
wholly unfamiliar language and then attempting the task of
constructing, from observation of the being’s behavior in its
environment, an understanding of that language (e.g., 1984, p.
135–137). Success in this enterprise would necessarily involve
attributing beliefs and desires to the being in question, in light of
which its utterances make sense. An entity with beliefs is a being for
whom such a project is practicable in principle—a being that
emits, or is disposed to emit, a complex pattern of behavior that can
productively be interpreted as linguistic, rational, and expressive of
beliefs and desires.
Dennett and Davidson both endorse the “indeterminacy” of
belief attributions: In at least some cases, multiple incompatible
interpretive schemes may be equally good, and thus there may be no
fact of the matter which of those schemes is “really” the
correct one, and thus whether the subject “really”
believes P, if belief that P is attributed by one
scheme but not by the other.
Many philosophers identify themselves as functionalists (see
 functionalism)
 about mental states in general or belief in particular. Functionalism
about mental states is the view that what makes something a mental
state of a particular type are its actual and potential, or its
typical, causal relations to sensory stimulations, behavior, and other
mental states (seminal sources include Armstrong 1968; Fodor 1968;
Lewis 1972, 1980; Putnam 1975; Block 1978). Functionalists generally
contrast their view with the view that what makes something a mental
state of a particular type are facts about its internal structure. To
understand this distinction, it may be helpful to begin with some
non-mental examples. Arguably, what makes something a streptococcal
bacterium, or a cube, is its shape or internal structure; its causal
history or proneness to produce particular effects on particular
occasions is only secondarily relevant, if at all. In contrast,
whether something is a hard drive or not is not principally a matter
of internal structure. A hard drive could be made of plastic or steel,
employ magnetic tape or lasers. What matters are the causal
relationships it’s prone to enter with a computer: Under certain
promptings, it enters states such that, under certain further
promptings, it will generate outputs of a certain sort. Internal
structure is relevant only secondarily, insofar as it grounds these
causal capacities. Likewise, according to the functionalist, what
makes a state pain is not its particular neural
configuration. People and animals with very different neural
configurations could all equally be in pain (even, conceivably, a
Martian with an internal structure radically different from ours could
suffer pain). What matters is that the subject is in a state that
(roughly) is apt to be caused by tissue damage or tissue stress and
that, in turn, is apt to cause signs of distress, withdrawal, future
avoidance of the painful stimulus, and (in verbal subjects) thoughts
and utterances like “that hurts!”.
Philosophers frequently endorse functionalism about belief without
even briefly sketching out the various particular functional
relationships that are supposed to be involved, though Loar (1981) is
a notable exception to this tendency (see also Leitgeb 2017). However,
among the causal relationships contemporary philosophers have often
seen as characteristic of belief are the following (these are sketched
here only roughly; they come in many versions differing in
nuance):
(1) Reflection on propositions (e.g., [Q] and [if Q
then P]) from which P straightforwardly follows, if
one believes those propositions and is not antecedently committed to
the falsity of P, typically causes the belief that
P.
(2) Directing perceptual attention to the perceptible properties of
things, events, or states of affairs, in conditions favorable to
accurate perception, typically causes the belief that those things,
events, or states of affairs have those properties (e.g., visually
attending to a red shirt in good viewing conditions will typically
cause the belief that the shirt is red).
(3) Believing that performing action A would lead to event or
state of affairs E, conjoined with a desire for E
and no overriding contrary desire, will typically cause an intention
to do A.
(4) Believing that P, in conditions favoring sincere
expression of that belief, will typically lead to an assertion of
P.
Loar emphasizes versions of (2) and (3) over (1) and (4), but one sees
conditions of this sort at least briefly alluded to by a number of
functionalist philosophers, including Dennett (1969, 1978), Armstrong
(1973), Stalnaker (1984), Fodor (1990), Pettit (1993), Shoemaker
(2003), and Zimmerman (2018). For the functionalist, to believe just
is to be in a state that plays (something like) this sort of causal
role.
As the list of names of the previous paragraph suggests, functionalism
is compatible with either a representationalist approach to belief (as
in Fodor) or an interpretationist one (as in Dennett). (The
interpretationist, of course, will have to treat the relevant
functional states as posits of an interpretative theory or scheme.)
Dispositional accounts of belief, too, can be functionalist. Indeed,
dispositional accounts can be seen as a special or limiting case of
functional accounts. To see this, it’s helpful to divide the
causal relations appealed to by functionalism into the
backward-looking and the forward-looking.
Backward-looking causal relations pertain to what actually,
potentially, or typically causes the state in question;
forward-looking causal relations pertain to what effects the
state in question actually, potentially, or typically has. Thus (1)
and (2) above are backward-looking causal relations, while (3) and (4)
are forward-looking. We might, then, see the dispositionalist as a
functionalist who thinks only the forward-looking causal relations are
definitive of belief: To believe is to be in a state apt to cause
such-and-such behavioral (or other) manifestations. (This view is, of
course, compatible with accepting the existence of regularities like
(1) and (2), as long as they are not regarded as defining
characteristics of belief.) Two caveats, however, should accompany
this reduction of dispositionalism to functionalism: First, insofar as
functionalism about belief requires a causal relationship
between the belief state and its manifestations in behavior (or in
other mental states), it will exclude dispositionalists like Ryle
(1949) who don’t view the disposition-manifestation relationship
causally (for discussion, see Section 6 (‘The causal efficacy
of dispositions’) of the entry on
 dispositions).
 Second, the liberal dispositionalist may wish to demur from the
functionalist’s usual commitment to the reducibility of facts
about functionally-definable mental states, en masse and in
principle (allowing for the intricate network of interrelationships
among them), to facts about sensory inputs and outward behavior.
The compatibility of functionalism and representationalism is not
evident on its face, though a number of prominent contemporary
philosophers appear to embrace both positions (e.g., Fodor 1968, 1975,
1981, 1990; Armstrong 1973; Harman 1973; Lycan 1981a, 1981b; Stalnaker
1984; Lewis 1994). As Millikan (1984), Papineau (1984), and others
have suggested, it seems one thing to say that to believe is to be in
a state that fills a particular causal role, and it seems
quite another to say that beliefs are essentially states that
represent how things stand in the world. How can something
represent the world outside simply by virtue of playing a certain
causal role in a cognitive system? Suppose, for example, that a state
represents by virtue of having an indicator function of the sort
described at the end of §1.1 above. The indicator function of an
internal state or system would seem, at least sometimes and in part,
to depend constitutively on the evolutionary history of that state or
system, or its learning history, and not simply on the causal
relationships it is currently disposed to enter. Despite the
word “function” in “functionalism”, it’s
not clear that standard functionalist accounts, limited as they are to
appeal to a state’s actual, potential, or typical causal roles,
can incorporate facts about a system’s evolutionary history or
learning history: Conceivably, for example, two states in different
individuals may have exactly analogous causal roles, yet differ in
their (as Millikan says) “proper function” because of
differences in the evolutionary or learning history of those
systems.
Three escapes from this potential difficulty suggest themselves. One
is to endorse a version of “conceptual [or functional] role
semantics” according to which the representational status and
content of a mental state is reducible just to facts about what is apt
to cause and to be caused by the mental state in question—that
is, to deny the relevance of remote evolutionary or learning history
to mental representation as not part of a proper functional
characterization (e.g., Harman 1973, 1987). Another is to accept that
causal role determines the representational status of a
mental state (i.e., that it is a representation) but does not
fully specify representational content (i.e. how that
representation represents things as being); but this seems to involve
abandoning full-blown functionalism. A third is to interpret more
liberally what it is for a mental state to be “typically
caused” (or perhaps “normally caused”) by some event
or state of affairs: Perhaps it is enough that in the young organism,
or its evolutionary ancestors, mental states of that sort were caused
in a particular way, or the system was selected to be responsive to
certain sorts of environmental factors. Such claims may be more easily
reconcilable with certain canonical statements of functionalism (such
as Lewis 1980) than with others (such as Putnam 1975). The issue has
not been as fully discussed as it should be.
Some philosophers have denied the existence of beliefs altogether.
Advocates of this view, generally known as eliminativism,
include Churchland (1981), Stich (in his 1983 book; he subsequently
moderated his opinion), and Jenson (2016). On this view,
people’s everyday conception of the mind, their “folk
psychology”, is a theory on par with folk theories about the
origin of the universe or the nature of physical bodies. And just as
our pre-scientific theories on the latter topics were shown to be
radically wrong by scientific cosmology and physics, so also will folk
psychology, which is essentially still pre-scientific, be overthrown
by scientific psychology and neuroscience once they have advanced far
enough.
According to eliminativism, once folk psychology is overthrown, strict
scientific usage will have no place for reference to most of the
entities postulated by folk psychology, such as belief. Beliefs, then,
like “celestial spheres” or “phlogiston”, will
be judged not actually to exist, but rather to be the mistaken posits
of a radically false theory. We may still find it convenient to speak
of “belief” in informal contexts, if scientific usage is
cumbersome, much as we still speak of “the sun going
down”, but if the concept of belief does not map onto the
categories described by a mature scientific understanding of the mind,
then, literally speaking, no one believes anything.
Instrumentalists about belief regard belief attributions as
useful for certain purposes, but hold that there are no definite
underlying facts about what people really believe, or that beliefs are
not robustly real, or that belief attributions are never in the
strictest sense true (these are not exactly equivalent positions,
though they are closely related). One sort of
instrumentalism—what we might call hard
instrumentalism—denies that beliefs exist in any sense.
Hard instrumentalism is thus a form of eliminativism, conjoined with
the thesis that belief-talk is nonetheless instrumentally useful
(e.g., Quine 1960, p. 221 [but for a caveat see p. 262–266]).
Another type of instrumentalism, which we might call soft
instrumentalism, grants that beliefs are real, but only in a less
robust sense than is ordinarily thought. Dennett (1991) articulates a
view of this sort. Consider as an analogy: Is the equator real? Well,
not in the sense that there’s a red stripe running through the
Congo; but saying that a country is on the equator says something true
about its position relative to other countries and how it travels on
the spinning Earth. Are beliefs real? Well, not perhaps in the sense
of being representations stored somewhere in the mind; but attributing
a belief to someone says something true about that person’s
patterns of behavior and response. Beliefs are as real as equators, or
centers of gravity, or the average Canadian. The soft instrumentalist
holds that such things are not robustly real (if that makes
sense)—not as real as mountains or masses or individual, actual
Canadians. They are in some sense inventions that capture something
useful in the structure of more robustly real phenomena; and yet at
the same time they are not mere fictions. Soft
instrumentalism in this sense comports naturally with approaches to
belief such as dispositionalism and interpretationism, to the extent
those positions treat belief attribution simply as a convenient means
of pointing toward certain patterns in a subject’s real and
hypothetical behavior.
For further discussion of eliminativism and the considerations for and
against it, see the entry on
 eliminative materialism.
Philosophers often distinguish dispositional from
occurrent believing. This distinction depends on the more
general distinction between dispositions and
occurrences. Examples of dispositional statements
include:
(1a) Corina runs a six-minute mile,
(1b) Leopold is excitable,
(1c) salt dissolves in water. 
These statements can all be true even if, at the time they are
uttered, Corina is asleep, Leopold is relaxed, and no salt is actually
dissolved in any water. They thus contrast with statements about
particular occurrences, such as:
(2a) Corina is running a six-minute mile,
(2b) Leopold is excited,
(2c) some salt is dissolving in water. 
Although (1a-c) can be true while (2a-c) are false, (1a-c) cannot be
true unless there are conditions under which (2a-c) would be true. We
cannot say that Corina runs a six-minute mile unless there are
conditions under which she would in fact do so. A dispositional claim
is a claim, not about anything that is actually occurring at the time,
but rather that some particular thing is prone to occur,
under certain circumstances.
Suppose Harry thinks plaid ties are hideous. Only rarely does the
thought or judgment that they are hideous actually come to the
forefront of his mind. When it does, he possesses the belief
occurrently. The rest of the time, Harry possesses the belief only
dispositionally. The occurrent belief comes and goes, depending on
whether circumstances elicit it; the dispositional belief endures. The
common representationalist warehouse model of memory and belief
suggests a way of thinking about this. A subject dispositionally
believes P if a representation with the content P is
stored in her memory or “belief box” (in the central,
“explicit” case: see §2.2). When that representation
is retrieved from memory for active deployment in reasoning or
planning, the subject occurrently believes P. As soon as she
moves to the next topic, the occurrent belief ceases.
As the last paragraph suggests, one needn’t adopt a
dispositional approach to belief in general to regard some beliefs as
dispositional in the sense here described. In fact, a strict
dispositionalism may entail the impossibility of occurrent belief: If
to believe something is to embody a particular dispositional
structure, then a thought or judgment might not belong to the right
category of things to count as a belief. The thought or judgment,
P, may be a manifestation of an overall
dispositional structure characteristic of the belief that P,
but it itself is not that structure.
Though the distinction between occurrent and dispositional belief is
widely employed, it is rarely treated in detail. A few important
discussions are Price (1969), Armstrong (1973), Lycan (1986), Searle
(1992), and Audi (1994). David Hume (1740) famously offers an account
of belief that treats beliefs principally as occurrences (see the
section on Causation: The Positive Phase in
 Hume),
 in which he is partly followed by Braithwaite (1932–1933).
It seems natural to say that you believe that the number of planets is
less than 9, and also that the number of planets is less than 10, and
also that the number of planets is less than 11, and so on, for any
number greater than 8 that one cares to name. On a simplistic reading
of the representational approach, this presents a difficulty. If each
belief is stored individually in representational format somewhere in
the mind, it would seem that we must have a huge number of stored
representations relevant to the number of planets—more than it
seems plausible or necessary to attribute to an ordinary human being.
And of course this problem generalizes easily.
The advocate of the maps view of representational structure (see
§1.1.1, above) can, perhaps, avoid this difficulty entirely,
since it seems a map of the solar system does represent all
these facts about the number of planets within a simple, tractable
system. However, representationalists have more commonly responded to
this issue by drawing a distinction between explicit and implicit
belief. One believes P explicitly if a
representation with that content is actually present in the mind in
the right sort of way—for example, if a sentence with that
content is inscribed in the “belief box” (see §1.1
above). One believes P implicitly (or
tacitly) if one believes P, but the mind does not
possess, in a belief-like way, a representation with that content.
(Philosophers sometimes use the term dispositional to refer
to beliefs that are implicit in the present sense—but this
invites confusion with the occurrent-dispositional distinction
discussed above (§2.1). Implicit beliefs are, perhaps,
necessarily dispositional in the sense of the previous subsection, if
occurrently deploying a belief requires explicitly tokening a
representation of it; but explicit beliefs may plausibly be
dispositional or occurrent.)
Perhaps all that’s required to implicitly believe something is
that the relevant content be swiftly derivable from something one
explicitly believes (Dennett 1978, 1987). Thus, in the planets case,
we may say that you believe explicitly that the number of planets is 8
and only implicitly that the number of planets is less than 9, less
than 10, etc. Of course, if swift derivability is the criterion, then
although there may be a sharp line between explicit and implicit
beliefs (depending on whether the representation is stored or not),
there will not be a sharp line between what one believes implicitly
and what, though derivable from one’s beliefs, one does not
actually believe, since swiftness is a matter of degree (see Field
1978; Lycan 1986).
The representationalist may also grant the possibility of implicit
belief, or belief without explicit representation, in cases of the
following sort (discussed in Dennett 1978; Fodor 1987). A
chess-playing computer is explicitly programmed with a large number of
specific strategies, in consequence of which it almost always ends up
trying to get its queen out early; but nowhere is there any explicitly
programmed representation with the content “get the queen out
early”, or any explicitly programmed representation from which
“get the queen out early” is swiftly derivable. The
pattern emerges as a product of various features of the hardware and
software, despite its not being explicitly encoded. While most
philosophers would not want to say that any currently existing
chess-playing computer literally has the belief that it should get its
queen out early, it is clear that an analogous possibility could arise
in the human case and thus threaten representationalism, unless
representationalism makes room for a kind of emergent, implicit belief
that arises from more basic structural facts in this way. However, if
the representationalist grants the presence of belief
whenever there is a belief-like pattern of actual or
potential behavior, regardless of underlying representational
structure, then the position risks collapsing into dispositionalism or
interpretationism. The issue of how to account for apparent cases of
belief without explicit representation poses an underexplored
challenge to representationalism.
Empirical psychologists have drawn a contrast between implicit and
explicit memory or knowledge, but this distinction does not map neatly
onto the implicit/explicit belief distinction described in Section
2.2.1. In the psychologists’ sense, explicit memory involves the
conscious recollection of previously presented information, while
implicit memory involves the facilitation of a task or a change in
performance as a result of previous exposure to information, without,
or at least not as a result of, conscious recollection (Schacter 1987;
Schacter and Tulving 1994; though see Squire 2004). For example, if a
subject is asked to memorize a list of word pairs—bird/truck,
stove/desk, etc.—and is then cued with one word and asked to
provide the other, the subject’s explicit memory is being
tested. If the subject is brought back two weeks later, and has no
conscious recollection of most of the word pairs on the list, then she
has no explicit memory of them. However, implicit memory of the
word-pairs would be revealed if she found it easier to learn the
“forgotten” pairs a second time. Knowledge that is
“implicit” in this sense will normally not be
implicit in the sense of the previous subsection (if it were swiftly
derivable from what one explicitly believes, presumably one could
answer the test questions correctly); it’s also at least
conceptually possible that some such psychologically implicit
knowledge may be stored stored “explicitly” in the sense
of the previous subsection.
A rather different empirical literature addresses the issue of
“implicit attitudes”, for example implicit racism or
sexism, which are often held to conflict with verbally or consciously
espoused attitudes. Such implicit attitudes might be revealed by
emotional reactions (e.g., more negative affect among white
participants when assigned to a co-operative task with a black person
than with a white person) or by association or priming tasks (e.g.,
faster categorization responses when white participants are asked to
pair negative words with dark-skinned faces and positive words with
light-skinned faces than vice versa). (For reviews, see Wittenbrink
and Schwarz, eds., 2007; Petty, Fazio, and Briñol, eds., 2009.)
However, it remains controversial to what extent tests of this sort
reveal subjects’ (implicit) beliefs, as opposed to
merely culturally-given associations or attitudes other than
full-blown belief (Wilson, Lindsey, and Schooler 2000; Kihlstrom 2004;
Lane et al. 2007; Hunter 2011; Tumulty 2014; Levy 2015; Machery 2016;
Madva 2016; Zimmerman 2018). Gendler, for example, suggests that we
regard such implicit attitudes as arational and automatic
aliefs rather than genuine evidence-responsive
beliefs (Gendler 2008a–b; for critique see Schwitzgebel
2010; Mandelbaum 2013).
Quine (1956) introduced contemporary philosophy of mind to the
distinction between de re and de dicto belief
attributions (as it is now generally called) by means of examples like
the following. Ralph sees a suspicious-looking man in a trenchcoat,
and concludes that that man is a spy. Unbeknownst to him, however, the
man in the trenchcoat is the newly elected mayor, Bernard J. Ortcutt,
and Ralph would sincerely deny the claim that “the mayor is a
spy”. So does Ralph believe that the mayor is a spy? There
appears to be a sense in which he does and a sense in which he does
not. Philosophers have attempted to characterize the difference
between these two senses by saying that Ralph believes de re,
of that man (the man in the trenchcoat who happens also to be the
mayor), that “he is a spy”, while he does not believe
de dicto that “the mayor a spy”.
The standard test for distinguishing de re from de
dicto attributions is referential transparency or
opacity. A sentence, or more accurately a position in a
sentence, is held to be referentially transparent if terms or phrases
in that position that refer to the same object can be freely
substituted without altering the truth of the sentence. The
(non-belief attributing) sentence “Jill kicked X”
is naturally read as referentially transparent in this sense. If
“Jill kicked the ball” is true, then so also is any
sentence in which “the ball” is replaced by a term or
phrase that refers to that same ball, e.g., “Jill kicked
Davy’s favorite birthday present”, “Jill kicked the
thing we bought at Toys ‘R’ Us on August 26”.
Sentences, or positions, are referentially opaque just in case they
are not transparent, that is, if the substitution of co-referring
terms or phrases could potentially alter their truth value. De
dicto belief attribution is held to be referentially opaque in
this sense. On the de dicto reading of belief, “Ralph
believes that the man in the trenchcoat is a spy” may be true
while “Ralph believes that the mayor is a spy” is false.
Likewise, “Lois Lane believes that Superman is strong” may
be true while “Lois believes that Clark Kent is strong” is
false, even if Superman and Clark Kent are, unbeknownst to Lois, one
and the same person. (Regarding the Lois example, however, see also
§3.4, on Frege’s Puzzle, below.)
In some contexts, the liberal substitution of co-referential terms or
phrases seems permissible in ascribing belief. Shifting example,
suppose Davy is a preschooler who has just met a new teacher, Mrs.
Sanchez, who is Mexican, and he finds her too strict. Davy’s
mother, in reporting this fact to his father, might say “Davy
thinks Mrs. Sanchez is too strict” or “Davy thinks the new
Mexican teacher is too strict”, even though Davy does not know
the teacher’s name or that she is Mexican. Similarly, if Ralph
eventually discovers that the man in the trenchcoat was Ortcutt, he
might, in recounting the incident to his friends later, laughingly
say, “For a moment, I thought the mayor was a spy!” or
“For a moment, I thought Ortcutt was a spy”. In a de
re mood, then, we can say that Davy believes, of X, that
she is too strict and Ralph believes, of Y, that he is a spy,
where X is replaced by any term or phrase that picks out Mrs.
Sanchez and Y is replaced by any term or phrase that picks
out Ortcutt—though of course, depending on the situation,
pragmatic considerations will favor the use of some terms or phrases
over others. In a strict de re sense, perhaps we can even say
that Lois believes, of Clark Kent, that he is strong (though she may
also simultaneously believe of him that he is not strong).
The standard view, then, takes belief-attributing sentences to be
systematically ambiguous between a referentially opaque, de
dicto structure and a referentially transparent, de re
structure. Sometimes this view is conjoined with the view that de
re but not de dicto belief requires some kind of direct
acquaintance with the object of belief.
The majority of the literature on the de re / de
dicto distinction since at least the 1980s has challenged this
standard view in one way or another. The challenges are sufficiently
diverse that they resist brief classification, except perhaps to
remark that a number of them invoke pragmatics or conversational
context, instead of an ambiguity in the term “belief”, or
in the structure of belief ascriptions, to explain the fact that it
seems in some way appropriate and in some way inappropriate to say
that Ralph believes the mayor is a spy.
Among the more important discussions of the de re / de
dicto distinction are Quine (1956), Kaplan (1968), Burge (1977),
Lewis (1979), Stich (1983), Dennett (1987), Crimmins (1992), Brandom
(1994), Jeshion (2002), Taylor (2002), and Keshet (2010). See also the
section on the De Re/De Dicto Distinction in the entry on
 propositional attitude reports.
Jessie believes that Stalin was originally a Tsarist mole among the
Bolsheviks, that her son is at school, and that she is eating a
tomato. She feels different degrees of confidence with respect to
these different propositions. The first she recognizes to be a
speculative historical conjecture; the second she takes for granted,
though she knows it could be false; the third she regards as a
near-certainty. Consequently, Jessie is more confident of the second
proposition than the first and more confident of the third than the
second. We might suppose that every subject holds each of her beliefs
with some particular degree of confidence. In general, the greater the
confidence one has in a proposition, the more willing one is to depend
on it in one’s actions.
One common way of formalizing this idea is by means of a scale from 0
to 1, where 0 indicates absolute certainty in the falsity of a
proposition, 1 indicates absolute certainty in its truth, and .5
indicates that the subject regards the proposition just as likely to
be true as false. This number then indicates one’s
“credence” or “degree of belief”. Standard
approaches equate degree of belief with the maximum amount the subject
would, or alternatively should, be willing to wager on a bet that pays
nothing if the proposition is false and 1 unit if the proposition is
true. So, for example, if the subject thinks that the proposition
“the restaurant is open” is three times more likely to be
true than false, she should be willing to pay no more than $0.75 for a
wager that pays nothing if the restaurant is closed and $1 if it is
open. Consequently, the subject’s degree of belief is .75, or
75%. Such a formalized approach to degree of belief has proven useful
in decision theory, game theory, and economics. Standard philosophical
treatments of this topic include Jeffrey (1983) and Skyrms (2000).
However, the phrase “degree of belief” may be misleading,
because the relationship between confidence, betting behavior, and
belief is not straightforward. The dispositionalist or
interpretationist, for example, might regard exhibitions of confidence
and attitudes toward risk as only part of the overall pattern
underwriting belief ascription. Similarly, the representationalist
might hold that readiness to deploy a representation in belief-like
ways need not line up perfectly with betting behavior. Some people
also find it intuitive to say that a rational person holding a ticket
in a fair lottery may not actually believe that she will lose, but
instead regard it as an open question, despite having a “degree
of belief” of, say, .9999 that she will lose. If this person
genuinely believes some other propositions, such as that her son is at
school, with a “degree of belief” considerably less than
.9999, then it appears to follow that a rational person may in some
cases have a higher “degree of belief” in a proposition
that she does not believe than in a proposition she does believe (see
Harman 1986; Sturgeon 2008; Buchak 2014; Leitgeb 2017; Friedman
forthcoming).
Philosophers have sometimes drawn a distinction between
acceptance and belief. Generally speaking,
acceptance is held to be more under the voluntary control of the
subject than belief and more directly tied to a particular practical
action in a context. For example, a scientist, faced with evidence
supporting a theory, evidence acknowledged not to be completely
decisive, may choose to accept the theory or not to accept it. If the
theory is accepted, the scientist ceases inquiring into its truth and
becomes willing to ground her own research and interpretations in that
theory; the contrary if the theory is not accepted. If one is about to
use a ladder to climb to a height, one may check the stability of the
ladder in various ways. At some point, one accepts that the ladder is
stable and climbs it. In both of these examples, acceptance involves a
decision to cease inquiry and to act as though the matter is settled.
This does not, of course, rule out the possibility of re-opening the
question if new evidence comes to light or a new set of risks
arise.
The distinction between acceptance and belief can be supported by
appeal to cases in which one accepts a proposition without believing
it and cases in which one believes a proposition without accepting it.
Van Fraassen (1980) has argued that the former attitude is common in
science: the scientist often does not think that some particular
theory on which her work depends is the literal truth, and thus does
not believe it, but she nonetheless accepts it as an adequate basis
for research. The ladder case, due to Bratman (1999), may involve
belief without acceptance: One may genuinely believe, even before
checking it, that the ladder is stable, but because so much depends on
it and because it is good general policy, one nonetheless does not
accept that the ladder is stable until one has checked it more
carefully.
Important discussions of acceptance include van Fraassen (1980),
Harman (1986), Cohen (1989, 1992), Lehrer (1990), Bratman (1999),
Velleman (2000), and Frankish (2004).
The traditional analysis of knowledge, brought into contemporary
discussion (and famously criticized) by Gettier (1963), takes
propositional knowledge to be a species of belief—specifically,
justified true belief. Most contemporary treatments of knowledge are
modifications or qualifications of the traditional analysis and
consequently also treat knowledge as a species of belief. (For a
detailed treatment of this topic see the entry on the
 analysis of knowledge.
 For critique of the view that propositional knowledge entails belief,
see Radford 1966; Murray, Sytsma, and Livengood 2013; Myers-Schulz and
Schwitzgebel 2013)
There may also be types of knowledge that are not types of belief,
though they have received less attention from epistemologists. Ryle
(1949), for example, emphasizes the distinction between knowing
how to do something (e.g., ride a bicycle) and knowing
that some particular proposition is true (e.g., that Seoul is
the capital of Korea). In contemporary psychology, a similar
distinction is sometimes drawn between procedural knowledge
and semantic, or declarative, knowledge (see Squire
1987; Schacter, Wagner, and Buckner 2000; also the entry on
 memory).
 Although knowledge-that or declarative knowledge may plausibly be a
kind of belief, it is not easy to see how procedural knowledge or
knowledge-how could be so, unless one holds that people have a myriad
of beliefs about minute and non-obvious procedural details. At least,
there is no readily apparent relation between knowledge-how and
“belief-how” that runs parallel to the relation
epistemologists generally accept between knowledge-that and
belief-that. (For an influential attempt to subsume knowledge-how
under knowledge-that, see Stanley and Williamson 2001; Stanley
2011.)
The standard reference text in psychiatry, the Diagnostic and
Statistical Manual of Mental Disorders (DSM-V, 2013)
characterizes delusions (e.g., persecutory delusions, delusions of
grandiosity) as beliefs. However, delusions often do not appear to
connect with behavior in the usual way. For example, a victim of
Capgras delusion—a delusion in which the subject asserts that a
family member or close friend has been replaced by an
identical-looking imposter—may continue to live with the
“imposter” and make little effort to find the supposedly
missing loved one. Some philosophers have therefore suggested that
delusions do not occupy quite the functional role characteristic of
belief and thus are not, in fact, beliefs (e.g., Currie 2000; Stephens
and Graham 2004; Gallagher 2009; Matthews 2013). Others have defended
the view that delusions are beliefs (e.g., Campbell 2001; Bayne and
Pacherie 2005; Bortolotti 2010, 2012) or in-between cases, with some
features of belief but not other features (e.g., Egan 2009; Tumulty
2011). See the entry on
 delusion,
 especially §4.2
 Are Delusions Beliefs?
Philosophers generally say that the belief that P has the
(propositional) content P. A variety of issues arise
about how to characterize those contents and what determines them.
The standard view that the contents of beliefs are propositions gives
rise to a debate about belief contents parallel to, and closely
related to, a debate about the metaphysics of propositions. One
standard view of propositions takes propositions to be sets of
possible worlds; another takes propositions to have something more
closely resembling a linguistic logical structure (see
 structured propositions
 for a detailed exposition of this issue).
Stalnaker (1984) endorses the possible-worlds view of propositions and
imports it directly into his discussion of belief content: He contends
the content of a belief is specified by the set of “possible
worlds” at which that belief is true (see Lewis 1979 for a
similar approach). The structure of belief content is thus the
structure of set theory. Among the advantages Stalnaker claims for
this view is its smooth accommodation of gradual change and of what
might, from the point of view of a discrete linguistic structure, be
seen as problematically indeterminate belief contents. Developing an
example from Dennett (1969), he describes the gradual transition from
a child’s learning to say (without really understanding) that
“Daddy is a doctor” to having a full, adult appreciation
of the fact that her father is a doctor. At some point, Stalnaker
suggests, it’s best to say that child “sort of” or
“half” believes the proposition in question. It’s
not clear how to characterize such gradual shifts by means of a
linguistic or quasi-linguistic propositional structure (1984, p.
64–65; see also Schwitzgebel 2001). On Stalnaker’s view,
the child’s half-belief is handled by attributing her the
capacity to rule out some but not all of the possibilities
incompatible with Daddy’s being a doctor: As her knowledge
grows, so does her sense of the excluded possibilities.
The possible worlds approach to belief content is sometimes referred
to as a “coarse-grained” approach because it implies that
any two beliefs that would be true in exactly the same set of possible
worlds have the same content—as opposed to a
“fine-grained” approach on which beliefs that would be
true at exactly the same set of possible worlds may nonetheless differ
in content. The difference between these two approaches is brought out
most starkly by considering mathematical propositions. On standard
accounts of possibility, all mathematically true propositions are true
in exactly the same set of possible worlds—every world.
It seems to follow, on the coarse-grained view, that the belief that 1
+ 1 = 2 has exactly the same content as the belief that the cosine of
0 is 1, and thus that anyone who believes (or fails to believe) the
one accordingly believes (or fails to believe) the other. And that
seems absurd.
Stalnaker attempts to escape this difficulty by characterizing
mathematical belief as belief about sentences: The belief
that the sentence “1 + 1 = 2” expresses a truth
and the belief that the sentence “the cosine of 0 is
1” expresses a truth have different content and may differ in
truth value between possible worlds (due simply to possible variations
in the meanings of terms, if nothing else). However, it’s
probably fair to say that few philosophers follow Stalnaker in this
view (see discussion in Robbins 2004; and Rayo 2013 for a recent view
similar to Stalnaker’s). The apparent difficulty of sustaining
such a view of belief is often held to reflect badly on the a
coarse-grained possible-worlds view of propositions in general, since
it’s generally thought that one of the principal metaphysical
functions of propositions is to serve as the contents of belief and
other “propositional attitudes” (e.g., Field 1978; Soames
1987).
Ani believes that salmon are fish; not knowing that whales are
mammals, she also believes that whales are fish. Sanjay, like Ani,
believes that salmon are fish, but he denies that whales are fish. Do
Ani and Sanjay share exactly the same belief about
salmon—namely, that they are fish—or is the content of
their belief somehow subtly different in virtue of their different
attitude toward whales? With certain caveats, the atomist
will say the former, the holist the latter. In general, the
atomist holds that the content of one’s beliefs does not depend
in any general way on one’s related beliefs (though it may
depend on the contents of a few specially related beliefs such as
definitions) and thus, consequently, that people who sincerely and
comprehendingly accept the same sentence normally have exactly the
same belief. Holism is the contrary view that the content of every
belief depends to a large degree on a broad range of one’s
related beliefs, and consequently that two people will rarely believe
exactly the same thing.
Holism may be defended by a slippery-slope argument. It seems that we
can imagine Sanjay’s and Ani’s beliefs about the nature of
fish and the members of the class of fish slowly diverging. At some
point, it will seem plainly correct to say that even though they may
both say “salmon are fish”, they are not expressing the
same belief by that sentence. As an extreme case, we might imagine Ani
to be so benighted as to hold that to be a “fish” is
neither more nor less than to be an Earthly animal in regular contact
with Martians, and that only salmon, whales, leopards, and banana
slugs are in such contact. But if we deny, in the extreme case, that
Ani and Sanjay share the same belief, expressed by the sentence
“salmon are fish”, it seems artificial to draw a sharp
line anywhere in the progression of divergence, on one side of which
they share exactly the same belief about salmon and on the other side
of which they have divergent beliefs. One is thus led to the
conclusion that similarity in belief is a matter of degree, and it may
then be difficult to avoid accepting that even a relatively small
divergence in surrounding beliefs may be sufficient to generate subtle
differences between two beliefs expressed in the same words. Similar
slippery slope arguments can be constructed that emphasize gradual
belief change in concept acquisition (“Leibniz was a
metaphysician” agreed to before and after learning philosophy)
or gradual change in surrounding theory or in the meaning of a term
(“electrons have orbits” as uttered by Niels Bohr in 1913
and as uttered by Richard Feynman in 1980). (This argument is similar
in some ways to Stalnaker’s argument for a possible-worlds
analysis of the propositional contents of belief—see §3.1,
above—and indeed Stalnaker takes himself, there, to be committed
to holism.)
Dispositional and interpretational approaches to belief tend to be
holist. On these views, recall, to believe is to be disposed to
exhibit patterns of behavior interpretable or classifiable by means of
various belief attributions (see §1.2 and §1.3 above). It is
plausible to suppose that a subject’s match to the relevant
patterns will generally be a matter of degree. There may be few actual
cases in which two subjects exactly match in their behavioral patterns
regarding P, even if it gets matters approximately right to
attribute to each of them the belief that P. Since behavioral
dispositions are interlaced in a complex way, divergence in any of a
variety of attitudes related to P may be sufficient to ensure
divergence in the patterns relevant to P itself. As
Ani’s associated beliefs grow stranger, her overall behavioral
pattern, or dispositional structure, begins to look less and less like
one that we would associate with believing that salmon are fish.
It is sometimes objected to holism that, intuitively, both Shakespeare
and contemporary physicians believe that blood is red, while on the
holist view it is hard to see how their beliefs could even be similar,
given that they have so many different surrounding beliefs about both
blood and redness. Although in principle a holist could respond to
this objection by describing what sort of differences in surrounding
belief create only minor divergences and what differences create major
ones, there have been no influential attempts at such a project. It
may be possible to address the Shakespeare case by suggesting that the
content of both Shakespeare’s and the contemporary
physician’s belief is partly determined
externalistically by the actual nature of blood and the
actual nature of redness, despite the different conceptualizations
over time (see §3.3 below). Since neither blood nor redness have
changed much since Shakespeare’s day, Shakespeare’s and
the contemporary physician’s belief may be similar—though,
of course, if one holds them to be exactly the same, one
cannot be a holist.
Holism appears to be incompatible with a certain variety of
representationalism about belief. If beliefs, or the representations
underlying them, are stored symbols in the mind, somewhat like
sentences on a chalkboard or objects in a box (to use standard
Fodorian metaphors), then it is natural to suppose that those beliefs
can, in principle, exist independently of each other. Whether one
believes P depends on whether a representation with the
content “P” is present in the right sort of way
in the mind, which would not seem to be directly affected by whether
Q or not-Q, or R or not-R, is also
represented. If there is, in addition, an innate language of thought
of the sort advocated by Fodor and others, then the basic terms of
that language may also be exactly the same from person to person. If a
view of this sort about the mind can be sufficiently well supported,
holism would have to be rejected. Conversely, if holism is plausible,
it cuts against the more atomistic forms of representationalism.
Fodor and Lepore (1992) contains an excellent if dated review and
critique of arguments for holism. The foremost defenders of holism are
probably Quine (1951) and Davidson (1984).
A number of philosophers have suggested that the content of
one’s beliefs depends entirely on things going on inside
one’s head, and not at all on the external world, except via the
effects of the latter on one’s brain. Consequently, if a genius
neuroscientist were to create a molecule-for-molecule duplicate of
your brain and maintain it in a vat, stimulating it artificially so
that it underwent exactly the same sequence of electrical and chemical
events as your actual brain, that brain would have exactly the same
beliefs as you. Those who accept this position are
internalists about belief content. Those who reject it are
externalists.
Several arguments against internalism have prompted considerable
debate in philosophy of mind. Here is a condensed version of one
argument, due to Putnam (1975; though it should be said that
Putnam’s original emphasis was on linguistic meaning, not on
belief). Suppose that in 1750, in a far-off region of the universe,
there existed a planet that was physically identical to Earth,
molecule-for-molecule, in every respect but one: Where Earth had
water, composed of H2O, Twin Earth had something else
instead, “twater”, coming down as rain and filling
streams, behaving identically to water by all the chemical tests then
available, but having a different atomic formula, XYZ. Intuitively, it
seems that the inhabitants of Earth in 1750 would have beliefs about
water and no beliefs about twater, while the inhabitants of Twin Earth
would have beliefs about twater and no beliefs about water. By
hypothesis, however, each inhabitant of Earth will have a molecularly
identical counterpart on Twin Earth with exactly the same brain
structures (except, of course, that their brains will contain XYZ
instead of H2O, but reflection on analogous examples
regarding chemicals not contained in the brain suggests that this fact
is irrelevant). Consequently, the argument goes, the contents of
one’s beliefs do not depend entirely on internal properties of
one’s brain.
For further detail on the debate between internalists and
externalists, see the entries on
 content externalism
 and
 narrow content.
Recall that in the de dicto sense (see §2.3 above) it
seemed plausible to say that Lois Lane, who does not know that Clark
Kent is Superman, believes that Superman is strong but does not
believe that Clark Kent is strong. Despite the intuitive appeal of
this view, some widely accepted “Russellian” views in the
philosophy of language appear committed to attributing to Lois exactly
the same beliefs about Clark Kent as she has about Superman. On such
views, the semantic content of a name, or the contribution it makes to
the meaning or truth conditions of a sentence, depends only on the
individual picked out by that name. Since the names
“Superman” and “Clark Kent” pick out the same
individual, it follows that the sentence “Lois believes Superman
is strong” could not have a different meaning or truth value
from the sentence “Lois believes Clark Kent is strong”.
Philosophers of language have discussed this issue, known as
“Frege’s Puzzle”, extensively since the 1970s.
Although the issues here arise for all the propositional attitudes (at
least), generally the puzzle is framed and discussed in terms of
belief. See the entry on
 propositional attitude reports.
A number of philosophers have argued that beings without language,
notably human infants and non-human animals, cannot have beliefs. The
most influential case for this view has been Davidson’s (1982,
1984; Heil 1992). Three primary arguments in favor of the necessity of
language for belief can be extracted from Davidson.
The first starts from the observation that if we are to ascribe a
belief to a being without language—a dog, say, who is barking up
a tree into which he has just seen a squirrel run—we must
ascribe a belief with some particular content. At first blush, it
seems natural to say that, in the case described, the dog believes
that the squirrel is in the tree. However, on reflection, that
attribution may seem to be not quite right. The dog does not really
have the concept of a squirrel or a tree in the human sense. He may
not know, for instance, that trees have roots and require water to
grow. Consequently, according to Davidson, it is not really accurate
to say that he believes that the squirrel is in the
tree (at least in the de dicto sense: see §2.3
above). However, Davidson argues, neither does the dog have any
other particular belief. Embracing holism (see §3.2
above), Davidson asserts that to have a belief with a specific
content, that belief must be embedded in a rich network of other
beliefs with specific contents, but a dog’s cognitive life is
not complex enough to support such a network. “Belief”
talk thus cannot get traction (cf. Dennett 1969; Stich 1979,
1983).
Several philosophers (e.g., Routley 1981; Smith 1982; Allen 1992;
Glock 2010) have objected to this argument on the grounds that the
dog’s cognition about things such as trees, while perhaps not
much like ours, is nonetheless relatively rich, involving a number of
elements generally neglected by us, such as their scent and their use
in marking territory. The dog’s understanding of a tree may be
at least as rich as the human understanding of some objects about
which we seem to have beliefs. For example, it seems that a chemically
untrained person may believe that boron is a chemical element without
knowing very much about boron apart from that fact. Since we have no
language for doggy concepts, our belief ascriptions to dogs can only
be approximate—but if one accepts holism, then belief ascription
to other human beings may be similarly approximate.
Davidson also argues that to have a belief one must have the
concept of belief, which involves the ability to recognize
that beliefs can be false or that there is a mind-independent reality
beyond one’s beliefs; and one cannot have all that without
language. However, Davidson offers little support for the claim that
belief requires the concept of belief. On the face of it, it is not
evident why this should be so, any more than having a bad temper
requires the concept of a bad temper. Furthermore, many developmental
psychologists have suggested that children do not understand the
appearance-reality distinction and do not recognize that beliefs can
be false until they are at least three years old, well after they have
begun to talk (Perner 1991; Wellman, Cross, and Watson 2001; though
see Southgate, Senju, and Csibra 2007; Scott and Baillargeon 2017).
Davidson’s view thus requires him either to reject this
empirical thesis or embrace the seemingly implausible view that young
three-year-olds have no beliefs (see also Andrews 2002).
The view that belief requires language is a natural consequence of the
view that belief attribution is inextricably intertwined with the
interpretation of a subject’s linguistic utterances. Davidson,
as described above (§1.3), argues that the interpretation of
creature’s beliefs, desires, and its language must come
together as a package. This provides a third Davidsonian reason for
rejecting belief without language (a reason that, however, remains
largely implicit in Davidson): Creatures without language are missing
part of what is essential to a behavioral pattern of the sort that can
underwrite proper belief ascription (and recall that on an
interpretational view, all there is to having a belief is having a
pattern of behavior that is interpretable in that way to an outside
observer). Any view that ties belief attribution and the
subject’s language as closely together as Davidson’s
does—Sellars (1956, 1969), Brandom (1994), and Wettstein (2004)
also offer views of this sort—will have difficulty accommodating
the possibility of belief in creatures without language. Thus,
whatever draws us to such views will also provide reason to deny
belief (or at least robust, full-blown belief) to languageless
creatures.
Positive arguments for attributing beliefs to (at least) human infants
and non-linguistic mammals have tended to focus on the general
biological and behavioral similarity between adult human beings, human
infants, and non-human mammals; the intuitive naturalness of
describing the behavior of infants and non-linguistic mammals in terms
of their beliefs and desires; and the difficulty of usefully
characterizing their mental lives without relying on the ascription of
propositional attitudes (e.g., Routley 1981; Marcus 1995; Allen and
Bekoff 1997; Zimmerman 2018).