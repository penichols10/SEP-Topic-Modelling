Charles Babbage was Lucasian Professor of Mathematics at Cambridge
University from 1828 to 1839 (a post formerly held by Isaac Newton).
Babbage's proposed Difference Engine was a special-purpose digital
computing machine for the automatic production of mathematical tables
(such as logarithm tables, tide tables, and astronomical tables). The
Difference Engine consisted entirely of mechanical components —
brass gear wheels, rods, ratchets, pinions, etc. Numbers were
represented in the decimal system by the positions of 10-toothed metal
wheels mounted in columns. Babbage exhibited a small working model in
1822. He never completed the full-scale machine that he had designed
but did complete several fragments. The largest — one ninth of
the complete calculator — is on display in the London Science
Museum. Babbage used it to perform serious computational work,
calculating various mathematical tables. In 1990, Babbage's Difference
Engine No. 2 was finally built from Babbage's designs and is also on
display at the London Science Museum. 
The Swedes Georg and Edvard Scheutz (father and son) constructed a
modified version of Babbage's Difference Engine. Three were made, a
prototype and two commercial models, one of these being sold to an
observatory in Albany, New York, and the other to the
Registrar-General's office in London, where it calculated and printed
actuarial tables.
Babbage's proposed Analytical Engine, considerably more ambitious
than the Difference Engine, was to have been a general-purpose
mechanical digital computer. The Analytical Engine was to have had a
memory store and a central processing unit (or ‘mill’) and
would have been able to select from among alternative actions
consequent upon the outcome of its previous actions (a facility
nowadays known as conditional branching). The behaviour of the
Analytical Engine would have been controlled by a program of
instructions contained on punched cards connected together with ribbons
(an idea that Babbage had adopted from the Jacquard weaving loom).
Babbage emphasised the generality of the Analytical Engine, saying
‘the conditions which enable a finite machine to make
calculations of unlimited extent are fulfilled in the Analytical
Engine’ (Babbage [1994], p. 97).
Babbage worked closely with Ada Lovelace, daughter of the poet
Byron, after whom the modern programming language ADA is named.
Lovelace foresaw the possibility of using the Analytical Engine for
non-numeric computation, suggesting that the Engine might even be
capable of composing elaborate pieces of music.
A large model of the Analytical Engine was under construction at the
time of Babbage's death in 1871 but a full-scale version was never
built. Babbage's idea of a general-purpose calculating engine was never
forgotten, especially at Cambridge, and was on occasion a lively topic
of mealtime discussion at the war-time headquarters of the Government
Code and Cypher School, Bletchley Park, Buckinghamshire, birthplace of
the electronic digital computer.
The earliest computing machines in wide use were not digital but
analog. In analog representation, properties of the representational
medium ape (or reflect or model) properties of the represented
state-of-affairs. (In obvious contrast, the strings of binary digits
employed in digital representation do not represent by means
of possessing some physical property — such as length —
whose magnitude varies in proportion to the magnitude of the property
that is being represented.) Analog representations form a diverse
class. Some examples: the longer a line on a road map, the longer the
road that the line represents; the greater the number of clear plastic
squares in an architect's model, the greater the number of windows in
the building represented; the higher the pitch of an acoustic depth
meter, the shallower the water. In analog computers, numerical
quantities are represented by, for example, the angle of rotation of a
shaft or a difference in electrical potential. Thus the output voltage
of the machine at a time might represent the momentary speed of the
object being modelled. 
As the case of the architect's model makes plain, analog
representation may be discrete in nature (there is no such
thing as a fractional number of windows). Among computer scientists,
the term ‘analog’ is sometimes used narrowly, to indicate
representation of one continuously-valued quantity by another
(e.g., speed by voltage). As Brian Cantwell Smith has remarked:
James Thomson, brother of Lord Kelvin, invented the mechanical
wheel-and-disc integrator that became the foundation of analog
computation (Thomson [1876]). The two brothers constructed a device for
computing the integral of the product of two given functions, and
Kelvin described (although did not construct) general-purpose analog
machines for integrating linear differential equations of any order and
for solving simultaneous linear equations. Kelvin's most successful
analog computer was his tide predicting machine, which remained in use
at the port of Liverpool until the 1960s. Mechanical analog devices
based on the wheel-and-disc integrator were in use during World War I
for gunnery calculations. Following the war, the design of the
integrator was considerably improved by Hannibal Ford (Ford [1919]). 
Stanley Fifer reports that the first semi-automatic mechanical
analog computer was built in England by the Manchester firm of
Metropolitan Vickers prior to 1930 (Fifer [1961], p. 29); however, I
have so far been unable to verify this claim. In 1931, Vannevar Bush,
working at MIT, built the differential analyser, the first large-scale
automatic general-purpose mechanical analog computer. Bush's design was
based on the wheel and disc integrator. Soon copies of his machine were
in use around the world (including, at Cambridge and Manchester
Universities in England, differential analysers built out of kit-set
Meccano, the once popular engineering toy).
It required a skilled mechanic equipped with a lead hammer to set up
Bush's mechanical differential analyser for each new job. Subsequently,
Bush and his colleagues replaced the wheel-and-disc integrators and
other mechanical components by electromechanical, and finally by
electronic, devices.
A differential analyser may be conceptualised as a collection of
‘black boxes’ connected together in such a way as to allow
considerable feedback. Each box performs a fundamental process, for
example addition, multiplication of a variable by a constant, and
integration. In setting up the machine for a given task, boxes are
connected together so that the desired set of fundamental processes is
executed. In the case of electrical machines, this was done typically
by plugging wires into sockets on a patch panel (computing machines
whose function is determined in this way are referred to as
‘program-controlled’).
Since all the boxes work in parallel, an electronic differential
analyser solves sets of equations very quickly. Against this has to be
set the cost of massaging the problem to be solved into the form
demanded by the analog machine, and of setting up the hardware to
perform the desired computation. A major drawback of analog computation
is the higher cost, relative to digital machines, of an increase in
precision. During the 1960s and 1970s, there was considerable interest
in ‘hybrid’ machines, where an analog section is controlled
by and programmed via a digital section. However, such machines are now
a rarity.
In 1936, at Cambridge University, Turing invented the principle of the
modern computer. He described an abstract digital computing machine
consisting of a limitless memory and a scanner that moves back and
forth through the memory, symbol by symbol, reading what it finds and
writing further symbols (Turing [1936]). The actions of the scanner are
dictated by a program of instructions that is stored in the memory in
the form of symbols. This is Turing's stored-program concept, and
implicit in it is the possibility of the machine operating on and
modifying its own program. (In London in 1947, in the course of what
was, so far as is known, the earliest public lecture to mention
computer intelligence, Turing said, ‘What we want is a machine
that can learn from experience’, adding that the
‘possibility of letting the machine alter its own instructions
provides the mechanism for this’ (Turing [1947] p. 393). Turing's
computing machine of 1936 is now known simply as the universal Turing
machine. Cambridge mathematician Max Newman remarked that right from
the start Turing was interested in the possibility of actually building
a computing machine of the sort that he had described (Newman in
interview with Christopher Evans in Evans [197?]. 
From the start of the Second World War Turing was a leading
cryptanalyst at the Government Code and Cypher School, Bletchley Park.
Here he became familiar with Thomas Flowers' work involving large-scale
high-speed electronic switching (described below). However, Turing
could not turn to the project of building an electronic stored-program
computing machine until the cessation of hostilities in Europe in
1945.
During the wartime years Turing did give considerable thought to the
question of machine intelligence. Colleagues at Bletchley Park recall
numerous off-duty discussions with him on the topic, and at one point
Turing circulated a typewritten report (now lost) setting out some of
his ideas. One of these colleagues, Donald Michie (who later founded
the Department of Machine Intelligence and Perception at the University
of Edinburgh), remembers Turing talking often about the possibility of
computing machines (1) learning from experience and (2) solving
problems by means of searching through the space of possible solutions,
guided by rule-of-thumb principles (Michie in interview with Copeland,
1995). The modern term for the latter idea is ‘heuristic
search’, a heuristic being any rule-of-thumb principle that cuts
down the amount of searching required in order to find a solution to a
problem. At Bletchley Park Turing illustrated his ideas on machine
intelligence by reference to chess. Michie recalls Turing experimenting
with heuristics that later became common in chess programming (in
particular minimax and best-first).
Further information about Turing and the computer, including his
wartime work on codebreaking and his thinking about artificial
intelligence and artificial life, can be found in Copeland 2004.
With some exceptions — including Babbage's purely mechanical
engines, and the finger-powered National Accounting Machine - early
digital computing machines were electromechanical. That is to say,
their basic components were small, electrically-driven, mechanical
switches called ‘relays’. These operate relatively slowly,
whereas the basic components of an electronic computer —
originally vacuum tubes (valves) — have no moving parts save
electrons and so operate extremely fast. Electromechanical digital
computing machines were built before and during the second world war by
(among others) Howard Aiken at Harvard University, George Stibitz at
Bell Telephone Laboratories, Turing at Princeton University and
Bletchley Park, and Konrad Zuse in Berlin. To Zuse belongs the honour
of having built the first working general-purpose program-controlled
digital computer. This machine, later called the Z3, was functioning in
1941. (A program-controlled computer, as opposed to a stored-program
computer, is set up for a new task by re-routing wires, by means of
plugs etc.) 
Relays were too slow and unreliable a medium for large-scale
general-purpose digital computation (although Aiken made a valiant
effort). It was the development of high-speed digital techniques using
vacuum tubes that made the modern computer possible.
The earliest extensive use of vacuum tubes for digital
data-processing appears to have been by the engineer Thomas Flowers,
working in London at the British Post Office Research Station at Dollis
Hill. Electronic equipment designed by Flowers in 1934, for controlling
the connections between telephone exchanges, went into operation in
1939, and involved between three and four thousand vacuum tubes running
continuously. In 1938–1939 Flowers worked on an experimental
electronic digital data-processing system, involving a high-speed data
store. Flowers' aim, achieved after the war, was that electronic
equipment should replace existing, less reliable, systems built from
relays and used in telephone exchanges. Flowers did not investigate the
idea of using electronic equipment for numerical calculation, but has
remarked that at the outbreak of war with Germany in 1939 he was
possibly the only person in Britain who realized that vacuum tubes
could be used on a large scale for high-speed digital computation. (See
Copeland 2006 for m more information on Flowers' work.)
The earliest comparable use of vacuum tubes in the U.S. seems to have
been by John Atanasoff at what was then Iowa State College (now
University). During the period 1937–1942 Atanasoff developed
techniques for using vacuum tubes to perform numerical calculations
digitally. In 1939, with the assistance of his student Clifford Berry,
Atanasoff began building what is sometimes called the Atanasoff-Berry
Computer, or ABC, a small-scale special-purpose electronic digital
machine for the solution of systems of linear algebraic equations. The
machine contained approximately 300 vacuum tubes. Although the
electronic part of the machine functioned successfully, the computer as
a whole never worked reliably, errors being introduced by the
unsatisfactory binary card-reader. Work was discontinued in 1942 when
Atanasoff left Iowa State. 
The first fully functioning electronic digital computer was Colossus,
used by the Bletchley Park cryptanalysts from February 1944. 
From very early in the war the Government Code and Cypher School
(GC&CS) was successfully deciphering German radio communications
encoded by means of the Enigma system, and by early 1942 about 39,000
intercepted messages were being decoded each month, thanks to
electromechanical machines known as ‘bombes’. These were
designed by Turing and Gordon Welchman (building on earlier work by
Polish cryptanalysts).
During the second half of 1941, messages encoded by means of a
totally different method began to be intercepted. This new cipher
machine, code-named ‘Tunny’ by Bletchley Park, was broken
in April 1942 and current traffic was read for the first time in July
of that year. Based on binary teleprinter code, Tunny was used in
preference to Morse-based Enigma for the encryption of high-level
signals, for example messages from Hitler and members of the German
High Command.
The need to decipher this vital intelligence as rapidly as possible
led Max Newman to propose in November 1942 (shortly after his
recruitment to GC&CS from Cambridge University) that key parts of
the decryption process be automated, by means of high-speed electronic
counting devices. The first machine designed and built to Newman's
specification, known as the Heath Robinson, was relay-based with
electronic circuits for counting. (The electronic counters were
designed by C.E. Wynn-Williams, who had been using thyratron tubes in
counting circuits at the Cavendish Laboratory, Cambridge, since 1932
[Wynn-Williams 1932].) Installed in June 1943, Heath Robinson was
unreliable and slow, and its high-speed paper tapes were continually
breaking, but it proved the worth of Newman's idea. Flowers recommended
that an all-electronic machine be built instead, but he received no
official encouragement from GC&CS. Working independently at the
Post Office Research Station at Dollis Hill, Flowers quietly got on
with constructing the world's first large-scale programmable electronic
digital computer. Colossus I was delivered to Bletchley Park in January
1943.
By the end of the war there were ten Colossi working round the clock
at Bletchley Park. From a cryptanalytic viewpoint, a major difference
between the prototype Colossus I and the later machines was the
addition of the so-called Special Attachment, following a key discovery
by cryptanalysts Donald Michie and Jack Good. This broadened the
function of Colossus from ‘wheel setting’ — i.e.,
determining the settings of the encoding wheels of the Tunny machine
for a particular message, given the ‘patterns’ of the
wheels — to ‘wheel breaking’, i.e., determining the
wheel patterns themselves. The wheel patterns were eventually changed
daily by the Germans on each of the numerous links between the German
Army High Command and Army Group commanders in the field. By 1945 there
were as many 30 links in total. About ten of these were broken and read
regularly.
Colossus I contained approximately 1600 vacuum tubes and each of the
subsequent machines approximately 2400 vacuum tubes. Like the smaller
ABC, Colossus lacked two important features of modern computers. First,
it had no internally stored programs. To set it up for a new task, the
operator had to alter the machine's physical wiring, using plugs and
switches. Second, Colossus was not a general-purpose machine, being
designed for a specific cryptanalytic task involving counting and
Boolean operations.
F.H. Hinsley, official historian of GC&CS, has estimated that
the war in Europe was shortened by at least two years as a result of
the signals intelligence operation carried out at Bletchley Park, in
which Colossus played a major role. Most of the Colossi were destroyed
once hostilities ceased. Some of the electronic panels ended up at
Newman's Computing Machine Laboratory in Manchester (see below), all
trace of their original use having been removed. Two Colossi were
retained by GC&CS (renamed GCHQ following the end of the war). The
last Colossus is believed to have stopped running in 1960.
Those who knew of Colossus were prohibited by the Official Secrets
Act from sharing their knowledge. Until the 1970s, few had any idea
that electronic computation had been used successfully during the
second world war. In 1970 and 1975, respectively, Good and Michie
published notes giving the barest outlines of Colossus. By 1983,
Flowers had received clearance from the British Government to publish a
partial account of the hardware of Colossus I. Details of the later
machines and of the Special Attachment, the uses to which the Colossi
were put, and the cryptanalytic algorithms that they ran, have only
recently been declassified. (For the full account of Colossus and the
attack on Tunny see Copeland 2006.)
To those acquainted with the universal Turing machine of 1936, and
the associated stored-program concept, Flowers' racks of digital
electronic equipment were proof of the feasibility of using large
numbers of vacuum tubes to implement a high-speed general-purpose
stored-program computer. The war over, Newman lost no time in
establishing the Royal Society Computing Machine Laboratory at
Manchester University for precisely that purpose. A few months after
his arrival at Manchester, Newman wrote as follows to the Princeton
mathematician John von Neumann (February 1946):
Turing and Newman were thinking along similar lines. In 1945 Turing
joined the National Physical Laboratory (NPL) in London, his brief to
design and develop an electronic stored-program digital computer for
scientific work. (Artificial Intelligence was not far from Turing's
thoughts: he described himself as ‘building a brain’ and
remarked in a letter that he was ‘more interested in the
possibility of producing models of the action of the brain than in the
practical applications to computing’.) John Womersley, Turing's
immediate superior at NPL, christened Turing's proposed machine the
Automatic Computing Engine, or ACE, in homage to Babbage's Difference
Engine and Analytical Engine. 
Turing's 1945 report ‘Proposed Electronic Calculator’
gave the first relatively complete specification of an electronic
stored-program general-purpose digital computer. The report is
reprinted in full in Copeland 2005.
The first electronic stored-program digital computer to be proposed
in the U.S. was the EDVAC (see below). The ‘First Draft of a
Report on the EDVAC’ (May 1945), composed by von Neumann,
contained little engineering detail, in particular concerning
electronic hardware (owing to restrictions in the U.S.). Turing's
‘Proposed Electronic Calculator’, on the other hand,
supplied detailed circuit designs and specifications of hardware units,
specimen programs in machine code, and even an estimate of the cost of
building the machine (£11,200). ACE and EDVAC differed
fundamentally from one another; for example, ACE employed distributed
processing, while EDVAC had a centralised structure.
Turing saw that speed and memory were the keys to computing.
Turing's colleague at NPL, Jim Wilkinson, observed that Turing
‘was obsessed with the idea of speed on the machine’
[Copeland 2005, p. 2]. Turing's design had much in common with today's
RISC architectures and it called for a high-speed memory of roughly the
same capacity as an early Macintosh computer (enormous by the standards
of his day). Had Turing's ACE been built as planned it would have been
in a different league from the other early computers. However, progress
on Turing's Automatic Computing Engine ran slowly, due to
organisational difficulties at NPL, and in 1948 a ‘very fed
up’ Turing (Robin Gandy's description, in interview with
Copeland, 1995) left NPL for Newman's Computing Machine Laboratory at
Manchester University. It was not until May 1950 that a small pilot
model of the Automatic Computing Engine, built by Wilkinson, Edward
Newman, Mike Woodger, and others, first executed a program. With an
operating speed of 1 MHz, the Pilot Model ACE was for some time the
fastest computer in the world.
Sales of DEUCE, the production version of the Pilot Model ACE, were
buoyant — confounding the suggestion, made in 1946 by the
Director of the NPL, Sir Charles Darwin, that ‘it is very
possible that … one machine would suffice to solve all the
problems that are demanded of it from the whole country’
[Copeland 2005, p. 4]. The fundamentals of Turing's ACE design were
employed by Harry Huskey (at Wayne State University, Detroit) in the
Bendix G15 computer (Huskey in interview with Copeland, 1998). The G15
was arguably the first personal computer; over 400 were sold worldwide.
DEUCE and the G15 remained in use until about 1970. Another computer
deriving from Turing's ACE design, the MOSAIC, played a role in
Britain's air defences during the Cold War period; other derivatives
include the Packard-Bell PB250 (1961). (More information about these
early computers is given in [Copeland 2005].)
The earliest general-purpose stored-program electronic digital computer
to work was built in Newman's Computing Machine Laboratory at
Manchester University. The Manchester ‘Baby’, as it became
known, was constructed by the engineers F.C. Williams and Tom Kilburn,
and performed its first calculation on 21 June 1948. The tiny program,
stored on the face of a cathode ray tube, was just seventeen
instructions long. A much enlarged version of the machine, with a
programming system designed by Turing, became the world's first
commercially available computer, the Ferranti Mark I. The first to be
completed was installed at Manchester University in February 1951; in
all about ten were sold, in Britain, Canada, Holland and Italy. 
The fundamental logico-mathematical contributions by Turing and
Newman to the triumph at Manchester have been neglected, and the
Manchester machine is nowadays remembered as the work of Williams and
Kilburn. Indeed, Newman's role in the development of computers has
never been sufficiently emphasised (due perhaps to his thoroughly
self-effacing way of relating the relevant events).
It was Newman who, in a lecture in Cambridge in 1935, introduced
Turing to the concept that led directly to the Turing machine: Newman
defined a constructive process as one that a machine can carry
out (Newman in interview with Evans, op. cit.). As a result of his
knowledge of Turing's work, Newman became interested in the
possibilities of computing machinery in, as he put it, ‘a rather
theoretical way’. It was not until Newman joined GC&CS in
1942 that his interest in computing machinery suddenly became
practical, with his realisation that the attack on Tunny could be
mechanised. During the building of Colossus, Newman tried to interest
Flowers in Turing's 1936 paper — birthplace of the stored-program
concept - but Flowers did not make much of Turing's arcane notation.
There is no doubt that by 1943, Newman had firmly in mind the idea of
using electronic technology in order to construct a stored-program
general-purpose digital computing machine.
In July of 1946 (the month in which the Royal Society approved
Newman's application for funds to found the Computing Machine
Laboratory), Freddie Williams, working at the Telecommunications
Research Establishment, Malvern, began the series of experiments on
cathode ray tube storage that was to lead to the Williams tube memory.
Williams, until then a radar engineer, explains how it was that he came
to be working on the problem of computer memory:
Newman learned of Williams' work, and with the able help of Patrick
Blackett, Langworthy Professor of Physics at Manchester and one of the
most powerful figures in the University, was instrumental in the
appointment of the 35 year old Williams to the recently vacated Chair
of Electro-Technics at Manchester. (Both were members of the appointing
committee (Kilburn in interview with Copeland, 1997).) Williams
immediately had Kilburn, his assistant at Malvern, seconded to
Manchester. To take up the story in Williams' own words:
Elsewhere Williams is explicit concerning Turing's role and gives
something of the flavour of the explanation that he and Kilburn
received:
It seems that Newman must have used much the same words with
Williams and Kilburn as he did in an address to the Royal Society on
4th March 1948:
Following this explanation of Turing's three-address concept (source
1, source 2, destination, function) Newman went on to describe program
storage (‘the orders shall be in a series of houses X1, X2,
…’) and conditional branching. He then summed up:
In a letter written in 1972 Williams described in some detail what
he and Kilburn were told by Newman:
Turing's early input to the developments at Manchester, hinted at by
Williams in his above-quoted reference to Turing, may have been via the
lectures on computer design that Turing and Wilkinson gave in London
during the period December 1946 to February 1947 (Turing and Wilkinson
[1946–7]). The lectures were attended by representatives of
various organisations planning to use or build an electronic computer.
Kilburn was in the audience (Bowker and Giordano [1993]). (Kilburn
usually said, when asked from where he obtained his basic knowledge of
the computer, that he could not remember (letter from Brian Napper to
Copeland, 2002); for example, in a 1992 interview he said:
‘Between early 1945 and early 1947, in that period, somehow or
other I knew what a digital computer was … Where I got this
knowledge from I've no idea’ (Bowker and Giordano [1993], p.
19).)
Whatever role Turing's lectures may have played in informing
Kilburn, there is little doubt that credit for the Manchester computer
— called the ‘Newman-Williams machine’ in a
contemporary document (Huskey 1947) — belongs not only to
Williams and Kilburn but also to Newman, and that the influence on
Newman of Turing's 1936 paper was crucial, as was the influence of
Flowers' Colossus.
The first working AI program, a draughts (checkers) player written
by Christopher Strachey, ran on the Ferranti Mark I in the Manchester
Computing Machine Laboratory. Strachey (at the time a teacher at Harrow
School and an amateur programmer) wrote the program with Turing's
encouragement and utilising the latter's recently completed
Programmers' Handbook for the Ferranti. (Strachey later became Director
of the Programming Research Group at Oxford University.) By the summer
of 1952, the program could, Strachey reported, ‘play a complete
game of draughts at a reasonable speed’. (Strachey's program formed the
basis for Arthur Samuel's well-known checkers program.) The first
chess-playing program, also, was written for the Manchester Ferranti,
by Dietrich Prinz; the program first ran in November 1951. Designed for
solving simple problems of the mate-in-two variety, the program would
examine every possible move until a solution was found. Turing started
to program his ‘Turochamp’ chess-player on the Ferranti
Mark I, but never completed the task. Unlike Prinz's program, the
Turochamp could play a complete game (when hand-simulated) and operated
not by exhaustive search but under the guidance of heuristics.
The first fully functioning electronic digital computer to be built in
the U.S. was ENIAC, constructed at the Moore School of Electrical
Engineering, University of Pennsylvania, for the Army Ordnance
Department, by J. Presper Eckert and John Mauchly. Completed in 1945,
ENIAC was somewhat similar to the earlier Colossus, but considerably
larger and more flexible (although far from general-purpose). The
primary function for which ENIAC was designed was the calculation of
tables used in aiming artillery. ENIAC was not a stored-program
computer, and setting it up for a new job involved reconfiguring the
machine by means of plugs and switches. For many years, ENIAC was
believed to have been the first functioning electronic digital
computer, Colossus being unknown to all but a few. 
In 1944, John von Neumann joined the ENIAC group. He had become
‘intrigued’ (Goldstine's word, [1972], p. 275) with
Turing's universal machine while Turing was at Princeton University
during 1936–1938. At the Moore School, von Neumann emphasised the
importance of the stored-program concept for electronic computing,
including the possibility of allowing the machine to modify its own
program in useful ways while running (for example, in order to control
loops and branching). Turing's paper of 1936 (‘On Computable
Numbers, with an Application to the Entscheidungsproblem’) was
required reading for members of von Neumann's post-war computer project
at the Institute for Advanced Study, Princeton University (letter from
Julian Bigelow to Copeland, 2002; see also Copeland [2004], p. 23).
Eckert appears to have realised independently, and prior to von
Neumann's joining the ENIAC group, that the way to take full advantage
of the speed at which data is processed by electronic circuits is to
place suitably encoded instructions for controlling the processing in
the same high-speed storage devices that hold the data itself
(documented in Copeland [2004], pp. 26–7). In 1945, while ENIAC
was still under construction, von Neumann produced a draft report,
mentioned previously, setting out the ENIAC group's ideas for an
electronic stored-program general-purpose digital computer, the EDVAC
(von Neuman [1945]). The EDVAC was completed six years later, but not
by its originators, who left the Moore School to build computers
elsewhere. Lectures held at the Moore School in 1946 on the proposed
EDVAC were widely attended and contributed greatly to the dissemination
of the new ideas.
Von Neumann was a prestigious figure and he made the concept of a
high-speed stored-program digital computer widely known through his
writings and public addresses. As a result of his high profile in the
field, it became customary, although historically inappropriate, to
refer to electronic stored-program digital computers as ‘von
Neumann machines’.
The Los Alamos physicist Stanley Frankel, responsible with von
Neumann and others for mechanising the large-scale calculations
involved in the design of the atomic bomb, has described von Neumann's
view of the importance of Turing's 1936 paper, in a letter:
Other notable early stored-program electronic digital computers were: 
The EDVAC and ACE proposals both advocated the use of mercury-filled
tubes, called ‘delay lines’, for high-speed internal
memory. This form of memory is known as acoustic memory. Delay lines
had initially been developed for echo cancellation in radar; the idea
of using them as memory devices originated with Eckert at the Moore
School. Here is Turing's description: 
Mercury delay line memory was used in EDSAC, BINAC, SEAC, Pilot
Model ACE, EDVAC, DEUCE, and full-scale ACE (1958). The chief advantage
of the delay line as a memory medium was, as Turing put it, that delay
lines were "already a going concern" (Turing [1947], p. 380). The
fundamental disadvantages of the delay line were that random access is
impossible and, moreover, the time taken for an instruction, or number,
to emerge from a delay line depends on where in the line it happens to
be.
In order to minimize waiting-time, Turing arranged for instructions
to be stored not in consecutive positions in the delay line, but in
relative positions selected by the programmer in such a way that each
instruction would emerge at exactly the time it was required, in so far
as this was possible. Each instruction contained a specification of the
location of the next. This system subsequently became known as
‘optimum coding’. It was an integral feature of every
version of the ACE design. Optimum coding made for difficult and untidy
programming, but the advantage in terms of speed was considerable.
Thanks to optimum coding, the Pilot Model ACE was able to do a floating
point multiplication in 3 milliseconds (Wilkes's EDSAC required 4.5
milliseconds to perform a single fixed point multiplication).
In the Williams tube or electrostatic memory, previously mentioned,
a two-dimensional rectangular array of binary digits was stored on the
face of a commercially-available cathode ray tube. Access to data was
immediate. Williams tube memories were employed in the Manchester
series of machines, SWAC, the IAS computer, and the IBM 701, and a
modified form of Williams tube in Whirlwind I (until replacement by
magnetic core in 1953).
Drum memories, in which data was stored magnetically on the surface
of a metal cylinder, were developed on both sides of the Atlantic. The
initial idea appears to have been Eckert's. The drum provided
reasonably large quantities of medium-speed memory and was used to
supplement a high-speed acoustic or electrostatic memory. In 1949, the
Manchester computer was successfully equipped with a drum memory; this
was constructed by the Manchester engineers on the model of a drum
developed by Andrew Booth at Birkbeck College, London.
The final major event in the early history of electronic computation
was the development of magnetic core memory. Jay Forrester realised
that the hysteresis properties of magnetic core (normally used in
transformers) lent themselves to the implementation of a
three-dimensional solid array of randomly accessible storage points. In
1949, at Massachusetts Institute of Technology, he began to investigate
this idea empirically. Forrester's early experiments with metallic core
soon led him to develop the superior ferrite core memory. Digital
Equipment Corporation undertook to build a computer similar to the
Whirlwind I as a test vehicle for a ferrite core memory. The Memory
Test Computer was completed in 1953. (This computer was used in 1954
for the first simulations of neural networks, by Belmont Farley and
Wesley Clark of MIT's Lincoln Laboratory (see Copeland and Proudfoot
[1996]).
Once the absolute reliability, relative cheapness, high capacity and
permanent life of ferrite core memory became apparent, core soon
replaced other forms of high-speed memory. The IBM 704 and 705
computers (announced in May and October 1954, respectively) brought
core memory into wide use.