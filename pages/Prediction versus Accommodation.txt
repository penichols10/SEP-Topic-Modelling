There was in the eighteenth and nineteenth centuries a passionate
debate about scientific method—at stake was the ‘method of
hypothesis’ which postulated hypotheses about unobservable
entities which ‘saved the phenomena’ and thus were
arguably true (see Laudan 1981a). Critics of this method pointed out
that hypotheses could always be adjusted artificially to accommodate
any amount of data. But it was noted that some such theories had the
further virtue of generating specific predictions of heretofore
unobserved phenomena—thus scientists like John Herschel and
William Whewell argued that hypotheses that saved phenomena could be
justified when they were confirmed by such ‘novel’
phenomena. Whewell maintained that predictions carry special weight
because a theory that correctly predicts a surprising result cannot
have done so by chance, and thus must be true (Whewell 1849 [1968:
294]). It thus appeared that predicted evidence confirmed theory more
strongly than accommodated evidence. But John Stuart Mill (in his
debate with Whewell) categorically denied this claim, affirming that
(s)uch predictions and their fulfilment are, indeed, well calculated
to impress the ignorant vulgar, whose faith in science rests solely
upon similar coincidences between its prophecies and what comes to
pass.  But it is strange that any considerable stress should be laid
upon such a coincidence by scientific thinkers. (1843, Vol. 2, 23)
John Maynard Keynes provides a simple account of why predictivism has
a misleading appearance of truth in a brief passage in his book A
Treatise on Probability:
The peculiar virtue of prediction or predesignation is altogether
imaginary… The plausibility of the argument [for predictivism]
is derived from a different source. If a hypothesis is proposed a
priori, this commonly means that there is some ground for it,
arising out of our previous knowledge, apart from the purely inductive
ground, and if such is the case the hypothesis is clearly stronger
than one which reposes on inductive grounds only. But if it is merely
a guess, the lucky fact of its preceding some or all of the cases
which verify it adds nothing whatever to its value. It is the union of
prior knowledge, with the inductive grounds which arise out of the
immediate instances, that lends weight to any hypothesis, and not the
occasion on which the hypothesis is first proposed. (1921:
 305–306)[1]
By ‘the inductive ground’ for a hypothesis Keynes clearly
means the data that the hypothesis fits. Keynes means that when some
theorist who undertakes to test a hypothesis first proposes it,
typically some other (presumably theoretical) form of support prompted
the proposal. Thus hypotheses which are proposed without being built
to fit the empirical data (which they are subsequently shown to
entail) are typically better supported than hypotheses which are
proposed merely to fit the data—for the latter lack the
independent support possessed by the former. The appearance of
plausibility to predictivism arises because the role of the
preliminary hypothesis-inducing evidence is being suppressed. 
Karl Popper is probably the most famous proponent of prediction in the
history of philosophy. In his lecture “Science: Conjectures and
Refutations” Popper recounts his boyhood attempt to grapple with
the question “When should a theory be ranked as
scientific?” (Popper 1963: 33–65). Popper had become
convinced that certain popular theories of his day, including
Marx’s theory of history and Freudian psychoanalysis, were
pseudosciences. Popper deemed the problem of distinguishing scientific
from pseudoscientific theories ‘the demarcation problem’.
His solution to the demarcation problem, as is well known, was to
identify the quality of falsifiability (or ‘testability’)
as the mark of the scientific theory. 
The pseudosciences were marked, Popper claimed, by their vast
explanatory power. They could explain not only all the relevant actual
phenomena the world presented, they could explain any conceivable
phenomena that might fall within their domain. This was because the
explanations offered by the pseudosciences were sufficiently malleable
that they could always be adjusted ex post facto to explain anything.
Thus the pseudosciences never ran the risk of being inconsistent with
the data. By contrast, a genuinely scientific theory made specific
predictions about what should be observed and thus ran the risk of
falsification. Popper emphasized that what established the scientific
character of relativity theory was that it ‘stuck its neck
out’ in a way that pseudosciences never did. 
Like Whewell and Herschel, Popper appeals to the predictions a theory
makes as a way of separating the illegitimate uses of the method of
hypothesis from its legitimate uses. But while Whewell and Herschel
pointed to predictive success as a necessary condition for the
acceptability of a theory that had been generated by the method of
hypothesis, Popper focuses in his solution to the demarcation problem
not on the success of a prediction but on the fact that the theory
made the prediction at all. Of course, there was for Popper an
important difference between scientific theories whose predictions
were confirmed and those whose prediction were falsified. Falsified
theories were to be rejected, whereas theories that survived testing
were to be ‘tentatively accepted’ until falsified. Popper
did not hold, with Whewell and Hershel, that successful predictions
could constitute legitimate proof of a theory—in fact Popper
held that it was impossible to show that a theory was even probable
based on the evidence, for he embraced Hume’s critique of
inductive logic that made evidential support for the truth of theories
impossible. Thus, one should ascribe to Popper a commitment to
predictivism only in the broad sense that he held predictions to be
superior to accommodations—he did not hold that predictions
confirmed theory more strongly than accommodations. It would
ultimately prove impossible for Popper to reconcile his claim that a
theory which enjoyed predictive success ought to be ‘tentatively
accepted’ with his anti-inductivism (see, e.g., Salmon 1981).
Imre Lakatos (1970, 1971) proposed an account of scientific method in
the form of his ‘methodology of scientific research
programmes’ which was a development of Popper’s approach.
A scientific research program was constituted by a ‘hard
core’ of propositions which were retained throughout the life of
that programme together with a ‘protective belt’ which was
constituted by auxiliary hypotheses that were adjusted so as to
reconcile the hard core with the empirical data. The attempt on the
part of the proponents of the research programme to reconcile the
programme to empirical data produced a series of theories \(T_1\),
\(T_2\),… \(T_n\) where, at least in some cases, \(T_{i+1}\)
serves to explain some data that is anomalous for \(T_i\). Lakatos
held that a research programme was ‘theoretically
progressive’ insofar as each new theory predicts some novel
hitherto unexpected fact. A research programme is ‘empirically
progressive’ to the extent that its novel empirical content was
corroborated, that is, if each new theory leads to the discovery of
“some new fact” (Lakatos 1970: 118). Lakatos thus offered
a new solution to the demarcation problem: a research programme was
pseudoscientific to the extent that it was not theoretically
progressive. Theory evaluation is construed in terms of competing
research programmes: a research programme defeats a rival programme by
proving more empirically progressive over the long run.
According to Merriam-Webster’s Collegiate
 Dictionary,[2]
 something is ‘ad hoc’ if it is ‘formed or used for
specific or immediate problems or needs’. An ad hoc hypothesis
then is one formed to address a specific problem—such as the
problem of immunizing a particular theory from falsification by
anomalous data (and thereby accommodating that data). Consequently
what makes a hypothesis ad hoc, in the ordinary English sense of the
term, has nothing to do with the content of the hypothesis but simply
with the motivation of the scientist who proposes it—and it is
unclear why there would be anything suspicious about such a
motivation. Nonetheless, ad hoc hypotheses have long been suspect in
discussions of scientific method, a suspicion that resonates with the
predictivist’s skepticism about accommodation. 
For Popper, a conjecture is ad hoc “if it is
introduced…to explain a particular difficulty,
but…cannot be tested independently” (Popper 1974: 986).
Thus Popper’s conception of ad hocness added to the ordinary
English meaning a further requirement—in the case of an ad hoc
hypothesis that was simply introduced to explain a single phenomenon,
the ad hoc hypothesis has no testable consequences other than that
phenomenon. In the case of an ad hoc theory modification introduced to
resolve an anomaly for a theory, the modified theory had no testable
consequences other than those of the original theory. 
Popper offered two explications of why ad hoc hypotheses were suspect.
One was that if we offer T as an explanation of f, but
then cite f as the only reason we have to believe T,
Popper claims that we have engaged in reasoning that is suspicious for
reasons of circularity (Popper 1972: 192–3). This was arguably
fallacious on Popper’s part—a circular proof would offer
one proposition, p, in support of a second proposition
q, when q has already been offered in support of
p. But in the above example, while f is offered as
evidence for T, T is offered as an explanation of (not
as evidence for) f—and thus there is no circular
reasoning (Bamford 1993: 338). 
Popper’s other explanation of why ad hoc hypotheses were
regarded with suspicion was that they ran counter to the aim of
science, which for Popper included the proposal of theories with
increasing empirical content, viz., increasing falsifiability. Ad hoc
hypotheses, for Popper, suffer from a lack of independent testability
and thus reduce (or at least fail to increase) the testability of the
theories they modify (cf. above). However, Popper’s claim that
the process of modifying a theory ad hoc tends to lead to insufficient
falsifiability and is ‘unscientific practice’ has been
challenged (e.g., Bamford 1993: 350). 
Subsequent authors argued that a hypothesis proposed for the sake of
immunizing a theory from falsification could be
‘suspicious’ for various reasons, and thus could be
‘ad hoc’ in various ways. Zahar (1973) argued that a
hypothesis was ad hoc1 if it had no novel consequences as
compared with its predecessor (i.e. was not independently testable),
ad hoc2 if none of its novel predictions have actually been
verified (either because it has not yet been tested or has been
falsified), and ad hoc3 
if it is obtained from its predecessor through a modification of the
auxiliary hypotheses which does not accord with the spirit of the
heuristic of the programme. (1973: 101) 
Beyond Popper’s criterion of a lack of independent testability
then, a hypothesis introduced to accommodate some datum could be ad
hoc because it was simply unconfirmed (ad hoc2) or because
it failed to cohere with the basic commitments of the research
programme in which it is proposed (ad hoc3).
Another approach proposes that a hypothesis H introduced into a
theory T in response to an experimental result E is ad
hoc if it is generally unsupported and appears to be a superficial
attempt to paper over deep problems with a theory that is actually in
need of substantive revision. Thus to level the charge of ad hocness
against a hypothesis was actually to direct serious skepticism toward
the theory the hypothesis was meant to rescue. This concept of
ad hocness arguably makes sense of Einstein’s critique of the
Lorentz-Fitzgerald contraction hypothesis as ‘ad hoc’ as a
supplementary hypothesis to the aether theory, and Pauli’s
postulation of the neutrino as an ad hoc rescue of classical quantum
mechanics (Leplin 1975, 1982; for further discussion see Grünbaum
1976).
It seems clearly true that the scientific community’s judgment
about whether a hypothesis is ad hoc can change. Given this
revisability, and the aesthetic dimension of theory evaluation (which
leaves assessment to some degree ‘in the eye of the
beholder’) there may be no particular point to embracing a
theory of ad hocness, if by the term ‘ad hoc’ we mean
‘illegitimately proposed’ (Hunt 2012).
Popper wrote that 
Confirmations should count only if they are the result of risky
predictions; that is to say, if, unenlightened by the theory in
question, we should have expected an event which was incompatible with
the theory in question, we should have expected an event which was
incompatible with the theory—an event which would have refuted
the theory. (1963: 36) 
Popper (and subsequently Lakatos) thereby endorsed a temporal
condition of novelty—a prediction counts as novel is if it is
not known to be true (or is expected to prove false) at the time the
theory is constructed. But it was fairly obvious that this made
important questions of confirmation turn implausibly on the time at
which certain facts were known.
Thus Zahar proposed that a fact is novel “if it did not belong
to the problem-situation which governed the construction of the
hypothesis” (1973: 103). This form of novelty has been deemed
‘problem-novelty’ (Gardner 1982: 2). But in the same paper
Zahar purports to exemplify this concept of novelty by referring to
the case in which Einstein did not use the known behavior of
Mercury’s perihelion in constructing his theory of
 relativity.[3]
 Gardner notes that this latter conception of novelty, which he deemed
‘use-novelty’, is distinct from problem-novelty (Gardner
1982: 3). Evidence is use-novel for T if T was not built
to fit that evidence (whether or not it was part of the relevant
‘problem-situation’ the theory was intended to address).
In subsequent literature, the so-called heuristic conception of
novelty has been identified with use-novelty—it was further
articulated in Worrall 1978 and
 1985.[4]
Another approach argues that a novel consequence of a theory is one
that was not known to the theorist at the time she formulated the
theory—this seems like a version of the temporal conception, but
this point appeals implicitly to the heuristic conception: if a
theorist knew of a result prior to constructing a theory which
explains it, it may be difficult to determine whether that theorist
somehow tailored the theory to fit the fact (e.g., she may have done
so unconsciously). A knowledge-based conception is thus the best that
we can do to handle this difficulty (Gardner
 1982).[5]
The heuristic conception is, however, deeply
controversial—because it makes the epistemic assessment of
theories curiously dependent on the mental life of their constructors,
specifically on the knowledge and intentions of the theorist to build
a theory that accommodated certain data rather than others.
Leplin’s comment is typical: 
The theorist’s hopes, expectations, knowledge, intentions, or
whatever, do not seem to relate to the epistemic standing of his
theory in a way that can sustain a pivotal role for them….
(1997: 54) 
(For similar comments see Gardner 1982: 6; Thomason 1992: 195;
Schlesinger 1987: 33; Achinstein 2001: 210–230; and Collins
1994.) 
Another approach notes that scientists operate with competing theories
and that the role of novel confirmations is to decide between them.
Thus, a consequence of a theory T is a ‘novel
prediction’ if it is not a consequence of the best available
theory actually present in the field other than T (e.g., the
prediction of the Mercury perihelion by Einstein’s relativity
theory constituted a novel prediction because it was not a
(straightforward) consequence of Newtonian mechanics; Musgrave 1974:
18). Operating in a Lakatosian framework, Frankel claims a consequence
was novel with respect to a theory and its research programme if it is
not similar to a fact which already has been used by members of the
same research program to support a theory designed to solve the same
problems as the theory in question (1979: 25). Also in a Lakatosian
framework, Nunan claims that a consequence is novel if it has not
already been used to support, or cannot readily be explained in terms
of, a theory entertained in some rival research program (1984:
 279).[6]
There are clearly multiple forms of novelty and it is generally
recognized that a fact could be ‘novel’ in multiple
senses—as we will see, some carry more epistemic weight than
others (Murphy 1989).
Global predictivism holds that predictions are always superior to
accommodations, while local predictivism holds that this only holds in
certain cases. Strong predictivism asserts that prediction is
intrinsically superior to accommodation, whereas weak predictivism
holds that predictive success is epistemically relevant because it is
symptomatic of other features that have epistemic import. The
distinction between strong and weak predictivism cross classifies with
the distinctions between different types of novelty. For example, one
could maintain that temporal predictions are intrinsically superior to
temporal accommodations (strong temporal predictivism) or that
temporal predictions were symptomatic of some other good-making
feature of theories (weak temporal predictivism; Hitchcock and Sober
2004: 3–5). These distinctions will be further illustrated
below.
A version of global strong heuristic predictivism is the null support
thesis that holds that theories never receive confirmation from
evidence they were built to fit—precisely because of how they
were built. This thesis has been attributed to Bacon and Descartes
(Howson 1990: 225). Popper and Lakatos also subscribe to this thesis,
though it is important to remember that they do not recognize any form
of confirmational support—even from successful predictions. But
others who maintained that successful predictions do confirm theories
nonetheless endorsed the null support hypothesis. Giere provides the
following argument:
If the known facts were used in constructing the model and were thus
built into the resulting hypothesis…then the fit between these
facts and the hypothesis provides no evidence that the hypothesis is
true [since] these facts had no chance of refuting the hypothesis.
(1984: 161; Glymour 1980: 114 and Zahar 1983: 245 offer similar
arguments)
The idea is that the way the theory was built provided an illegitimate
protection against falsification by the facts—hence the facts
cannot support the theory. Others however find this argument specious,
noting that since the content of the hypothesis is fixed, it makes no
sense to think of any facts as having a ‘chance’ to
falsify the theory. The theory says what it says, and any particular
fact refutes it or it doesn’t. 
Giere has confused what is in effect a random variable (the
experimental setup or data source E together with its set of
distinct possible outcomes) with one of its values (the outcome
e)…Moreover, it makes perfectly good sense to say that
E might well have produced an outcome other than the one,
e, it did as a matter of fact produce. (Howson 1990: 229; see
also Collins 1994: 220) 
Thus Giere’s argument collapses.
Howson argued in a series of papers (1984, 1988, 1990) that the null
support thesis is falsified using simple examples, such as the
following:
An urn contains an unknown number of black and white tickets, where
the proportion p of black tickets is also unknown. The data
consists simply in a report of the relative frequency \(r/k\) of black
tickets in a large number k of draws with replacement from the
urn. In the light of the data we propose the hypothesis that \(p =
(r/k)+\epsilon\) for some suitable \(\epsilon\) depending on k.
This hypothesis is, according to standard statistical lore, very well
supported by the data from which it is clearly constructed. (1990:
231) 
In this case there is, Howson notes, a background theory that supplies
a model of the experiment (it is a sequence of Bernoulli trials, viz.,
a sequence of trials with two outcomes in which the probability of
getting either outcome is the same on each trial; it leaves only a
single parameter to be evaluated). As long as we have good reason to
believe that this model applies, our inference to the high probability
of the hypothesis is a matter of standard statistical methodology, and
the null support thesis is refuted.
It has been argued that one of the limitations of Bayesianism is that
it is fatally committed to the (clearly false) null support thesis
(Glymour 1980). The standard Bayesian condition by which evidence
e supports h is given by the inequality \(p(h\mid e) \gt
p(h)\). But where e is known (and thus \(p(e) = 1\)), we have
\(p(h\mid e) = p(h)\). This came to be known as the ‘Bayesian
problem of old evidence’. Howson (1984) noted that this problem
could be overcome by selecting a probability function \(p^*\) based on
the assumption that e was not known—thus even if \(p(h\mid e)
= p(h)\), it could still hold that \({p^*}(h\mid e) \gt {p^*}(h)\). Thus
followed an extensive literature on the old evidence problem which
will not be summarized here (see, e.g., Christiansen 1999; Eells &
Fitelson 2000; Barnes 1999, 2008: Ch. 7; and Hartmann & Fitelson
2015).
Patrick Maher (1988, 1990, 1993) presented a seminal thought
experiment and a Bayesian analysis of its predictivist implications.
The thought experiment contained two scenarios: in the first scenario,
a subject (the accommodator) is presented with E, a sequence of
99 coin flips. E forms an apparently random sequence of heads
and tails. The accommodator is then instructed to tell us the outcome
of the first 100 flips—he responds by reciting E and then
adding the prediction that the 100th toss will be
heads—the conjunction of E and this last toss is
T. In the other scenario, another subject (the predictor) is
asked to predict the first 100 flip outcomes without witnessing any
outcomes—the predictor endorses theory T. Thereafter the
coin is flipped 99 times, E is established, and the
predictor’s first 99 predictions are confirmed. The question is
in which of these two scenarios is T better confirmed. It is
strongly intuitive that T is better confirmed in the
predictor’s scenario than in the accommodator’s scenario,
suggesting that predictivism holds true in this case. If we allow
‘O’ to assert that evidence E was input into
the construction of T, predictivism asserts:
Maher argues that the successful prediction of the initial 99 flips
constitutes persuasive evidence that the predictor ‘has a
reliable method’ for making predictions of coin flip outcomes.
T’s consistency with E in the case of the
accommodator provides no particular evidence that the
accommodator’s method of prediction is reliable—thus we
have no particular reason to endorse his prediction about the
100th flip. Allowing R to assert that the method in
question is reliable, and \(M_T\) that method M generated
hypothesis T, this amounts to:
Maher’s (1988) provides a rigorous proof of (2), which is shown
to entail (1) on various assumptions. 
Maher’s (1988) makes the simplifying assumption that any method
of prediction used by a predictor is either completely reliable (this
is the claim abbreviated by ‘R’) or is no better
than a random method (\(\neg R\)). (Maher [1990] shows that this
assumption can be surrendered and a continuum of degrees of
reliability of scientific methods assumed; the predictivist result is
still generated.) In qualitative terms, where M generates
T (and thus predicts E) without input of evidence
E, we should infer that it is much more likely that the method
that generated E is reliable than that E just happened
to turn out true though R was no better than a random method.
In other words, we judge that we are much more likely to stumble on a
subject using a reliable method M of coin flip prediction than
we are to stumble on a sequence of 99 true flip predictions that were
merely lucky guesses—because
Maher has articulated a weak heuristic predictivism because he claims
that predictive success is symptomatic of the use of a reliable
discovery
 method.[7]
For critical discussion of Maher’s theory of predictivism see
Howson and Franklin 1991 (and Maher’s 1993 reply); Barnes
1996a,b; Lange 2001; Harker 2006; and Worrall 
 2014.[8]
It was noted above that ad hoc hypotheses stand under suspicion for
various reasons, one of which was that a hypothesis that was proposed
to resolve a particular difficulty may not cohere well with the theory
it purports to save or relevant background
 beliefs.[9]
 This could result from the fact that there is no obvious way to
resolve the difficulty in a way that is wholly ‘natural’
from the standpoint of the theory itself or operative criteria of
theory choice. For example, the phlogiston theory claimed that
substances emitted phlogiston while burning. However, it was
established that some substances actually gained weight while burning.
To accommodate the latter phenomenon it was proposed that phlogiston
had negative weight—but the latter hypothesis was clearly ad hoc
in the sense of failing to cohere with the background belief that
substances simply do not have negative weight, and with the knowledge
that many objects lost weight when burned (Partington & McKie
1938a: 33–38).
Thus the ‘fudging explanation’ defends predictivism by
pointing out that the process of accommodation lends itself to the
proposal of hypotheses that do not cohere naturally with operative
constraints on theory choice, while successful predictions are immune
from this worry (Lipton 1990, 1991: Ch. 8). Of course, it is an
important question whether scientists actually rely on the fact that
evidence was predicted (or accommodated) in their assessment of
theories—if a theory was fudged to accommodate some datum,
couldn’t the scientist simply note that the fudged theory
suffers a defect of coherence and pay no attention to whether the data
was accommodated or predicted? Some argue, however that scientists are
imperfect judges of such coherence—a scientist who accommodates
some datum may think his accommodation is fully coherent, while his
peers may have a more accurate and objective view that it is not. The
scientist’s ‘assessed support’ of his proposed
accommodation may thus fail to coincide with its ‘objective
support’, and the scientist might rely on the fact that his
evidence was accommodated as evidence that it was fudged (or
conversely, that his evidence was predicted as evidence that it was
not fudged; Lipton 1991: 150f).
Lange (2001) offers an alternate interpretation of the coin flip
example that claims that the process of accommodation (unlike
prediction) tends to generate theories that are not strongly supported
by confirming data. He imagines a ‘tweaked’ version of the
coin flip example in which the initial 99 outcomes form a strict
alternating sequence ‘tails heads tails heads…’
(instead of forming the ‘apparently random sequence’ of
outcomes provided in the original case). Again we imagine a predictor
who correctly predicts 99 outcomes in advance and an accommodator who
witnesses them. Both the predictor and the accommodator predict that
the 100th outcome will be tails. Now there is little or no
difference in our assessed probability that the subject will correctly
predicted the 100th outcome. 
This suggests that the intuitive difference between Maher’s
original pair of examples does not reflect a difference between
prediction and accommodation per se. (Lange 2001: 580)
Lange’s analysis appeals to what Goodman called an
‘arbitrary conjunction’—the mark of which is that
establishment of one component endows the whole statement with no
credibility that is transmitted to other component statements. (1983:
68–9) 
An example of an arbitrary conjunction is “The sun is made of
helium and August 3rd 2017 falls on a Thursday and 17 is a
prime number”. In the original coin flip case, we judge that
H is weakly supported in the accommodator’s scenario
because we judge that the apparently random sequence of outcomes is
probably an arbitrary conjunction—thus the fact that the initial
99 conjuncts are confirmed implies almost nothing about what the
100th outcome will be. But the success of the predictor in
predicting the initial 99 outcomes strongly implies that the sequence
is not an arbitrary conjunction after all: 
(w)e now believe it more likely that the agent was led to posit this
particular sequence by way of something we have not noticed that ties
the sequence together—that would keep it from being a
coincidence that the hypothesis is accurate to the 100th
toss…. (Lange 2001: 581) 
Having judged it not to be an arbitrary conjunction, we are now
prepared to recognize the first 99 outcomes as strongly confirming the
prediction in the 100th case. What accounts for the
difference between the two scenarios, in other words, is not primarily
whether E was predicted or accommodated, but whether we judge
H to be an arbitrary conjunction, and thus whether E
provides support for the remaining portion of H. 
Thus in Lange’s tweaked case, the non-existence of the
predictivist effect is due to the fact that it is clear from the
initial 99 flips that the sequence is not an arbitrary
conjunction—thus E confirms H equally strongly in
both scenarios. 
Lange goes on to suggest that in actual science the practice of
constructing a hypothesis by way of accommodating known evidence has a
tendency to generate arbitrary conjunctions. Thus Lorentz’s
contraction hypothesis, when appended to his electrodynamics to
accommodate the failure to detect optically any motion with respect to
the aether, resulted in an arbitrary conjunction (since evidence that
supported the contraction hypothesis did not support the
electrodynamics, or vice versa)—essentially for this reason,
Lange argues, it was rejected by Einstein as ad hoc. When evidence is
predicted by a theory, by contrast, this is typically because the
theory is not an arbitrary conjunction. The evidential significance of
prediction and accommodation for Lange is that they tend to be
correlated (negatively and positively) with the construction of
theories that are arbitrary conjunctions. Lange’s view might
thus be classed as a weak heuristic predictivism, though Lange never
takes a stand on whether scientists actually rely on such correlations
in assessing theories. 
For critical discussion of Lange’s theory see Worrall 2014:
59–61 and Harker 2006: 317f.
Deborah Mayo has argued (particularly in Mayo 1991, 1996, and 2014)
that the intuition that predictivism is true derives from a premium on
severe tests of hypotheses. A test of a hypothesis H is severe
to the extent that H is unlikely to pass that test if H
is false. Intuitively, if a novel consequence N is shown to
follow from H, and the probability of N on the
assumption \({\sim}H\) is very low (for the reason of its being
novel), then testing for N would seem to count as a severe test
of H, and a positive outcome should strongly support H.
Here novelty and severity appear to coincide—but Mayo observes
that there are cases in which they come apart. For example, it has
seemed to many that if H is built to fit some body of evidence
E then the fact that H fits E does not support
H because this fit does not constitute H’s having
survived a severe test (or a test at all). One of Mayo’s central
objectives is to expose the fallacies that this latter reasoning
involves.
Giere (1984: 161, 163) affirms that evidence H was built to fit
cannot support H because, given how H was built, it was
destined to fit that evidence. Mayo summarizes his reasoning as
follows:
But Mayo notes that ‘no matter what’ can be interpreted in
two ways: (a) no matter what the data are, and (b) no matter whether
H is true or false. (1) is true when interpreted as (a), but in
order to establish that accommodated evidence fails to support
H (as Giere intends) (1) must be interpreted as (b). However,
(1) is false when so interpreted. Mayo (1996: 271) illustrates this
with a simple example: let the evidence e be a list of SAT
scores from students in a particular class. Use this evidence to
compute the average score x, and set h = the mean SAT
score for these students is x. Now of course h has been
use-constructed from e. It is true that whatever mean score was
computed would fit the data no matter what the data are—but
hardly true that h would have fit the evidence no matter
whether h was true or false. If h were false it would
not fit the data, because the data will inevitably fit only a true
hypothesis. Thus h has passed a maximally severe test: it is
virtually impossible for h to fit the data if h is
false—despite the fact that h is built to fit e.
Mayo gives an additional example of how a use-constructed hypothesis
can count as having survived a severe test that pertains to the famous
1919 Eddington eclipse experiment of Einstein’s General Theory
of Relativity. GTR predicted that starlight that passed by the sun
would be bent to a specific degree (specifically 1.75 arcseconds). There
were actually two expeditions carried out during the eclipse—one
to Sobral in Northern Brazil and the other to the island of Principe
in the Gulf of Guinea. Each expedition generated a result that
supported GTR, but there was a third result generated by the Sobral
expedition that appeared to refute GTR. This result was however
disqualified because it was determined that a mirror used to acquire
the images of the stars’ position had been damaged by the heat
of the sun. While one might worry that such dismissing of anomalous
evidence was the kind of ad hoc adjustment that Popper warned against,
Mayo notes that this is instead a perfectly legitimate case of using
evidence to support a hypothesis (that the third result was
unreliable) that amounted to that hypothesis having passed a severe
test. Mayo concludes that a general prohibition on use-constructed
hypothesis “fails to distinguish between problematic and
unproblematic use-constructions (or double countings)” (1996:
285). However, Hudson (2003) argues that there is historical evidence
that suggests there was legitimate reason to question the hypothesis
that the third result was unreliable (he uses this point to support
his own contention that the fact that a hypothesis was use-constructed
is prima facie evidence that the hypothesis is suspect). Mayo (2003)
replies that insofar as the third result was nonetheless suspect the
physicists involved were right to discard it. 
Mayo (1996: Ch. 9) defends a predictivist-like position attributed to
Neyman-Pearson statistical methods—the prohibition on
after-trial constructions of hypotheses. To illustrate: Kish (1959)
describes a study that investigated the statistical relationship
between a large number of infant training experiences (nursing, toilet
training, weaning, etc.) and subsequent personality and behavioral
traits (e.g., school adjustment, nail biting, etc.) The study found a
number of high correlations between certain training experience and
later traits. The problem was that the study investigated so many
training experiences that it was quite likely that some correlations
would appear in the data simply by chance—even if there would
ultimately prove to be no such correlation. An investigator who
studied many possible correlations thus could survey that data and
simply look for statistically significant differences and proclaim
evidence for correlations despite such evidence being
misleading—thus engaging in the dubious practice of the
‘after-trial construction of
 hypothesis’.[10]
 Mayo notes that such hypotheses should not count as having passed a
severe test, thus she endorses the Neyman-Pearson prohibition on such
construction. Hitchcock and Sober (2004) note that Mayo’s
definition of severity as applied in this case differs from the one
she employs in dealing with cases like her SAT example; Mayo (2008)
replies at length to their criticism and argues that while she does
employ two versions of the severity definition they nonetheless
reflect a unified conception of severity.
For critical discussion of Mayo’s account see Iseda 1999 and
Worrall 2006: 56–60, 2010: 145–153—see also
Mayo’s (1996: 265f, 2010) replies to Worrall. 
John Worrall has been an important contributor to the predictivism
literature from the 1970s until the present time. He was, along with
Elie Zahar, one of the early proponents of the significance of
heuristic novelty (e.g., Worrall 1978, 1985). In his more recent work
(cf. his 1989, 2002, 2005, 2006, 2010, 2014; also Scerri & Worrall
2001) Worrall has laid out a detailed theory of predictivism that,
while sometimes presented in heuristic terms, is “at root a
logical theory of confirmation” (2005: 819)—it is thus a
weak heuristic account that takes use-novelty of evidence to be
symptomatic of underlying logical features that establish strong
confirmation of theory. 
Worrall’s mature account is based on a view of scientific
theories that he credits to Duhem—which claims that a scientific
theory is naturally thought of as consisting of a core claim together
with some set of more specific auxiliary claims. It is commonly the
case that the core theory will leave undetermined certain ‘free
parameters’ and the auxiliary claims fix values for such
parameters. To cite an example Worrall often uses, the wave theory of
light consists of the core theory that light is a periodic disturbance
transmitted though some sort of elastic medium. This core claim by
itself leaves open various free parameters concerning the wavelengths
of particular types of monochromatic light. Worrall proposes to
understand the diminished status of evidential support associated with
accommodation as follows: when evidence e is ‘used’
in the construction of a theory, it is typically used to establish the
value of a free parameter in some core theory T. The fixed
version will be a specific version \(T'\) of T.
e serves to confirm \(T'\), then, only on the
condition that there is independent support for T—thus
accommodation provides only ‘conditional confirmation’.
Importantly, evidence e that is used in this way will by itself
typically provide no evidence for core theory T. Worrall (2002:
201) offers as an illustration the support offered to the wave theory
of light (W) by the two slit experiment using light from a
sodium arc—the data will consist of various alternating light
and dark ‘fringes’. The fringe data can be used to compute
the wavelength of sodium light—and thus used to generate a more
specific version of the wave theory of light \(W'\)—one which
conjoins W with a claim about the wavelength of this particular
sort of light. But the data offer merely conditional support to
\(W'\)—that is the data support \(W'\) only on the condition
that there is independent evidence for W. 
Predicted evidence for Worrall is thus evidence that is not used to
fix free parameters. Worrall cites two forms that predictions can
take: one is when a particular evidential consequence falls
‘immediately out of the core’, i.e., is a consequence of
the core, together with ‘natural auxiliaries’, and the
other is when it is a consequence of a specific version of a theory
whose free parameters have been fixed using other data. To illustrate
the first: retrograde
 motion[11]
 was a natural consequence of the Copernican core (the claim that the
earth and planets orbit the sun) because observation of the planets
was carried out on a moving observatory that periodically passed other
planets—however it could only be accommodated by Ptolemaic
astronomy by proposing and adjusting auxiliary hypotheses that
supposed the planet to move on an epicycle (retrograde motion did not
follow naturally from the Ptolemaic core idea that the Sun, stars and
planets orbit the earth). Thus retrograde motion was predicted by the
Copernican theory and thus offered unconditional support to that
theory, while it offered only conditional confirmation to the
Ptolemaic theory. The second form of prediction is one which follows
from a specific version of a theory but was not used to fix a
parameter—imagine \(W'\) in the preceding paragraph makes a new
prediction p (say for another experiment, such as the one slit
experiment)—p offers unconditional confirmation of \(W'\)
(and W; Worrall 2002: 203).
However it is important to understand that Worrall’s repeated
expression of his position in terms of the heuristic conception of
novelty (particularly after his 1985) does not amount to an
endorsement of strong heuristic predictivism. Worrall clarifies this
in his 1989 article that focuses on the evidential significance of the
‘white spot’ confirmation of Fresnel’s version of
the wave theory of light. The reason the white spot datum carried such
important weight is not ultimately that it was not used by Fresnel in
the construction of the theory but because this datum followed
naturally from the core theory that light is a wave. The reason the
fringe data that was used to compute the wavelength of sodium light
(cf. above) did not carry such weight is that it is not a consequence
of this core idea (nor has the wavelength of sodium light been fixed
by some other data). Thus d is novel for T when
“there is a heuristic path to [T] that does not
presuppose [d’s] existence” (Scerri & Worrall 2001:
418). As Worrall sometimes puts it, whether d carries
unconditional confirmation for T does not depend on whether
d was actually used in constructing T, but whether it
was ‘needed’ to construct T (e.g., 1989:
149–151). Thus Worrall is actually a proponent of
‘essential use-novelty’ (Alai 2014: 304). For Worrall,
facts about heuristic prediction and accommodation serve to track
underlying facts about the logical relationship between theory and
evidence. Thus Worrall is ultimately a proponent of weak (not strong)
heuristic predictivism. Worrall categorically rejects temporal
predictivism, arguing that the fact that the white spot was a
temporally novel consequence in itself was of no epistemic importance.
For further discussion of Worrall’s theory of predictivism see
Mayo 2010: 155f; Schurz 2014; Votsis 2014; and Douglas & Magnus
2013: 587–8. 
Scerri and Worrall 2001 contains a detailed rendering of the
historical episode of the scientific community’s assessment of
Mendeleev’s theory of the periodic law—it is argued that
this story ultimately vindicates Worrall’s theory of
predictivism. 
For discussion of Scerri and Worrall see Akeroyd 2003; Barnes 2005b
(and replies from Worrall 2005 and Scerri 2005); Schindler 2008, 2014;
and Brush 2007. 
A common argument for predictivism is that we should avoid inferring
that a theory T is true on the basis of evidence E that
it is built to fit because we can explain why T entails
E by simply noting how T was built—but if T
was not built to fit E then only the truth of T can
explain the fact that T fits E. Various philosophers
have noted that this reasoning is fallacious. As noted above it makes
no sense to offer an explanation (for example, in terms of how the
theory was built) for the fact that T entails
E—for this latter fact is a logical fact for which no
causal explanation can be given. Insofar as there is an
explanandum in need of an explanans here it is
rather the fact that the theorist managed to construct or
‘choose’ a theory (which turned out to be T) that
correctly entailed E (Collins 1994; Barnes 2002)—that
explanandum could be explained by noting that the theorist
built a theory (which turned out to be T) to fit E, or
endorsed it because it fit E. 
White (2003) offers a theory of predictivism that begins with this
same insight—the relevant explanandum is:
This explanandum could be explained in one of two ways:
White explains that (RA) means “roughly that the mechanisms
which led to her selection of a theory gave her a good chance of
arriving at the truth” (2003: 664). (Thus White analogizes the
theorist to an ‘archer’ who is more or less reliable in
‘aiming’ at the truth in selecting a theory.) Then White
offers a simple argument for predictivism: assuming ~DS, ES provides
evidence for RA. But assuming DS, ES provides no evidence for RA.
Thus, heuristic predictivism is true.
Interestingly, White bills his account as a strong heuristic account.
In making this claim he is claiming that the epistemic advantage of
prediction would not be entirely erased for an observer who was
completely aware of all relevant evidence and background knowledge
possessed by the scientific community at the relevant point in time.
This is because the degree to which theorizing is reliable depends
upon principles of evidence assessment and causal relations (including
the reliability of our perceptual faculties, accuracy of measuring
instruments, etc.) that are not entirely “transparent” to
 us.[12]
 Insofar as fully informed scientists may not be fully convinced of
just how reliable these principles and relations are, evidence that
they lead to the endorsement of theories which are predictively
successful continues to redound to their assessed reliability. Thus,
White concludes, strong heuristic predictivism is vindicated (2003:
671–4). 
Hitchcock and Sober (2004) provide an original theory of weak
heuristic predictivism that is based on a particular worry about
accommodation. On the assumption that data are noisy (i.e. imbued with
observational error), a good theory will almost never fit the data
perfectly. To construct a theory that fits the data better than a good
theory should, given noisy data, is to be guilty of
“overfitting”—if we know a theorist built her theory
to accommodate data, we may well worry that she has overfit the data
and thus constructed a flawed theory. If we know however that a
theorist built her theory without access to such data, or without
using it in the process of theory construction, we need not worry that
overfitting that data has occurred. When such a theory goes on to make
successful predictions, Hitchcock and Sober moreover argue, this
provides us with evidence that the data on which the theory was
initially based were not overfit in the process of constructing the
theory. 
Hitchcock and Sober’s approach derives from a particular
solution to the curve-fitting problem presented in Forster and Sober
1994. The curve fitting problem is how to select an optimally
supported curve on the basis of a given body of data (e.g., a set of
\([X,Y]\) points plotted on a coordinate graph). A well-supported
curve will feature both ‘goodness of fit’ with the data
and simplicity (intuitively, avoiding highly bumpy or irregular
patterns). Solving the curve-fitting problem requires some precise way
of characterizing a curve’s simplicity, a way of characterizing
goodness of fit, and a method of balancing simplicity against goodness
of fit to identify an optimal curve. 
Forster and Sober cite Akaike’s (1973) result that an unbiased
estimate of the predictive accuracy of a model can be computed by
assessing both its goodness of fit and its simplicity as measured by
the number of adjustable parameters it contains. A model is a
statement (a polynomial, in the case of a proposed curve) that
contains at least one adjustable parameter. For any particular model
M, a given data set, and identifying \(L(M)\) as the likeliest
(i.e. best data fitting) curve from M, Akaike showed:
An unbiased estimate of the predictive accuracy of model
This estimate is deemed a model’s ‘Akaike Information
Criterion’ (AIC) score—it measures goodness of fit in
terms of the log likelihood of the data on the assumption of \(L(M)\).
The simplicity of the model is inversely proportion to k, the
number of adjustable parameters in the model. The intuitive idea is
that models with a high k value will provide a large variety of
curves that will tend to fit data more closely than models with a
lower k value—and thus large k values are more
prone to overfitting than small k values. So the AIC score
assesses a model’s likely predictive accuracy in a way that
balances both goodness of fit and simplicity, and the curve-fitting
problem is arguably solved. 
Hitchcock and Sober (2004) consider a hypothetical example involving
two scientists, Penny Predictor and Annie Accommodator. Working
independently, they acquire the same set of data D—Penny
proposes theory Tp while Annie proposes Ta. The critical
difference however was that Penny proposed Tp on the basis of
an initial segment of the data D1—thereafter she
predicted the remaining data D2 to a high degree of accuracy
\((D = D1 \cup D2)\). Annie however was in possession of all the data
in D prior to proposing Ta and in proposing this theory
accommodated D. Hitchcock and Sober ask whether there might be
reason to suspect that Penny’s theory will be more predictively
accurate in the future, and in this precise sense be better confirmed.
Hitchcock and Sober argue that there is no one answer to this
question—and then present a series of several cases. Insofar as
predictivism holds in some and not others, their account of
predictivism is clearly a local (rather than global) account. In cases
in which Penny and Annie propose the same theory, or propose theories
whose AIC scores can be computed and directly compared, there is no
reason to regard facts about how they built the theory to carry
further significance. But if we do not know which theories were
proposed, or by what method they were constructed, the fact that Penny
predicted data that Annie accommodated can argue for Penny’s
theory having a higher AIC score than Annie’s, and thus carry an
epistemic advantage.
Insofar as predictivism holds in some cases but not the others, the
question whether predictivism holds in actual episodes of science
depends on which cases such actual episodes tend to resemble, but
Hitchcock and Sober “take no stand on how often the various
cases arise” (2004: 21). 
Although their account of predictivism is tailored initially to the
curve-fitting problem, it is by no means limited to such cases. They
note that it is natural to think of a model as analogous to the
ontological framework of a scientific theory where the various
ontological commitments can function as ‘adjustable
parameters’—for example, the Ptolemaic and Copernican
world pictures both begin with a claim that a certain entity (the sun
or the earth) is at the center, and these models are articulated by
producing models with adjustable parameters.
For critical discussion of Sober and Hitchcock’s account, see
Lee 2012, 2013 and Douglas & Magnus 2013: 582–584. 
Barnes (2005a, 2008) maintains that predictivism is frequently a
manifestation of a phenomenon he calls ‘epistemic
pluralism’. A ‘T-evaluator’ (a scientist who
assigns some probability to theory T) is an epistemic pluralist
insofar as she regards one form of evidence to be the probabilities
posted (i.e. publicly presented) by other scientists for and against
T and other relevant claims (she is an epistemic individualist
if she does not do this but considers only the scientific evidence
‘on her own’). One form of pluralistic evidence is the
event in which a reputable scientist endorses a theory—this
takes place when a scientist posts a probability for T that is
(1) no lower than the evaluator’s probability and (2) high
enough that subsequent predictive confirmation of T would
redound to the scientist’s credibility (2008: 2.2). 
Barnes rejects the heuristic conception of novelty on the grounds that
it is a mistake to think that what matters epistemically is the
process by which the theory was constructed—what matters is on
what basis the theory was endorsed (2008: 33f) . In the example above,
confirmation of N (a consequence of T) could carry
special weight for an evaluator who learned that the theorist endorsed
the theory without appeal to observational evidence for N
(irrespective of how the theory was constructed). He proposes to
replace the heuristic conception with his endorsement conception of
novelty: N (a known consequence of T) counts as a novel
confirmation of T relative to agent X insofar as
X posts an endorsement-level probability for T that is
based on a body of evidence that does not include observation-based
evidence for N. 
Barnes claims that the notion of endorsement novelty has several
advantages over the heuristic conception—one is that endorsement
novelty can account for the fact that prediction is a matter of
degree: the more strongly the theorist endorses T, the more
strongly its consequence N is predicted (and thus the more
evidence for T for pluralist evaluators who trust the
endorser). Another is that the orthodox distinction between the
context of discovery and the context of justification is preserved.
According to the latter distinction, it does not matter for purposes
of theory evaluation how a theory was discovered. But this turns out
not to be true on the heuristic conception given the central
importance it accords to how a theory was built (cf. Leplin 1987).
Endorsement novelty respects the irrelevance of the process by which
theories are discovered (Barnes 2008: 37–8). 
One claim central to this account is that confirmation is a three-way
relation between theory, evidence, and background belief (cf. Good
1967). Barnes distinguishes between two types of theory endorser: (1)
virtuous endorsers post probabilities for theories that cohere with
their evidence and background beliefs and (2) unvirtuous endorsers who
post probabilities that do not so cohere. A common way of explaining
the predictivist intuition is to note that accommodators tend to be
viewed with a certain suspicion—their endorsement of T
based on accommodated evidence may reflect a kind of social pressure
to endorse T whatever its merits (cf. the ‘fudging
explanation’ above). Such an endorser may post a probability
for T that is too high given her total evidence and background
belief—predictivism thus becomes a strategy by which pluralist
endorsers protect themselves from unvirtuous accommodators (Barnes
2008: 61–69). 
Barnes then presents a theory of predictivism that is designed to
apply to virtuous endorsers. Virtuous predictivism has two roots: (1)
the prediction per se, which is constituted by an endorser’s
posting an endorsement level probability for T that entails
empirical consequence N on a basis that does not include
observation-based evidence for T, and (2) predictive success,
constituted by the empirical demonstration that N is true. The
prediction per se carries epistemic significance for a pluralist
endorser because it implies that the predictor possesses reason
R (consisting of background beliefs) that supports T. If
the endorser views the predictor as credible, this simple act of
prediction carries epistemic weight. Predictive success then confirms
the truth of R, which thereby counts as evidence for T.
Novel confirmation thus has the special virtue of confirming the
background beliefs of the predictor—accommodative confirmation
lacks this virtue. 
Barnes presents two Bayesian thought experiments that purport to
establish virtuous predictivism. In each experiment an evaluator Eva
faces two scenarios—one in which she confronts Peter who posts
an endorsement probability for T without appeal to
N-supporting observations (thus Peter predicts N) and
another in which she confronts Alex who posts an endorsement
probability for T on a basis that includes observations that
establish N (thus Alex accommodates N). The idea behind
both thought experiments is to make the scenarios otherwise as similar
as possible—Barnes makes a number of ceteris paribus
assumption that render the probability functions of Peter and Alex
maximally similar. However it turns out that there is more than one
way to keep the scenarios maximally similar: in the first experiment,
Peter and Alex have the same likelihood ratio but have different
posteriors for T. In the second scenario they have the same
posteriors but different likelihood ratios. Barnes demonstrates that
Eva’s posterior probability is higher in the predictor scenario
in both experiments—thus vindicating virtuous predictivism
(2008: 69–80).
Although his defense of virtuous predictivism is the centerpiece of
his account, Barnes claims that predictivism can hold true of actual
theory evaluation in a variety of ways. He maintains that the position
deemed ‘weak predictivism’ is actually ambiguous—it
could refer to the claim that scientists actually rely on knowledge
that evidence was (or was not) predicted because prediction is
symptomatic of a some other feature(s) of theories that is
epistemically important (‘tempered
 predictivism’[13])
 or simply to the fact that there is a correlation between prediction
and this other feature(s) (‘thin predictivism’). The
distinction between tempered and thin predictivism cross classifies
with the distinction between virtuous and unvirtuous predictivism to
produce four varieties of weak predictivism. Barnes then turns to the
case of Mendeleev’s periodic law and argues that all four
varieties can be distinguished in the scientific community’s
reaction to Mendeleev’s theory of the elements (2008:
82–122). In particular, he argues that it was specifically
Mendeleev’s predicted evidence, not his accommodated evidence,
that had the power to confirm his scientific and methodological
background beliefs from the standpoint of the scientific community.
Critical responses to Barnes’s account are presented in Glymour
2008; Leplin 2009; and Harker 2011. Barnes 2014 responds to these. See
also Magnus 2011 and Alai 2016.
It was noted in
 Section 1
 that John Maynard Keynes rejected predictivism—he argued that
when a theory T is first constructed it is usually the case
that there are reasons R that favor T. If T goes
on to generate successful novel predictions E then those
reasons combine with R to support T—but if some
\(T'\) is constructed ‘merely because it fit
E’ then \(T'\) will be less supported than
T. This has been deemed the “Keynesian dissolution of the
paradox of predictivism” (Barnes 2008: 15–18) 
Colin Howson cites with approval the Keynesian dissolution (1988: 382)
and provides the following illustration: consider h and
\(h'\) which are rival explanatory frameworks.
\(h'\) independently predicts e; h does not
entail e but has a free parameter which is fixed on the basis
of e to produce \(h(a_{0})\)—this latter hypothesis thus
entails e. So \(h'\) predicts e while
\(h(a_{0})\) merely accommodates e. Let us assume that the
prior probabilities of h and \(h'\) are equal (i.e.,
\(p(h) = p(h')\)). Now it stands to reason that \(p(h(a_0)) \lt
p(h)\) since \(h(a_{0})\) entails h but not vice
versa—thus Howson shows it follows that the effect of
e’s confirmation will be to leave \(h'\) no less
probable—and quite possibly more probable—than
\(h(a_{0})\) (1990: 236–7). Thus predictivism appears true but
the operating factor is the role of unequal prior
 probabilities.[14]
The argument from Keynes and Howson against predictivism holds that
the evidence which appears to support predictivism is
illusory—they are clearly asserting that strong predictivism is
false, presumably in its temporal and heuristic forms.
However, it is important to note that the arguments of Keynes and
Howson cited above predate the injection of the concept of ‘weak
predictivism’ into the
 literature.[15]
 It is thus unclear what stand Keynes or Howson would take on weak
predictivism. Likewise, Collins’ 1994 paper “Against the
Epistemic Value of Prediction” strongly rejects predictivism,
but what he is clearly denying is what has since been deemed strong
heuristic predictivism. He might endorse weak heuristic predictivism
as he concedes that 
all sides to the debate agree that knowing that a theory predicted,
instead of accommodated, a set of data can give us an additional
reason for believing it is true by telling us something about the
structural/relational features of a theory. (1994: 213) 
Similarly Harker argues that “it is time to leave predictivism
behind” but also concedes that “some weak predictivist
theses may be correct” (2008: 451); Harker worries that
proclaiming weak predictivism may mislead some into thinking that
predictive success is somehow more important than other epistemic
indicators (such as endorsement by reliable scientists). White goes so
far as to claim that weak predictivism “is not
controversial” (2003: 656).
Stephen Brush is the author of a body of historical work much of which
purports to show that temporal predictivism does not hold in various
episodes of the history of
 science.[16]
 These include the case of starlight bending in the assessment of the
General Theory of Relativity (Brush 1989), Alfvén’s
theories of space plasma phenomena (Brush 1990), and the revival of
big bang cosmology (Brush 1993). However, Brush (1996) argues that
temporal novelty did play a role in the acceptance of
Mendeleev’s Periodic Table based on Mendeleev’s
predictions. Scerri and Worrall (2001) presents considerable
historical detail about the assessment of Mendeleev’s theory and
dispute Brush’s claim that temporal novelty played an important
role in the acceptance of the theory (2001: 428–436). 
Scientific realism holds that there is sufficient evidence to believe
that the theories of the ‘mature sciences’ are at least
approximately true. Appeals to novelty have been important in
formulating two arguments for realism—these are the ‘no
miracle argument’ and the realist reply to the so-called
‘pessimistic
 induction’.[17]
The no-miracle argument for scientific realism holds that realism is
the only account that does not make the success of science a miracle
(Putnam 1975: 73). ‘The success of science’ here refers to
the myriad verified empirical consequences of the theories of the
mature sciences—but as we have seen there is a long standing
tendency to regard with suspicion those verified empirical
consequences the theory was built to fit. Thus the ‘ultimate
argument for scientific realism’ refers to a version of the no
miracle argument that focuses just on the verified novel consequences
of theories—it would be a miracle, this argument proclaims, if a
theory managed to have a sustained record of successful novel
predictions if the theory were not at least approximately true. Thus,
assuming there are no competing theories with comparable records of
novel success, we ought to infer that such theories are at least
approximately true (Musgrave
 1988).[18]
Insofar as the ultimate argument for realism clearly emphasizes a
special role for novel successes, the nature of novelty has been an
important focus in the realist account. Leplin 1997 is a book length
articulation of the ultimate argument for realism; Leplin proposes a
sufficient condition for novelty consisting of two conditions:
An observational result O is novel for T if: 
Leplin clarifies that a ‘minimally adequate
reconstruction’ of such reasoning will be a valid deduction
D of the ‘basic identifying hypotheses’ of T
from independently warranted background assumptions—the premises
of D cannot be weakened or simplified while preserving
D’s validity. Thus for Leplin what establishes whether
O is a novel consequence of T is not whether O
was actually used in the construction of T, but rather whether
it was ‘needed’ for T’s construction. As with
Worrall’s mature ‘essential use’ conception of
novelty, what matters is whether there is a heuristic path to T
that does not appeal to O, whether or not O was used in
constructing T. The Uniqueness Condition helps bolster the
argument for the truth of theories with true novel consequences, for
if there were another theory \(T'\) (incompatible with
T) that also provides a viable explanation of O, the
imputation of truth could not explain the novel success of both
T and \(T'\). The success of at least one would have
to be due to chance, but if chance could explain one such success it
could explain the other as well. 
Both of these conditions for novelty have been questioned. Given the
Independence Condition, it is unclear that any observational result
O will count as novel for any theory, for it may always be true
that the logically weakest set of premises that entail T (which
will be cited in a minimally adequate reconstruction of the reasoning
that led to T) will include O as a disjunct of one of
the premises (Healey 2001: 779). The Uniqueness Condition insists that
there be no available alternative explanation of O at the time
T first explains O—but clearly, theories that
explain O could be subsequently proposed and would threaten the
imputation of truth to T no less. This condition seems
arbitrarily to privilege theories depending on when they were proposed
(Sarkar 1998: 206–8; Ladyman 1999: 184). 
Another conception of novelty whose purpose is to bolster the ultimate
argument for realism is ‘functional novelty’ (Alai 2014).
A datum d is ‘functionally novel’ for theory
T if (1) d was not used essentially in constructing
T (viz., there is a heuristic path to T and related
auxiliary hypotheses that does not cite d), (2) d is
a priori improbable, and (3) d is heterogeneous with
respect to data that is used in constructing T and related
auxiliary hypotheses (i.e. d is qualitatively different from
such data). Functional novelty is a ‘gradual’ concept
insofar as a priori improbability and data heterogeneity come
in degrees. If there is more than one theory for which d is
functionally novel then the dispute between these theories cannot be
settled by the ultimate argument (Alai 2014: 306). 
Anti-realists have argued that insofar as we adopt a naturalistic
philosophy of science, the same standards should be used for assessing
philosophical theories as scientific theories. Consequently, if novel
confirmations are necessary for inferring a theory’s truth then
scientific realism should not be accepted as true, as the latter
thesis has no novel confirmations to its credit (Frost-Arnold 2010,
Mizrahi 2012). 
Another component of the realist/anti-realist debate in which appeals
to novel success figure importantly is the debate over the
‘pessimistic induction’ (or ‘pessimistic
meta-induction’). According to this argument, the history of
science is almost entirely a history of theories that were judged
empirically successful in their day only to be shown subsequently to
be entirely false. There is no reason to think that currently accepted
theories are any different in this regard (Laudan 1981b). 
In response some realists have defended ‘selective
realism’ which concedes that while the majority of theories from
the history of science have proven false, some of them have components
that were retained in subsequent theories—these tend to be the
components that were responsible for novel successes. Putative
examples of this phenomenon are the caloric theory of heat and
nineteenth century optical theories (Psillos 1999: Ch. 6), both of
which were ultimately rejected as false but which had components that
were retained in subsequent theories; these were the portions that
were responsible for their novel
 confirmations.[19]
 So in line with the ultimate argument the claim is made that novel
successes constitute a serious argument for the truth of the theory
component which generates them. However, antirealists have responded
by citing cases of theoretical claims that were subsequently
determined to be entirely false but which managed nonetheless to
generate impressive records of novel predictions. These include
certain key claims made by Johannes Kepler in his Mysterium
Cosmographicum (1596), assumptions used by Adams and Leverrier in
the prediction of the planet Neptune’s existence and location
(Lyons 2006), and Ptolemaic astronomy (Carman & Díez
2015).