Default interpretations, interpretations producing the standard
content, are defined differently depending on how
‘default’ is defined: as a default for the lexical item, a
default for the syntactic structure, a default for a particular
construction, or even a default for a particular context (where, in
addition, there is a necessary correlation with the adopted definition
of ‘context’). The delimitation of such defaults can
proceed according to different methods that, again, can affect the
results and as such further contribute to the definition of
defaults. For example, the psychological route is associated with
automatic, inference-free interpretations, while the statistical route
appeals to quantitative analyses of data, where the latter can pertain
to corpora of conversations or big databases of word co-occurrence as
used in statistical, distributional approaches in computational
semantics.
When analysed in standard truth-conditional semantics, defaults can
contribute to the truth-conditional content or affect what is implicit
– presupposed or implicated (see e.g. Potts 2015). The side on
which we find defaults in this distinction is largely dictated by the
orientation concerning the semantics/pragmatics boundary, where the
choice ranges from traditional semantic minimalism to radical versions
of contextualism. I discuss these in more detail in the following
sections. But it has to be remembered that the category is tangential
to such concepts as what is said, conversational implicature,
conventional implicature, presupposition, or, to use a more general
term, projective content (on universals in projective content see
Tonhauser et al. 2013). For example, presuppositions are
stronger than defaults: presupposition triggers such as
‘know’, ‘regret’, ‘again’ or
‘manage’ do not give the hearer much option of
interpretation, save admitting some form of metalinguistic or
quotative reading when these are negated as in (3).
(3) I didn’t forget about your birthday again; it is the first
time it happened.
What is said can rely on various types of defaults (Section 2) and
contextually salient interpretations (Section 3), but likewise it can
rely on effortful pragmatic inference from a variety of sources
available in the situation of discourse. Relevant implicatures can be
conventional (Grice 1975; Potts 2005) and conversational generalised,
the latter understood either as grammar-driven (Chierchia 2004) or,
more loosely, language-system-driven (Levinson 2000 and Section 1.3),
but implicatures can also be entirely context-dependent
(particularised). To add to this multi-dimensionality,
context-dependent implicatures can on some occasions arise
automatically, so when our definition of defaults relies on the
definitional criterion of the automaticity of the process, as
discussed above, then, by this definition, such implicatures can also
be dubbed ‘defaults’ (Giora & Givoni 2015; Jaszczolt 2016a). In
short, pursuing the standard route of analysing the types of content
will not get us far with analysing defaults. Said/implicated,
at-issue, or question-under-discussion-driven analyses (e.g. Roberts
2004) will encounter defaults on either side of the pertinent
dichotomies.
A further complication in linking defaultness with the categories of
the said or the unsaid is the fact that even weak implicatures or
presuppositions adopted through accommodation can enjoy either status.
In (4), we can accommodate globally the presupposition in (5) –
either via inference or automatically.
(4) Tom says that Ian hasn’t finished writing a novel.

(5) Ian is writing a novel.
As a result, (5) can enjoy the default status according
to some of the standard understandings of defaults as automatic, or
more frequent, more salient, or even more ‘literal’ interpretations,
or, alternatively, it can simply be an interpretation that is easier
to process – arguably, in itself a plausible criterion for
‘defaultness’.
Next, conventional implicatures, that is lexical meanings that,
according to Grice (1975), do not contribute to what is said, have,
at first glance, less to do with defaultness: they are entrenched,
non-cancellable and form-dependent (detachable), and they cannot be
calculated from maxims, principles or heuristics (Horn 1988: 123).
However, more recent inquiries into the related category of
expressives gives more scope for pursuing defaultness. Slurs are,
arguably, offensive by default but their derogatory import does not
carry across to context of banter and camaraderie. As to whether the
expressive content is an implicature or part of what is said, the
matter is still hotly discussed (see e.g. Richard 2008; Sileo 2017).
In what follows, I try to bring some order into this unwieldy term
vis-à-vis the semantics/pragmatics distinction and finish with some
reflections on its usefulness for semantics and pragmatics.
Be that as it may, default meanings come from default reasoning.
According to Kent Bach (1984), in utterance interpretation we use
‘jumping to conclusions’, or ‘default
reasoning’. In other words, speakers know when context-dependent
inference from the content of the sentence is required and when it is
not. When it is not required, they progress, unconsciously, to the
first available and unchallenged alternative. This step is cancellable
when it becomes obvious to the addressee that the resulting meaning is
not what the speaker had intended. What is important in this view is
the proposed distinction between (conscious) inference and the
unconscious act of ‘taking a step’, as Bach (1984: 40)
calls it, towards the enriched, default interpretation. Such a move to
the default meaning is not preceded by a conscious act of deliberation
as to whether this meaning was indeed intended by the speaker. Rather,
it just goes through unless it is stopped by some contextual or other
factors that render it implausible.
Bach founds his account on the Gricean theory of intentional
communication and therefore he has a ready explanation for the fact
that different meanings come with different salience. He makes an
assumption that intentions allow for different degrees of strength
(Bach 1987). He also adds that salience has a lot to do with
standardisation (Bach 1995; 1998) which consists of interpreting an
utterance according to a pattern that is established by previous usage
and as such short-circuits the process of (conscious) inference. In
short, ‘jumping to conclusions’ is performed unconsciously
and effortlessly.
For Bach, such default meanings are neither implicatures nor what is
said (or explicatures): they are implicit in what is said, or
implicitures. They are a result of ‘fleshing out’
the meaning of the sentence in order to arrive at the intended
proposition, or ‘filling in’ some conceptual gaps in the
semantic representation that, only after this filling in, becomes a
full proposition. An example of ‘fleshing out’ is given in
(6b), where the minimal proposition is expanded. ‘Filling
in’ is exemplified in (7b), where a so-called propositional
radical is completed.
(6a) Tom is too young.

(6b) Tom is too young to drive a car.

(7a) Everybody likes philosophy.

(7b) Everybody who reads the SEP likes philosophy. 
But default meanings do not exhaust the category
membership of the impliciture: implicitures can be a result of default
reasoning as well as a context-dependent process of inference.
Analogous to the distinctions discussed before, default meanings are
orthogonal to the distinction between what is said, impliciture, and
implicature: the default/inferential distinction cuts across all
three. 
Stephen Levinson (1995, 2000) argues for default interpretations that
he calls presumptive meanings and classifies as implicatures.
He uses the term borrowed from Grice, generalized conversational
implicatures (GCIs), but ascribes some properties to them that
differentiate them from Grice’s GCIs. For Levinson, GCIs are neither
properly semantic nor properly pragmatic. They should not be regarded
as part of semantics as, for example, in Discourse Representation
Theory (Kamp and Reyle 1993), nor should they be seen as a result of
context-dependent inference performed by the hearer in the process of
the recovery of the speaker’s intention. Instead, “they sit
midway, systematically influencing grammar and semantics on the one
hand and speaker-meaning on the other.” (Levinson 2000: 25).
Such presumed meanings are the result of rational, communicative
behaviour and arise through three assumed heuristics: (1) ‘What
isn’t said, isn’t’; (2) ‘What is expressed simply is
stereotypically exemplified’, and (3) ‘What’s said in an
abnormal way isn’t normal’, called Q, I, and M heuristics
(principles) respectively. Levinson’s GCIs, unlike their Gricean
progenitors, can arise at various stages in utterance processing: the
hearer need not have processed the whole proposition before arriving
at some presumed meanings. Also, unlike Grice’s GCIs that are taken to
be speaker’s intended meanings, Levinson’s presumptive meanings seem
to be hearer’s meanings, obtained by the hearer as a result
of the assumptions he or she made in the process of utterance
interpretation (see Saul 2002 and Horn 2006 for discussion). On the
other hand, like Grice’s GCIs, they are cancellable without
contradiction.
Now, when defaults are delimited by contextual salience, arguably,
cancellation may not occur except for cases of miscommunication. In
other words, when the meaning is salient in a given context, it is
likely that it had been meant by the speaker unless the speaker
misjudged the common ground. But when they are understood as
language-system-driven meanings, à la Levinson’s GCIs, cancellability
constitutes direct evidence of such defaultness. Salient components of
meaning added to the overtly expressed content (in the form of
additional information or choices of interpretation) tend to be
entrenched and as such difficult to cancel. But as Jaszczolt (2009a,
2016a) demonstrates, cancellability is a property that does not side
with implicit as opposed to explicit content but rather with salience.
If the main intended message is communicated indirectly, as in (8b),
then it is the implicature (8c) that is difficult to cancel. 
(8) (Fred and Wilma talking about Wilma’s piano recital)

(8a) Fred: Was the recital a success?

(8b) Wilma: Lots of people left before the end.

(8c) The recital was not a success.
The presence or absence of cancellation in utterance interpretation is
still a matter of dispute. It is difficult at present to decide
between the rival views (i) that a particular GCI arose and was
subsequently cancelled or (ii) that it did not arise at all due to
being blocked by the context. There is not sufficient experimental
evidence to support either stance. The answer to this question is
closely dependent on the answer to the so-called globalism-localism
dispute. If, as Levinson claims, default interpretations arise
‘locally’, out of the processing of a pre-propositional
unit such as a word or a phrase, then they have to be subjected to
frequent cancellation once the proposition has been processed. If,
however, despite the incrementality of the interpretation process they
arise post-propositionally, or ‘globally’, in accordance
with Grice’s original assumption, then utterance interpretation can
proceed without costly backtracking (see Geurts 2009; Jaszczolt 2008,
2009a, 2016a; Noveck & Sperber 2004).
Gricean pragmatics is not the only approach in which defaults are
discussed. Defaults and nonmonotonic reasoning are also well
entrenched in
 computational linguistics.
 Defaults are distinguished there with respect to various units of
meaning, from morphemes and words to multisentential units (Asher
& Lascarides 1995; Lascarides & Copestake 1998). In this
section I focus on intersentential default links and in the next I
place ‘glue logic’ in the context of some other understandings of
defaults in computational semantics. 
The tradition of defaults in nonmonotonic reasoning can be traced back
to Humboldt, Jespersen and Cassirer, and more recently to Reiter’s
(1980) default logic and his default rules of the form:
A:B

C
where C can be concluded if A has been concluded and
B can be assumed (and not B cannot be proven). Such
defaults can be built into standard logic:
But the resulting logic will become nonmonotonic because there are
default rules and default operators in the language. The literature on
the topic is vast and is best considered as a separate topic from our
current concern (see e.g., Thomason (1997) for an overview).
The best example of how default interpretations can be accounted for
in formal semantic theory is Segmented Discourse Representation Theory
(SDRT, e.g., Asher & Lascarides 2003). SDRT is an offshoot of
Discourse Representation Theory, a dynamic semantic approach to
meaning according to which meaning arises incrementally through
context change. In SDRT, defaults are regarded as highly probable
routes that an interpretation of a sentence may take in a particular
situation of discourse. There are rules of discourse, so-called
rhetorical structure rules, that produce such default
interpretations. These rules spell out the overall assumption that
discourse is coherent and that this coherence can be further
elaborated on by proposing a set of regularities. For example, two
events represented as two consecutive utterances are presumed to stand
in the relation of Narration, where the event described in
the first utterance precedes the one from the second utterance. If the
second utterance describes a state, then it stands in the relation of
Background to the first one. There are many other types of
such relations, among them Explanation and Elaboration. Axioms prevent
a relation from being of two incompatible types at the same time. The
relations between states and events are computed as strong
probabilities, in the process called defeasible reasoning. The laws of
reasoning are ‘defeasible’ in the sense that if the
antecedent of a default rule is satisfied, then its consequent is
normally, but not always, satisfied. The inference normally,
but not always, obtains: ceteris paribus, the relation
predicted by the law obtains, but in certain circumstances it may not.
It is also nonmonotonic in that the relation may disappear with the
growth of information.
SDRT includes the following components: (i) the semantics of sentences
alone, that is the underspecified output of the syntactic processing
of the sentences; (ii) the semantics of information content, that is,
further addition to these underdetermined meanings, including default
additions summarised by rhetorical structure rules; and (iii) the
semantics of information packaging that ‘glues’ such
enriched representations by means of the rules of the rhetorical
structure of discourse. This ‘gluing together’ is
defeasible, in that the rules result in the dependency
A>B, that is ‘if A, then normally
B’, where A and B stand for the
enriched propositional representations of two sentences. In other
words, they stand for the meanings of two consecutive utterances.
The main strength of this approach is that it is fully formalized and
it allows for computational modelling of discourse that takes
pragmatic links between utterances seriously and incorporates them in
the semantics. Next, it also aspires to cognitive reality and although
the cognitive reality of the particular rules can be disputed, the
view of discourse processing that they jointly produce is highly
plausible. Finally, as the authors often stress, SDRT allows them, for
most part, to model discourse without recourse to speakers’
intentions. However, a direct comparison with Gricean accounts of
defaults is precluded by the fact that we would not be comparing like
with like. In SDRT, the default interpretations are the defaults that
are formalized with respect to the actually occurring discourse: there
are rules that tell us how to take two events represented in two
consecutive sentences, there are also rules that specify the relation
between them depending on some features of their content. Gricean
defaults are, on the contrary, defaults for speakers’ overall
knowledge state: they may arise because the speaker did not
say something he or she could have said or because the
speaker assumed some cultural or social information to be shared
knowledge. For example, we cannot formalize the interpretation of (9a)
as (9b) by means of rhetorical structure rules. The interpretation of
(9a) as (9b) fits under the SDRT component (ii) rather than (iii)
above, i.e., the semantics of information content rather than
packaging.
(9a) Pablo’s painting is of a crying woman.

(9b) Picasso’s painting is of a crying woman.
Finally, it has to be mentioned that the discourse relations that for
Asher and Lascarides belong to the ‘glue logic’ can alternatively be
conceived of as part of the grammar proper: Lepore & Stone (2015),
for example, incorporate conventions into minimalistically understood,
grammar-driven semantics, and a fortiori into grammar proper;
following Lewis’s (1979) ideas on convention and ‘scorekeeping’, they
propose that “semantics describes interlocutors’ social competence in
coordinating on the conversational record” (Lepore & Stone 2015:
256). Merits of putting conventions into grammar are, however, not
easy to find (for a review see Jaszczolt 2016b). 
The computational semantics landscape contains a few landmarks in
which the concept of a default figures prominently, albeit under
different labels. I have already discussed the role of defaults and
inheritance reasoning in artificial intelligence research in the
example of SDRT. This kind of research in computational linguistics is
arguably the closest to theoretical linguistic semantics and
pragmatics in that it directly appeals to human practices in
reasoning. Pelletier & Elio (2005) refer to this characteristic as
the psychologism of nonmonotonic logics, and thus a property that was
so fiercely banished from logic by Frege as a form of a ‘corrupting
intrusion’, in that ‘being true is quite different from
being held as true’ (Frege 1893: 202). Pelletier and Elio
write:
Other landmarks include research on default feature specification in
syntactic theory and default lexical inheritance (e.g. Gazdar et
al. 1985; Boguraev & Pustejovsky 1990; Lascarides et al.
1996), where default inheritance comes from a simple idea pertaining
to all taxonomies: regular features belonging to an entity of a
certain type are inherited from the categories higher up in the
taxonomic hierarchy, that is simply by virtue of the membership of a
certain ontological type. As a result, only the non-default features
have to be attended to (on various semantic networks in computational
linguistics see also Stone 2016). To generalize, this line of research
can lead to incorporation of information into logical forms,
including, as can be seen in the example of SDRT, dynamic logical
forms of discourses. In a different camp there are statistical,
distributional approaches to meaning where meaning is derived from
information about co-occurrence of items gleaned from corpora and then
quantitatively analysed. This orientation gave rise to current
vector-based approaches (see, e.g., Jurafsky & Martin 2017 [Other
Internet Resources]; Coecke et al. 2010 and for discussion
Liang & Potts 2015). Vector semantics exploits the finding that
dates back at least to Harris (1954) and Firth (1957) that the meaning
of a word can be computed from the distribution of the words in its
immediate context.  The term ‘vector semantics’ derives
from the representation of the quantitative values in this
distribution called a ‘vector’, where the latter is
defined as a distributional model that presents information in the
form of a co-occurrence matrix. Vectors have been around since the
1950s but it is only recently that such distributional methods have
been combined with logic-based approaches to meaning (see Liang &
Potts 2015). Vectors can measure the similarity of texts with respect
to a lexical item, the similarity of lexical items with respect to
sources, or, what interests us most, the co-occurrence of selected
words in a selection of contexts (using additional methods to rule out
co-occurrence by chance). In distributional semantics therefore the
salient or default meaning is the meaning given by the observed high
co-occurrence or, in other words, delimited by the high conditional
probability of its occurrence in the context of other words.
Current compositional semantics is beginning to combine compositional
semantic theory (logic-based approaches discussed above) with
statistical models, conforming to the standard view of
compositionality on which complex meanings are a function of lexical
meanings and the mode of combination, arrived at through a recursive
process, but at the same time aiming at capturing the generalization
from (finite) past experiences that would inform machine learning.
Defaults arise in this context in several different forms: (i) as
shortcuts to standard meanings of more semantically predictable
categories, that is, closed-class words such as determiners, pronouns
or sentential connectives. (This can be extended perhaps to types of
predictable projective content such as various types of implicature or
presupposition; see Tonhauser et al. 2013); (ii) as
predictable cross-sentential discourse relations; (iii) as predictable
discourse-anaphoric links; (iv) as meaning arising from frequent
syntagmatic associations; (v) as meaning arising from frequent
conversational scenarios, to name a few salient concepts. In this new,
positively eclectic orientation in computational linguistics that
combines logical and statistical approaches, the label ‘default’ is
likely to lead to more confusion than utility in that it can pertain
to either of the two contributing orientations. On the other hand, if
the findings lead to the same set of what we can call ‘shortcuts
through possible interpretations’, the confusion may be of merely a
methodological rather than ontological importance.
Optimality-Theory pragmatics (OT pragmatics, Blutner 2000; Blutner and
Zeevat 2004; ) is another attempt at a computational modelling of
discourse but unlike SDRT it makes use of a post-Gricean,
intention-based account of discourse interpretation. The process of
interpretation is captured in a set of pragmatic constraints. The
pragmatic additions to the underdetermined output of syntax are
governed by a rationality principle called an optimization procedure
that is spelled out as a series of constraints. These constraints are
ranked as to their strength and they are defeasible, that is, they can
be violated (see Zeevat 2000, 2004). The resulting interpretation of
an utterance is the outcome of the working of such constraints. OT
pragmatics formalizes and extends the Gricean principles of
cooperative communicative behaviour as found in Horn (1984) and
Levinson (1995, 2000). For example, STRENGTH means preference for
readings that are informationally stronger, CONSISTENCY means
preference for interpretations that do not conflict with the extant
context, FAITH-INT stands for ‘faithful interpretation’,
that is interpreting the utterance without leaving out any aspect of
what the speaker says. The ordering of these constraints is FAITH-INT,
CONSISTENCY, STRENGTH. The interaction of such constraints, founded on
Levinson’s heuristics, explains how the hearer arrives at the intended
interpretation. At the same time, this model can be regarded as
producing default, presumed interpretations. With respect to finding
an antecedent for an anaphor, for example, the interaction of the
constraints explains the general tendency to look for the referent in
the immediately preceding discourse rather than in the more remote
fragments or, rather than constructing a referent ad hoc. In
other words, it explains the preference for binding over accommodation
(van der Sandt 1992, 2012).
Defaults in OT pragmatics combine the precision of a formal account
with the psychological reality of Gricean intention-based
explanations. The main difference is that they don’t seem to be
defeasible: OT pragmatics tells us how an actual interpretation arose,
rather than what the default interpretation could be. Constraints are
ranked, so to speak, post hoc: they explain what actually
happened and why, rather than what should happen according to the
rules of rational communicative behaviour. In other words, context is
incorporated even sooner into the process of utterance interpretation
than in Gricean accounts and allows for non-defeasible, albeit
standard, default, interpretations. With respect to this feature they
resemble defaults of Default Semantics discussed in Section 1.8.
In truth-conditional pragmatics (Recanati, e.g. 2004, 2010), the
meaning of an utterance consists of the output of syntactic processing
combined with the output of pragmatic processing. Pragmatic
processing, however, is not necessarily fulfilled by conscious
inference: processes that enrich the output of syntax are
sub-doxastic, direct, and automatic. The resulting representation of
utterance meaning is the only representation that has cognitive
reality and it is subject to truth-conditional analysis. On this
account, the content of an utterance is arrived at directly, similar
to the act of perception of an object. Recanati calls this view
anti-inferentialist in that “communication is as direct as
perception” (Recanati 2002: 109): the processing of the
speaker’s intentions is (at least normally) direct, automatic, and
unreflective. Such processes enriching the actually uttered content
are called primary pragmatic processes. Some of them make use of
contextual information, others are context-independent. So, they
include some cases of Grice’s GCIs as well as some particularised
implicatures (PCIs; on implied content see also Tonhauser et
al. 2013) – but only the ones which further develop the
logical form of the uttered sentence. When the pragmatic addition
constitutes a separate thought, it is, on this account, an implicature
proper, arrived at through a secondary, conscious, and reflective
pragmatic process.
There are two kinds of enrichment of the content obtained through the
syntactic processing: (i) completing of a semantically incomplete
proposition as in (10b), called saturation, and (ii) further
elaboration of the meaning of the sentence that is not guided by any
syntactic or conceptual gaps but instead is merely triggered by the
hearer’s opinion that something other than the bare meaning of the
sentence was intended, as in (11b). The latter process is called
free enrichment.
(10a) The fence isn’t strong enough.

(10b) The fence isn’t strong enough to withstand the gales.

(11a) John hasn’t eaten.

(11b) John hasn’t eaten dinner yet.
Default interpretations are here defaults for processing of an
utterance in a particular context. Automatic and unconscious
enrichment produces a default interpretation of the utterance and
“[o]nly when there is something wrong does the hearer suspend or
inhibit the automatic transition which characterizes the normal cases
of linguistic communication”. (Recanati 2002: 109). To sum up,
such defaults ensue automatically, directly, without the effort of
inference. They are cancellable, they can make use of contextual
clues, but they are not ‘processes’ in any cognitively
interesting sense of the term: they don’t involve conscious inference,
albeit, in Recanati’s terminology, they involve inference in the broad
sense: the agent is not aware of performing an inference but is aware
of the consequences of this pragmatic enrichment of the interpreted
sentence.
One of the main questions to ask about any theory of utterance
interpretation is what sources information about meaning comes from.
In Default Semantics, on the revised version of the theory (Jaszczolt,
e.g., 2009, 2010, 2016a), utterance meaning is the outcome of merging
of information that comes from five sources: (i) word meaning and
sentence structure (WS); (ii) situation of discourse (SD); (iii)
properties of human inferential system (IS); (iv) stereotypes and
presumptions about society and culture (SC); and world knowledge (WK).
WS is the output of the syntactic processing of the sentence, or its
logical form. SD stands for the broadly understood context in which
the discourse is immersed. IS pertains to properties of mental states
which trigger certain types of interpretations. For example, the
property of intentionality ensures that we normally use referring
expressions with a referential intention that is the strongest for the
given context. SC pertains to the background knowledge of societal
norms and customs and cultural heritage. WK encompasses information
about physical laws, nature, environment, etc. It is important to
stress that the four sources that accompany WS do not merely enrich
the output of the latter. All of the sources are equally powerful and
can override each other’s output. This constitutes a substantial
breakaway from the established boundary between explicit and implicit
content. 
The identification of the sources also allows us to propose a
processing model in Default Semantics in which three types of
contribution to utterance interpretation are distinguished: (i)
processing of the sentence (called combination of word meaning and
sentence structure, WS); (ii) conscious pragmatic inference (CPI) from
three of the sources distinguished above: SD, SC, and WK; and (iii)
two kinds of default, automatic meanings: cognitive defaults (CD)
triggered by the source IS, and social, cultural and world-knowledge
defaults (SCWD). 
The primary meaning is arrived at through the interaction of these
processes and therefore need not bear close resemblance to the logical
form of the sentence; the output of WS can vary in significance as
compared with the output of other types of processes. For example, to
borrow Bach’s (1994) scenario, let us imagine little Johnny cutting
his finger and crying, to which his mother reacts by uttering
(12a).
(12a) You are not going to die.
The what is said/explicature of (12a) is something to the
effect of (12b). There may also be other communicated meanings but
those fall in the domain of implicatures.
(12b) You are not going to die from this cut.
In Default Semantics, the primary content of an utterance is its most
salient meaning. This is so even when this meaning does not bear any
resemblance to the logical form derived from the syntactic structure
of the uttered sentence. In other words, CPI can override WS and
produce, say, (12c) as utterance meaning (called  primary
meaning, represented in a merger representation) for the
given context. The explicit content of the utterance need not be even
partially isomorphic with the meaning of the uttered sentence: it need
not amount to the development of the sentence’s logical form.
(12c) There is nothing to worry about.
CDs and SCWDs are default interpretations. Similar to Recanati’s
automatic free enrichment, these default meanings cut across Grice’s
GCI/PCI divide. Some of them arise due to the properties of words or
constructions used and are present by default independently of the
context of the utterance, while others are default meanings for the
particular situation of discourse. CDs are default interpretations
that are triggered by the properties of mental states. For example,
when speakers use a definite description in an utterance, they
normally use it referentially (about a particular, known,
intersubjectively recognisable individual) rather than attributively
(about whoever fits the description). This default referential use can
be given a functional as well as a cognitive explanation. Firstly, it
can be explained in terms of the strength of the referential intention
associated with the act of utterance: ceteris paribus, humans provide
the strongest information relevant and available to them. At the same
time, in cognitive terms, it can be explained through the property of
mental states that underlie the speaker’s speech act: this is the
property of intentionality or aboutness, in the
sense in which the mental state is about a particular object, be it a
person, thing, or situation. Like the strongest referring, so the
strongest aboutness, is the norm, the default. For example, the
description ‘the architect who designed St Paul’s
cathedral’ in (13a) is likely to be interpreted as
‘Christopher Wren’, as in (13b).
(13a) The architect who designed St Paul’s cathedral was a genius.

(13b) Sir Christopher Wren was a genius.
Next, SCWDs are default interpretations that arise due to the shared
cultural and social background of the interlocutors. To use a well
worn example, in (14a), it is the shared presumption that babies are
raised by their own mothers that allows the addressee to arrive at
(14b).
(14a) The baby cried and the mother picked it up.

(14b) The baby cried and the baby’s mother picked it up.
In CDs and SCWDs, no conscious inference is involved. The natural
concomitant of reducing the role of the logical form (WS) to one of
four equally potent constituents of utterance meaning is a revised
view of compositionality. The compositional nature of meaning is
retained as a methodological assumption but this compositionality is
now sought at the level of the merger of information from the five
sources, arrived at through the interaction of the four identified
processes. The output of these processes is called merger
representation and is expected to be a compositional structure.
Current research focuses of providing an algorithm for the interaction
of the output of the identified processes.
It is evident from the sample of approaches presented above that the
notion of default meaning is used slightly differently in each of
them. We can extract the following differences in the understanding of
default interpretations:
[1a] Defaults belong to competence.

vs.

[1b] Defaults belong to performance.
[2a] Defaults are context-independent.

vs.

[2b] Defaults can make use of contextual information.
[3a] Defaults are easily defeasible.

vs.

[3b] Defaults are not normally defeasible.
[4a] Defaults are a result of subdoxastic, automatic process.

vs.

[4b] Defaults can involve conscious pragmatic inference.
[5a] Defaults are developments of the logical form of the uttered
sentence.

vs.

[5b] Defaults need not enrich the logical form of the sentence but may
override it.
[6a] Defaults can all be classified as one type of pragmatic process.

vs.

[6b] Defaults come from qualitatively different sources in utterance
processing.
There is also disagreement concerning the following properties, to be
discussed below:
[7a] Defaults are always based on a complete proposition.

vs.

[7b] Defaults can be ‘local’,
‘sub-propositional’, based on a word or a phrase.
[8a] Defaults necessarily arise quicker than non-default meanings.
Hence they can be tested for experimentally by measuring the time of
processing of the utterance.

vs.

[8b] Defaults do not necessarily arise quicker than non-default
meanings because both types of meaning can be based on conscious,
effortful inference. Hence, the existence of defaults cannot be tested
experimentally by measuring the time of processing of the utterance.
These are the most standardly accepted characteristics of default
interpretations in theoretical semantics and pragmatics. We shall not
include here definitional characteristics of defaults in computational
linguistics as these are a subject for a separate study. Some of the
properties in [1]–[8] are interrelated, some of the others just tend
to occur together. Levinson’s presumptive meanings, for example, are
defeasible, i.e., fulfil [3a], local [7b], pertain to competence [1a],
and are faster to process than inferential meanings [8a]. They are
competence defaults of the type [1a] because they arise independently
of the situation of discourse and are triggered by the construction
alone, due to the presumed default scenario that it pertains to. For
example, scalar inference from ‘many’ to ‘not
all’ is a case of a competence-based, context-independent, local
default. Similarly, the rhetorical structure rules of SDRT give rise to
competence defaults. (15b) is a result of the common, shared knowledge
that pushing normally results in falling.
(15a) You pushed me and I fell.

(15b) You pushed me and as a result I fell.
On Levinson’s account, such defaults arise as soon as the relevant
word or expression is processed and as soon as the situation is clear
to the addressee. Such meanings can subsequently be cancelled if
further context witnesses against them.
As regards feature 7, it is at least conceivable that presumed
meanings arise as soon as the triggering word or construction has been
processed by the hearer. For Levinson (1995, 2000), salient meanings
have this property of arising even before the processing of the
sentence is completed. In other words, they arise
pre-propositionally or locally. Discourse
interpretation proceeds incrementally and similarly the assignment of
default meanings to the processed segments is incremental. For
example, the scalar term many in (16a) triggers the presumed
meaning not all as soon as it has been processed. The
subscript d in (16b) stands for the default meaning and is
placed immediately after the triggering construction.
(16a) Many people liked Peter Carey’s new novel.

(16b) Many (d many but not all) people liked Peter Carey’s
new novel.
Similarly, ‘paper cup’ and ‘tea cup’ give rise
to presumed meanings locally, as in (17b) and (18b) respectively.
(17a) Those paper cups are not suitable for hot drinks.

(17b) Those paper cups (d cups made of paper) are not
suitable for hot drinks.
(18a) I want three tea cups, three saucers and three spoons please.

(18b) I want three tea cups (d cups used for drinking tea),
three saucers and three spoons please.
Inferences such as those in (17b) and (18b) are very common. They are,
however, substantially different from the inference in (16b) in that
the resulting meaning is the lexical meaning of the collocation,
similar to that of a compound. Other examples include ‘pocket
knife’ vs. e.g., ‘bread knife’, and ‘coffee
spoon’ vs. e.g., ‘silver spoon’. It is worth
remembering that on Levinson’s account, presumed, salient
interpretations can be explained through the principles of rational
communicative behaviour summed up as his Q, I and M heuristics (see
Section 1.2 and Levinson 1995, 2000). (16b) arises through the
Q-heuristic, ‘What isn’t said isn’t’, while (17b) and
(18b) arise through the I-heuristic, ‘What is expressed simply
is stereotypically exemplified’. Most generally, the defaults
that arise through the Q-heuristic exploit a comparison with what was
not, but might have been, said. For example, ‘most’
triggers an inference to a denial of a stronger item
‘all’; ‘believe’ triggers an inference to
‘not know’. At the same time, they are all easily
cancellable, as (16c) illustrates.
(16c) Many, and possibly all, people liked Peter Carey’s new novel.
I-heuristic exploits only what there is in the sentence: it is an
inference to a stereotype and as such is not so easily cancellable.
For example, (19) and (20) seem rather bizarre.
(19) Those paper cups, I mean cups used for storing paper, are full.

(20) I want three tea cups, I mean cups used for storing tea leaves.
Perhaps the fact that these defaults are not so easily cancellable
comes from their property of resembling lexical compounds and, like in
the case of compounds, the link between the juxtaposed lexemes is very
strong in their case. If indeed it is plausible to treat them on a par
with compounds, then they are not very useful as a supporting argument
for local defaults: instead of defaults, we have lexical meaning of
compounds.
Local defaults allow us to dispose of the level of an underspecified
propositional representation in semantic theory. Since the inferences
proceed incrementally, then as soon as the triggering expression is
encountered, there is no level of a minimal proposition that would
constitute a foundation for further inferences. If there is one, it is
just accidental, in that the triggering item may just happen to be
placed at the end of the sentence, for example ‘tea cups’
in the first clause of (20) above. But it is also important to note
that the status of such defaults is still far from clear. For example,
Levinson’s defaults are local, but at the same time
“cancellable” to the extent that the context may prevent
them from arising. This leads to a difficulty in examples such as
(21)–(22).
(21) You are allowed five attempts to get the prize.

(22) You are allowed to do five minutes of piano practice today
because it is late.
It is clear that in (21) ‘five’ is to be understood as
‘at most five’. How are we to model the process of
utterance interpretation for this case? Are we to propose that the
inference from ‘at least five’ to ‘exactly
five’ takes place and is then cancelled? Or are we to propose
that ‘five’ is by default ‘at least five’ (or
underdetermined five, or ‘exactly five’, depending on the
orientation (see Horn 1992; Koenig 1993; Bultinck 2005) and becomes
altered in the process of pragmatic inference to ‘at most
five’ in the context of ‘allow’? But then,
‘allow’ is also present in (22) and the inference to
‘at most’ is not at all salient: doing a longer piano
practice is generally preferred but may not be what the addressee
likes doing and ‘five’ may end up, in this context, to
mean ‘as little as five’ or ‘five or more’,
stressing that more than five is not expected but allowed. In (23),
the problem is even more salient. If ‘five’ triggers
locally the ‘exactly’ meaning, then the default has to be
cancelled immediately afterwards when ‘are needed’ has
been processed and the ‘at least’ interpretation becomes
obvious.
(23) Five votes are needed to pass the proposal.
Alternatively, we can stipulate that the first inference takes place
after the word ‘needed’. It is clear that a lot needs to
be done to clarify the notion of local defaults: most importantly, (i)
what counts as the triggering unit, (ii) to what extent context is
consulted, and (iii) how common cancellation is. But it seems that if
defaults prove to be so local as to arise out of words or even
morphemes, then they are part of the computational power of grammar
and they belong to grammar and lexicon rather than to semantics and
pragmatics. Chierchia (2004) and Landman (2000) represent this view.
Chierchia argues that since scalar implicatures do not arise in
downward-entailing contexts (contexts that license inference from a
set to its subset), there is a clear syntactic constraint on their
behaviour (but see Chemla et al. 2011). Jaszczolt (2012)
calls such units that give rise to defaults or inferential
modification ‘fluid characters’, employing Kaplan’s (1989)
content-character distinction, to emphasise the fact that the unit of
meaning that leads to inference or to a default meaning varies from
context to context and from speaker to speaker: characters are ‘fluid’
because they correspond to ‘flexible inferential bases’ or ‘flexible
default bases’. But much more theorizing and substantial empirical
support are needed to establish the exact size of such local domains
and the corresponding fluid characters.
As far as feature [8] is concerned, some experimental work has been
performed to help decide between [8a] and [8b], measuring the recovery
time for the default meaning as opposed to the non-default one. The
development of the ability to use scalar inferences has also been
tested (Noveck 2001; Papafragou & Musolino 2003; Musolino 2004;
Noveck and Sperber 2004; Geurts 2010). It has been argued on the basis
of some evidence that default interpretations are not faster to
produce and can be absent altogether from processing in the case of
five-year old subjects. Noveck (2004) provides the following evidence
against Levinson’s automatic and fast defaults. Children were
presented with some descriptions of situations in which the order of
the events was inverted in narration. They had to assess whether the
description was true or false. The outcome was that the children who
agreed with the inverted description reacted faster than the ones who
disagreed. It was then concluded that enriching ‘and’ to
‘and then’ is not automatic: it takes time. And, if
pragmatically enriched responses take longer, then they cannot be the
default ones (see Noveck 2004: 314). Similarly, with scalar terms, if
one could demonstrate that the enriched readings, such as ‘some
but not all’ for ‘some’, arise faster than
‘some but not necessarily not all’, one would have strong
evidence in support of the defaults view.
The problem is that all these experiments assume Levinson’s notion of
a fast and inference-free default while this is, as we have seen, by
no means the only understanding of default interpretations, and,
arguably, not even the most commonly assumed one. The experimenters
talk of arguments for and against ‘the Default View’,
‘the Default Model’ (see also Bezuidenhout and Morris
2004, Breheny et al 2006), while, in fact there is no such unique
model to be falsified: there are very different understandings of
defaultness even in post-Gricean pragmatics alone, as is evident from
Section 1. The list of possible defining characteristics of default
interpretations in [1]–[8] shows that one cannot talk about
the default meaning. At the same time, it is much harder to
provide any experimental evidence for or against salient meaning that
draw on some contextual information, arise late in utterance
processing, and are not normally cancellable. The latter also seem
much more intuitively plausible than Levinson’s rigid defaults
in that they are simply shortcuts through costly pragmatic inference
and as such can be triggered by the situation itself rather than by
the properties of a lexical item or construction at large. They are
just normal, unmarked meanings for the context at hand and it is not
improbable that such default, salient interpretations will prove to
constitute just the polar end of a scale of degrees of inference
rather than have qualitatively different properties from non-default,
clearly inference-based interpretations. They will occupy the area
towards the ‘zero’ end of the scale of inference but will
not trigger the dichotomy ‘default vs. inferential
interpretation’. But since it is debatable whether salience
ought to be equated with defaultness in he first place, our
terminological quandary comes back with full strength (see Section
3).
It is also difficult to pinpoint the boundary between default and
non-default interpretations when we allow context and inference to
play a role in default meanings, that is when we allow [2b] and [8b].
This does not mean, however, that we should pour them out with the
bath water and resort to proposing nonce-inference in the case of
every single utterance produced in discourse. When context-dependence
of defaults is allowed, then the main criterion for such meanings is
their subdoxastic arrival. When conscious inference is allowed, then
the main criterion is the fact that only minimal contextual input is
allowed, such as, say, the co-text in (24). In (24), the definite
description ‘the first daughter’ has the attributive
rather than referential reading.
(24) The first daughter to be born to Mr and Mrs Brown will be called
Scarlett.
On a traditional Gricean view of post-propositional, sentence-based
pragmatic inference, we have here the default attributive reading: the
expression ‘to be born’ and the future auxiliary
‘will’ signal that no particular, extant, known individual
is referred to. This is also the view followed in Default Semantics
(Section 1.8) where both inference and defaults are ‘ assumed to
be global’ – ‘assumed’ in the sense of a methodological
assumption put in place until we have the means to test the actual
length of fluid characters and the content of the corresponding
default bases. In other words, information arrived at through WS
merges with that from CPI, CD and SCWD when all of the WS is
ready. But in Default Semantics there is no default involved in (24):
we have WS merging with CPI to produce the attributive reading. On
Levinson’s presumptive meanings account (Section 1.3), it can be
stipulated that (24) would fall in-between GCIs and PCIs: the only
context that is required is the sentence itself, so the example is not
different from any other cases of GCIs. But the locality of the GCI is
the problem: depending on how we construct the length of the
triggering expression, we obtain a GCI or a PCI. When we construe it
as ‘the first daughter’, the sub-part of the definite noun
phrase, then we obtain the referential reading as the default, to be
cancelled by ‘to be born’. In short, we don’t know yet, at
the current state of theorizing and experimenting, which of the
potential defining characteristics of defaults to employ. Neither are
we ready to propose the demarcation line between default and
non-default interpretations. We can conceive of the first as shortcuts
through inference but such a definition will not suffice for
delimiting a category. We can, however, concede that default
interpretations are governed by principles of rational behaviour in
communication, be it Gricean maxims, neo-Gricean principles or
heuristics, the logic of information structuring of SDRT, or a version
of defeasible logic as presented above.
All in all, it appears that the diversity of default interpretations
pertains not only to their features listed in [1]–[8] but also
to their provenance. This diversified use makes the term heavily
theory-dependent.
In pragmatic theory, the term ‘default’ is often used in association
with the term ‘salience’, so it is important to clarify the
similarities and differences between them. For Giora (e.g. 2003; Giora
& Givoni 2015), salience is independent from literality of an
interpretation and depends merely on ‘accessibility in memory due to
such factors as frequency of use or experiential familiarity’ (Giora
2003: 33). She clearly distinguishes salience from defaultness in that
for her salience concerns meanings, while defaultness concerns
interpretations:
This, however, can lead to a problem with reconciling defaultness with
salience in particular cases, making her propose degrees of
defaultness (ibid.), and to a rather counterintuitive diluting of the
concept of a default. For example, sarcasm can rely on non-salient but
default interpretation – non-salient when the interpretation is
compositionally put together instead of being processed as a
conventionalised unit.
On the other hand, for Jaszczolt (e.g. 2016a) defaultness relies
precisely on salience that leads to automatic meaning retrieval. She
calls this view Salience-Based Contextualism: the meaning of an
utterance is derived through a variety of interacting processes, some
of which rely on automatic interpretations such as cognitive defaults
or socio-cultural and world knowledge defaults identified in the
theory of Default Semantics and discussed in Section 1.8. Such
defaults are defaults for the context and for the speaker, but
salience that predicts them is not. According to Salience-Based
Contextualism, words and structures can trigger salient, automatically
retrieved meanings. This is guaranteed by the fact that language is a
socio-cultural as well as a cognitive phenomenon and as such is shaped
by its common use in discourse on the one hand, and by the structure
and operations of the brain on the other (see Jaszczolt 2016a: 50).
Salience is situation-free (although not always co-text free –
see above on fluid characters), defaultness is not: it is easy to
imagine a speaker who does not make use of the available salient
interpretation because he or she lacks the necessary sociocultural
background, knowledge of the laws of physics, or is guided by the
context towards a different interpretation. Defaultness applies to a
flexible unit on the basis of which an interpretation is formed (a
fluid character). Defaults in conversation result from emergent
intentionality (Haugh 2008, 2011): they rely on the process of
co-construction of meaning. So, conversational defaults subsume
salient meanings – literal or not, such as Giora’s salient
meanings understood as conventional, prototypical, familiar, or
frequent.
One of the important corollaries of this salience-based defaultness is
the possible redundancy of the literal/nonliteral distinction. If
words exhibit strong influences on each other in a string as well as
influences from the situation of discourse (sometimes called
‘lateral’ and ‘top-down influences’,
respectively – see e.g. Recanati 2012), then we have little
reason for postulating literal meaning. For example, in (25), we have
little reason for postulating ‘literal’ meanings of
‘city’ or ‘asleep’: either of the words may
accommodate to fit the other, and the appropriate interpretation will
follow.
(25) The city is asleep. (from Recanati 2012: 185).
If ‘city’ means ‘inhabitants of the city’,
‘asleep’ applies to it directly. If it means the place
with its infrastructure, ‘asleep’ has to adjust to mean
something to the effect of ‘quiet’, ‘dark’ or
‘showing no movement’. But neither of the processes has a
clear point of departure: there is no obvious, clearly definable move
from literal to non-literal. Such co-occurrence comes with degrees of
salience of certain meanings. Such options can also be viewed as
‘probabilistic meanings’ that are ‘contextually
affirmed’, to use Allan’s (2011: 185) terminology, through
nonmonotonic inferences either in the co-text or by some other factor
in the common ground.
However, probabilistic meanings presuppose ambiguity, while in general
the salience- and default-oriented semantics and pragmatics relies on
the assumption of underspecification. While in the case of (26)-(27)
an ambiguity account appears justified when viewed from the
perspective of the inventory of the lexicon in a language system (in
that one would expect an analogy from ‘leopard’ and
‘fox’ to ‘lamb’ and ‘goat’, while in the
context of the sentence this analogy is not present, as shown in (26a)
and (27a)), most cases of lexical adjustment cannot be traced back to
properties of entries in the lexical inventory.
(26) Jacqueline prefers leopard to fox.

(27) Harry prefers lamb to goat.
(26a) Jacqueline prefers leopard skin to fox fur.

(27a) Harry prefers eating lamb to eating goat.
(from Allan 2011: 180). Probabilistic meaning, according to Allan,
ought to be included in the lexicon: a lexeme ought to be listed in
the lexicon with different interpretations, annotated for their
probability and circumstances in which this meaning is likely to
occur. He calls such probabilistic meanings ‘grades of
salience’.
It is evident that this proposal brings us to the territory of
vector-based semantics in computational linguistics, while, on the
other hand, accounts based on intention- and context-driven adjustment
such as Recanati’s and Jaszczolt’s pull in the direction
of post-Gricean pragmatics. But as Sections 1.1-1.8 demonstrate, the
two traditions are not necessarily incompatible. Lexical salience and
radical contextualism about the lexicon point in the same direction as
distributional accounts in computational semantics: we have
probabilities of certain meanings because these are meanings derived
from the ‘company a word keeps’, to adapt Firth’s
(1957: 11) famous dictum. All in all, content words are strongly
context-dependent – to the extent that perhaps indexicality
ought to be viewed not as a defining feature of some lexical items but
as a gradable feature of the entire lexicon. But what counts as
indexicality is a separate theoretical question that cannot be pursued
here.
Next, there is one more reason why salience has to be clearly
distinguished from defaultness. Let us consider demonstratives. The
object referred to by using ‘that’ can be located with the
help of (i) the recognition of the speaker’s intention, or (ii)
the act of pointing, or even (iii) the presence of a particular
prominent object in the visual field of the interlocutors. All these
combine to delimit the concept of salience: such an object has to be
(made) salient for the linguistic demonstration to succeed. Here
salient meaning is entirely, or almost entirely (allowing for
the grammatical rendering of e.g. the proximal/distal distinction)
determined by the given context and by the speaker’s knowledge
that it is the relevant context (see Lewis 1979 on
scorekeeping; Cappelen and Dever 2016 for a discussion). What is
important for us here is that salience can be produced by the
use of a context-dependent term: objects are brought to salience by
the use of an indexical. Salience so understood is still compatible
with the situation-independent concept of salience discussed above (to
distinguish it from defaultness) in that it is the semantic meaning,
the character (Kaplan 1989) of the demonstrative that triggers the
bringing-to-salience process.
Now, on the one hand, linguistic research informs us that expressions
with thin semantic content such as anaphorically used demonstrative
pronouns or personal pronouns are employed for referents whose
cognitive status is high. In other words, they are used when the
object is in focus of attention or at least activated in memory
(Gundel et al. 1993; see Jaszczolt 2002: 140–149 for a discussion). On
the other hand, when combined with an act of demonstration, the object
can be made salient. Cappelen and Dever (2016) discuss here
two types of successful referring by demonstratives: pointing and
intending (i.e. (i) and (ii) above), contrasted with prominence (i.e.
(iii) above). The first creates salience, while the latter pertains to
extant salience; the first brings entities into focus, while the
latter exploits their in-focus cognitive status. What is of particular
interest to semanticists (of both orientations discussed here, Gricean
and computational) is that the first type allows for accommodation
(Lewis 1979): objects are made more salient when communication
requires it.
To conclude, salience clearly differs from defaultness but for
expressions (words, phrases, sentences) for which delimiting default
interpretations makes sense, salience can provide an explanans.
This comparison of various selected approaches to default
interpretations in semantics and pragmatics allows for some
generalizations. Firstly, it is evident from the surveyed literature
that, contrary to the assumptions of some experimental pragmaticists,
there is no one, unique ‘default model’ of utterance
interpretation. Instead, default (and salient) meanings are recognised
in many approaches to utterance interpretation but they are defined by
different sets of characteristic features. Next, in the present state
of theorizing, data-based analyses and experimenting, while the
rationale for default interpretations is strong, some of the
properties of such interpretations are still in need of further
investigation. For example, the discussions of locality of defaults
and their subdoxastic arrival are in need of empirical support before
they can be taken any further. In other words, a fluid character is in
need of empirical identification.
Moreover, the principle and method for delimiting default
interpretation as distinguished from inferential interpretation is
still a task for the future. The existence of a shortcut through
costly inference is an appealing and hardly controversial thesis but
the exact properties of such meanings are still subject to
disputes.
Next, the automatic arrival at context-dependent meanings has to be
discussed as part of the debate between the direct access view and the
modular view of language processing. Direct access predicts that
context is responsible for activating relevant senses to the extent
that the salience of the particular sense of a lexical item does not
play a part. According to the modular view, lexical meanings that are
not appropriate for the context are also activated, only to be
suppressed at the next stage of processing. With the rise of theories
that sit between these polar views, the question of the compatibility
of the salience in the lexicon and the default status of utterance
interpretations requires more attention. What can be attributed to the
lexicon (or, in terms of Default Semantics, what exactly is the scope
of WS) and what to the context of utterance, remains an unresolved
question.
Finally, whether we approach defaults through distributional
computational semantics, theoretical truth-conditional semantics,
post-Gricean pragmatics, or some version of a combined view, progress
in research on defaults in human reasoning will necessarily require
progress in technology. No matter how powerful our theories are, they
will have to be tested either on large corpora, or through
neuroimaging: more traditional methods of psycholinguistic experiments
or small databases will always leave a wide margin of doubt as to ‘is
it really how human reasoning works’? If there are big generalizations
to be made regarding how we jump to conclusions, these will have to be
modern equivalents of the 19th century phenomenological ideas, founded
on intentionality of mental states, informativeness of acts of
communication, all predicted by assumed efficiency (but not
necessarily cooperation), but with access to modern methods of
empirical corroboration (or falsification).