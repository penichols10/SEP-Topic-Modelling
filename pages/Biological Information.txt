Biology is concerned with living organisms—with their
structure, activities, distribution in space and time, and
participation in evolutionary and developmental histories. Many of
these organisms engage in activities that seem best understood in
terms of information processing or representation. These include
perception, cognition, signalling, and language use. We will not much
be concerned with the role of concepts of information and
representation in these cognitive contexts. For over the second half
of the 20th century, biology came to apply informational
concepts (and their relatives) far more broadly than this. For many
biologists, the most basic processes characteristic of living
organisms should now be understood in terms of the expression of
information: the response to signals, the execution of programs, and
the interpretation of codes. So although contemporary mainstream
biology is an overtly materialist field, it has come to employ
concepts that seem intentional or semantic. These come with a long
history of causing foundational problems for materialists (and, to
some extent, for everyone else).
The embrace of informational and other semantic concepts has been
especially marked within genetics, and other fields—evolutionary
theory and developmental biology—with strong ties to
genetics. But informational language is not only found there: for
example, hormones are routinely seen as long
distance signals that enable one organ system to coordinate with
others (Levy 2011). That said, the usage that has generated the most
discussion is found in the description of the relations between genes
and the various structures and processes to which genes
contribute. For many biologists, the causal role of genes should be
understood in terms of their carrying information about their
various products; and perhaps as well about the environments in which
these products enhance fitness (Lorenz 1965; Shea 2013). The
expression of that information might depend on the presence of various
environmental factors, but the same can be said of other kinds of
message. Breaking things down further, we can recognize two causal
roles for genes, and hence two potential explanatory roles for genetic
information, within biology. Genes are crucial to both explaining the
development of individual organisms, and to explaining the inheritance
of characteristics across generations. Information has been invoked in
both explanatory contexts.
One important use of informational language is relatively
uncontroversial, through being anchored in striking and well
established facts about the role of DNA and RNA in the manufacture of
protein molecules in cells, a set of facts summarized in the familiar
chart representing the “genetic code”, that maps triplets
of the DNA bases (C, A, T, G) to individual amino acids, which are the
building blocks of protein molecules. Not even this use
of “coding” language is completely uncontroversial (Sarkar
1996). More importantly, enthusiasm for the use of informational
language in biology both predates the discovery of the DNA-RNA-amino
acid mapping (Schrödinger 1944), and goes far beyond it, so this
mapping is only a partial explanation of the uptake of informational
notions in biology. Current applications of informational concepts in
biology include:
There is no consensus about the status of these ideas. Indeed, the
use of informational notions is controversial even when giving
accounts of animal communication, with some theorists denying that
such communication is the flow of information from one animal to
another (Krebs and Dawkins 1984; Owren et al. 2010). The result has
been a growing foundational discussion within biology and the
philosophy of biology. Some have hailed the employment of
informational concepts as a crucial advance (Williams 1992). Others
have seen almost every biological application of informational
concepts as a serious error, one that distorts our understanding and
contributes to lingering genetic determinism (Francis 2003). Most of
the possible options between these extreme views have also been
defended. Perhaps most commentators within philosophy have seen their
project as one of sorting through the various kinds of informational
description that have become current, distinguishing legitimate ones
from illegitimate ones (Godfrey-Smith 2000; Griffiths 2001;
Godfrey-Smith 2007; Shea 2013; Lean 2014). Philosophers have also
tried to give a reductive or naturalistic explanation for the
legitimate ones.
This entry proceeds as follows. In the next section we discuss the
most unproblematic technical use of information in biology, which
draws on Shannon and the mathematical theory of information. Against
that background, some of the more contentious uses are both motivated
and introduced. In section 3, we discuss
recent attempts to build a richer concept out of Shannon’s
technical notion. We then discuss the status of the “genetic
code” in its original sense (section 4),
and signalling systems inside and outside of genetics
(section 5). We then look at ways of rejecting
informational forms of description (section
6). The key sceptical idea is the so-called “parity
thesis”: the idea that while genes play an immensely important
role in evolution and development, so do other factors, and so it is a
mistake to think of genes and only genes as instructing or controlling
these processes. The final two sections discuss the idea that
biological development in individuals proceeds by the execution of a
program (section 7), and the use of information
in understanding the role of inheritance systems in the “major
transitions” in evolution (section 8).
It is common to begin the analysis of information in biology with
an uncontroversial but minimal notion: a causal or correlational
conception. Smoke in the air and fire in the forest are correlated
with one another. If you see a forest burning at night, you can
predict the air will be smoky. If you see a plume of smoke rising from
above the trees, you can predict that below there is a fire. The
correlation does not have to be perfect to be informative. Putting the
point generally, a signal carries information about a source, in this
sense, if we can predict the state of the source from the signal. This
sense of information is associated with Claude Shannon (1948) who
showed how the concept of information could be used
to quantify facts about contingency and correlation in a
useful way, initially for use in communication technology. For
Shannon, anything is a source of information if it has a
number of alternative states that might be realized on a particular
occasion. Any other variable contains some information about
that source, or carries information about it, if its state is correlated with the state of the source. This is a matter of degree; a signal carries more information
about a source if its state is a better predictor of the source, less
information if it is a worse predictor. 
In this sense, a signal can carry information about a source
without there being any biological system designed to produce that
signal, nor any to use it once produced. When a biologist employs this
sense of information in a description of gene action or of other
processes, he or she is just adopting a quantitative framework for
describing ordinary correlations or causal connections within those
systems. So it is one thing for there to be a signal carrying
information about a source; quite another to explain biological
processes as the result of signalling. The familiar example of tree
rings is helpful here. When a tree lays down rings, it establishes a
structure that can be used by us to make inferences about the
past. The number and size of the rings carry information, in
Shannon’s sense, about the history of the tree and its
circumstances (the technique, “dendrochronology”, is
scientifically important). Despite the usefulness of the informational
description, there is no sense in which we are explaining how
the tree does what it does in informational terms. The only reader or
user of the information in the tree rings is the human observer. The
tree itself does not exploit the information in its rings to control
its growth or flowering. Similarly, we might note ways in which the
distribution of different DNA sequences within and between biological
populations “carries information about” the historical
relationships between those populations, and the histories of the
individual populations themselves. Using this information has given
evolutionary biologists a much richer and more reliable picture of the
history of life (for a good introduction to these uses see Bromham
2008). It has been argued, for example, that the greater diversity in
mitochondrial DNA in African populations, in comparison to other human
populations, is an indicator of a comparatively recent human dispersal
from an African origin. In making these inferences, informational
concepts can be useful tools. But this is just a more complicated
version of what is going on in the case of tree rings. The appeal to
information has an inferential use that is no
way explanatory. A large proportion of the informational
descriptions found in biology have this character.
The border between these two categories, however, can be hard to
discern. Steven Frank (2012) has developed a detailed mapping between
equations describing change due to natural selection and equations
used in information theory. Change across generations can be seen as
the accumulation of information by a population about its
environment. So far, there is no claim that populations use this
information in any way, and the situation is akin to that found in
tree rings: a tree accumulates, in its rings, information about
climatic conditions in its past, though the tree does nothing with
that information. Frank may view the role of information acquired by
evolution in a more substantive way: “selection causes
populations to accumulate information about the fit of characters to
the environment” (2012: 2391). But it is doubtful that anything more
than an inferential role has yet been shown for this kind of
information.
Consequently, philosophers have sometimes set the discussion up by
saying that there is one kind of “information” appealed to
in biology, the kind originally described by Shannon, that is
unproblematic and does not require much philosophical attention. The
term “causal” information is also sometimes used to refer
to this concept, and this is essentially Grice’s “natural
meaning”, (see entry on Grice) from his
work in the philosophy of language (Grice 1957). Information in this
sense exists whenever there is contingency and correlation. So we can
say that genes contain information about the proteins they make, and
also that genes contain information about the whole-organism
phenotype. But when we say that, we are saying no more than what we
are saying when we say that there is an informational connection
between smoke and fire, or between tree rings and a tree’s
age. The more contentious question becomes whether or not biology
needs another, richer concept of information as
well. Information in this richer sense is sometimes called
“semantic” or “intentional” information.
Why might we think that biology needs to employ a richer concept?
One thought is that genes play a special, instructional role in
development, telling the embryo how to grow. It is true that genes
carry Shannon-information about phenotypes: the genome of a fertilized
egg predicts much of the resulting phenotype. In mammals, for example,
chromosome structure predicts the sex of the adult animal. But if an
informational relationship between gene and phenotype is supposed to
involve a distinctive instruction-like mode of causation, then this
cannot be information in Shannon’s sense.  For environmental
factors, not just genes, “carry information” about
phenotypes, in Shannon’s sense. With respect to Shannon
information, there is a “parity” between the roles of
environmental and genetic causes (Griffiths and Gray 1994). Moreover,
if we are using Shannon’s concept, then the informational
relationship between genotype and phenotype is symmetrical. For
example, once a reader knows that both authors of this article are
male, they can predict that we both carry a Y-chromosome.  Some talk
about information in biology is consistent with these features of
Shannon information, but some is not. In particular, it is usually
thought that at least some applications of informational language to
genes ascribe to them a property that is not ascribed to
environmental conditions, even when the environment is predictively
important.
In addition, a message that carries “semantic
information”, it is often thought, has the capacity
to mis-represent, as well as accurately represent, what it is
about. There is a capacity for error. Shannon information does not
have that feature; we cannot say that one variable “carried
false information” about another, if we are using Shannon’s
sense of the term. But biologists do apparently want to use language
of that kind when talking about genes. Genes carry a message that
is supposed to be expressed, whether or not it actually is
expressed. Thus the developmental disorders caused by thalidomide in
the 1960s were due to an inability to use genetic signals
that were supposed to control limb development. These genes
were present and active; thalidomide did not do its harm by causing
mutations.
These are the usual “marks” that are taken in the
literature to show that a richer sense of information than
Shannon’s has been introduced to biology. But the most crucial
difference between the less and more contentious applications of
informational concepts is that, in the richer cases, information use
is supposed to help explain how biological systems do what
they do—how cells work, how an egg can develop into an adult,
how genetic inheritance mechanisms make the evolution of complex
phenotypes possible.
At this point, there are a number of options on the table. One is
to deny that genes, cells and other biological structures literally
traffic in information in ways that explain their behaviour, but to
argue nevertheless that this is a useful analogy or model. The idea is
that there are useful similarities between paradigm cases of information and
representation using systems—cognitively sophisticated agents,
thinking and communicating with one another—and biological
systems. Hormones, for example, are usefully thought of as messages
because they are small, stable, energetically inexpensive items that
can travel long distances (relative to their own size) without
decaying, to specialized locations where they have
predictable, specific, and important effects on arrival. But while
this is a helpful way of thinking, perhaps we should not take talk of
messages too seriously. For example, we should not treat a question
about whether prolactin is a report of pregnancy or an instruction to
the mammary glands as if it had a literally correct answer. Arnon Levy
has developed the most sophisticated version of this view of
biological information (Levy 2011). 
A second option is to argue that genes and other biological
structures literally carry semantic information, and their
informational character explains the distinctive role of these
structures in biological processes. If we think of genes or cells as
literally carrying semantic information, our problem changes; who or
what could count as composing or reading these messages? Paradigm
cases of structures with semantic information—pictures,
sentences, programs—are built by the thought and action of
intelligent agents. So we need to show how genes and
cells—neither intelligent systems themselves nor the products of
intelligence—can carry semantic information, and how the
information they carry explains their biological role. We need some
kind of reductive explanation of semantic information (arguably, we
need this to understand cognition, too). One place we might look for
such an analysis is naturalistic philosophy of mind.
A third option is to argue that causal information itself can
explain biological phenomena, and no additional concept of information
is needed. Biological systems, on this view, can be adapted to send or
receive signals that carry causal information. Elevated prolactin, for
example, does covary with pregnancy, and that is no
accident. Likewise, the production of milk is a designed response to
the registration of these elevated levels at the mammary glands. While
it is true that lactation carries as much causal information about
prolactin levels as those levels carry about milk flow, there is a
physical asymmetry, hence directionality, between source and
receiver. (After birth, suckling stimulates local production of
prolactin, so the physical asymmetry is the signal of pregnancy from
the pituitary gland to the breasts, tuning them for lactation). Brian
Skyrms’ work has been very important in returning causal
information to centre stage, in part because his work shows that
sender-receiver signalling systems need not be cognitively
sophisticated agents (Skyrms 2010). Sending and receiving causal
information can emerge and stabilise amongst simple systems; certainly
amongst systems no more complex than a cell.
As noted above, several philosophers and biologists have argued
that much informational talk about genes uses a richer concept than
Shannon’s, but this concept can be given a naturalistic
analysis.  The aim has been to make sense of the idea that
genes semantically specify their normal products, in a sense
similar to that seen in some paradigm cases of symbolic phenomena.
If genes are seen as “carrying a message” in this
sense, the message apparently has a prescriptive or imperative
content, as opposed to a descriptive or indicative one. Their
“direction of fit” to their effects is such that if the
genes and the eventual structure produced (the phenotype) do not
match, what we have is a case of unfulfilled instructions rather than
inaccurate descriptions.  Alternatively, perhaps it is possible to
think that the genes are telling the developing phenotype the
environment it can expect. For the gene pool from which those genes
have been drawn has been filtered by selection. In arid regions of
Australia, genes which contributed to the development of leaves whose
shape and surface restricted water loss have become common. Hence
perhaps we can see these genes as telling the trees: conditions will
be dry (for this line of thought, see Shea 2011, 2013).
In making sense of these ideas, the usual way to proceed has been
to make use of a rich concept of biological function, in
which the function of an entity derives from a history of natural
selection (see Teleological Notions in
Biology).  This move is familiar from the philosophy of mind,
where similar problems arise in the explanation of the semantic
properties of mental states (Millikan 1984). When an entity has been
subject to and shaped by a history of natural selection, this can
provide the grounding for a kind of purposive or normative description
of the causal capacities of that entity. To use the standard example
(Wright 1973), the function of a heart is to pump blood, not to make
thumping sounds, because it is the former effect that has led to
hearts being favored by natural selection. The hope is that a similar
“teleofunctional” strategy might help make sense of the
semantic properties of genes, and perhaps other biological structures
with semantic properties.
The idea of a teleosemantic approach to genetic information was
developed in an early form by Sterelny et al. (1996); see also
Maclaurin (1998). The eminent biologist John Maynard Smith took a
similar approach, when he tried to make sense of his own enthusiasm
for informational concepts in biology (Maynard Smith 2000; and see
also the commentaries that follow Maynard Smith’s article). Eva
Jablonka has also defended a version of this idea (Jablonka 2002). Her
treatment is more unorthodox, as she seeks to treat environmental
signals as having semantic information, along with genes, if they are
used by the organism in an appropriate way. Nick Shea has developed
the most sophisticated teleosemantic treatment of genes to date (see
Shea 2007a,b, 2011, 2013).
One way of developing these ideas is to focus on the evolved
functions of the genetic machinery as a whole (Godfrey-Smith
1999). Carl Bergstrom and Martin Rosvall take this approach,
emphasising the adapted, impressively engineered features of
intergenerational gene flow in developing their “transmission
sense of information”. Bergstron and Rosvall point out that
intergeneration gene flow is structured so as to make possible the
transmission of arbitrary sequences (so the message is relatively
unconstrained by the medium); that information is stored compactly and
stably; that the bandwidth is large and indefinitely extendable. DNA
sequences are reliably replicable with very high fidelity, so
transmission is accurate, yet the mapping between DNA and amino acid sequence seems
optimised to reduce the impact of those errors that do occur. Many
point mutations are mapped onto the same or a chemically similar amino
acid. Bergstrom and Rosvall conclude both that DNA replication is an
exquisitely designed information channel, and that we can tell this
from the characteristics of the DNA-amino acid system itself. We do
not need to know what signals from the parental generation say to the
offspring generation, in order to know that the characteristics of DNA
replication are explained by its information-carrying capacities
(Bergstrom and Rosvall 2011).
The more usual route, and the one taken both by Sterelny et
al. (1996) and by Maynard Smith (2000) is to focus on the natural
selection of particular genetic elements. That view faces an
immediate problem, for the fact that specific genetic elements (or the
genetic system as a whole) have an evolved function is clearly not
sufficient for genes to carry semantic information.  Legs are for
walking, but they do not represent walking. Enzymes are for catalyzing
reactions, but they do not instruct this activity. There are things
that legs and enzymes are supposed to do, but this does not
make them into information-carriers, in a rich beyond-Shannon
sense. Why should it do so for genes? Sterelny, Smith and Dickison
(1996), suggested that the differences between genes and legs is that
genes have been selected to play a causal role
in developmental processes. They add, however, that any
non-genetic factors that have a similar developmental role, and have
been selected to play that role, also have semantic properties. So
Sterelny, Smith and Dickison want to ascribe very rich semantic
properties to genes, but not only to genes. Some non-genetic factors
have the same status. Even so, many quite plausible cases turn out not
to be informational on this view: prolactin, like most hormones, does
not have a specifically developmental function, so it would not count
as carrying information to the mammary glands. This is one reason why
Levy thinks of talk of information as merely metaphorical (Levy
2011). In contrast to the views of Sterelny, Smith and Dickison, in
his 2000 paper, John Maynard Smith argued that only genes carry
semantic information about phenotypes. He suggested that in contrast
to other developmental resources, the relationship between adapted
gene and phenotypic outcome is arbitrary; the gene-trait relationship
is like the word-object relationship. This idea is intriguing but hard
to make precise. One problem is that any causal relation can look
“arbitrary” if it operates via many intervening links, for
there are many possible interventions on those links which would
change the product of the causal chain. The problem of arbitrariness
is discussed further in section 4.
To distinguish between ordinary biological functions and
representational functions, Nicholas Shea draws upon the more
elaborate machinery of Ruth Millikan’s teleosemantic theory. For
Millikan, any object that has semantic properties plays a role that
involves a kind of mediation between two “cooperating
devices”, a producer and a consumer device. In the case of an
indicative signal or representation, the representation is supposed to
affect the activities of the consumer in a way that will only further
the performance of the consumer’s biological functions if some
state of affairs obtains. In the case of an imperative representation,
the representation is supposed to affect the activities of the
consumer by inducing them to bring some state of affairs about. Shea
treats genetic messages as having both indicative and imperative
content, and depending on both producers and consumers (Shea 2007b,
2011, 2013). Where Shea follows Millikan in emphasizing the roles of
both producers and consumers, Jablonka (2002) tries to achieve as much
as possible with an emphasis on consumer mechanisms alone. It seems
clear that the attention to producer and consumer mechanisms is a step
forward in this discussion, but we will see
in section 5 that it also poses problems. In
thinking about both inheritance and development, it turns out to be
unclear whether there are independently identifiable mechanisms which
count as senders and receivers, producers and consumers.
The overall picture envisaged by the teleosemantic approach has
undeniably appealing structural features. If this program succeeds, we
would have an uncontroversial sense of information, via Shannon, that
applies to all sorts of physical correlations. This picture can then
be developed by identifying a subset of cases in which these signals
have been co-opted or produced to drive biological processes. In
addition, perhaps we can appeal to rich semantic properties in cases
where we have the right kind of history of natural selection to
explain the distinctive role of genes, and perhaps other factors, in
development.  Genes and a handful of non-genetic factors would have
these properties; most environmental features that have a causal role
in biological development would not. There remain many problems of
detail, but the appeal of the overall picture provides, at least for
some, good reason to persevere with some account along these
lines.
So far we have mostly discussed the concept of information; there
has not been much talk of “coding”. With the exception of
our discussion of Bergstrom and Rosvall’s “transmission
sense” of information, the discussion so far has not emphasised
the distinctive features of the cell-level processes in which genes
figure, such as the language-like combinatorial structure of the
“genetic code”. These structural features of DNA and its
relation to amino acids are not central to some of the ideas about
information in biology, even when the focus is on development and
inheritance. As noted above, the enthusiasm for semantic
characterization of biological structures extends back before the
genetic code was discovered (see Kay 2000 for a detailed historical
treatment). But one line of thought in the literature, overlapping
with the ideas above, has focused on the special features of genetic
mechanisms, and on the idea of “genetic coding” as a
contingent feature of these mechanisms.
Both Peter Godfrey-Smith and Paul Griffiths have argued that there
is one highly restricted use of a fairly rich semantic notion within
genetics that is justified (Godfrey-Smith 2000; Griffiths 2001).  This
is the idea that genes “code for” the amino acid sequence
of protein molecules, in virtue of the peculiar features of the
“transcription and translation” mechanisms found within
cells. Genes specify amino acid sequence via a templating process that
involves a regular mapping rule between two quite different kinds of
molecules (nucleic acid bases and amino acids). This mapping rule
is combinatorial, and apparently arbitrary (in a
sense that is hard to make precise, though see Stegmann 2004 for
discussion of different versions of this idea).
Figure 1 below has the standard genetic code
summarized. The three-letter abbreviations such as “Phe”
and “Leu” are types of amino acid molecules.
Figure 1. The Standard Genetic
Code
This very narrow understanding of the informational properties of
genes is basically in accordance with the influential early proposal
of Francis Crick (1958). The argument is that these low-level
mechanistic features make gene expression into a causal process that
has significant analogies to paradigmatic symbolic phenomena.
Some have argued that this analogy becomes questionable once we
move from the genetics of simple prokaryotic organisms (bacteria and
archaea), to those in eukaryotic cells (Sarkar 1996). Mainstream
biology tends to regard the complications that arise in the case of
eukaryotes as mere details that do not compromise the basic picture we
have of how gene expression works (for an extensive discussion of
these complexities, by those who think they really matter, see
(Griffiths and Stotz 2013)). An example is the editing and
“splicing” of mRNA transcripts. The initial stage in gene
expression is the use of DNA in a template process to construct an
intermediate molecule, mRNA or “messenger RNA”, that is
then used as a template in the manufacture of a protein. The protein
is made by stringing a number of amino acid molecules together. In
eukaryotes the mRNA is often extensively modified
(“edited”) prior to its use. Moreover, much of the DNA in
a eukaryotic organism is not transcribed and translated at all. Some
of this “non-coding” DNA (note the informational language
again) is certainly functional, serving as binding sites for proteins
that bind to the DNA thus regulating the timing and rate of
transcription. The extent of wholly nonfunctional DNA remains
unclear. These facts make eukaryotic DNA a much less straightforward
predictor of the protein’s amino acid sequence than it is in
bacteria, but it can be argued that this does not much affect the
crucial features of gene expression mechanisms that motivate the
introduction of a symbolic or semantic mode of description.
So the argument in Godfrey-Smith (2000) and Griffiths (2001) is
that there is one kind of informational or semantic property that
genes and only genes have: coding for the amino acid sequences of
protein molecules. But this relation “reaches” only as far
as the amino acid sequence. It does not vindicate the idea that genes
code for whole-organism phenotypes, let alone provide a basis for the
wholesale use of informational or semantic language in biology.  Genes
can have a reliable causal role in the production of a whole-organism
phenotype, of course. But if this causal relation is to be described
in informational terms, then it is a matter of ordinary Shannon
information, which applies to environmental factors as well. That
said, it is possible to argue that the specificity of gene action, and
the existence of an array of actual and possible alternatives at a
given site on a chromosome, means that genes exert fine-grained causal
control over phenotypes, and that few other developmental resources
exert this form of causal control (Waters
2007; Maclaurin 1998;
Stegmann 2014). In contrast, Griffiths and Stotz
(2013) say that these
other factors are often a source of “Crick information”,
as they contribute to specifying the linear sequence of a gene
product.  We return to this issue in section
6.
One of the most appealing, but potentially problematic, features of
the idea that genes code for amino acid sequences concerns the alleged
“arbitrariness” of the genetic code. The notion of
arbitrariness figures in other discussions of genetic information as
well (Maynard Smith 2000; Sarkar 2003; Stegmann 2004). It is common to
say that the standard genetic code has arbitrary features, as many
other mappings between DNA base triplets and amino acids would be
biologically possible, if there were compensating changes in the
machinery by which “translation” of the genetic message is
achieved. Francis Crick suggested that the structure of the genetic
code should be seen as a “frozen accident”, one that was
initially highly contingent but is now very hard for evolution to
change (Crick 1958). But the very idea of arbitrariness, and the
hypothesis of a frozen accident, have become problematic. For one
thing, as noted in our discussion of Bergstrom and Rosvall (2011)
above, the code is not arbitrary in the sense that many others would
work as well. To the contrary, the existing code is near-optimal for
minimising error costs. Conceptually, it seems that any causal
relation can look “arbitrary” if it operates via many
intervening links. There is nothing “arbitrary” about the
mechanisms by which each molecular binding event occurs. What makes
the code seem arbitrary is the fact that the mapping between base
triplets and amino acids is mediated by a causal chain with a number
of intervening links (especially involving “transfer RNA”
molecules, and enzymes that bind amino acids to these intervening
molecules). Because we often focus on the “long-distance”
connection between DNA and protein, the causal relation appears
arbitrary. If we focused on steps in any other biological cascade that
are separated by three or four intervening links, the causal relation
would look just as “arbitrary”. So the very idea of
arbitrariness is elusive. And empirically, the standard genetic code
is turning out to have more systematic and non-accidental structure
than people had once supposed (Knight, Freeland, and Landweber
1999). The notion of
arbitrariness has also been used in discussions of the links between
genes and phenotypes in a more general sense. Kirscher and Gerhart
(2005) discuss a kind of arbitrariness that derives from the details
of protein molecules and their relation to gene regulation. Proteins
that regulate gene action tend to have distinct binding sites, which
evolution can change independently. To bind, a protein must be able to
attach to a site, but that requires congruence only with a local
feature of the protein, not sharply constraining its overall shape
(Planer 2014). This gives rise to a huge range of possible processes
of gene regulation. So there is a perennial temptation to appeal to
the idea of arbitrariness when discussing the alleged informational
nature of some biological causation.
In section 2, we noted that Brian
Skyrms’ work on signalling systems has made this framework a
very natural fit for a quite large range of biological phenomena. As
we remarked above, it is very intuitive to see hormones such as
insulin, testosterone, and growth hormone as signals, as they are
produced in one part of the body, and travel to other parts where they
interact with “receptors” in a way that modifies the
activities of various other structures. It is routine to describe
hormones as “chemical messages”. The Skyrms framework fits
these designed cause and response systems within biological agents for
three reasons. First, as noted, this framework shows that signalling
does not require intelligence or an intelligent grasp of signal
meaning.
Second, the simplest cases for models of signalling are cases in which there is common interest.
The sender and receiver are advantaged or disadvantaged by the same
outcomes. While complete common interest is atypical of
organism-to-organism communication, the cells and other structures
within an organism share a common fate (with complex exceptions). So
in this respect the base models might fit organ-to-organ communication
better than they fit between-organism phenomena.
Third, in many of these biological systems, the abstract structure
specified as a signalling system—environmental source, sender,
message, receiver, response—maps very naturally onto concrete
biological mechanisms. For example, Ron Planer has recently argued
that we should see gene expression as the operation of a signalling
system (Planer 2014). His view is quite nuanced, for the identity of
the sender, receiver, and message vary somewhat from case to case. For
example, when a protein is a transcription factor, then the gene
counts as a sender. He treats other gene-protein relations
differently. The details of his view need not concern us here. The
point is that there is machinery in the cell—genes, proteins
mRNA transcripts, ribosomes and their associated tRNA—that can
be plausibly mapped onto sender-receiver systems. There is nothing
forced about mapping the information-processing sender-receiver
structure onto the molecular machinery of the cell.
However, while this framework very naturally fits within cell and
between cell processes, it is much less clear how naturally other
suggestions mesh with this framework. For example, in the
Bergstrom-Rosvall picture of intergenerational transmission, who or
what are the senders and receivers? Perhaps in the case of multicelled
organisms, the receiver exists independently of and prior to the
message. For an egg is a complex and highly structured system, before
gene expression begins in the fertilised nucleus, and that structure
plays an important role in guiding that gene expression (Sterelny
2009; Gilbert 2013). But most organisms are single celled prokaryotes,
and when they fission, it is not obvious that there is a daughter who exists prior to and
independently of the intergenerational genetic message she is to
receive.
Likewise, Nicholas Shea’s Millikan-derived analysis does not
seem to map naturally onto independently specifiable biological
mechanisms. For him, the sender of genetic messages is natural
selection operating on and filtering the gene pool of a population;
messages are read by the developmental system of the organism as a
whole (Shea 2013). But the less clearly a sender-receiver or
producer-consumer framework maps onto independently recognised
biological mechanisms, the more plausible a fictionalist or
analogical analysis of that case becomes. So we see an important
difference between a reader being the developmental system as a whole,
and reader being a receptor on a cell membrane. The cell-level
machinery of transcription and translation (the ribosomal/tRNA
machinery, especially) really is a reader or consumer of nucleic acid
sequences, with the function of creating protein products that will
have a variety of uses elsewhere in the cell. But this realization of
the causal schematism applies only at the cell level, at the
level at which the transcription and translation apparatus shows up as
a definite part of the machinery. One of the most extraordinary
features of ontogeny is that it proceeds reliably and predictably
without any central control of the development of the organism as a
whole. There is nothing, for example, that checks whether the
left-side limbs are the same size as the right side limbs, intervening
to ensure symmetry. Similarly, there are DNA sequence-readers, and some
intercellular sender-receiver systems with clear “readers”
(such as neural structures), but there are no higher-level readers
that interpret a message specifying the structure of a whole limb-bud in the embryo.
Some biologists and philosophers have argued that the introduction
of informational and semantic concepts has had a bad effect on
biology, that it fosters various kinds of explanatory illusions and
distortions, perhaps along with ontological confusion. Here we will
survey some of the more emphatic claims of this kind, but some degree
of unease can be detected in many other discussions (see, for example,
Griesemer 2005).
The movement known as Developmental Systems Theory (DST) has often
opposed the mainstream uses of informational concepts in biology,
largely because of the idea that these concepts distort our
understanding of the causal processes in which genes are involved. For
a seminal discussion, see Oyama (1985), and also Lehrman (1970);
Griffiths and Gray (1994); Griffiths and Neumann-Held (1999). These
theorists have two connected objections to the biological use of
informational notions.  One is the idea that informational models are
preformationist. Preformationism, in its original form, in effect
reduces development to growth: within the fertilized egg there exists
a miniature form of the adult to come. Preformationism does not
explain how an organized, differentiated adult develops from a much
less organized and more homogeneous egg; it denies the phenomenon.
DST’s defenders suspect that informational models of development
do the same. In supposing, for example, that instructions for a
“language organ” are coded in the genome of a new-born
baby, you do not explain how linguistic abilities can develop in an
organism that lacks them, and you foster the illusion that no such
explanation is necessary. (See Francis 2003 for a particularly
vigorous version of the idea that the appeal to information leads to
pseudo-explanation in biology.)
Second, DST theorists have often endorsed a “parity
thesis”: genes play an indispensable role in development, but so
do other causal factors, and there is no reason to privilege
gene’s contribution to development. This claim is often
buttressed by reference to Richard Lewontin’s arguments for the
complexity and context sensitivity of developmental interaction, and
his consequent arguments that we cannot normally partition the causal
responsibility of the genetic and the environmental contributions to
specific phenotypic outcomes (Lewontin 1974, 2000). DST theorists
think that informational models of genes and gene action make it very
tempting to neglect parity, and to attribute a kind of
causal primacy to these factors, even though they are just
one of a set of essential contributors to the process in
question. Once one factor in a complex system is seen in informational
terms, the other factors tend to be treated as mere background, as
supports rather than bona fide causal actors. It becomes
natural to think that the genes direct, control, or organise
development; other factors provide essential resources. But, the
argument goes, in biological systems the causal role of genes is in
fact tightly interconnected with the roles of many other factors
(often loosely lumped together as “environmental”).
Sometimes a gene will have a reliable effect against a wide range of
environmental backgrounds; sometimes an environmental factor will have
a reliable effect against a wide range of genetic
backgrounds. Sometimes both genetic and environmental causes are
highly context-sensitive in their operation. Paul Griffiths has
emphasised this issue, arguing that the informational mode of
describing genes can foster the appearance of
context-independence:
Genes are instructions—they provide
information—whilst other causal factors are merely
material…. A gay gene is an instruction to be gay even when
[because of other factors] the person is straight. (Griffiths 2001:
395–96)
The inferential habits and associations that tend to go along with
the use of informational or semantic concepts are claimed to lead us to think of
genes as having an additional and subtle form of extra causal
specificity. These habits can have an effect even when people are
willing to overtly accept context-dependence of (most) causes in
complex biological systems. So DST theorists suggest that it is
misleading to treat genes and only genes as carrying
“messages” that are expressed in their effects. To say
this is almost inevitably to treat environmental factors as secondary
players.
The parity thesis has been the focus of considerable discussion and
response. In a helpful paper, Ulrich Stegmann shows that the parity
thesis is really a cluster of theses rather than a single thesis
(Stegmann 2012). Some ways of interpreting parity make the idea quite
uncontroversial, as no more than an insistence on the complex and
interactive character of development, or as pointing to the fact that
just as genes come in slightly different versions, with slightly
different effects (holding other factors constant), the same is true
of nongenetic factors. Epigenetic markers on genes, due to nutritional
environments, litter position and birth order, may also come in
slightly different variants, with slightly different effects. Other
versions of the claim are much more controversial.
One response to the parity thesis has been to accept the view that
genes are just one of a set of individually necessary and collectively
sufficient developmental factors, but to argue nonetheless that genes
play both a distinctive and especially important role in development
(Austin 2015; Lean 2014;
Planer 2014). As noted above, perhaps the most promising suggestion
along these lines is that genes exert a form of causal control over
development that is universal, pervasive and fine-grained. Many
features of the phenotypes of every organism exist in an array of
somewhat different versions, as a result of allelic variation in
causally relevant genes. No other developmental factor exerts control
that is similarly universal, pervasive and fine-grained (Woodward
2010; Stegmann 2014). Thus Stegmann illustrates Woodward’s idea
that genes exert specific control over phenotypes by contrasting the
effects of intervening on, say, the quantity of polymerase on cell
activity with intervening on the DNA template itself. Polymerase is
critically causally important, but varying its concentration will
modify the rate of synthesis, but not the sequences produced. That is
not true of modifications of the DNA sequence itself, so the DNA
sequence is more causally specific than polymerase.
Shea takes a different approach, arguing that different causal
factors have different evolutionary histories. Some causal factors are
simply persisting features of the environment (gravity being one
example). Others are experienced by the developing organism as a
result of histories of selection. Burrows, for example, ensure that
eggs and nestlings develop in fairly constant temperature and
humidity. But burrows are not naturally selected inheritance
mechanisms. They have not come into existence to ensure that a seabird
chick resembles its parents. In contrast, some other developmental
features are present and act in development because of histories of
selection in which the selective advantage is that these mechanisms
help ensure parent-offspring similarity. Shea argues that genes,
probably epigenetic markings on genes, and perhaps a few other
developmental resources are shaped by this form of natural
selection. So genes, and perhaps a few other developmental factors,
play a distinctive developmental role, even though many other factors
are causally necessary (Shea 2011).
In sum then, there are good reasons to be cautious about the use of
informational terminology in thinking about development. But it is
also possible to over-estimate the strength of the connection between
informational conceptions of development and the idea that genes play
a uniquely important role in development. There are ways of defending
the idea that genes play a special role while acknowledging the
interactive character of development. Moreover, an ambitious use of
informational concepts is not confined to those within mainstream
biological thinking. Eva Jablonka and Marion Lamb defend quite
heterodox views of inheritance and evolution, while basing key parts
of their work—including an advocacy of “Lamarckian”
ideas—around informational concepts (Jablonka and Lamb
2005). They suggest that one of the useful features of informational
descriptions is that they allow us to generalize across different
heredity systems, comparing their properties in a common currency. In
addition, one of the present authors has used informational concepts
to distinguish between the evolutionary role of genes from that of
other inherited factors whilst demonstrating the evolutionary
importance of non-genetic inheritance (Sterelny 2004, 2011). So in
various ways, an informational point of view may facilitate discussion
of unorthodox theoretical options, including non-genetic mechanisms of
inheritance.
Talk of genetic “programs” is common both in popular
presentations of biology, and in biology itself. Often, the idea is
just a vivid (but perhaps misleading) way of drawing attention to the
orderly, well-controlled and highly structured character of
development. In its overall results, development is astonishingly
stable and predictable, despite the extraordinary complexity of
intracellular and intercellular interactions, and despite the fact
that the physical context in which development takes place can never
be precisely controlled. So when biologists speak, for example, of
“programmed cell death”, they could just as well say that
in an important class of cases, cell death is predictable, organised,
and adaptive.
There are attempts to draw closer and more instructive parallels
between computational systems and biological development. In
particular, Roger Sansom has made a sustained and detailed attempt to
develop close and instructive parallels between biological development
and connectionist computational models (Sansom 2008b,a, 2011). This
view has the merit of recognising that there is no central control of
development; organisms develop as a result of local interactions
within and between cells. However, the most promising ideas about
program-development parallels seem to us to be ones that point to an
apparently close analogy between processes within cells, and
the low-level operation of modern computers. One crucial kind
of causal process within cells is cascades of up and down-regulation
in genetic networks. One gene will make a product that binds to and
hence down-regulates another gene, which is then prevented from making
a product that up-regulates another… and so on. What we have
here is a cascade of events that can often be described in terms of
Boolean relationships between variables. One event might only follow
from the conjunction of another two, or from a disjunction of
them. Down-regulation is a kind of negation, and there can be double
and triple negations in a network. Gene regulation networks often have
a rich enough structure of this kind for it to be useful to think of
them as engaged in a kind of computation. Computer chip
“and-gates”, neural “and-gates” and genetic
“and-gates” have genuine similarities.
While talking of signalling networks rather than programs, Brett
Calcott has shown that positional information in the developing
fruitfly embryo depends on this kind of Boolean structure, with limb
bud development depending on the cells that differentiate into limb
buds integrating one positive with two negative signals, so the buds
develop in a regular pattern on the anterior midline of the embryo.
Calcott shows that thinking of development in terms of these
signalling networks with their Boolean structures has real explanatory
value, for it enables us to explain how positional information, for
example, can easily be reused in evolution. Wing spots on fruitflies
can evolve very easily, for the networks that tell cells where they
are on the wing already exist, so the evolution of the wingspot just
need a simple mutational change that links that positional information
to pigment production (Calcott 2014). Ron Planer agrees that gene
regulation has this Boolean structure, and that we can, in effect,
represent each gene as instantiating a conditional instruction. The
“if” part of the conditional specifies the molecular
conditions which turn the gene on; the “then” part of the
conditional specifies the amino acid sequence made from the gene. As
with Calcott, Planer goes on to point out that these conditional
instructions can be and often are, linked together to build complex
networks of control. Even so, Planer argues that while these are
signalling networks, they should not be thought of as computer
programs. For example, the combinations of instructions have no intrinsic order; we
can represent each of the genes as a specific conditional instruction,
but there is nothing in the set of instructions itself that tells us
where to begin and end (Planer 2014).
Information has also become a focus of general discussion of
evolutionary processes, especially as they relate to the mechanisms of
inheritance. One strand of this discussion misconceives information
and its role in biological processes. In particular, G.C.  Williams
argues that, via reflection on the role of genes in evolution, we can
infer that there is an informational “domain” that exists
alongside the physical domain of matter and energy (Williams
1992). Richard Dawkins defends a similar view, arguing that the
long-term path of evolution is made up of gradual changes in inherited
information—as a river that “flows through time, not
space” (Dawkins 1995: 4). This is an extension of a more common
idea, that there exists such things as “informational
genes” that should be understood as distinct from the
“material genes” that are made of DNA and localized in
space and time (Haig 1997). It is a mistake to think that there are
two different things; that there is both a physical entity—a
string of bases—and an informational entity, a message. It is
true that for evolutionary (and many other) purposes genes are often
best thought of in terms of their base sequence (the sequence of C, A,
T and G), not in terms of their full set of material properties. This
way of thinking is essentially a piece of abstraction (Griesemer
2005). We rightly ignore some properties of DNA and focus on
others. But it is a mistake to treat this abstraction as an extra
entity, with mysterious relations to the physical domain.
Other ways of linking informational ideas to general issues in
evolutionary theory seem more promising. As John Maynard Smith, Eors
Szathmáry, Mark Ridley and Richard Dawkins have emphasized in
different ways, inheritance mechanisms that give rise to significant
evolutionary outcomes must satisfy some rather special conditions
(Dawkins 1995; Jablonka and Szathmáry 1995; Szathmáry and Maynard
Smith 1997; Ridley 2000). Maynard Smith and Szathmáry claim, for
example, that the inheritance system must be unlimited or
“indefinite” in its capacity to produce new combinations,
but must also maintain high fidelity of transmission. This fact about
the relationship between inheritance systems and biological structure
is often thought to reveal one of the most pressing problems in
explaining the origins of life. If reproduction depends on the
replication of a crucial set of ingredients to kick-start the new
generation (whether or not we think of those ingredients as
instructions), these ingredients must be replicated accurately. Yet
accurate replication apparently depends on complex molecular and
intracellular machinery that itself is the result of a long regime of
adaptive evolution, and hence on deep lineages of living systems. So
how could reproduction have begun? (see Ridley 2000 for a thoughtful
discussion).
So life itself, the argument goes, depends on the evolution of
mechanisms that support a high fidelity flow of information from one
generation to the next. More ambitiously still, Maynard Smith and
Szathmáry argue that many of the crucial steps in the last four
billion years of evolution—their “major transitions in
evolution”—involve the creation of new ways of
transmitting information across generations—more reliable, more
fine-grained, and more powerful ways of making possible the reliable
re-creation of form across events of biological reproduction. The
transition to a DNA-based inheritance system (probably from a system
based on RNA) is one central example. But Maynard Smith and Szathmáry
suggest that the transition from great ape forms of social life to
human social life is a major transition, in part because of the novel
forms of large scale cooperation that typify human social life, but
mostly because they see human language as a breakthrough informational
technology, revolutionising the possibilities of high fidelity
intergenerational cultural learning (MacArthur 1958; Maynard Smith and
Szathmáry 1995, 1999).
Maynard Smith and Szathmáry’s work on major transitions has been a
landmark in macroevolutionary thinking—in thinking about large
scale patterns in the history of life. But the informational dimension
of their work has not been taken up. A minor exception is Sterelny
(2009). This paper argued that multicelled animal life depended not
just on the transmission of more information with high fidelity, but
on the control of that information in development, suggesting that the
evolution of the egg—a controlled, structured, information-rich
developmental environment—was critical to complex animal
life. But most focus has been on the other strand of their work on
major transitions: on the solution of cooperation problems that then
allow previously independent agents to combine into new, more complex
agents. Thus in Calcott and Sterelny (2011), none of the papers
focused primarily on the expansion or control of intergenerational
information flow. That may change. The evolutionarily crucial features
of inheritance mechanisms are often now discussed in informational
terms, and the combinatorial structure seen in both language and DNA
provides a powerful basis for analogical reasoning.