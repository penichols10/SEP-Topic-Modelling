General relativity, Einstein's theory of space, time, and gravity,
allows for the existence of singularities. Everyone agrees on this.
When it comes to the question of how, precisely, singularities are to
be defined, however, there is widespread disagreement. Singularities
in some way signal a breakdown of the geometry of spacetime itself,
but this presents an obvious difficulty in referring to a singularity
as a “thing” that resides at some location in
spacetime: without a well-behaved geometry, there can be no location.
For this reason, some philosophers and physicists have suggested that
we should not speak of “singularities” at all, but rather
of “singular spacetimes”. In this entry, the two
formulations will generally be treated as equivalent, but the
distinction will be highlighted when it becomes significant. 
Singularities are often conceived of metaphorically as akin to a tear
in the fabric of spacetime. The most common attempts to define
singularities center on one of two core ideas that this image readily
suggests. The first is that a spacetime has a singularity if it
contains an incomplete path, one that cannot be continued
indefinitely, but draws up short, as it were, with no possibility of
extension. (“Where is the path supposed to go after it runs into
the tear? Where did it come from when it emerged from the
tear?”) The second is that a spacetime is singular just in case
there are points “missing from it”. (“Where are the
spacetime points that should be where the tear is?”) 
Another common thought, often adverted to in discussion of the two
primary notions, is that singular structure, whether in the form of
missing points or incomplete paths, must be related to pathological
behavior of some sort in the singular spacetime's curvature, that is,
the fundamental deformation of spacetime that manifests itself as
“the gravitational field”. For example, some measure of
the intensity of the curvature (“the strength of the
gravitational field”) may increase without bound as one
traverses the incomplete path. 
In recent years it was realized that there is another kind of singular
behavior that spacetimes may manifest, distinct conceptually and
physically from the idea that singularities come in the form of
incomplete curves or missing points. These are known as ‘sudden
singularities’, and are particularly important in cosmological
contexts. Besides their intrinsic interest, they also call into
question much of the standard, traditional conceptions and claims made
about singular structure in general relativity. 
Finally, there is considerable disagreement over the significance of
singularities. Many eminent physicists believe that general
relativity's prediction of singular structure signals a serious
deficiency in the theory: singularities are an indication that the
description offered by general relativity is breaking down. Others
believe that singularities represent an exciting new possibility for
physicists to explore in astrophysics and cosmology, holding out the
promise of physical phenomena differing so radically from any that we
have yet experienced as to signal, in our attempt to observe, quantify
and understand them, a profound advance in our comprehension of the
physical world. 
Each of these issues will be considered in turn below. 
The history of singular structure in general relativity is
fascinating, with debate over it dating back to the earliest days of
the theory, but discussion of it is beyond the scope of this article;
the interested reader should consult Earman (1999), Earman and
Eisenstaedt (1999), Senovilla and Garfinkle (2015), and references
therein. 
While there are competing definitions of spacetime singularities, the
most central and widely accepted criterion rests on the possibility
that some spacetimes contain incomplete, inextendible paths. Indeed,
the rival definitions (in terms of missing points or curvature
pathology), as we will see, rely on the notion of path incompleteness.
A path in spacetime is a continuous chain of events through space and
time. If I snap my fingers continually, without pause, then the
collection of snaps forms a path. The paths used in the most important
singularity theorems represent possible trajectories of particles and
observers. Such paths are known as world-lines; they consist of the
continuous sequence of events instantiated by an object's existence at
each instant of its lifetime. That the paths be incomplete and
inextendible means, roughly speaking, that, after a finite amount of
time, a particle or observer following that path would “run out
of world”, as it were—it would hurtle into the tear in the
fabric of spacetime and vanish. (See
 Figure 1.)
 Alternatively, a particle or observer could leap out of the tear to
follow such a path. While there is no logical or physical
contradiction in any of this, it appears on the face of it physically
suspect for an observer or a particle to be allowed to pop in or out
of existence right in the middle of spacetime, so to speak—if
that does not suffice for concluding that the spacetime is singular,
it is difficult to imagine what else would. At the same time as this
criterion for singularities was first proposed, the ground-breaking
work predicting the existence of such pathological paths (Penrose
1965, 1968; Hawking 1965, 1966a, 1966b, 1966c, 1966d; Geroch 1966,
1967, 1968b, 1970; Hawking and Penrose 1970) produced no consensus on
what ought to count as a necessary condition for singular structure
according to this criterion, and thus no consensus on a fixed
definition for it. 
Figure 1: a tear in spacetime 
In this context, an incomplete path in spacetime is one that is both
inextendible and of finite proper length, which means that any
particle or observer traversing the path would experience only a
finite interval of existence that in principle cannot be continued any
longer. For this criterion to do the work we want it to, however, we
will need to limit the class of spacetimes under discussion.
Specifically, we shall be concerned with spacetimes that are maximally
extended (or just ‘maximal’, for short). In effect, this
condition says that one's representation of spacetime is “as big
as it possibly can be”. There is, from the mathematical point of
view, no way to treat the spacetime as being a proper subset of a
larger, more extensive spacetime. (See
 figure 2.)
 
Figure 2: a non-maximal spacetime 
If there is an incomplete path in a spacetime, goes the thinking
behind the requirement, then perhaps the path is incomplete only
because one has not made one's model of spacetime big enough. If one
were to extend the spacetime manifold maximally, then perhaps the
previously incomplete path could be extended into the new portions of
the larger spacetime, indicating that no physical pathology underlay
the incompleteness of the path. The inadequacy would merely have
resided in the incomplete physical model we had been using to
represent spacetime. 
An example of a non-maximally extended spacetime can be easily had,
along with a sense of why they intuitively seem in some way or other
deficient. For the moment, imagine spacetime is only two-dimensional,
and flat, like an endless sheet of paper. Now, excise from somewhere
on this plane a closed set shaped like Ingrid Bergman. Any path that
had passed through one of the points in the removed set is now
incomplete. 
In this case, the maximal extension of the resulting spacetime is
obvious, and does indeed fix the problem of all such incomplete paths:
re-incorporate the previously excised set. (See
 Figure 3.)
 The seemingly artificial and contrived nature of such examples, along
with the ease of rectifying them, seems to militate in favor of
requiring spacetimes to be maximal. Also, inextendibility is sometimes
argued for on the grounds that there is no known physical process that
could cause spacetime to draw up short, as it were, and not continue
on as it could have, were it to have an extension (Clarke 1975; Ellis
and Schmidt 1977). 
Figure 3: a non-maximal spacetime made
maximal by filling its holes 
In recent important work, Manchak has questioned the need and even the
reasonableness of requiring spacetimes to be maximal (i.e.,
inextendible), pointing out problems with the condition's epistemic
status (Manchak 2011), its conceptual cogency (Manchak 2016a), and its
metaphysical character (Manchak 2016b). Because inextendibility is the
most common assumption made in the physics literature when singular
structure is discussed, however, we will continue to assume it for the
purposes of this discussion, Manchak's interesting arguments
notwithstanding. (Manchak's arguments will be discussed further in
 section 4
 below.) 
Once we have established that we are interested in maximal spacetimes,
the next issue is what sort of path incompleteness is relevant for
singularities. Here we find a good deal of controversy. Criteria of
incompleteness typically look at how some parameter naturally
associated with the path (such as its proper length) grows. One
generally also places further restrictions on the paths that one
considers—for example, one may rule out paths that could be
traversed only by particles undergoing unbounded acceleration in a
finite period of time. A spacetime, then, is said to be
singular if it possesses a path such that the specified
parameter associated with that path cannot increase without bound as
one traverses the entirety of the maximally extended path. The idea is
that the parameter at issue will serve as a marker for some manifestly
physical property, such as the time experienced by a particle or
observer, and so, if the value of that parameter remains finite along
the whole path, then we have run out of path in a finite amount of
time, as it were. We have hit an edge or a “tear” in
spacetime. 
For a path that is everywhere timelike, i.e., that does not
involve speeds at or above that of light, it is natural to take as the
parameter the proper time a particle or observer would experience
along the path, that is, the time measured along the path by a natural
clock, such as one based on the vibrational frequency of an atom.
(There are also natural choices that one can make for spacelike paths,
e.g., those that consist of points at a single
“time”, and for null paths, those followed by light
signals; however, because the spacelike and null cases add yet another
level of technical complexity, we shall not discuss them here.) The
physical interpretation of this sort of incompleteness for timelike
paths is more or less straightforward: a timelike path incomplete with
respect to proper time in the future direction would represent the
possible trajectory of a massive body that would never age beyond a
certain point in its existence. (An analogous statement can be made,
mutatis mutandis, if the path were incomplete in the past
direction.) 
We cannot, however, simply stipulate that a maximal spacetime is
singular just in case it contains paths of finite proper time that
cannot be extended. Such a criterion would imply that even the flat
spacetime described by special relativity is singular, which is surely
unacceptable. This would follow because, even in flat spacetime, there
are timelike paths with unbounded acceleration that have only a finite
proper time and are also inextendible. 
The most obvious option is to define a spacetime as singular if and
only if it contains incomplete, inextendible timelike geodesics,
i.e., paths representing the possible trajectories of
inertial observers, those in free-fall. This criterion, however, seems
too permissive, in that it would count as non-singular some spacetimes
whose geometry seems otherwise pathological. For example, Geroch
(1968c) describes a spacetime that is geodesically complete and yet
possesses an incomplete timelike path of bounded total
acceleration—that is to say, an inextendible path in spacetime
traversable by a rocket with a finite amount of fuel, along which an
observer could experience only a finite amount of proper time. Surely
the intrepid astronaut in such a rocket, who would never age beyond a
certain point, but who also would never necessarily die or cease to
exist, would have just cause to complain that something was singular
about this spacetime. 
When deciding whether a spacetime is singular, therefore, we want a
definition that is not restricted to geodesics. We need, however, some
way of overcoming the fact that non-singular spacetimes include
inextendible paths of finite proper length that are not prima
facie pathological (e.g., flat spacetimes with
inextendible paths of unbounded total acceleration). The most widely
accepted solution to this problem makes use of a slightly different,
technically complex notion of length, known as ‘generalized
affine length’ (Schmidt
 1971).[1]
 Unlike proper time, this generalized affine length depends on some
arbitrary choices. (Roughly speaking, the length will vary depending
on the coordinates one chooses to compute it; see
 note 1.)
 If the length is infinite for one such choice, however, it will be
infinite for all other choices. Thus the question of whether a path
has a finite or infinite generalized affine length is a well-defined
question, and that is all we will need. 
The definition that has won the most widespread
acceptance—leading Earman (1995, p. 36) to dub this the
semiofficial definition of singularities—is the
following: 
To say that a spacetime is singular then is to say that there is at
least one maximally extended path that has a bounded (generalized
affine) length. To put it another way, a spacetime is nonsingular when
it is complete in the sense that the only reason any given path might
not be extendible is that it's already infinitely long (in this
technical sense). 
The chief problem facing this definition of singularities is that the
physical significance of generalized affine length is opaque, and thus
it is unclear what the physical relevance of singularities, defined in
this way, might be. It does nothing, for example, to clarify the
physical status of the spacetime described by Geroch (geodesically
complete but containing incomplete paths of bounded total
acceleration), which it classifies as non-singular, as the
curve at issue indeed has infinite generalized affine length, even
though it has only a finite total proper time (to the future). The new
criterion does nothing more than sweep the troubling aspects of such
examples under the rug. It does not explain why we ought not to take
such prima facie puzzling and troubling examples as
physically pathological; it merely declares by fiat that they are not.
Recently, Manchak (2014a) proposed a condition spacetimes may satisfy,
manifestly relevant to the issue of what characterizes singular
behavior, which he calls ‘effective completeness’. The
idea is to try to give what may be thought of as a quasi-local
characterization of path
 incompleteness.[2]
 Manchak (2014a, p. 1071) describes the intended physical significance
as follows: “If a space-time fails to be effectively complete,
then there is a freely falling observer who never records some
particular watch reading but who ‘could have’ in the sense
that nothing in her vicinity precludes it.” This condition has
the pleasant property of being logically intermediate between the
condition of geodesic incompleteness for spacetime, on the one hand,
generally conceded to be too strong to capture the general idea of
singular behavior (because of examples such that of Geroch 1968c,
discussed above), and, on the other hand, the condition of being
extendible, generally conceded to be too weak, for effective
completeness is implied by geodesic completeness and in turn implies
inextendibility. While this new condition appears promising as a clear
and useful characterization of singular structure (in the sense of
path incompleteness), and does so in a way that avoids the problems of
physical opacity plaguing the semi-official definition, it is too new
and unexplored for definitive judgments to be made about it. One wants
to know, among other things, whether it can be used to prove novel
theorems with the same physical depth and reach as the standard
singularity theorems (Penrose 1965, 1968; Hawking 1965, 1966a, 1966b,
1966c, 1966d; Geroch 1966, 1967, 1968b, 1970; Hawking and Penrose
1970), and whether it can shed real light on the philosophical issues
discussed below in
 section 2.
 
So where does all this leave us? The consensus seems to be that, while
it is easy in specific examples to conclude that incomplete paths of
various sorts represent singular structure, no entirely satisfactory,
strict definition of singular structure in their terms has yet been
formulated (Joshi 2014). As we will see in
 section 1.4
 below, moreover, spacetimes can evince entirely different kinds of
behavior that manifestly are singular in an important sense, and yet
which are independent of path incompleteness. For a philosopher, the
issues offer deep and rich veins for those contemplating, among other
matters, the role of explanatory power in the determination of the
adequacy of physical theories, the role of metaphysics and intuition
in the same, questions about the nature of the existence attributable
to physical entities in spacetime and to spacetime itself, and the
status of mathematical models of physical systems in the determination
of our understanding of those systems as opposed to the mere
representation of our knowledge of them. All of these issues will be
touched upon in the following. 
We have seen that one runs into difficulties if one tries to define
singularities as “things” that have locations, and how
some of those difficulties can be avoided by defining singular
spacetimes using the idea of incomplete paths. It would be desirable
for many reasons, however, to have a characterization of a spacetime
singularity in general relativity as, in some sense or other, a
spatiotemporal “place”. If one had a precise
characterization of a singularity based on points that are missing
from spacetime, one might then be able to analyze the structure of the
spacetime “locally at the singularity”, instead of taking
troublesome, perhaps ill-defined, limits along incomplete paths. Many
discussions of singular structure in relativistic spacetimes,
therefore, are premised on the idea that a singularity represents a
point or set of points that in some sense or other is missing from the
spacetime manifold, that spacetime has a “hole” or
“tear” in it that we could fill in, or patch, by attaching
a boundary to it. 
In trying to determine whether an ordinary web of cloth has a hole in
it, for example, one would naturally rely on the fact that the web
exists in space and time. In this case one can point to a hole in the
cloth by specifying points of space at a particular moment of time not
currently occupied by any of the cloth, but which would complete the
cloth were they so occupied. When trying to conceive of a singular
spacetime, however, one does not have the luxury of imagining it
embedded in a larger space with respect to which one can say there are
points missing from it. In any event, the demand that the spacetime be
maximal rules out the possibility of embedding the spacetime manifold
in any larger spacetime manifold of any ordinary sort. It would seem,
then, that making precise the idea that a singularity is a marker of
missing points ought to involve some idea of intrinsic structural
incompleteness in the spacetime manifold rather than extrinsic
incompleteness with respect to an external structure. 
The most obvious route, especially in light of the previous
discussion, and the one most often followed, is to define a spacetime
to have points missing from it if and only if it contains incomplete,
inextendible paths, and then try to use these incomplete paths to
construct in some fashion or other new, properly situated points for
the spacetime, the addition of which will make the previously
inextendible paths extendible. These constructed points would then be
our candidate singularities. Missing points on this view would
correspond to a boundary for a singular spacetime—actual points
of a (non-standard) extended spacetime at which paths incomplete in
the original spacetime would terminate. (We will, therefore, alternate
between speaking of missing points and speaking of
boundary points, with no difference of sense intended.) The
goal then is to construct this extended space using the incomplete
paths as one's guide. 
Now, in trivial examples of spacetimes with missing points such as the
one offered before, flat spacetime with a closed set in the shape of
Ingrid Bergman excised from it, one does not need any technical
machinery to add the missing points back in. One can do it by hand.
Many spacetimes with incomplete paths, however, do not allow missing
points to be attached in any obvious way by hand, as that example
does. For this program to be viable, which is to say, in order to give
substance to the idea that there really are points that in some sense
ought to have been included in the spacetime in the first place, we
require a physically natural completion procedure that can be applied
to incomplete paths in arbitrary spacetimes. There are several
proposals for such a construction (Hawking 1966c, Geroch 1968a,
Schmidt
 1971).[3]
Several problems with this kind of program make themselves felt
immediately. Consider, for example, a spacetime representing the final
state of the complete gravitational collapse of a spherically
symmetric body resulting in a black hole. (See
 section 3
 below for a description of black holes in general, and
 Figure 4
 for a representation of a body collapsing to form a black hole.) In
this spacetime, any timelike path entering the black hole will
necessarily be extendible for only a finite amount of proper
time—it then “runs into the singularity” at the
center of the black hole. In its usual presentation, however, there
are no obvious points missing from the spacetime at all. By any
standard measure, as a manifold in its own right it is as complete as
the Cartesian plane, excepting only the existence of incomplete
curves, no class of which indicates by itself a place in the manifold
at which to add a point so as to make the paths in the class complete.
Likewise, in our own spacetime every inextendible, past-directed
timelike path is incomplete (and our spacetime is singular): they all
run into the Big Bang. Insofar as there is no moment of time at which
the Big Bang occurred (no moment of time at which time began, so to
speak), there is no point to serve as the past endpoint of such a
path. We can speak of the cosmic epoch, the time after the big bang.
That makes it easy to imagine that cosmic time zero is some initial
event. That, however, is an illusion of our labeling. Cosmic time
“zero” is a label attached to no event. If instead we had
labeled epochs with the logarithm of cosmic time, then the imaginary
moment of the big bang would be assigned the label of minus infinity
and its fictional character would be easier to accept. (One can make
the point a little more precise: the global structure of our universe,
as modeled by our best cosmological theories, is essentially the same
as a well known mathematical space, either ℝ4 or
𝕊3 x ℝ, which are both complete and
inextendible as manifolds independent of any spacetime metrical
structure, in every reasonable sense of those terms.) 
Even more troublesome examples are given by topologically compact
regions of spacetimes containing incomplete, inextendible paths, as in
a simple example due to Misner (1967). In a sense that can be made
precise, compact sets, from a topological point of view,
“contain every point they could possibly be expected to
contain”, one manifestation of which is that a compact manifold
cannot be embedded as an open submanifold of any other manifold, a
necessary pre-requisite for attaching a boundary to a singular
spacetime. It is not only with regard to the attachment of a boundary,
however, that compact sets already contain all points they possibly
could: every sequence of points in a compact set has a subsequence
that converges to a point in the set. Non-convergence of sequences is
the standard way that one probes geometrical spaces for
“missing” points that one can add in by hand, as it were,
to complete the space; thus, compact sets, in this natural sense,
cannot have any missing points. 
Perhaps the most serious problem facing all the proposals for
attaching boundary points to singular spacetimes, however, is that the
boundaries necessarily end up having physically pathological
properties (Geroch et al. 1982): in a sense one can make
precise, the boundary points end up being arbitrarily
“near” to every point in the interior of the spacetime.
Attaching boundary points to our own universe, therefore, to make the
Big Bang into a real “place”, ends up making the Big Bang
arbitrarily close to every neuron in my brain. Far from making
tractable the idea of localizing singular structure in a physically
fruitful way, then, all the proposals only seem to end up making the
problems worse. 
The reaction to the problems faced by these boundary constructions is
varied, to say the least, ranging from blithe acceptance of the
pathology (Clarke 1993), to the attitude that there is no satisfying
boundary construction currently available while leaving open the
possibility of better ones in the future (Wald 1984), to not even
mentioning the possibility of boundary constructions when discussing
singular structure (Joshi 1993, 2007b, 2014), to rejection of the need
for such constructions at all (Geroch et al. 1982; Curiel
1999). 
Nonetheless, many eminent physicists seem convinced that general
relativity stands in need of such a construction, and have exerted
extraordinary efforts in trying to devise one. This fact raises
several philosophical problems. Though physicists sometimes offer as
strong motivation the possibility of gaining the ability to analyze
singular phenomena locally in a mathematically well-defined manner,
they more often speak in terms that strongly suggest they suffer a
metaphysical itch that can be scratched only by the sharp point of a
localizable, spatiotemporal entity serving as the locus of their
theorizing. Even were such a construction forthcoming, however, what
sort of physical and theoretical status could accrue to these missing
points? They would not be idealizations of a physical system in any
ordinary sense of the term, since they would not represent a
simplified model of a system formed by ignoring various of its
physical features, as, for example, one may idealize the modeling of a
fluid by ignoring its viscosity. Neither would they seem necessarily
to be only convenient mathematical fictions, as, for example, are the
physically impossible dynamical evolutions of a system one integrates
over in the variational derivation of the Euler-Lagrange equations. To
the contrary, as we have remarked, many physicists and philosophers
seem eager to find such a construction for the purpose of bestowing
substantive and clear ontic status on singular structure. What sorts
of theoretical entities, then, could they be, and how could they serve
in physical theory? 
While the point of this project may seem at bottom identical to the
path-incompleteness account discussed in
 section 1.1,
 insofar as singular structure will be defined by the presence of
incomplete, inextendible paths, there is a crucial conceptual and
logical difference between the two. Here, the existence of the
incomplete path does not constitute the singular structure, but rather
serves only as a marker for the presence of singular structure in the
sense of missing points: the incomplete path is incomplete because it
“runs into a hole” in the spacetime that, were it filled,
would allow the path to be continued; this hole is the singular
structure, and the points constructed to fill it constitute its locus.
Indeed, every known boundary construction relies on the existence of
incomplete paths to “probe” the spacetime, as it were,
looking for “places” where boundary points should be
appended to the spacetime; the characterization of singular structure
by incomplete paths seems, therefore, logically, perhaps even
conceptually, prior to that by boundary points, at least, again, for
all known constructions of boundary points. 
Currently, there seems to be even less consensus on how (and whether)
one should define singular structure based on the idea of missing
points than there is regarding definitions based on path
incompleteness. Moreover, this project also faces even more technical
and philosophical problems. For these reasons, path incompleteness is
generally considered the default definition of singularities. For the
remainder of this article, therefore, singular structure will be
assumed to be characterized by incomplete, inextendible paths, with
the exception of the discussion of
 section 1.4
 below. 
There is, however, one special case in which it seems a boundary can
be placed on singular spacetimes in such a way as to localize the
singularity in a physically meaningful way: for so-called conformal
singularities. Their properties are discussed at the end of
 section 1.3,
 and their physical and philosophical significance explored in more
detail in
 section 7.
 
While path incompleteness seems to capture an important aspect of the
intuitive picture of singular structure, it completely ignores another
seemingly integral aspect of it: curvature pathology. If there are
incomplete paths in a spacetime, it seems that there should be a
reason that the path cannot go further. The most obvious candidate
explanation of this sort is that something going wrong with the
dynamical structure of the geometry of spacetime, which is to say,
with the curvature of the spacetime. This suggestion is bolstered by
the fact that local measures of curvature do in fact blow up as one
approaches the singularity of a standard black hole or the Big Bang
singularity. There is, however, one problem with this line of thought:
no species of curvature pathology we know how to define is either
necessary or sufficient for the existence of incomplete paths. (For a
discussion of foundational problems attendant on attempts to define
singularities based on curvature pathology, see Curiel 1999; for a
recent survey of technical issues, see Joshi 2014.) 
To make the notion of curvature pathology more precise, we will use
the manifestly physical idea of tidal force. Tidal force is generated
by the difference in intensity of the gravitational field at
neighboring points of spacetime. For example, when you stand, your
head is farther from the center of the Earth than your feet, so it
feels a (practically negligible) smaller pull downward than your feet.

 Tidal forces are a physical manifestation of spacetime curvature, and
one gets direct observational access to curvature by measuring the
resultant relative difference in accelerations of neighboring test
bodies. For our purposes, it is important that in regions of extreme
curvature tidal forces can grow without bound. 
It is perhaps surprising that the state of motion of an object as it
traverses an incomplete path (e.g., whether it is
accelerating or spinning) can be decisive in determining its physical
response to curvature pathology. Whether an object is spinning or not,
for example, or accelerating slightly in the direction of motion, may
determine whether the object gets crushed to zero volume along such a
path or whether it survives (roughly) intact all the way along it, as
shown by examples offered by Ellis and Schmidt (1977). Indeed, the
effect of the observer's state of motion on his or her experience of
tidal forces can be even more pronounced than this. There are examples
of spacetimes in which an observer cruising along a certain kind of
path would experience unbounded tidal forces and so be torn apart,
while another observer, in a certain technical sense approaching the
same limiting point as the first observer, accelerating and
decelerating in just the proper way, would experience a perfectly
well-behaved tidal force, though she would approach as near as she
likes to the other fellow who is in the midst of being ripped to
 shreds.[4]
Things can get stranger still. There are examples of incomplete
geodesics contained entirely within a well-defined, bounded region of
a spacetime, each having as its limiting point an honest-to-goodness
point of spacetime, such that an observer freely falling along such a
path would be torn apart by unbounded tidal forces; it can easily be
arranged in such cases, however, that a separate observer, who
actually travels through the limiting point, will experience perfectly
well-behaved tidal
 forces.[5]
 Here we have an example of an observer being ripped apart by
unbounded tidal forces right in the middle of spacetime, as it were,
while other observers cruising peacefully by could reach out to touch
him or her in solace during the final throes of agony. This example
also provides a nice illustration of the inevitable difficulties
attendant on attempts to localize singular structure in the senses
discussed in
 section 1.2.
 
It would seem, then, that curvature pathology as characterized based
on the behavior of tidal forces is not in any physical sense a
well-defined property of a region of spacetime simpliciter.
When we consider the physical manifestations of the curvature of
spacetime, the motion of the device that we use to probe a region (as
well as the nature of the device) becomes crucially important for the
question of whether pathological behavior manifests itself. This fact
raises questions about the nature of quantitative measures of
properties of entities in general relativity, and what ought to count
as observable, in the sense of reflecting the underlying physical
structure of spacetime. Because apparently pathological phenomena may
occur or not depending on the types of measurements one is performing,
it seems that purely geometrical pathology does not necessarily
reflect anything about the state of spacetime itself, or at least not
in any localizable way. What then does it reflect, if anything? Much
work remains to be done by both physicists and by philosophers in this
area, i.e., the determination of the nature of physical
quantities in general relativity and what ought to count as an
observable with intrinsic physical significance. See Bertotti (1962),
Bergmann (1977), Rovelli (1991, 2001 in
 Other Internet Resources, henceforth OIR,
 2002), Curiel (1999) and Manchak (2009a) for discussion of many
different topics in this area, approached from several different
perspectives. 
There is, however, one form of curvature pathology associated with a
particular form of an apparently important class of singularities that
recently has been clearly characterized and analyzed, that associated
with so-called conformal singularities, also sometimes called
isotropic singularities (Goode and Wainright 1985; Newman 1993a,
1993b; Tod 2002). The curvature pathology of this class of
singularities can be precisely pinpointed: it occurs solely in the
conformal part of the curvature; thus, what is singular in one
spacetime will not necessarily be so in a conformally equivalent
 spacetime.[6]
 This property allows for a boundary to be attached to the singular
spacetime in a way that seems to be physically meaningful (Newman
1993a, 1993b; Tod 2002). Many physicists hold that, in a sense that
can be made precise, all “purely gravitational degrees of
freedom” in general relativity are encoded in the conformal
structure (Penrose 1979; Gomes et al. 2011). These
properties, along with the fact that the Big Bang singularity almost
certainly seems to be of this form, make conformal singularities
particularly important for the understanding and investigation of many
issues of physical and philosophical interest in contemporary
cosmology, as discussed below in
 section 7.
 
In 2004, it was discovered that general relativity admits even more
kinds of singularities than those known before, so-called
‘sudden singularities’ (Barrow 2004a, 2004b). The
characterization of this kind of singularity has, so far, been
confined to the context of cosmological models, including essentially
all spacetimes whose matter content consists of homogeneous perfect
fluids and a very wide class of spacetimes consisting of inhomogeneous
fluids. The dynamics of those cosmological models is largely governed
by the behavior of the cosmological expansion factor, a measure of the
relative sizes of local regions of space (not spacetime) at different
cosmological times. In an expanding spacetime, such as the one we
believe ourselves to live in, the expansion factor continually
increases, having “started from zero at the Big Bang”. If
the universe's expansion stops, and the net gravitational effect on
cosmological scales results in the universe's collapsing in on itself,
this would be marked by a continual decrease in the expansion factor,
eventuating in a Big Crunch singularity as the expansion factor
asymptotically approached zero. The remaining dynamics of these
cosmological models is encoded in the behavior of the Hubble
parameter, a natural measure of the rate of change of the expansion
factor. A sudden singularity, then, is defined by the divergence of a
time derivative of the expansion factor or the Hubble parameter,
though the factor or parameter itself remains finite. 
Because important physical quantities, such as spatial pressure of the
cosmological fluid, are proportional to such time derivatives, the
physical interpretation of sudden singularities is often, in at least
one sense, perspicuous: depending on the time derivative that
diverges, a sudden singularity can mark the divergence of a physically
important quantity such as pressure, within a finite interval of
proper time (Cattoën and Visser 2005; Cotsakis and Klaoudatou
2005; Fernández-Jambrina 2014; Jiménez et
al. 2016). In such cases, it may happen that the mass-density
of the fluid itself, the expansion factor and its first derivative,
and even the Hubble parameter and its first derivative, all remain
finite: only the pressure (and so the second derivative of the
expansion factor) diverges. Because the physical significance of
quantities such as pressure is thought to be unambiguous, this feature
of sudden singularities stands in marked contrast to the problems of
physical interpretation that plague the standard type of singularity,
discussed in
 section 1.3.
 
Of most interest, however, is the way that sudden singularities may
differ in an even more fundamental way from standard singularities:
there need be no path-incompleteness associated with them
(Fernández-Jambrina and Lazkoz 2004, 2007). In effect, although
the values of some physically important quantities diverge, the metric
itself remains well defined, allowing curves “running
into” the pathological point to continue through it. Indeed,
point particles passing through the sudden singularity would not even
notice the pathology, as only tidal forces may diverge (and not even
all sudden singularities involve divergence of those): point
particles, having no extension, cannot experience tidal force. If one
wants to count sudden singularities as true singularities—and
there seems every reason to do so—then this would put the nail
in the coffin for the idea that singularities always can or should be
associated with “missing
 points”.[7]
Although the discovery of sudden singularities has reinvigorated the
study of singular spacetimes in the physics community (Cotsakis 2007),
they remain so far almost entirely unexamined by the philosophy
community. Nonetheless, they raise questions of manifest philosophical
interest and import. The fact that they are such radically different
structures from all other previously known kinds of singularity, for
example, raises methodological questions about how to understand the
meaning of terms in physical theories when those terms refer to
structurally quite different but obviously still intimately related
phenomena—the reasons for thinking of them as singularities are
compelling, even though they violate essentially every standard
condition known for characterizing singularities. 
Another unusual kind of singularity characterized only recently
characterized deserves mention here, because of its possible
importance in cosmology. The physical processes that seem to eventuate
in most known kinds of singular structure involve the unlimited
clumping together of matter, as in collapse singularities associated
with black holes, and the Big Bang and Big Crunch singularities of
standard cosmological models. A big rip, contrarily, occurs
when the expansion of matter increasingly accelerates without bound in
a finite amount of proper time (Caldwell 2002; Caldwell et
al. 2003; Chimento and Lazkov 2004; Dabrowski 2006;
Fernández-Jambrina 2014). Rather than the volume of spacetime
shrinking to zero, its volume increases without bound—spacetime
literally tears itself apart, not even fundamental particles being
able to maintain their structural unity and integrity (Chimento and
Lazkoz 2004; Fernández-Jambrina 2014). Again, standard concepts
and arguments about singularities characterized as incomplete paths do
not seem easily applicable here. Although big rips do have incomplete
paths associated with them as well as curvature pathology, they are of
such radically different kinds as to prima facie warrant
separate analysis. 
Recent work, codified by Harada et al. (2018), shows just how
different such cosmological singularities can be. For homogeneous
cosmological models filled with perfect fluids with a linear equation
of state—the standard cosmological model—certain values of
the barotropic index yield past, future, or past and future big rips
that are such that every timelike geodesic runs into them, but every
null geodesic avoids them. (See
 note 7
 for an explanation of the barotropic index.) In other words, any body
traveling more slowly than light will run into the singularity, but
every light ray will escape to infinity. This is not a situation that
lends itself to easy and perspicuous physical interpretation. 
When considering the implications of spacetime singularities, it is
important to note that we have good reasons to believe that the
spacetime of our universe is singular. In the late 1960s, Penrose,
Geroch, and Hawking proved several singularity theorems, using path
incompleteness as a criterion (Penrose 1965, 1968; Hawking 1965,
1966b, 1966c, 1966d; Geroch 1966, 1967, 1968b, 1970; Hawking and
Penrose 1970). These theorems showed that if certain physically
reasonable premises were satisfied, then in certain circumstances
singularities could not be avoided. Notable among these conditions is
the positive energy condition, which captures the idea that energy is
never negative. These theorems indicate that our universe began with
an initial singularity, the Big Bang, approximately 14 billion years
ago. They also indicate that in certain circumstances (discussed
below) collapsing matter will form a black hole with a central
singularity. According to our best current cosmological theories,
moreover, two of the likeliest scenarios for the end of the universe
is either a global collapse of everything into a Big Crunch
singularity, or the complete and utter diremption of everything, down
to the smallest fundamental particles, in a Big Rip singularity. (See
Joshi 2014 for a recent survey of singularities in general, and Berger
2014 for a recent survey of the different kinds of singularities that
can occur in cosmological models.) 
Should these results lead us to believe that singularities are real?
Many physicists and philosophers resist this conclusion. Some argue
that singularities are too repugnant to be real. Others argue that the
singular behavior at the center of black holes and at the beginning
(and possibly the end) of time indicates the limit of the domain of
applicability of general relativity. Some are inclined to take general
relativity at its word, however, and simply accept its prediction of
singularities as a surprising but perfectly consistent account of the
possible features of the geometry of our world. (See Curiel 1999 and
Earman 1995, 1996 for discussion and comparison of these opposing
points of view.) In this section, we review these and related problems
and the possible responses to them. 
Let us summarize the results of
 section 1:
 there is no commonly accepted, strict definition of singularity;
there is no physically reasonable characterization of missing points;
there is no necessary connection between singular structure, at least
as characterized by the presence of incomplete paths, and the presence
of curvature pathology; and there is no necessary connection between
other kinds of physical pathology (such as divergence of pressure) and
path incompleteness. 
What conclusions should be drawn from this state of affairs? There
seem to be two basic kinds of response, illustrated by the views of of
Clarke (1993) and Earman (1995) on the one hand, and those of Geroch
et al. (1982) and Curiel (1999) on the other. The former
holds that the mettle of physics and philosophy demands that we find a
precise, rigorous and univocal definition of singularity. On this
view, the host of philosophical and physical questions surrounding
general relativity's prediction of singular structure would best be
addressed with such a definition in hand, so as better to frame and
answer these questions with precision, and thus perhaps find other,
even better questions to pose and attempt to answer. The latter view
is perhaps best summarized by a remark of Geroch et al.
(1982): “The purpose of a construction [of ‘singular
points’], after all, is merely to clarify the discussion of
various physical issues involving singular space-times: general
relativity as it stands is fully viable with no precise notion of
‘singular points’.” On this view, the specific
physics under investigation in any particular situation should dictate
which definition of singularity to use in that situation if, indeed,
any at all. 
In sum, the question becomes the following: is there a need for a
single, blanket definition of singularity or does the urge for one
betray only an old Aristotelian, essentialist prejudice? This question
has obvious connections to the broader question of natural kinds in
science. One sees debates similar to those canvassed above when one
tries to find, for example, a strict definition of biological species.
Clearly, part of the motivation for searching for a single
exceptionless definition is the impression that there is some real
feature of the world (or at least of our spacetime models) that we can
hope to capture precisely. Further, we might hope that our attempts to
find a rigorous and exceptionless definition will help us to better
understand the feature itself. Nonetheless, it is not clear why we
should not be happy with a variety of types of singular structure,
taking the permissive attitude that none should be considered the
“right” definition of singularities, but each has its
appropriate use in context. 
Even without an accepted, strict definition of singularity for
relativistic spacetimes, the question can be posed: what would it mean
to ascribe existence to singular structure under any of the available
open possibilities? It is not far-fetched to think that answers to
this question may bear on the larger question of the existence of
spacetime points in general (Curiel 1999, 2016; Lam 2007). (See the
entries
 The Hole Argument
 and
 Absolute and Relational Theories of Space and Motion
 for discussions of the question of the existence of spacetime
itself.) 
It would be difficult to argue that an incomplete path in a maximal
relativistic spacetime does not exist in at least some sense of the
term. It is not hard to convince oneself, however, that the
incompleteness of the path does not exist at any particular
point of the spacetime in the same way, say, as this glass of beer
exists at this point of spacetime. If there were a point on the
manifold where the incompleteness of the path could be localized,
surely that would be the point at which the incomplete path
terminated. But if there were such a point, then the path could be
extended by having it pass through that point. It is perhaps this fact
that lies behind much of the urgency surrounding the attempt to define
singular structure as missing points. 
The demand that singular structure be localized at a particular place
bespeaks an old Aristotelian substantivalism that invokes the maxim,
“To exist is to exist in space and time” (Earman 1995, p.
28). Aristotelian substantivalism here refers to the idea
contained in Aristotle's contention that everything that exists is a
substance and that all substances can be qualified by the Aristotelian
categories, two of which are location in time and location in space.
Such a criterion, however, may be inappropriate for features and
properties of spacetime itself. Indeed, one need not consider anything
so outré as incomplete, inextendible paths in order to
produce examples of entities that seem undeniably to exist in some
sense of the term or other, and yet which cannot have any even vaguely
determined location in time and space predicated of them. Several
essential features of a relativistic spacetime, singular or not,
cannot be localized in the way that an Aristotelian substantivalist
would demand. For example, the Euclidean (or non-Euclidean) nature of
a space is not something with a precise location. (See Butterfield
2006 for discussion of these issues.) Likewise, various spacetime
geometrical structures (such as the metric, the affine structure, the
topology, etc.) cannot be localized in the way that the Aristotelian
would demand, whether that demand be for localization at a point,
localization in a precisely determinate region, or even just
localization in a vaguely demarcated region. The existential status of
such entities vis-à-vis more traditionally considered
objects is an open and largely ignored issue (Curiel 1999, 2016;
Butterfield 2006). Because of the way the issue of singular structure
in relativistic spacetimes ramifies into almost every major open
question in relativistic physics today, both physical and
philosophical, it provides a peculiarly rich and attractive focus for
these sorts of questions. 
An interesting point of comparison, in this regard, would be the
nature of singularities in other theories of gravity besides general
relativity. Weatherall's (2014) characterization of singularities in
geometrized Newtonian gravitational theory, therefore, and his proof
that the theory accommodates their prediction, may serve as a possible
testing ground for ideas and arguments on these issues. 
Many of these questions, in the end, turn upon the issue of what
constitutes “physically reasonable” spacetime structure.
General relativity admits spacetimes exhibiting a vast and variegated
menagerie of structures and behaviors, even over and above
singularities, that most physicists and philosophers would consider,
in some sense or other, not reasonable possibilities for physical
manifestation in the actual world. But what is to count as
“reasonable” here: who is to decide, and on what basis
(Curiel 1999)? Manchak (2011) has argued that there cannot be purely
empirical grounds for ruling out the seemingly unpalatable structures,
for there always exist spacetimes that are, in a precise sense,
observationally indistinguishable from our own (Malament 1977; Manchak
2009a) that have essentially any set of properties one may stipulate.
Norton (2011) argues that this constitutes a necessary failure of
inductive reasoning in cosmology, no matter what one's account of
induction. Butterfield (2012) discusses the relation of Manchak's
results to standard philosophical arguments about under-determination
of theory by data. 
The philosopher of science interested in the definition and status of
theoretical terms in scientific theories has at hand here a rich
possible case-study, enlivened by the opportunity to watch eminent
scientists engaged in fierce, ongoing debate over the definition of a
term—indeed, over the feasibility of and even need for defining
it—that lies at the center of attempts to unify our most
fundamental physical theories, general relativity and quantum field
theory. 
At the heart of all of our conceptions of a spacetime singularity is
the notion of some sort of failure: a path that disappears, points
that are torn out, spacetime curvature or some other physical quantity
such as pressure whose behavior becomes pathological. Perhaps the
failure, though, lies not in the spacetime of the actual world (or of
any physically possible world), but rather in our theoretical
description of the spacetime. That is, perhaps we should not think
that general relativity is accurately describing the world when it
posits singular structure—it is the theory that breaks down, not
the physical structure of the world. 
Indeed, in most scientific arenas, singular behavior is viewed as an
indication that the theory being used is deficient, at least in the
sense that it is not adequate for modeling systems in the regime where
such behavior is predicted (Berry 1992). It is therefore common to
claim that general relativity, in predicting that spacetime is
singular, is predicting its own demise, and that classical
descriptions of space and time break down at black hole singularities
and the Big Bang, and all the rest (Hawking and Ellis 1973; Hawking
and Penrose 1996). Such a view denies that singularities are real
features of the actual world, and rather asserts that they are merely
artifacts of our current, inevitably limited, physical theories,
marking the regime where the representational capacities of the theory
at issue breaks down. This attitude is widely adopted with regard to
many important cases, e.g., the divergence of the Newtonian
gravitational potential for point particles, the singularities in the
equations of motion of classical electromagnetism for point electrons,
the singular caustics in geometrical optics, and so on. No one
seriously believes that singular behavior in such models in those
classical theories represents truly singular behavior in the physical
world. We should, the thought goes, treat singularities in general
relativity in the same way. 
One of the most common arguments that incomplete paths and non-maximal
spacetimes are physically unacceptable, and perhaps the most
interesting one, coming as it does from physicists rather than from
philosophers, invokes something very like the Principle of Sufficient
Reason: if whatever creative force responsible for spacetime could
have continued on to create more of it, what possible reason could
there have been for it to have stopped at any particular point
(Penrose 1969; Geroch
 1970)?[8]
 An opponent of this view could respond that it implicitly relies on a
certain picture of physics that may not sit comfortably with general
relativity, that of the dynamical evolution of a system. An advocate
of this viewpoint would argue that, from a point of view natural for
general relativity, spacetime does not evolve at all. It just sits
there, once and for all, as it were, a so-called block universe
(Putnam 1967; the entries
 Time Machines,
 Time Travel and
 Being and Becoming in Modern Physics).
 If it happens to sit there non-maximally, well, so be it. This kind
of response, however, has problems of its own, such as with the
representation of our subjective experience, which seems inextricably
tied up with ideas of evolution and change. Those sorts of problem,
however, do not seem peculiar to this dispute, but arise from the
character of general relativity itself: “dynamical
evolution” and “time” are subtle and problematic
concepts in the theory no matter what viewpoint one takes (Stein 1968,
1970, 1991). 
One can produce other metaphysical arguments against the view that
spacetime must be maximal. To demand maximality may lead to Buridan's
Ass problems, for it can happen that global extensions exist in which
one of a given set of incomplete curves is extendible, but no global
extension exists in which every curve in the set is
extendible (Ellis and Schmidt 1977). Also, there may exist several
physically quite different global extensions: the spacetime covered by
the usual Schwarzschild coordinates outside the Schwarzschild radius,
for instance, can be extended analytically to Kruskal-Schwarzschild
spacetime with a spacetime “tunnel” or
“bridge” to an otherwise disconnected part of the universe
(Hawking and Ellis 1973, sec. 5.5), or it can be extended to a
solution representing the interior of a massive spherical body. It is,
in any event, difficult to know what to make of the invocation of such
overtly metaphysical considerations in arguments in this most hard of
all hard sciences. See Curiel (1999) and Earman (1996) for critical
survey of such arguments, and Doboszewski (2017) for a recent
comprehensive survey of all these issues, including discussion of the
most recent technical results. 
A common hope is that when quantum effects are taken into account in
the vicinity of such extreme conditions of curvature where
singularities are predicted by the classical theory, the singular
nature of the spacetime geometry will be suppressed, leaving only well
behaved spacetime structure. Advocates of various programs of quantum
gravity also argue that in such a complete, full theory, singularities
of the kinds discussed here will not appear. Recent important work by
Wall (2013a, 2013b) shows that these hopes face serious problems. We
pick up these issues below, in
 section 5.4.4
 and
 section 6.3
 respectively, for it is in those contexts that many of the explicit
debates play out over the limits of general relativity. 
In any event, it is well to keep in mind that, even if singularities
are observed one day, and we are able to detect regularity in their
behavior of a sort that lends itself to formulation as physical law,
it seems likely that this law will not be a consequence of general
relativity but will rather go beyond its bounds in radical ways, for,
as we have seen, general relativity by itself does not have any
mechanism for constraining the possible behavior that singular
structure of various types may manifest. It is perhaps just this
possibility that excites a frisson of pleasure in those of the
libertine persuasion at the same time as it makes the prudish shudder
with revulsion. 
For a philosopher, the issues mooted here offer deep and rich veins
for those contemplating, among other matters: the role of explanatory
power in the assessment of physical theories; the interplay among
observation, mathematical models, physical intuition and metaphysical
predilection in the genesis of scientific knowledge; questions about
the nature of the existence attributable to physical entities in
spacetime and to spacetime itself; and the role of mathematical models
of physical systems in our understanding of those systems, as opposed
to their role in the mere representation of our knowledge of them.
The simplest picture of a black hole is that of a system whose gravity
is so strong that nothing, not even light, can escape from it. Systems
of this type are already possible in the familiar Newtonian theory of
gravity. The escape velocity of a body is the velocity at which an
object would have to begin to travel to escape the gravitational pull
of the body and continue flying out to infinity, without further
acceleration. Because the escape velocity is measured from the surface
of an object, it becomes higher if a body contracts and becomes more
dense. (Under such contraction, the mass of the body remains the same,
but its surface gets closer to its center of mass; thus the
gravitational force at the surface increases.) If the object were to
become sufficiently dense, the escape velocity could therefore exceed
the speed of light, and light itself would be unable to escape. 
This much of the argument makes no appeal to relativistic physics, and
the possibility of such Newtonian black holes was noted in the late
18th Century by Michell (1784) and Laplace (1796, part ii,
p. 305). These Newtonian
objects, however, do not precipitate the same sense of crisis as do
relativistic black holes. Although light emitted at the surface of the
collapsed body cannot escape, a rocket with powerful enough motors
firing could still push itself free. It just needs to keep firing its
rocket engines so that the thrust is equal to or slightly greater than
the gravitational force. Since in Newtonian physics there is no upper
bound on possible velocities, moreover, one could escape simply by
being fired off at an initial velocity greater than that of light.
Taking relativistic considerations into account, however, we find that
black holes are far more exotic entities. Given the usual
understanding that relativity theory rules out any physical process
propagating faster than light, we conclude that not only is light
unable to escape from such a body: nothing would be able to
escape this gravitational force. That includes the powerful rocket
that could escape a Newtonian black hole. Further, once the body has
collapsed down to the point where its escape velocity is the speed of
light, no physical force whatsoever could prevent the body from
continuing to collapse further, for that would be equivalent to
accelerating something to speeds beyond that of light. Thus once this
critical point of collapse is reached, the body will get ever smaller,
more and more dense, without limit. It has formed a relativistic black
hole. Here is where the intimate connection between black holes and
singularities appears, for general relativity predicts that, under
physically reasonable and generic conditions, a spacetime singularity
will form from the collapsing matter once the critical point of
black-hole formation is reached (Penrose 1965; Schoen and Yau 1983;
Wald 1984). 
For any given body, this critical stage of unavoidable collapse occurs
when the object has collapsed to within its so-called Schwarzschild
radius, which is proportional to the mass of the body. Our sun has a
Schwarzschild radius of approximately three kilometers; the Earth's
Schwarzschild radius is a little less than a centimeter; the
Schwarzschild radius of your body is about 10-27
cm—ten times smaller than a neutrino and 1010 times
smaller than the scale characteristic of quark interactions. This
means that if you could collapse all the Earth's matter down to a
sphere the size of a pea, it would form a black hole. 
It is worth noting, however, that one does not need an extremely high
density of matter to form a black hole if one has enough mass. If all
the stars in the Milky Way gradually aggregate towards the galactic
center while keeping their proportionate distances from each other,
they will all fall within their joint Schwarzschild radius and so form
a black hole long before they are forced to collide. Or if one has a
couple hundred million solar masses of water at its standard density
(1 gm/cm3)—so occupying in total a region of about
1027 cubic kilometers, the approximate size of the smallest
sphere containing the orbit of Uranus—it will be contained
within its Schwarzschild radius. (In this case, of course, the water
would indeed eventually collapse on itself to arbitrarily high
densities.) Some supermassive black holes at the centers of galaxies
are thought to be even more massive than the example of the water, at
several billion solar masses, though in these cases the initial
density of the matter thought to have formed the black holes was
extraordinarily high. 
According to the standard definition (Hawking and Ellis 1973; Wald
1984), the event horizon of a black hole is the surface formed by the
points of no return. That is, it is the boundary of the collection of
all events in the spacetime closest to the singularity at which a
light signal can still escape to the external universe. Everything
including and inside the event horizon is the black hole itself. (See
 section 3.4
 for a discussion of different ways to define a black hole, and the
problems these competing definitions raise.) For a standard
(uncharged, non-rotating) black hole, the event horizon lies at the
Schwarzschild radius. A flash of light that originates at an event
inside the black hole will not be able to escape, but will instead end
up in the central singularity of the black hole. A light flash
originating at an event outside of the event horizon will escape
(unless it is initially pointed towards the black hole), but it will
be red-shifted strongly to the extent that it started near the
horizon. An outgoing beam of light that originates at an event on the
event horizon itself, by definition, remains on the event horizon
until the temporal end of the universe. 
General relativity tells us that clocks running at different locations
in a gravitational field will, in a sense that can be made precise,
generally not agree with one another. In the case of a black hole,
this manifests itself in the following way. Imagine someone falls into
a black hole, and, while falling, she flashes a light signal to us
every time her watch hand ticks. Observing from a safe distance
outside the black hole, we would find the times between the arrival of
successive light signals to grow larger without limit, because it
takes longer for the light to escape the black hole's gravitational
potential well the closer to the event horizon the light is emitted.
(This is the red-shifting of light close to the event horizon.) That
is, it would appear to us that time were slowing down for the falling
person as she approached the event horizon. The ticking of her watch
(and every other process as well) would seem to go ever more slowly as
she approached ever more closely to the event horizon. We would never
actually see the light signals she emits when she crosses the event
horizon; instead, she would seem to be eternally “frozen”
just above the horizon. (This talk of seeing the person is somewhat
misleading, because the light coming from the person would rapidly
become severely red-shifted, and soon would not be practically
detectable.) 
From the perspective of the infalling person, however, nothing unusual
happens at the event horizon. She would experience no slowing of
clocks, nor see any evidence that she is passing through the event
horizon of a black hole. Her passing the event horizon is simply the
last moment in her history at which a light signal she emits would be
able to escape from the black hole. The concept of an event horizon is
a global one that depends on the overall structure of the
spacetime, and in particular on how processes physically evolve into
the indefinite future. Locally there is nothing noteworthy about the
points on the event horizon. In particular, locating the event horizon
by any combination of strictly local measurements is impossible in
principle, no matter how ingeniously the instruments are arranged
and precisely the measurements made. The presence of an event horizon
in this global sense is a strictly unverifiable hypothesis. One need
not be a verificationist about scientific knowledge to be troubled by
this state of affairs (Curiel 2019). Indeed, the global nature of the
event horizon manifests in an even more striking way: they are
“prescient”, in the sense that where the event horizon
horizon is located today depends on what I will throw in the black
hole tomorrow. How should a good empiricist feel about all of this?
The global and geometrical nature of black holes also raises
interesting questions about the sense in which one may or should think
of them as physical objects or systems (Curiel 2019). A black hole is
simply a geometrically characterized surface in spacetime, with no
ordinary matter at the event horizon, and no other local feature that
would allow one to detect it. The same questions as with singularities
 (section 2.1),
 therefore, force themselves on us here: in what sense, if any, should
we attribute existence to black holes, in so far as, considered
locally, they are an undistinguished region of spacetime whose
physically important properties manifest only as global structure?
Because of the peculiar nature of black holes as physical systems, the
attempt to observe them also raises interesting epistemic problems
about, inter alia, under-determination of theoretical models
by data, the way that theoretical assumptions play ineliminable roles
in the interpretation of data, and what it means at all to
“observe” a physical system that is, in principle, able to
emit no signal directly. Eckart et al. (2017) provides a
comprehensive survey of the issues; see also Collmar et al.
(1998) for the record of a round-table discussion on these questions
by a group of eminent theoreticians and observational astronomers. In
light of the recent epoch-making detection by LIGO of gravitational
waves with a signature indicating they were generated by a binary
black-hole system coalescing (Abbott et al. 2016), these
issues become even more urgent for philosophers to explore. 
One of the most remarkable features of relativistic black holes is
that they are purely gravitational entities: all the standard
black-hole spacetime models (Schwarzschild, Reissner-Nordström,
Kerr, Kerr-Newman) contain no matter whatsoever. They are vacuum
solutions to the Einstein field equations, which just means a solution
in which the matter density is everywhere zero. (Of course, one can
also consider a black hole with matter present, as standard
astrophysical models do for the supermassive black holes that are
believed to live at the center of most galaxies, which are thought to
be surrounded by strong magnetic fields and accretion disks of
super-heated matter.) In pre-relativistic physics we think of gravity
as a force produced by the mass associated with some matter. In the
context of general relativity, however, we do away with gravitational
force, and instead postulate a curved spacetime geometry that produces
all the effects we standardly attribute to gravity. One of the most
characteristic features of general relativity that sets it apart from
Newtonian gravitational theory is that it admits the possibility of
such curvature (“gravitational effects”) in the absence of
matter, such as at the boundary of a black hole. Thus a black hole is
not a thing in spacetime; it is instead a feature of
spacetime itself. 
A careful definition of a relativistic black hole will therefore rely
only on the geometrical features of spacetime. We will need to be a
little more precise about what it means to be “a region from
which nothing, not even light, can escape”. First, there will
have to be someplace to escape to if our definition is to
make sense. The most common method of making this idea precise and
rigorous employs the notion of escaping to infinity. The idea is that
if a particle or light ray cannot travel arbitrarily far from a
definite, bounded region in the interior of spacetime but must remain
always in the region, then that region is one of no escape, and is
thus a black hole. The boundary of the region is the event horizon.
Once a physical entity crosses the event horizon into the black hole,
it never crosses it again. 
Second, we will need a clear notion of the kind of geometry that
allows for escape, or makes such escape impossible. For this, we need
the notion of the causal structure of spacetime. At any event in the
spacetime, the possible trajectories of all light signals form a cone
(or, more precisely, the four-dimensional analogue of the boundary of
a cone). Since light travels at the fastest speed allowed in the
spacetime, these cones map out the boundaries of the propagation of
possible causal processes in the spacetime. If an occurrence at an
event A is able to causally affect another occurrence at event
B, there must be a continuous trajectory in spacetime from
event A to event B such that the trajectory lies in or
on the light cones of every event along it. (For more discussion, see
the Supplementary Document:
 Light Cones and Causal Structure.)
 
Figure 4
 is a spacetime diagram of a sphere of matter collapsing to form a
black hole. The curvature of the spacetime is represented by the
tilting of the light cones away from 45 degrees. Notice that the light
cones tilt inwards more and more as one approaches the center of the
black hole. The jagged line running vertically up the center of the
diagram depicts the central singularity inside the black hole. As we
emphasized in Section 1, this is not actually part of the spacetime,
but might be thought of as the “place” where the structure
of spacetime breaks down. Thus, one should not imagine the possibility
of traveling through the singularity; this would be as nonsensical as
something's leaving the diagram (i.e., the spacetime)
altogether. 
Figure 4: a spacetime diagram of black
hole formation 
What makes this a black hole spacetime is the fact that it contains a
region from which it is impossible to exit while traveling at or below
the speed of light. This region is marked off by the events at which
the outside edge of the forward light cone points straight upward. As
one moves inward from these events, the light cone tilts so much that
one is always forced to move inward toward the central singularity.
This set of points of no return is, of course, the event horizon; and
the spacetime region inside it is the black hole. In this region, one
inevitably moves towards the singularity; the impossibility of
avoiding the singularity is just the impossibility of preventing
ourselves from moving forward in time. (Again, see
 section 3.4
 for a discussion of other ways to define a black hole.) 
Notice that, as represented in
 Figure 4, the matter of the collapsing
 star eventually disappears into the black hole singularity. All the
 details of the matter are then completely lost; all that is left is
 the geometrical properties of the black hole. Astonishingly, those
 properties can be identified with a small, fixed set of physical
 quantities. Indeed, the remarkable No-Hair Theorems (Israel 1967,
 1968; Carter 1971, 1973, 1997 [Other Internet Resources]; Robinson
 1975; Mazur 1982; Heusler 1996; Chruściel et al. 2012)
 make rigorous the idea that a black hole in equilibrium is entirely
 characterized by just three numbers, viz., its mass, its
 angular momentum, and its electric
 charge.[9] This
 has the remarkable consequence that no matter what the particulars
 may be of any body that collapses to form a black hole—it may
 be as intricate, complicated and Rococo as one likes, composed of the
 most exotic materials—the final result after the system has
 settled down to equilibrium will be identical in every respect to a
 black hole that formed from the collapse of any other body
 having the same total mass, angular momentum and electric charge
 (Carter 1997 [Other Internet Resources]). Because of this extremity
 of simplicity, Chandrasekhar (1983, Prologue,
 p. xxiii) called black
 holes “the most perfect macroscopic objects … in the
 universe.” (The fact that their physical state is entirely
 characterized by only three numbers plays an important role in the
 ascription of thermodynamical properties to black holes, discussed in
 5.2
 below.) 
Remarkably, not only are black holes in and of themselves objects of
the utmost simplicity. They enforce simplicity on all else in the
universe as well, no matter how far away from themselves. In a sense
that can be made precise, one of the most basic structures of the
spacetime manifold itself, its topology, is as simple as possible
everywhere outside a well behaved black
 hole.[10]
 This is known as the Topological Censorship Theorem (Friedman et
al. 1983; Chruściel and Wald 1994; Galloway 1995). As its
name suggests, it bears on the larger question of the Cosmic
Censorship Hypothesis (Galloway and Woolgar 1997), discussed in
 section 4
 below. In itself, though, it raises fascinating questions about the
relation of topological to metrical structure in a spacetime,
questions almost completely unexplored by philosophers. (See Geroch
and Horowitz 1979 for a long list of conceptual and technical problems
and questions about this relation.) For a philosopher interested in
the nature of spacetime, however, the way that its different
structures relate to and constrain each other must be of fundamental
importance. 
For the reasons discussed in
 section 3.1,
 the standard definition of a black hole, based on the idea of a
global event horizon, has limited application to the modeling of real
astrophysical systems (except in so far as one can idealize them as
essentially isolated). In an attempt to rectify this situation,
Hayward (1994b) offered a generalized definition of a black hole, not
requiring any of the special global structures that the traditional
definition relies on. Hayward defines a black hole based on what he
calls a trapping horizon. This is, roughly speaking, a surface on
which all inward-directed light rays are converging, and to which all
outward-directed light rays are tangent. This definition tries to
capture the idea that a black hole is a surface at which the
gravitational intensity is such that not even light can escape: any
light ray incident on the surface the smallest bit inward will get
sucked in; otherwise, light rays can be only tangent to the surface.
The surface does not admit incident light rays traveling away from its
interior. This definition has the virtue that the boundary of the
black hole now has clear, local physical significance in principle: an
observer could determine when she crossed it by making only local
measurements. (More precisely, a team of synchronized observers, whose
combined instrumental reach encompasses the entire surface at a given
moment of time, could jointly make this determination, with enough
background knowledge of the spacetime geometry outside the boundary.)
Perhaps one of the most intriguing aspects of Hayward's definition is
that a black hole would not necessarily be a region of no escape: in
some black holes so defined, an observer entering the trapped region
could later escape it (Hayward 1994a, in OIR).
 
Ashtekar et al. (1999, 2000) offer a different, related
generalization of the idea of a black hole, based on what they call
isolated horizons. This definition is somewhat more restrictive than
Hayward's in so far as, as the name suggests, it requires that no
stress-energy cross such a horizon. Subsequent work by Ashtekar and
Krishnan (2003), Ashtekar (2007) and Hayward (2006, in OIR,
 2009) clarified the relationship between the two, showing that the
isolated horizon can be considered, in a sense, a special case of the
trapping horizon. (See Hayward 2013 for a recent comprehensive review,
and Faraoni 2013 for one with special attention to its relevance to
cosmology.) For lack of a better term, we shall call black holes
defined by trapping or isolated horizons “quasi-local black
holes”. (‘Local’ because they are not global objects
in the sense that black holes as tradionally defined are, and
‘quasi’ because they still can extend arbitrarily far
throughout spacetime.) 
The status of these competing definitions of a quasi-local black hole
and of the differences among them, and what their respective virtues
and demerits may be, appear to be open questions, though both Hayward
and Ashtekar et al., in the works just cited, go some way
towards answering some of them by using their respective definitions
to prove generalizations of the so-called laws of black hole mechanics
 (section 5.1
 below). Hayward also demonstrates that analogues to some of the
classical singularity theorems hold for his definition as well. Still,
many questions remain open. To take one example, it is not clear
whether or not the new definitions coincide with the traditional
definition in those spacetimes in which the traditional definition can
be formulated, or whether collateral conditions must be met for the
two to coincide. It is also not clear whether the analogues to the
classical No Hair Theorems hold using the new definitions or even what
those analogues may be. 
Perhaps the most fascinating feature of quasi-local black holes is the
fact that, in a sense that can be made precise, they are
“clairvoyant”: they are aware of and respond to changes in
the geometry in spacetime regions that they cannot be in causal
contact with (Bengtsson and Senovilla 2011). Indeed, they can
encompass regions whose entire causal past is flat! This subject
exemplifies the exuberant weirdness that causal structure in general
relativity can manifest. 
Besides the standard definition of a black hole based on the presence
of a global event horizon, and the quasi-local definitions just
discussed, there is an enormous and greatly variegated menagerie of
different definitions and conceptions of a black hole that physicists
in different fields (and sometimes those in the same field) use in
their day to day work, none agreeing with the standard or quasi-local
definitions, many of them manifestly inconsistent with each other
(Curiel 2019). However one views this situation, it is clear, as a
brute fact about the practice of physics today, that there is no
single definition of “black hole” that will serve all
investigative purposes in all fields in which black holes are objects
of study.
 Table 1
 lists the core concepts most commonly used in definitions and
characterizations of black holes across several different fields of
physics, sketched with only the broadest and crudest of brushes. It
should be kept in mind that many investigators in each of these fields
do not use, or even accept as reasonable, what is given in the table.
Table 1: core concepts in different
definitions of black holes common to different fields 
What seems to be the most common practice today is, during the course
of an investigation, to fix a list of important, characteristic
properties of and phenomena associated with black holes required for
one's purposes in the context of interest, and then to determine which
of the known definitions imply the members of that list. If no known
definition implies the list, one either attempts to construct a new
definition that does (and is satisfactory in other ways), or else one
concludes that there is an internal inconsistency in one's list, which
may already be of great interest to learn. Examining the way the idea
of black holes are used across physics—in astrophysics,
cosmology, classical general relativity, semi-classical gravity,
particle physics, various programs in quantum gravity, fluid
mechanics, condensed matter, and analogue gravity—yields a list
of potentially characteristic properties and phenomena some subset of
which may plausibly be required or wanted in a characterization of a
black hole in a given investigative context (Curiel 2019): 
This list is not meant to be exhaustive. There are many other such
properties and phenomena that might be needed for a given purpose. It
is already clear from this partial list, however, that no single
definition can accommodate all of them. It is also clear from
examining the literature, moreover, that, even within the same
communities, different workers will choose different subsets of these
properties for different purposes in their thinking about black holes.
As in the case of singularities, these alternative definitions of
black holes raise philosophical questions about the relations among
the different definitions that attempt to capture different aspects
of, intuitively speaking, the “same kind” of physical
object. One can, for instance, view the standard definition of a black
hole, with its global event horizon, as an extreme idealization of an
isolated system (one with no neighboring systems at all), and the
definitions based on isolated or trapping horizons as trying to
capture a more general, less idealized representation of an isolated
system, one that has neighboring systems at a finite remove, or a
representation of a system that may be non-trivially interacting with
other systems. For the looser, less precise definitions used by
astrophysicists, for example, and some of the gestures at definitions
proposed in some programs of quantum gravity, however, it is difficult
to know how even to begin to compare them to the precise global and
quasi-local ones. It is simply not clear that the same type of
physical system is being characterized. 
This situation provides a fascinating case study, from both a physical
and a philosophical point of view, for questions about the nature of
idealization and de-idealization, and the definition of theoretical
entities more generally. On what grounds, e.g., could one
ascertain the relative merits of each type of definition on its own,
and each as proposed for a particular sort of investigation, in the
absence of empirical data? In what sense do the different definitions
characterize the “same” type of physical system, if they
do so at all? Is there a need to settle on a single canonical
definition of a black hole? What would be gained or lost with or
without one? The situation is closely analogous to that of the lack of
a canonical definition of a singularity, except it is even more
extreme here: the different definitions of singularities used by
different physicists are (almost always) not actually
inconsistent with each other. 
For the remainder of this encyclopedia entry, unless explicitly stated
otherwise, when we speak of a black hole it should be understood that
we mean one as determined by the standard definition of a global event
horizon, because this is the one most often used in current
foundational work. 
While spacetime singularities in general are frequently viewed with
suspicion, physicists often offer the reassurance that, even if they
are real, we expect most of them to be hidden away behind the event
horizons of black holes. Such singularities therefore could not affect
us unless we were actually to jump into the black hole. A naked
singularity, on the other hand, is one that is not hidden behind an
event horizon. Such singularities appear much more threatening because
they are uncontained, freely accessible to the rest of spacetime. 
The heart of the worry is that singular structure seems to signify so
profound a breakdown in the fundamental structure of spacetime that it
could wreak havoc on any region of the universe that it were visible
to. Because the structures that break down in singular spacetimes are
in general required for the formulation of our known physical laws,
and of initial-value problems for individual physical systems in
particular, one such fear is that determinism would collapse entirely
wherever the singular breakdown were causally visible. In Earman's
(1995, pp. 65–6) evocative conceit, nothing would seem to stop
the singularity from disgorging any manner of unpleasant jetsam, from
TVs showing Nixon's Checkers Speech to old lost socks, in a way
completely undetermined by the state of spacetime in any region
whatsoever. As a result, there could be no reasonable expectation of
determinism, nor even just predictability, for any region in causal
contact with what it spews out. 
One form that such a naked singularity could take is that of a
white hole, which is a time-reversed black hole. Imagine
taking a film of a black hole forming from the collapse of a massive
object, say, a star, with light, dust, rockets, astronauts and old
socks falling into it during its subsequent evolution. Now imagine
that film being run backwards. This is the picture of a white hole:
one starts with a naked singularity, out of which might appear people,
rockets, socks—anything at all—with eventually a star
bursting forth. Absolutely nothing in the causal past of such a white
hole would determine what would pop out of it, since, as follows from
the No Hair Theorems
 (section 3.2),
 items that fall into a black hole leave no trace on the future
outside of it. (This description should feel familiar to the canny
reader: it is the same as the way that increase of entropy in ordinary
thermodynamics as embodied in the Second Law makes retrodiction
impossible; the relationship of black holes to thermodynamics is
discussed in
 section 5.)
 Because the field equations of general relativity do not pick out a
preferred direction of time, if the formation of a black hole is
allowed by the laws of spacetime and gravity, then those laws also
permit white holes. 
Roger Penrose (1969, 1973) famously suggested that although naked
singularities are compatible with general relativity, in physically
realistic situations they will never form; that is, any process that
results in a singularity will safely ensconce that singularity behind
an event horizon. This conjecture, known as the Cosmic Censorship
Hypothesis, has met with some success and popularity; however, it also
faces several difficulties. As in our previous discussions of
singularities and black holes, there are questions about how exactly
to formulate the hypothesis, and, once formulated, about whether or
not it holds in general relativity as a whole, or at least in some
physically reasonable subset of spacetimes—where, again,
“physically reasonable” will likely be a vague and
controversial notion. 
Penrose's original formulation relied on black holes: a suitably
generic singularity will always be contained in a black hole (and so
causally invisible outside the black hole). As the counter-examples to
various ways of articulating the hypothesis based on this idea have
accumulated over the years, however, it has gradually been abandoned
(Geroch and Horowitz 1979; Krolak 1986; Penrose 1998; Joshi et
al. 2002; Joshi 2003, 2007a; Joshi and Malafarina 2011a, 2011b).
More recent approaches either begin with an attempt to provide
necessary and sufficient conditions for cosmic censorship itself,
yielding an indirect characterization of a naked singularity as any
phenomenon violating those conditions, or else they begin with an
attempt to provide a characterization of a naked singularity without
reference to black holes and so conclude with a definite statement of
cosmic censorship as the absence of such phenomena (Geroch and
Horowitz 1979). The variety of proposals made using both approaches is
too great to canvass here; the interested reader is referred to
Ringström (2010) for a review of the current state of the art for
standard black holes, Nielsen (2012) for cosmic censorship regarding
Hayward's quasi-local black holes
 (section 3.3),
 Ringström (2010) for a review of the bearing of the
initial-value problem in general relativity on cosmic censorship, and
to Earman (1995, ch. 3) and Curiel (1999) for philosophical discussion
of many of the earlier proposals. Manchak (2011) gives reasons for
thinking that the question of providing a completely satisfactory
formulation of the Cosmic Censorship Hypothesis may never be settled,
on the grounds that the idea of what counts as “physically
reasonable” is not an empirically determinable question. Still,
the possibility may remain open that there be several different,
inequivalent formulations of the Cosmic Censorship Hypothesis, each
having its own advantages and problems, none “canonical”
in a definitive sense, as may be the case for definitions of
singularities and black holes themselves. 
There is another area of investigation intimately related to issues of
Cosmic Censorship in general, and issues of determinism in general
relativity in particular: whether or not spacetime is
“hole-free”. This has been the subject of recent
philosophical work, primarily by Manchak (2009b, 2014a, 2016a). Geroch
(1977) originally proposed the idea of a generic “hole” in
spacetime in trying to capture in as general terms as possible the
idea that spacetime has no obstruction of any kind that would prevent
it from “evolving as far into the future as it reasonably
could”. (Recall the discussion of maximality and extendibility
in
 section 1.1.)
 Although Geroch's definition had powerful conceptual appeal, in the
event it has proven untenable: Krasnikov (2009) showed that, according
to it, even Minkowski spacetime fails to be hole-free. Manchak (2009b)
showed how an emendation of Geroch's definition could fix the problem.
He then showed that, under the assumption of global hyperbolicity (a
strong condition of causal well-behavedness for a spacetime), one gets
a nice hierarchy of conditions relating to determinism: geodesic
completeness implies effective completeness (Manchak's own condition),
which implies inextendibility, which implies hole-freeness (Manchak
2014a; see
 section 1.1
 for the definitions of these conditions). In related work, Manchak
(2014b) showed that, in a sense one can make precise, it should be
easier to construct a machine that would result in spacetime's having
such a hole than one that would result in time-travel. In short,
creating the possibility for indeterminism seems easier in the theory
than the possibility for causal paradox! 
Manchak (2016a) also recently introduced a new kind of pathology a
spacetime can have, an “epistemic hole”: roughly speaking,
a spacetime has an epistemic hole if two observers in initially
identical epistemic states can evolve in such a way that what one can
learn can only ever be, in principle, a proper subset of what the
other can learn. Manchak shows that, if a spacetime has no epistemic
holes then (under mild conditions on the niceness of the causal
structure) the spacetime has no naked singularities as standardly
construed. The condition differs also in its modal character from
other such hole-freeness conditions, for it makes significantly weaker
and more conceptually and technically tractable modal claims. 
Issues of determinism, from an epistemic perspective, are intimately
bound up with the possibility of reliable prediction. (See the entry
 Causal Determinism.)
 The general issue of predictability itself in general relativity,
even apart from the specific problems that singular structure may
raise, is fascinating, philosophically rich, and very much unsettled.
One can make a prima facie strong argument, for example, that
prediction is possible in general relativity only in spacetimes that
possess singularities (Hogarth 1997; Manchak 2013)! See
Geroch (1977), Glymour (1977), Malament (1977), and Manchak (2008,
2013) for discussion of these and many other related issues. 
Here again, as with almost all the issues discussed up to this point
in this entry regarding singularities and black holes, is an example
of a sizable subculture in physics working on matters that have no
clearly or even unambiguously defined physical parameters to inform
the investigations and no empirical evidence to guide or even just
constrain them, the parameters of the debate imposed by and large by
the intuitions of a handful of leading researchers. From sociological,
physical, and philosophical vantage points, one may well wonder, then,
why so many physicists continue to work on it, and what sort of
investigation they are engaged in. Perhaps nowhere else in general
relativity, or even in physics, can one observe such a delicate
interplay of, on the one hand, technical results, definitions and
criteria, and, on the other hand, conceptual puzzles and even
incoherence, largely driven by the inchoate intuitions of physicists.
Not everyone views the situation with excitement or even equanimity,
however: see Curiel (1999) for a somewhat skeptical discussion of the
whole endeavor. 
The challenge of uniting quantum theory and general relativity in a
successful theory of quantum gravity has arguably been the greatest
challenge facing theoretical physics for the past eighty years. One
avenue that has seemed particularly promising is the attempt to apply
quantum theory to black holes. This is in part because, as purely
gravitational entities, black holes present an apparently simple but
physically important case for the study of the quantization of
gravity. Further, because the gravitational force grows without bound
as one nears a standard black-hole singularity, one would expect
quantum gravitational effects (which should come into play at
extremely high energies) to manifest themselves in the interior of
black holes. 
In the event, studies of quantum mechanical systems in black hole
spacetimes have revealed several surprises that threaten to overturn
the views of space, time, and matter that general relativity and
quantum field theory each on their own suggests or relies on. Since
the ground-breaking work of Wheeler, Penrose, Bekenstein, Hawking and
others in the late 1960s and early 1970s, it has become increasingly
clear that there are profound connections among general relativity,
quantum field theory, and thermodynamics. This area of intersection
has become one of the most active and fruitful in all of theoretical
physics, bringing together workers from a variety of fields such as
cosmology, general relativity, quantum field theory, particle physics,
fluid dynamics, condensed matter, and quantum gravity, providing
bridges that now closely connect disciplines once seen as largely
independent. 
In particular, a remarkable parallel between the laws of black holes
and the laws of thermodynamics indicates that gravity and
thermodynamics may be linked in a fundamental (and previously
unimagined) way. This linkage strongly suggests, among many things,
that our fundamental ideas of entropy and the nature of the Second Law
of thermodynamics must be reconsidered, and that the standard form of
quantum evolution itself may need to be modified. While these
suggestions are speculative, they nevertheless touch on deep issues in
the foundations of physics. Indeed, because the central subject matter
of all these diverse areas of research lies beyond the reach of
current experimentation and observation, they are all speculative in a
way unusual even in theoretical physics. In their investigation,
therefore, physical questions of a technically sophisticated nature
are inextricable from subtle philosophical considerations spanning
ontology, epistemology, and methodology, again in a way unusual even
in theoretical physics. 
Because this is a particularly long and dense section of the article,
we begin with an outline of it.
 Section 5.1
 states the laws of black holes in classical general relativity, and
expounds the formal analogy they stand in with the laws of ordinary
thermodynamics.
 Section 5.2
 briefly describes how taking account of quantum effects in the
neighborhood of a black hole leads to the prediction of Hawking
radiation and the subsequent conclusion that the analogy with the laws
of ordinary thermodynamics is more than just formal, but represents a
true and intimate physical connection.
 Section 5.3
 discusses the puzzles that arise when trying to understand the
attribution of a physical entropy to a black hole.
 Section 5.4
 consists of several subsections, each examining a different puzzling
aspect of the so-called Generalized Second Law: the hypothesis that
the total entropy of the universe, that of ordinary matter plus that
of black holes, never decreases. We conclude in
 Section 5.5
 with a brief account of attempts to extend the attribution of a
physical entropy to gravitational systems more general than just black
holes. 
Suppose one observes a quiescent black hole at a given moment,
ignoring any possible quantum effects. As discussed above in
 section 3.2,
 there are three physical quantities of dynamical interest the black
hole possesses that are, more or less, amenable to measurement, and
that completely characterize the physical state of the black hole: its
mass, its angular momentum, and its electric charge. These quantities,
like those of systems in classical mechanics, stand in definite
relation to each other as the black hole dynamically evolves, which is
to say, they satisfy a set of equations characterizing their
 behavior.[11]
(A black hole is stationary if, roughly speaking, it does not
change over time; more precisely, it is stationary if its event
horizon is generated by an asymptotically timelike Killing field.)
On the face of it, the Zeroth, First and Third Laws are
straightforward to understand. The Second Law, however, is not so
“obvious” as it may at first glance appear. It may seem
that, because nothing can escape from a black hole once it has
entered, black holes can only grow larger or, at least, stay the same
size if nothing further falls in. This assumes, however, that
increased mass always yields increased surface area as opposed to some
other measure of spatial extent. Surprising as it may sound, it is
indeed the case that, although nothing that enters a black hole can
escape, it is still possible to extract energy (i.e., mass)
from a spinning black hole, by means of what is known as the Penrose
process (Penrose and Floyd 1971). It is therefore not obvious that one
could not shrink a black hole by extracting enough mass-energy or
angular momentum from it. It also seems to be at least possible that a
black hole could shrink by radiating mass-energy away as gravitational
radiation, or that the remnant of two colliding black holes could have
a smaller surface area than the sum of the original two. 
It is most surprising, therefore, to learn that the Second Law is a
deep, rigorous theorem that follows only from the fundamental
mathematics of relativistic spacetimes (Hawking 1971), and does not
depend in any essential way on the particulars of relativistic
dynamics as encapsulated in the Einstein field equation (Curiel
2017). This is in strict opposition to the Second Law in classical
thermodynamics, which stands as a more or less phenomenological
principle derived by empirical generalization, perhaps justified in
some sense by a “reduction” to statistical mechanics, with
the temporal asymmetry of entropy non-decrease argued to hold based on
the likelihood of initial states in conjunction with the forms of
dynamical evolution “physically permissible” for matter
fields. (See the entry
 Philosophy of Statistical Mechanics.)
 
For those who know classical thermodynamics, the formal analogy
between its laws and the laws of black hole as stated should be
obvious. (For exposition and discussion of the laws of classical
thermodynamics, see, e.g.: Fermi 1937 for a less technical,
more physically intuitive approach; Fowler and Guggenheim 1939 for a
more technical and rigorous one; and Uffink 2007 for a more
historically and philosophically oriented one.) One formulation of the
Zeroth Law of thermodynamics states that a body in thermal equilibrium
has constant temperature throughout. The First Law is a statement of
the conservation of energy. It has as a consequence that any change in
the total energy of a body is compensated for and measured by changes
in its associated physical quantities, such as entropy, temperature,
angular momentum and electric charge. The Second Law states that
entropy never decreases. The Third Law, on one formulation, states
that it is impossible to reduce the temperature of a system to zero by
any physical process. Accordingly, if in the laws for black holes one
takes ‘stationary’ to stand for ‘thermal
equilibrium’, ‘surface gravity’ to stand for
‘temperature’, ‘mass’ to stand for
‘energy’, and ‘area’ to stand for
‘entropy’, then the formal analogy is perfect. 
Indeed, relativistically mass just is energy, so at least the
First Law seems already to be more than just formal analogy. Also, the
fact that the state of a stationary black hole is entirely
characterized by only a few parameters, completely independent of the
nature and configuration of any micro-structures that may underlie it
(e.g., those of whatever collapsed to form the thing),
already makes it sound more than just a little thermodynamical in
character. (Recall the discussion of the No Hair Theorems in
 section 3.2
 above.) Still, although the analogy is extremely suggestive in
toto, to take it seriously would require one to assign a non-zero
temperature to a black hole, which, at the time Bardeen, Carter and
Hawking first formulated and proved the laws in 1973, almost everyone
agreed was absurd. All hot bodies emit thermal radiation (like the
heat given off from a stove, or the visible light emitted by a burning
piece of charcoal); according to general relativity, however, a black
hole ought to be a perfect sink for energy, mass, and radiation,
insofar as it absorbs everything (including light), and emits nothing
(including light). So it seems the only temperature one might be able
to assign it would be absolute zero. (See
 section 5.4.2
 below for more detailed arguments to this effect.) 
In the early 1970s, nonetheless, Bekenstein (1972, 1973, 1974) argued
that the Second Law of thermodynamics requires one to assign a finite
entropy to a black hole. His worry was that one could collapse any
amount of highly entropic matter into a black hole—which, as we
have emphasized, is an extremely simple object—leaving no trace
of the original disorder associated with the high entropy of the
original matter. This seems to violate the Second Law of
thermodynamics, which asserts that the entropy (disorder) of a closed
system—such as the exterior of an event horizon—can never
decrease. Adding mass to a black hole, however, will increase its
size, which led Bekenstein to suggest that the area of a black hole is
a measure of its entropy. This conjecture received support in 1971
when Hawking proved that the surface area of a black hole, like the
entropy of a closed system, can never decrease (Hawking 1971). Still,
essentially no one took Bekenstein's proposals seriously at first,
because all black holes manifestly have temperature absolute zero, as
mentioned above, if it is even meaningful to ascribe temperatures to
them in the first
 place.[12]
Thus it seems that the analogy between black holes and thermodynamical
objects, when treated in the purely classical theory of general
relativity, is merely a formal one, without real physical
significance. 
The “obvious fact” that the temperature of a black hole
can be, at best, only absolute zero was shown to be illusory when
Hawking (1974, 1975) demonstrated that black holes are not completely
black after all. His analysis of the behavior of quantum fields in
black-hole spacetimes revealed that black holes will emit radiation
with a characteristically thermal spectrum: a black hole generates
heat at a temperature that is inversely proportional to its mass and
directly proportional to its surface gravity. It glows like a lump of
smoldering coal even though light should not be able to escape from
it! The temperature of this Hawking radiation is extremely low for
stellar- and galactic-scale black holes, but for very, very small
black holes the temperatures would be high. (The Hawking temperature
of the black hole at the center of the Milky Way, Sagittarius
A*, having a mass of approximately 4 million solar masses,
is approximately 10-14 Kelvin; for a black hole to be room
temperature, it would have to have a mass of about 1018
kg—about 1000 times the mass of Mt. Everest—and so be
about 10-7 m across, the size of a virus.) This means that
a very, very small black hole should rapidly evaporate away, as all of
its mass-energy is emitted in high-temperature Hawking radiation.
Thus, when quantum effects are taken into account, black holes will
not satisfy the Area Theorem, the second of the classical laws of
black hole, as their areas shrink while they evaporate. (Hayward
et al. 2009 discuss the status of deriving a
“local” flux of Hawking radiation for quasi-local black
holes; Nielsen 2009 discusses this along with the status of attempts
to prove the laws of black holes for quasi-local black holes.) 
These results—now referred to collectively as the Hawking
effect—were taken to establish that the parallel between the
laws of black hole and the laws of thermodynamics was not a mere
formal fluke: it seems they really are getting at the same deep
physics. The Hawking effect establishes that the surface gravity of a
black hole can, indeed must, be interpreted as a physical temperature.
(The surface gravity, therefore, is often referred to as the
‘Hawking temperature’.) Connecting the two sets of laws
also requires linking the surface area of a black hole with entropy,
as Bekenstein had earlier suggested: the entropy of a black hole is
proportional to the area of its event horizon, which is itself
proportional to the square of its mass. (The area, therefore, is often
referred to as the ‘Bekenstein entropy’.) Furthermore,
mass in black hole mechanics is mirrored by energy in thermodynamics,
and we know from relativity theory that mass and energy are identical,
so the black hole's mass is its thermodynamical energy. The
overwhelming consensus in the physics community today, therefore, is
that black holes truly are thermodynamical objects, and the laws of
black hole mechanics just are the laws of ordinary thermodynamics
extended into a new regime, to cover a new type of physical system.
We will return to discuss Hawking radiation in more detail in
 section 6.1
 below, but for now we note that this all raises deep questions about
inter-theoretic relations that philosophers have not yet come to grips
with: although it seems undeniable, what does it mean to say that a
purely gravitational system is also “a thermodynamical
 object”?[13]
 How can the concepts and relations of one theory be translated so as
to be applicable in the context of a radically different one? (See the
entries
 Scientific Unity
 and
 Intertheory Relations in Physics.)
 
Although it is still orthodoxy today in the physics community that
there is no consistent theory of thermodynamics for purely classical
black holes (Unruh and Wald 1982; Wald 1999, 2001), i.e.,
when quantum effects are not taken into account, primarily because it
seems that they must be assigned a temperature of absolute zero, if
any at all. Curiel (2017a, Other Internet Resources)
 has recently argued that this is not so. He argues, to the contrary,
that there is a consistent way of treating purely classical black
holes as real thermodynamical systems, that they should be assigned a
temperature proportional to their surface gravity, and, in fact, that
not to do so leads to the same kinds of inconsistencies as occur if
one does not do so for black holes emitting Hawking radiation. 
In a recent article, Dougherty and Callender (2019) challenge the
orthodoxy from the opposite direction. They argue that we should be
far more skeptical of the idea that the laws of black holes are more
than just formal analogy, and that, indeed, there are strong reasons
to think that they are not physically the laws of thermodynamics
extended into a new domain. Their main argument is that the Zeroth Law
of black holes cannot do the work that the standard formulation of the
Zeroth Law does in classical thermodynamics. In classical
thermodynamics, the standard formulation of the Zeroth Law is
transitivity of equilibrium: two bodies each in equilibrium with a
third will be in equilibrium with each other. They point out that this
transitivity of equilibrium underlies many of the most important
constructions and structures in classical thermodynamics, which mere
constancy of temperature (surface gravity) for a single system in
equilibrium does not suffice for. Curiel (2018), however, recently
proposed a strengthened version of the Zeroth Law for black holes,
based on a characterization of transitivity of equilibrium among them,
in an attempt to address this challenge. It suffers from several
problems, however, most importantly the fact that it relies on a
notion of “approximate symmetry” in general relativity
that is not well defined. This is an area of active dispute. 
Wallace (2018, 2019) provides a more comprehensive exposition and
defense of the claim that black holes truly are thermodynamical
objects, attacking the problem from several different directions, and
offers specific rejoinders to several of the other arguments made by
Dougherty and Callender (2019). 
The most initially plausible and promising way to explain what the
entropy of a black hole measures, and why a black hole has such a
property in the first place, is to point to the Hawking radiation it
emits, and in particular the well defined temperature the radiation
has. (For exposition and discussion of the standard relations between
temperature and entropy in classical thermodynamics, see,
e.g.: Fermi 1936 for a less technical, more physically
intuitive approach; Fowler and Guggenheim 1939 for a more technical
and rigorous one; and Uffink 2007 for a more historically and
philosophically oriented one.) Indeed, it is not uncommon to see such
“explanations”, not only in popular accounts but even in
serious research papers. There are, however, many technical and
conceptual reasons why such an explanation is not viable (Visser
1998b, 2003), summed up in the slogan that Hawking radiation is a
strictly kinematical effect, whereas black hole entropy is a dynamical
phenomenon. (This fact is discussed in more detail in
 section 8
 below.) What, then, is the origin and nature of the entropy we
attribute to a black hole? 
In classical thermodynamics, that a system possesses entropy is often
attributed to the fact that in practice we are never able to give it a
“complete” description (Jaynes 1967). When describing a
cloud of gas, we do not specify values for the position and velocity
of every molecule in it; we rather describe it using quantities, such
as pressure and temperature, constructed as statistical measures over
underlying, more finely grained quantities, such as the momentum and
energy of the individual molecules. On one common construal, then, the
entropy of the gas measures the incompleteness, as it were, of the
gross description. (See the entry
 Philosophy of Statistical Mechanics.)
 In the attempt to take seriously the idea that a black hole has a
true physical entropy, it is therefore natural to attempt to construct
such a statistical origin for it. The tools of classical general
relativity cannot provide such a construction, for it allows no way to
describe a black hole as a system whose physical attributes arise as
gross statistical measures over underlying, more finely grained
quantities. Not even the tools of quantum field theory on curved
spacetime can provide it, for they still treat the black hole as an
entity defined entirely in terms of the classical geometry of the
spacetime (Wald 1994). Any such statistical accounting, therefore,
must come from a theory that attributes to the classical geometry
itself a description based on an underlying, perhaps discrete
collection of microstates, themselves describing the fine-grained
dynamics of “entities”, presumably quantum in nature,
underlying the classical spacetime description of the black hole. Note
that any program aimed at “counting black-hole
microstates” need not accept a subjectivist interpretation of
entropy à la Jaynes. In any event, on any view of the
nature of entropy, there arises a closely related problem,
viz., to locate “where” black-hole entropy
resides: inside, on, or outside the event horizon? See Jacobson et
al. (2005) for a thoughtful dialogue among three eminent
physicists with different point of views on the matter. 
Explaining what these microstates are that are counted by the
Bekenstein entropy has been a challenge that has been eagerly pursued
by quantum gravity researchers. In 1996, superstring theorists were
able to give an account of how M-theory (an extension of
superstring theory) generates the number of string-states underlying a
certain class of classical black holes, and this number matched that
given by the Bekenstein entropy (Strominger and Vafa 1996). At the
same time, a counting of black-hole states using loop quantum gravity
also recovered the Bekenstein entropy (Rovelli 1996). It is
philosophically noteworthy that this is treated as a significant
success for these programs (i.e., it is presented as a reason
for thinking that these programs are on the right track), even though
no quantum effect in the vicinity of a black hole, much less Hawking
radiation itself, has ever been experimentally observed. (Sadly, we
have no black holes in terrestrial laboratories, and those we do have
good reason to think we indirectly observe are too far away for
anything like these effects to be remotely detectable, given their
minuscule temperatures.) It is also the case that all known
derivations held only for a very special class of black holes
(“extremal” ones), which everyone agrees are unphysical.
There are no convincing derivations for more general, physically
relevant black holes. 
Nonetheless, the derivation of the Bekenstein entropy by the counting
of “microstates” has become something of a sine qua
non for programs of quantum gravity, even if only for the special
case of extremal black holes: if one cannot do it from something like
the first principles of one's program, no one will take you seriously.
This is noteworthy because it poses a prima facie problem for
traditional accounts of scientific method, and underscores the
difficulties faced by fundamental physics today, that in many
important areas it cannot make contact with empirical data at all. How
did a theoretically predicted phenomenon, derived by combining
seemingly incompatible theories in a novel way so as to extend their
reach into regimes that we have no way of testing in the foreseeable
future, become the most important touchstone for testing novel ideas
in theoretical physics? Can it play that role? Philosophers have not
yet started to grapple seriously with these issues. 
In a thoughtful survey, Sorkin (2005) concisely and insightfully
characterizes in ten theses what seems to be a popular view on the
nature of black-hole entropy when studied as an essentially quantum
phenomenon, which is distilled into the essential parts for our
purposes as follows. The entropy: 
These theses concisely capture how radically different black-hole
entropy is from ordinary thermodynamical entropy. The first, as is
already obvious from the Second Law of black hole mechanics,
underscores the fact that black-hole entropy is proportional to the
surface area of the system, not to the bulk volume as for ordinary
thermodynamical systems. The second articulates the fact that the
underlying entities whose statistics are conjectured to give rise to
the entropy are the constituents of perhaps the most fundamental
structure in contemporary physics, spacetime itself, not high-level,
derivative entities such as atoms, which are not fundamental in our
deepest theory of matter, quantum field theory. The third emphasizes
the fact that, contrary to the way that there is no
“natural” coarse-graining of underlying micro-degrees of
freedom in the statistical mechanics of ordinary matter, there is a
unique natural one here, intimately related to the fact that the
geometry of the event horizon is unique, and the Planck scale provides
a measure of units of area thought by many to be physically privileged
(albeit in a sense never made entirely clear). The fourth states that
the Second Law of black hole thermodynamics, generalized to include
contributions from both black holes and ordinary matter (as discussed
in
 section 5.4
 below), is not a phenomenologically derived empirical generalization,
as is the Second Law for ordinary matter; rather, it follows directly
from the most fundamental dynamical principle, quantum evolution, in
conjunction with the basic geometry of spacetimes in general
relativity. (This will be discussed further in
 section 6.2
 below.) This is of a piece with the fact that the Second Law for
black holes in the classical regime is a theorem of pure differential
geometry
 (section 5.1).
 
In so far as one takes Bekenstein entropy seriously as a true
thermodynamical entropy, then, these differences strongly suggest that
the extension of entropy to black holes should modify and enrich our
understanding not only of entropy as a physical quantity, but
temperature and heat as well, all in ways perhaps similar to what that
of the extension of those classical quantities to electromagnetic
fields did at the end of the 19th century (Curiel 2017a, 
Other Internet Resources).
 This raises immediate questions concerning the traditional
philosophical problems of inter-theoretic relations among physical
quantities and physical principles as formulated in different
theories, and in particular problems of emergence, reduction, the
referential stability of physical concepts, and their possible
incommensurability across theories. One could not ask for a more novel
case study to perhaps enliven these traditional debates. (See the
entries
 Scientific Unity,
 Scientific Reduction, and
 Intertheory Relations in Physics.)
 
Dougherty and Callender (2019) have challenged orthodoxy here, as
well, by arguing that the many ways in which the area of a black hole
does not behave like classical entropy strongly suggests that we
should be skeptical of treating it as such. Curiel (2017b, Other
Internet Resources) attempts to rebut them using exactly the idea that
any extension of a known physical quantity into a new regime will
inevitably lead to modifications of the concept itself, and
emendations in the relations it may enter into with other physical
quantities. Thus, we should expect that black-hole entropy will not
behave like ordinary entropy, and it is exactly those differences that
may yield physical and philosophical insight into old puzzles. 
In the context of thermodynamic systems containing black holes, one
can easily construct apparent violations of both the ordinary laws of
thermodynamics and the laws of black holes if one applies these laws
independently of each other. So, for example, if a black hole emits
radiation through the Hawking effect, then it will lose mass—in
apparent violation of the classical Second Law of black hole
mechanics. Likewise, as Bekenstein argued, we could violate the
ordinary Second Law of thermodynamics simply by dumping matter with
high entropy into a black hole: for then the outside of the black
hole, a causally isolated system, spontaneously decreases in entropy.
The price of dropping matter into the black hole, however, is that its
event horizon will increase in size. Likewise, the price of allowing
the event horizon to shrink by giving off Hawking radiation is that
the entropy of external matter fields will increase. This suggests
that we should formulate a combination of the two laws that stipulates
that the sum of a black hole's area and the entropy of external
systems can never decrease. This is the Generalized Second Law of
thermodynamics (Bekenstein 1972, 1973, 1974). 
The Second Law of ordinary thermodynamics has a long, distinguished,
and contentious history in the Twentieth Century debates about the
philosophical foundations of physics, ramifying into virtually every
important topic in the philosophy of physics in particular, and into
many important topics in philosophy of science in general, including:
the relation between thermodynamics and statistical mechanics; the
Measurement Problem of quantum mechanics, and the status and meaning
of theories of quantum information and computation; the definition of
various arrows of time and the relations among them; the so-called
Past Hypothesis in cosmology; determinism; causation; prediction
versus retrodiction; the nature of reasoning based on idealization and
approximation; emergence and reduction; and problems with theory
confirmation. 
That black holes and other purely gravitational and geometrical
systems possess an entropy naturally leads to the idea that the Second
Law of thermodynamics ought to be modified in order to accommodate that
fact. It is an almost completely unexplored issue how this Generalized
Second Law itself may require modifications to the traditional
questions about the Second Law, and possibly lead to new insights
about them. Thus the postulation of the Generalized Second Law and its
broad acceptance by the physics community raises many interesting
puzzles and questions. 
In the remainder of this section, we will review the issues raised by
the Generalized Second Law that bear on those puzzles and questions,
namely that: contrary to the case in classical thermodynamics, the
Generalized Second Law admits not only of proof, but of many kinds of
proof
 (Section 5.4.1);
 several different physically plausible mechanisms have been proposed
that seem to violate the Generalized Second Law
 (Section 5.4.2)
 under relatively benign conditions; the Generalized Second Law seems
to allow for the possibility of formulating and proving the existence
of a universal bound on the amount of entropy any physical system can
have, along with a related constellation of ideas known as
‘holography’
 (Section 5.4.3);
 and, contrary to the Second Law of classical thermodynamics, the
Generalized Second Law seems to imply novel and deep propositions of
interest in their own right
 (Section 5.4.4).
 The possible connection of the Generalized Second Law to the arrow of
time is discussed in
 Section 7
 below. 
The ordinary Second Law of thermodynamics is, at bottom, an empirical
generalization based on observation of the behavior of ordinary
material systems, albeit one with confirmation and thus entrenchment
more profound than probably any other single principle in all of
physics. One of the most remarkable features of the Generalized Second
Law, by contrast, is that it seems to admit of proof in ways much more
mathematically rigorous than does the ordinary Second Law (such as,
e.g., the proof of Flanagan et al. 2000, in the
context of classical general relativity and theories of matter, and a
number of proofs in different contexts given in and discussed by Wall
2009). That already raises interesting philosophical questions about
the relations between what seems prima facie to be the
“same” fundamental principle as formulated, evaluated and
interpreted in different physical theories. 
At least as interesting, from both a physical and a philosophical
point of view, is the fact that the Generalized Second Law in fact
admits a wide variety of different ways of being proven (Wall 2009).
Some of those ways are more mathematically rigorous than others, some
more physically perspicuous and intuitive, some more general, and
almost all have their respective validity in different regimes than
the others, using different types of physical systems, different
approximations and idealizations, and different physical and
mathematical starting points. “Proofs” have been given,
for example, in the classical, hydrodynamic, semiclassical, and full
quantum gravity regimes of black holes. 
Although the results of all those proofs are called by the same
name—the Generalized Second Law—they seem prima
facie to be different physical principles, just because of the
extreme differences in the assumptions and content of their respective
proofs. Here is just a sample of some of the many questions and issues
one must take a stand on in order to formulate a version of the
Generalized Second Law and attempt to prove it. 
The dizzying variety of proofs on offer, which can be roughly
classified by how each answers these (and other related) questions,
thus prompts the question: what is the relation among all the
different principles actually derived by each proof? Do they represent
the same physical principle as it manifests itself in different
regimes, and as it is viewed from different perspectives? Again, the
answer one gives to this question will depend sensitively on,
inter alia, one's views on inter-theoretic relations. Indeed,
because different answers to these questions can lead to
“proofs” that have, respectively, contradictory
assumptions, one may well worry that the derived principle, if it is
to be the same in all cases, will turn out to be a tautology! 
Even putting aside the contradictory assumptions used in different
derivations, one should, in any event, note that one cannot try to
justify the multifariousness of proofs by using an argument based on
something like consilience, for it will not be consilience in anything
like the standard form. (See the entry on
 Scientific Discovery.)
 This is not a case in which the same equations or relations or model,
or values of quantities, are being derived for a given phenomenon
based on studies of different types of interactions among different
types of physical systems, as in the classic case of Perrin's
calculation of Avogadro's number. This is rather a case in which
different physical assumptions are made about the very same class of
physical systems and interactions among them, and calculations and
arguments made in very different physical and mathematical frameworks,
with no clear relation among them. 
When Bekenstein first proposed that a black hole should possess
entropy, and that it should be proportional to its area, difficulties
that appeared insurmountable immediately appeared. In a colloquium
given at Princeton at 1970, Geroch proposed a mechanism that seemed to
show that, if one could attribute a temperature to a black hole at
all, it should be absolute zero; an immediate consequence of the
working of the mechanism showed that to do otherwise would seem to
allow arbitrarily large violations of what was to become known as the
Generalized Second
 Law.[14]
 Far away from a black hole, prepare an essentially massless box to be
full of energetic radiation with a high entropy; then the mass of the
radiation will be attracted by the black hole's gravitational force.
One can use this weight to drive an engine to produce energy
(e.g., to produce friction from the raising of a
counter-weight) while slowly lowering the box towards the event
horizon of the black hole. This process extracts energy, but not
entropy, from the radiation in the box. One can then arrange for all
the mass-energy of the radiation to have been exhausted when the box
reaches the event horizon. If one then opens the box to let the
radiation fall into the black hole, the size of the event horizon will
not increase (because the mass-energy of the black hole does not
increase), but the thermodynamic entropy outside the black hole has
decreased. Thus we seem to have violated the Generalized Second Law.
Many ways to try to defuse the problem have been mooted in the
literature, from entropy bounds (discussed below in
 section 5.4.3)
 to the attribution of an effective buoyancy to the object being
lowered due to its immersion in radiation generated by its
acceleration (Unruh and Wald 1982), a consequence of the so-called
Unruh effect (for an account of which, see
 note 16).
 None of them is completely satisfying. 
The question of whether we should be troubled by this possible
violation of the Generalized Second Law touches on several issues in
the foundations of physics. The status of the ordinary Second Law is
itself a thorny philosophical puzzle, quite apart from the issue of
black holes. Many physicists and philosophers deny that the ordinary
Second Law holds universally, so one might question whether we should
insist on its validity in the presence of black holes. On the other
hand, the Second Law clearly captures some significant
feature of our world, and the analogy between black holes and
thermodynamics seems too rich to be thrown out without a fight.
Indeed, the Generalized Second Law is the only known physical law that
unites the fields of general relativity, quantum mechanics, and
thermodynamics. As such, it seems currently to be the most promising
window we have into the most fundamental structures of the physical
world (for discussion of which, see
 section 6.3
 below). 
In response to the apparent violation of the Generalized Second Law
consequent on Geroch's proposed process, Bekenstein postulated a limit
to how much entropy can be contained in a given region of spacetime in
order to try to avoid such seeming violations, the limit being given
by the entropy of a black hole whose horizon would encompass the
region. Current physics imposes no such limit, so Bekenstein (1981)
postulated that the limit would be enforced by the underlying theory
of quantum gravity that, it is hoped, black hole thermodynamics
provides our best current insight into. There is, moreover, a further,
related reason that one might think that black hole thermodynamics
implies a fundamental upper bound on the amount of entropy that can be
contained in a given spacetime region. Suppose that there were more
entropy in some region of spacetime than the Bekenstein entropy of a
black hole of the same size. Then one could collapse that entropic
matter into a black hole, which obviously could not be larger than the
size of the original region (or the matter would have already
collapsed to form a black hole). But this would violate the
Generalized Second Law, for the Bekenstein entropy of the resulting
black hole would be less than that of the matter that formed it. Thus
the Generalized Second Law itself appears to imply a fundamental limit
on how much entropy a region can contain (Bekenstein 1983; Bousso
1999a, 2006). If this is right, it seems to be a deep insight into the
fundamental structure of the world, and in particular it should
provide an important clue to the nature of an adequate theory of
quantum gravity. 
Arguments along these lines led ’t Hooft (1993, in
 OIR)
 to postulate the Holographic Principle (though the name is due to
Susskind 1995). This principle claims that the number of fundamental
degrees of freedom in any spherical spatial region is given by the
Bekenstein entropy of a black hole of the same size as that region.
The Holographic Principle is notable not only because it postulates a
well-defined, finite number of degrees of freedom for any region, but
also because this number grows in proportion to the area surrounding
the region, not the volume. This flies in the face of the standard
picture of the dynamics of all other known types of physical systems,
whether particles or fields. According to that picture, the entropy is
measured by the number of possible ways something can be, and that
number of ways increases as the volume of any spatial region. To the
contrary, if the Holographic Principle is correct then one spatial
dimension of any physical system can, in a sense, be viewed as
superfluous: the fundamental “physical story” of a spatial
region is actually a story that can be told merely about the boundary
of the region (Luminet 2016). 
Still, there are reasons to be skeptical of the validity of the
proposed universal entropy bounds, and the corresponding Holographic
Principle. Unruh and Wald (1982), in response to Bekenstein's
postulated entropy bound, argued convincingly that there is a less
ad hoc way to save the Generalized Second Law, namely by
exploiting the Unruh effect (for an explanation of which, see
 note 16).[15]
 Flanagan et al. (2000), moreover, have offered strong
arguments that the validity of the Generalized Second Law is
independent of Bousso's proposed entropy bound (widely thought to be
superior to Bekenstein's original one), thus removing much of the
primary historical and conceptual motivation for the Holographic
Principle. 
Again, all these questions are of great interest in their own right in
physics, but there is strong reason to believe that their analysis may
shed new light on several ongoing philosophical discussions about the
nature of spacetime, with which they have obvious direct connections,
especially concerning the dimensionality of space and spacetime, and
the substantivalism-versus-relationalism debate. The interested reader
should see de Haro et al. (2015) for a discussion of the
relation of holography to gauge/gravity dualities in general, and a
review of the philosophical issues that raises, and Castellani (2016)
for philosophical discussion of the ontological issues raised by these
dualities. 
The ordinary Second Law has profound philosophical implications. It
is, however, rarely if ever used to prove other physical principles or
results of real depth, all of its important consequences being more or
less immediate. Once again, the Generalized Second Law stands in
contrast to the ordinary Law, for, as has recently been realized, it
can be used to prove several physical results of deep interest, over
and above heuristically motivating the Holographic Principle. 
In a tour de force of physical argument, Wall (2013a, 2013b)
showed that assumption of the Generalized Second Law rules out
traversable wormholes, other forms of faster-than-light travel between
distant regions, negative masses for physical systems, and closed
timelike curves. (See the Encyclopedia entries
 Time Machines
 and
 Time Travel,
 and Visser 1996.) Furthermore, if the Generalized Second Law is to be
satisfied, then it is impossible for “baby universes” that
eventually become causally independent of the mother universe to form.
Such baby universes and their eventual independence, however,
constitute the fundamental mechanism for currently popular
“multiverse” scenarios in cosmology. 
In the same work, Wall also shows that the Generalized Second Law has
a striking positive conclusion: a “quantum singularity
theorem”, which shows that, even when quantum effects are taken
into account, spacetime will still be geodesically incomplete inside
black holes and to the past in cosmological models (like the currently
most well supported ones, which start with a Big Bang singularity).
This flies directly in the face of the pious hopes of most physicists
that quantum effects, and in particular the hoped-for theory of
quantum gravity, will efface singularities from spacetime. (See,
e.g., Ashtekar and Bojowald 2006, Ashtekar et al.
2006, and Kiefer 2010 for typical sentiments along these lines, along
with typical arguments forwarded to support them, in the context of
canonical quantum gravity, and Roiban 2006 and Das 2007 for the same
in the context of string theory; it is noteworthy that Roiban also
discusses known cases where it appears that string theory does
not necessarily efface singularities.) 
Another striking positive consequence of the Generalized Second Law is
that it allows one to derive energy conditions in the context of
general relativity. An energy condition is, crudely speaking, a
relation one demands the properties of matter to satisfy in order to
try to capture the idea that “mass-energy should be
positive”. Energy conditions play a central and fundamental role
in general relativity, since they are needed as assumptions in
essentially every deep, major result proven in the last 60 years,
especially those pertaining to singularities and black holes (Curiel
2017). One thing that makes them unusual is the fact that, uniquely
among the central and fundamental tenets of general relativity, they
themselves do not admit of derivation or proof based on other such
principles. At least, no such derivations or proofs were known until
very recently, when Wall (2010) argued that the Generalized Second Law
implies one. There are several problematic aspects to Wall's argument
(Curiel 2017), but the mere fact that he was able to produce a
prima facie decent one at all is remarkable, showing that the
Generalized Second Law may be a very deep physical principle indeed.
One, however, may contrarily conclude that the argument shows rather
that the Generalized Second Law is a contingent matter, depending
sensitively on the kinds of matter fields that actually exist—if
matter fields were such as to violate the energy condition Wall argued
for, then his argument would show that the Generalized Second Law is
not valid. 
Finally, Bousso et al. (2016) showed that a form of the
Generalized Second Law applicable to generalized horizons strongly
suggests that causal geodesics in the regime where quantum field
theory effects become important will focus and converge on each even
when the standard energy conditions are violated. This is significant
because it is propositions about the focusing properties of geodesics
that lie at the heart of all the standard singularity theorems and
most other results about horizons of all kinds, and all of the
propositions that show focusing assume a standard energy condition. If
this conjecture is correct, it would provide further strong evidence
that quantum effects may not remove singularities from generic
spacetimes. 
That black holes, purely gravitational objects, possess a physical
entropy strongly suggests that the gravitational field itself in
general may possess entropy, as Penrose (1979) hypothesized. Indeed,
there are a number of reasons to suspect that the thermodynamical
character of gravity should extend to gravitational systems and
structures beyond just those provided by black holes. Because
gravitational “charge” (i.e., mass-energy) comes
with only one sign (as opposed, e.g., to electromagnetic
charge, which can be of either positive or negative sign), bits of
matter tend to accelerate towards each other, other things being
equal. This already suggests that gravity has a built-in
thermodynamical character, since it provides an objective, invariant
measure of a direction for time: it is characteristic of future
time-flow that bits of matter tend to accelerate towards each other,
and so become more inhomogeneous in the aggregate. (See
 section 7
 for discussion of the possible relation of gravitational entropy to
an arrow of time.) 
Since the work of Gibbons and Hawking (1977), Bekenstein (1981),
Smolin (1984), Bousso (1999a), Jacobson and Parentani (2003), and
Padmanabhan (2005), among others, it has been known that an entropy
and a temperature can be attributed to spacetime horizons more general
than just the event horizon of a black hole. From another direction,
the attractiveness of Penrose's Conformal Curvature Hypothesis
(Penrose 1979), discussed below in
 section 7,
 along with subsequent work attempting to make the Hypothesis precise,
all suggest that certain types of cosmological singularities, such as
the Big Bang, should themselves be attributed an entropy. This has led
in recent years to several interesting proposals for a completely
general measure of gravitational entropy, such as that of Clifton
et al. (2013). Indeed, Anastopoulos and Savvido (2012) have
even attempted to attribute entropy directly to non-cosmological
singularities, those associated with collapse phenomena. Pavón
and Zimdahl (2012), in a similar spirit, attempt to provide a
thermodynamical analysis of future cosmological singularities and so
characterize them by their thermodynamical properties. 
These facts raise several fundamental puzzles about the nature of
entropy as a physical quantity and the relations among the different
theories that involve it. How can such a quantity, which hitherto has
been attributed only to material systems such as fluids and Maxwell
fields, be attributed to simple regions of spacetime itself? How does
general gravitational entropy relate to more standard forms of
entropy, and how may the nature of general gravitational entropy
itself inform our understanding of the standard forms? Does it shed
new light on traditional general philosophical topics of interest,
such as questions about reduction and emergence of thermodynamics to
and from statistical mechanics? 
As discussed already in
 Section 5.2
 and
 Section 5.3,
 it is the addition of quantum field theory to general relativity that
definitively settles the issue of the thermodynamical character of
black holes. There are, however, many other fascinating phenomena that
arise when one adds quantum field theory to the mix of black holes and
singularities, and general relativity in general, than just that, and
a concomitant broadening and deepening of the philosophical issues and
puzzles that confront us. 
In
 Section 6.1,
 we discuss the Hawking effect (the predicted emission by black holes
of thermal radiation) and its associated problems and puzzles in
detail. One puzzle in particular that seems to follow the prediction
of the Hawking effect has exercised physicists and philosophers the
most, the so-called Information Loss Paradox: the evaporation of black
holes by the emission of Hawking radiation seems to lead in the end to
a violation of one of the most fundamental tenets of quantum
mechanics. We discuss that in
 Section 6.2.
 We conclude in
 Section 6.3
 with an examination of the claims that black hole thermodynamics
provides the best evidence to guide us in the search for a theory of
quantum gravity. 
In light of the notorious difficulty of constructing a theory that
incorporates and marries quantum mechanics and general
relativity—a theory of quantum gravity—it may come as a
surprise to learn that there is a consistent, rigorous theory of
quantum fields posed on the background of a classical relativistic
spacetime. (Wald 1994 is a standard text on the subject; Jacobson
[2003, in
 OIR]
 gives a less rigorous overview, discussing possible relations to
proposed theories of quantum gravity; Wald [2006b, in
 OIR]
 gives a synoptic history of the technical aspects of the entire
subject, and an exposition of the advances in the field subsequent to
the publication of Wald 1994; and Hollands and Wald 2015 provides a
technically sophisticated overview of the most recent results.)
Quantum field theory on curved spacetime, however, differs from
standard quantum field theory (set on the flat Minkowski spacetime of
special relativity) in one profound respect, that difference ramifying
into every part of the theory: a generic relativistic spacetime has no
group of symmetries comparable to the Poincaré Group for
special relativity. There is correspondingly no distinguished vacuum
state and no natural notion of a particle. This means, for instance,
that one cannot employ many familiar and useful techniques from
standard quantum field theory, and one must take care in the use of
most of the others. 
One expects that such a framework would find its most natural
application in the treatment of problems in which, in some sense or
other, the curvature of spacetime is well above the Planck length, in
so far as there are some theoretical grounds for suspecting that in
this regime one can safely ignore any quantum properties of the
spacetime geometry itself. (Hence, the framework is often called
‘the semi-classical approximation’ or
‘semi-classical gravity’.) In this vein, its most popular
and successful applications have been to problems involving particle
creation in the early universe and in the vicinity of black holes.
Now, according to general relativity a black hole ought to be a
perfect sink for energy, mass and radiation, in so far as it absorbs
everything (including light), and emits nothing (including light). It
was therefore more than shocking when Hawking (1974, 1975) predicted
that, when quantum effects are taken into account, a black hole ought
to behave rather like a perfect black body, in the sense of ordinary
statistical thermodynamics: a stationary black hole should emit
thermal radiation with the Planckian power spectrum characteristic of
a perfect blackbody at a fixed temperature. It glows like a lump of
smoldering coal even though light should not be able to escape from
 it![16]
As with the Generalized Second Law, one of the most fascinating
aspects of Hawking radiation from a foundational point of view is the
multiplicity and multifariousness of the derivations known for it.
They also differ radically among themselves with regard to the
mathematical rigor of the framework they adopt and the mathematical
character of the structures they assume, and almost all are valid in
different regimes than the others, using different types of physical
systems and different approximations and idealizations, basing their
arguments on different physical principles, with varying degrees of
physical perspecuity and intuitiveness. In consequence, these
different derivations seem to suggest different physical
interpretations of Hawking radiation itself, both for its origin and
for its character (Brout et al. 1995). It is thus not even
clear, at a foundational level, what the physical content of the
prediction of Hawking radiation is. Indeed, as in the case of the
Generalized Second Law, some of the derivations of Hawking radiation
make assumptions that seem to contradict some of the assumptions of
other derivations—but if A implies B and
not-A implies B, then B must be a tautology.
Since this is an unappealing attitude to take towards Hawking
radiation, some other way must be found to reconcile the contrary
derivations. Again, standard consilience cannot be invoked here, for
the same reasons as discussed at the end of
 section 5.4.1
 for different proofs of the Generalized Second Law. 
Because the interpretation of quantum field theory itself, even in the
flat spacetime of special relativity, is already so contested, fraught
with problems, and just poorly understood in general (see the
Encyclopedia entry
 Quantum Field Theory),
 one may think that there is even less of a chance here to get a grip
on such issues. Contrarily, one may also think that the very fact that
the phenomena are so different here than in ordinary quantum field
theory may suggest or afford us new avenues of approach to the
traditional problems that have so long frustrated us. 
The existence of Hawking radiation has a remarkable consequence: as
Hawking (1976) pointed out and Unruh (1976) elaborated, the fact that
a black hole radiates implies that it loses mass-energy, and so will
shrink, in seeming violation of the Area Theorem. (The Area Theorem is
not in fact violated; rather, one of its assumptions is,
viz., that locally energy is always strictly positive.)
Because there is no limit to this process except that imposed by the
total initial mass of the black hole itself, eventually the black hole
will radiate itself entirely away—it evaporates. This prediction
clearly bears on the issue of cosmic censorship: if the end-state of
the evaporation leaves the previously hidden singularity open for the
rest of the universe to see, all the potential problems raised in
 section 4
 can arise. 
There is, however, a seemingly even deeper problem posed by the
possibility of black-hole evaporation, one that raises doubts about
the possibility of describing black holes using any standard
formulation of quantum theory. According to standard quantum theory,
the entropy of a closed system never changes; this is captured
formally by the nature of the evolution of a quantum system, by the
technical property of unitarity. Unitary evolution guarantees that the
initial conditions, together with the Schrödinger equation (the
equation governing the temporal evolution of quantum systems), will
fix the future state of the system. Likewise, a reverse application of
the Schrödinger equation will take us from the later state back
to the original initial state. In other words, the states at each time
contain enough information to fix the states at all other times, given
the unitarity of dynamical evolution for quantum systems. Thus there
is a sense in which the completeness of the state is
maintained by the standard time evolution in quantum theory. (See the
entry
 Quantum Theory.)
 
It is usual to characterize this feature by the claim that quantum
evolution “preserves information”. If one begins with a
system in a precisely known quantum state, then quantum theory
guarantees that the details about that system will evolve in such a
way that one can infer the precise state of the system at some later
time, and vice versa. This quantum preservation of details implies
that if we burn a chair, for example, it would in principle be
possible to perform a complete set of measurements on all the outgoing
radiation, the smoke, and the ashes, and reconstruct exactly what the
chair looked like. If we were instead to throw the chair into a black
hole, however, then orthodoxy holds that as a consequence of the No
Hair Theorems (discussed in
 section 3.2
 above) it would be physically impossible for the details about the
chair ever to escape to the outside universe. This might not be a
problem if the black hole continued to exist for all time, since one
could then assume the information encoded in the chair still existed
behind the event horizon, preserved by the unitary evolution in that
region. The existence of Hawking radiation, however, tells us that the
black hole is giving off energy, and thus it will shrink down and
presumably will eventually disappear altogether, along with whatever
stuff had fallen past the event horizon before that. At that point,
the details about the chair will be irrevocably lost; thus such
evolution cannot be described by the standard laws of quantum theory.
This is the Information Loss Paradox of quantum black holes
(Hawking
 1976).[17]
 Although the paradox is usually formulated in terms of
“information”, the issues is often put here as being the
maintenance of correlations between different systems, as this is a
physically more perspicuous notion that lies at the bottom of the
paradox, and is much less problematic than the notoriously vexed and
nebulous concept of “information”. 
The attitude that individual physicists adopt towards this problem is
strongly influenced by their intuitions about which theory, general
relativity or quantum theory, will have to be modified to achieve a
consistent theory of quantum gravity. Spacetime physicists tend to
view non-standard quantum evolution as a fairly natural consequence of
singular spacetimes: one would not expect all the details of systems
at earlier times to be available at later times if they were lost in a
singularity. Hawking (1976), for example, argued that the paradox
shows that the full theory of quantum gravity will be a theory that
does not obey the standard dynamical principles of quantum theory, and
he began working to develop such a theory very soon after first
promulgating the paradox. (He has since abandoned this position.)
Unruh and Wald (2017) develop an extended review and defense of this
position. Particle physicists (such as superstring theorists),
however, tend to view black holes as being just another state of a
quantum field. If two particles were to collide at extremely high
energies, they would form a very small black hole. This tiny black
hole would have a very high Hawking temperature, and thus it would
very quickly give off many high-energy particles and disappear.
(Recall, as discussed in
 section 5.2
 above, that Hawking temperature is inversely proportional to the mass
of black hole.) Such a process would look very much like a standard
high-energy scattering experiment: two particles collide and their
mass-energy is then converted into showers of outgoing particles. The
fact that all known scattering processes obey the standard dynamical
principles of quantum theory, and above all unitarity, then, seems to
give us some reason to expect that black hole formation and
evaporation should also do so. 
The reactions to the puzzle are legion. (A helpful overview of earlier
stages of this debate can be found in Belot et al. 1999.) It
is useful to classify them as belonging to one of six broad groupings:
In particular, today there are four main ways of trying to address the
problem that have a fair amount of support in different segments of
the physics community: 
We will briefly sketch each of them, along with their pros and cons.
Chakraborty and Lochan (2017), Bryan and Medved (2017), Marolf (2017),
and Unruh and Wald (2017) provide recent reviews of the most popular
approaches, with Marolf's emphasizing possible approaches that save
unitarity, and Unruh and Wald's emphasizing ones that violate it. (See
Mathur 2009 and Chen et al. 2015 for recent discussions of
approaches based on remnants, which we will not cover here.) 
The arguments that we should accept the calculations that predict
failure of unitarity at face value are straightforward (Unruh and Wald
2017). The calculations represent a regime (the semi-classical one) in
which we have good theoretical grounds for trusting our theoretical
machinery, and nothing is required that deviates from standard
applications of quantum field theory and general relativity,
respectively. Even though there is failure of unitarity, there is no
violation of conservation of probability—all quantum
probabilities sum to 1 over the course of the entire
evolution—and there is no other manifest form of indeterminism
present. Nor is there any violation of energy conservation attendant
on the failure of unitarity, as some have alleged must happen. Unitary
evolution, moreover, is arguably not a fundamental tenet of
quantum theory: so long as probability is conserved, one can calculate
with confidence. Indeed, there are examples of just such non-unitary,
but probability-conserving and energy-conserving evolution in standard
applications of ordinary quantum theory, with no need for anything as
high-falutin' as quantum field theory on curved spacetime and black
hole thermodynamics (Unruh 2012). 
The conclusion, however—that what many still take to be one of
the most fundamental principles of quantum theory is violated—is
too distasteful for many physicists to swallow, especially those
trained in the tradition of particle physics, where unitarity is taken
to be inviolate. The sanguine acceptance of the loss of unitarity
seems to come mostly from the trust the physicists in question have in
general relativity. This raises the question why general relativity
ought to be trusted enough in this regime to conclude that unitarity
will fail in any deeper quantum theory, but not trusted enough when it
comes to the prediction of singularities
 (section 2.2)—on
 what grounds do they pick and choose when and when not to trust it?
This question becomes especially piquant when one considers that loss
of unitarity is, on its face, an extraordinarily strong constraint to
place on any proposed theory of quantum gravity, especially when it
comes from a calculation made in the context of a merely effective and
not a fundamental theory, and when it is exactly that still unknown
fundamental theory that is supposed to efface singularities. In any
event, Manchak and Weatherall (2018) have recently argued that, even
if one does accept loss of unitarity—what seems to be a
straightforward conclusion of the standard calculations—the
state of affairs is still justly called paradoxical. 
The idea of black-hole complementarity, initiated by Susskind et
al. (1993), tries to resolve the paradox by pointing out that the
self-description of the experience of an astronaut falling into a
black hole will differ from the description made by an external
observer, and then playing the contrary descriptions off each other in
a dialectical fashion. It has been the subject of philosophical
controversy because it includes apparently incompatible claims, and
then tries to reconcile them by appeal to a form of so-called quantum
complementarity, or (so charge the critics) simple-minded
verificationism (Belot et al. 1999). An outside observer will
never see the infalling astronaut pass through the event horizon.
Instead, she will seem to hover just above the horizon for all time
(as discussed in
 section 3.1
 above). But all the while, the black hole will also be giving off
heat, shrinking, and getting ever hotter. The black hole
complementarian therefore suggests that an outside observer should
conclude that the infalling astronaut gets burned up before she
crosses the event horizon, with the result that all the details about
her state will be returned in the outgoing radiation, just as would be
the case if she and her belongings were incinerated in a more
conventional manner; thus the information (and standard quantum
evolution) is saved. 
This suggestion, however, flies in the face of the fact that for an
infalling observer, nothing out of the ordinary would be experienced
at the event horizon (as discussed in
 section 3.1
 above). Indeed, in general she could not even know that she was
passing through an event horizon at all, unless classical general
relativity were very wrong in regimes where we expect no quantum
effects to show themselves. This obviously contradicts the suggestion
that she might be burned up as she passes through the horizon. The
black hole complementarian tries to resolve this contradiction by
agreeing that the infalling observer will notice nothing
remarkable at the horizon, but then suggests that the account of the
infalling astronaut should be considered to be complementary to the
account of the external observer, rather in the same way that position
and momentum are complementary descriptions of quantum particles
(Susskind et al. 1993). The fact that the infalling observer
cannot communicate to the external world that she survived her passage
through the event horizon is supposed to imply that there is no
genuine contradiction here. This solution to the information
loss paradox has been criticized for making an illegitimate appeal to
verificationism (Belot et al. 1999). Bokulich (2005), to the
contrary, argues that the most fruitful way of viewing black hole
complementarity is as a novel suggestion for how a non-local theory of
quantum gravity will recover the local behavior of quantum field
theory while accommodating the novel physics of black holes. 
Almheiri et al. (2013) have recently claimed that black hole
complementarity is not viable on different, more physically oriented
grounds. They argue that the following three statements, assumed by
black-hole complementarity, cannot all be true: (i) Hawking radiation
is in a pure state; (ii) the information carried by the radiation is
emitted from the region near the horizon, with low energy effective
field theory (i.e., the standard semi-classical
approximation) valid beyond some microscopic distance from the
horizon; and (iii) the infalling observer encounters nothing unusual
at the horizon. Based on powerful grounds for believing the first two
propositions, they conclude that the appropriate response to the
paradox is to posit that there is a “firewall” at the
event horizon: the flux of Hawking radiation from the black hole
becomes in general so intense that highly accelerated infalling bodies
are themselves incinerated as soon as they enter the black hole. This
proposal is too recent for any consensus to have been reached about
its viability; vigorous debate about it is ongoing. 
Perhaps the physically most conservative—and correlatively the
philosophically least thrilling—proposal is to deny the implicit
assumption that during black-hole evaporation the deviations of
Hawking radiation from exact thermality are negligible. Thus the
problem prima facie does not ever arise, because all the
quantum information does manage to escape in those non-thermal
correlations. This proposal faces the serious challenge of showing
that such non-thermal corrections are rich and large enough to carry
away all possible information encoded in all possible bodies falling
into black holes. Hawking et al. (2016) argue, in this vein,
that black holes do indeed have hair, violating the No Hair theorems,
which makes possible the maintenance of correlations between early and
late time Hawking radiation in such a way as to preserve information.
Dvali (2015) argues that exact thermality of Hawking radiation, in
conjunction with other well established results about black hole
thermodynamics and quantum field theory on curved spacetime, imply
that the black-hole entropy would be infinite; thus, he concludes,
there must be large deviations from thermality. Any such
argument, note, will have to conclude that the deviations from perfect
thermality are large—otherwise there would be no hope of
encoding enough information to record recovery data about every
physical system that fell into the black hole before evaporation.
Again, the particular arguments in favor of this sort of proposal are
too recent for real consensus one way or another to have been
achieved. 
The evaporation of black holes has another startling consequence that
raises far-reaching philosophical and physical problems for our
current picture of quantum field theory and particle physics: it
implies that baryon and lepton number need not be conserved. Suppose a
neutron star composed of ∼1057 baryons collapses to
form a black hole. After evaporation, the resultant baryon number is
essentially zero, since it is overwhelmingly likely that the black
hole will radiate particles of baryon number zero. (The radiation is
not energetic enough to produce baryons, until, perhaps, the very late
stages of the evaporation.) This issue seems not to have agitated
researchers in either the particle physics or the general relativity
community so much as the idea of non-standard quantum evolution even
though conservation of baryons and leptons are surely principles as
well entrenched as that of the unitarity of quantum
 evolution.[18]
 One could perhaps argue that they are even more entrenched, since our
empirical evidence for the conservation principles is simple and
immediate in a way that our evidence for standard evolution is not:
one simply counts particles before and after an observed
interaction—interpretational questions arising from the
Measurement Problem in quantum theory and a possible
“collapse” of the wave function do not bear on it. (See
the entry
 Quantum Mechanics.)
 
Okon and Sudarsky (2017) have in fact recently argued that there is an
intimate connection between the Information Loss Paradox and the
Measurement Problem in quantum mechanics. Their arguments raise
further questions about the Information Loss Paradox. Why are
physicists so exercised by the possible violation of unitarity
seemingly entailed by black-hole evaporation, when almost all of those
self-same physicists do not worry at all about the Measurement Problem
of quantum mechanics, and the seeming violations of unitarity that
happen every time a measurement is performed? One possible explanation
is perhaps best described as “sociological”: most
theoreticians, as the ones involved in this debate, never model
experiments, and so do not face the Measurement Problem directly in
their work. Thus it is generally not an issue that is at the forefront
of their thought. Along the same lines, many theoreticians in this
area also work in cosmology, in which one considers the “wave
function of the universe”, an object that seems not to admit of
external observers making measurements on it, and so the issue of
collapse does not arise in their work. Perhaps a more intriguing
explanation, one not discussed by Okon and Sudarsky, is that the
Information Loss Paradox provides an explicit physical mechanism for
violations of unitarity. It is perhaps easier to dismiss seeming
violations of unitarity during measurements as an artifact of our lack
of understanding of quantum mechanics, not as a reflection of what
happens in the world. One cannot dismiss the possible violation of
unitarity in the Information Loss Paradox with such equanimity: it
appears to be an integral, explicit part of a model of the behavior of
a physically possible system, with an articulated mechanism for
bringing it about. 
Recently, Wallace (2020) has introduced philosophers to another
puzzle, intimately related to information loss in the context of
black-hole evaporation. For lack of a better term, and so as to
distinguish it from the standard problem, we call this
‘Page-time paradox’, as it was first formulated by Page
(1993), and turns on calculation of a distinguished time in the
evolution of an evaporating black hole, the so-called Page time, that
time at which half of the black hole's original entropy has been
radiated away. Page showed that there is a manifest inconsistency
between a treatment of black hole evaporation that is wholly
formulated in the terms of statistical mechanics, and the standard
semi-classical treatment used in derivations of Hawking radiation.
Wallace argues forcefully that this puzzle is incontrovertibly
paradoxical, completely divorced from the issue of whether or not
unitarity fails, and raises deep philosophical problems of its own.
In sum, the debate over the Information Loss Paradox highlights the
conceptual importance of the relationship between different effective
theories. At root, the debate is over where and how our effective
physical theories will break down: where can they be trusted, and
where must they be replaced by a more adequate theory? This has
obvious connections to the issue of how we are to interpret the
ontology of merely effective physical descriptions, and how we are to
understand the problems of emergence and reduction they raise. (See,
e.g., Williams 2017 for an interesting survey of such issues
in the context of quantum field theory on flat spacetime.) The
Information Loss Paradox ramifies into questions of ontology in other
ways as well. When matter forms a black hole, it is transformed into a
purely gravitational entity. When a black hole evaporates, spacetime
curvature is transformed into ordinary matter. Thus black holes appear
to be crucial for our understanding of the relationship between matter
and spacetime, and so provide an important arena for investigating the
ontology of spacetime, of material systems, and of the relations
between them. 
Black hole thermodynamics and results concerning quantum fields in the
presence of strong gravitational fields more generally are without a
doubt the most widely accepted, most deeply trusted set of conclusions
in theoretical physics in which our current best, deepest
theories—general relativity and quantum field theory—work
together in seemingly fruitful harmony. Indeed, that black holes
possess a physical temperature and entropy, and correlatively that
there is a hitherto unsuspected and profound connection among gravity,
quantum field theory and thermodynamics, is now as widely accepted an
idea in theoretical physics as an idea with no direct empirical
substantiation can be. As such, the study of black hole thermodynamics
prima facie holds out the most promise for guidance in our
search for a deeper theory of quantum gravity, in which the two would
be intimately combined in a unified account of all known physical
phenomena, from the behavior of quarks at the scale of
10-17 cm, to the cosmological structure of superclusters of
galaxies at the scale of 1032 cm. (See the entry
 Quantum Gravity.)
 What is not widely shared is the vision of the path that this
guidance purportedly shows us. 
I list only a small sample of the many foundational and philosophical
issues that arise here. A full discussion is beyond the scope of this
article. 
Wallace (2019) provides an overview of the relation of black hole
thermodynamics to a few programs in quantum gravity, especially those
related to string theory and the AdS/CFT correspondence, and
associated foundational problems. 
The Second Law of thermodynamics has long been connected to the
seeming asymmetry of the arrow of time, that time seems to flow, so to
speak, in only one direction for all systems no matter how different
in kind they are and no matter how spatiotemporally separated. Indeed,
one of the fundamental problems is that different types of system seem
prima facie to give rise to independent arrows of time,
e.g., thermodynamical, electromagnetic, cosmological, and so
on, with no a priori reason why they should all point in the
same direction. (See Zeh 2014 for a thorough recent review; see also
the Encyclopaedia entry
 Thermodynamic Asymmetry in Time.)
 The Generalized Second Law and the corresponding idea of general
gravitational entropy
 (section 5.5)
 introduces a new possible arrow of time, the gravitational. 
There is a peculiar and intimate relation between the Second Law of
ordinary thermodynamics and time. That physical systems always seem to
change in such a way that entropy never decreases picks out a
privileged direction in time, as it were. At the present moment, there
are two “directions” in time one may consider: that
pointing to the future, and that to the past. The Second Law says,
roughly speaking, that order never spontaneously increases toward the
future. Looking back towards the past, however, that is exactly how it
may appear to us: if one thinks of the ordinary change of physical
systems as running backwards in time, then it will exactly appear as
though order is spontaneously increasing. 
That fact is already on its own remarkable: all other known
fundamental principles and laws of physics are time-symmetric (putting
aside the minuscule violations of time-reversal symmetry manifested by
the weak nuclear force). That means that if a given sequence of
changes of a physical system governed by those principles and laws is
allowed, then the same sequence in reversed order is also allowed as a
physical possibility. If a tea cup drops to the floor and smashes into
little bits, then the reverse process is also possible: the smashed
bits can spontaneously leap up into the air towards each other and
re-assemble into an undamaged tea cup. If an antenna can absorb a
given type of radio wave, it can also emit that same wave. And so on.
That is not what we see in physical systems governed by thermodynamics
and the Second Law. An ice cube in a glass of warm water spontaneously
melts, and the water cools a bit. We never see a cool glass of water
spontaneously warm while an ice cube forms in the middle of it. This
is even more mysterious when one considers the fact that we know that
the water and ice are, at a deeper level of description, really just a
collection of an enormous number of molecules and atoms themselves
moving around, bouncing off each other, and connected
together—and, to the best of our knowledge, the principles and
laws governing the changes in that collection of molecules
are time symmetric. Why is it that the laws governing the
micro-structure of water and ice are time symmetric, but, when one
looks at the water and ice in the aggregate, ignoring the fine details
of the micro-structure, the governing principle becomes time
asymmetric? That is one of the deepest and most hotly debated
questions in the foundations of physics. 
This all raises a second question: if entropy tends only to increase,
and so order in the universe continually degrades, where did all the
order around us come from in the first place? Life, for instance,
seems like an extraordinarily highly structured phenomenon. Living
organisms are much more highly structured than the air and earth and
water surrounding us, and certainly more so than the food we consume
to build and replenish our highly structured bodies. The same holds
true of planets themselves, stars, galaxies, and clusters and
superclusters of galaxies—they all are prima facie much
more highly ordered and structured than the homogeneous and highly
rarefied plenum of interstellar dust surrounding them, and the vast
reaches of empty space itself. How did such highly structured physical
systems evolve in the first place? Are they all not manifestly a
violation of the Second Law? (See Schrödinger 1944 for the
locus classicus of discussion of these issues.) 
Indeed, the problem for physical systems on the cosmological scale
(planetary systems and larger) is made even more urgent by what we
know about conditions in the very early universe, very soon after the
Big Bang, that we think obtained at the start of the cosmos. We have
strong evidence that the very early universe consisted of a highly
homogeneous, extremely hot and condensed gaseous soup of fundamental
particles. According to ordinary thermodynamics, however, that is a
state of extremely high entropy. That such a physical system evolved
into ordered structures such as stars and galaxies—prima
facie a state of much lower entropy for the same matter and
energy now redistributed—seems on the face of it to be a massive
violation of the Second Law. 
One might with some justice ask: well, so what? Entrenched scientific
theories and principles get overthrown all the time. The caloric
theory of heat got overthrown by thermodynamics and the theory of
molecular kinetics. Classical Newtonian mechanics got overthrown by
quantum mechanics. Newtonian gravitational theory got overthrown by
general relativity. Now the evidence from cosmology tells us that the
Second Law is just one more in a long line of principles that have not
passed the test of confrontation with empirical data. That response,
however, does not do justice to the profound faith that physicists
have in the Second Law. When Einstein was once asked what he thought
physics would look like a century from then, he famously said he
thought nothing currently believed would still be held as fundamental,
except only the Second Law. Everything else—quantum theory,
general relativity—could go, but he could not imagine the Second
Law being overthrown. Contemporary physicists feel the same
 way.[19]
 They love the Second Law. There must be, they demand, a way to
reconcile the universality of the Second Law with its seeming
violation in the way the universe has evolved on cosmological scales.
What does all this have to do with black holes? At first glance,
nothing. On deeper reflection, however, quite a lot. Hawking's Area
Theorem, that black holes never decrease in size and can only
increase, is time asymmetric in the same way as the behavior of
ordinary physical systems governed by the Second Law. This, recall,
was the basis for the postulation of the Generalized Second Law, based
on the idea that black holes themselves possess entropy, itself one of
the motivating reasons that have led physicists to hypothesize that
the gravitational field in general, not just black holes, possess an
intrinsic entropy
 (section 5.5),
 as Penrose (1979) hypothesized. Indeed Penrose did far more than just
argue that the gravitational field itself possesses a generalized
entropy. He also proposed what has come to be known as the Conformal
Curvature Hypothesis, which states that the gravitational and
cosmological arrows of time are driven, if not determined, by this
generalized gravitational entropy. 
The existence of such a general gravitational entropy may provide a
key to answering the question about the development of stars,
galaxies, and other large-scale structure in the universe, as well as
the puzzle about the fact that the very early universe seems prima
facie to have been already a state of very high
 entropy.[20]
 Just as the thermodynamical behavior of ordinary matter picks out a
preferred direction in time, the idea goes, so does the way gravity
tends to shape the evolution of matter on cosmological scales, and,
moreover, it picks out the very same direction in time. If
one could show that the extremely homogeneous conditions of the very
early universe was a state of low gravitational entropy, and
that the current inhomogeneous clumping of matter into stars,
galaxies, etc., is a state of high gravitational entropy, and
that the difference in gravitational entropy is enough to
counterbalance the decrease in the entropy of ordinary matter as the
universe evolved from homogeneity to clumpiness, then one would have
saved the Second Law by replacing it with the Generalized Second Law.
And that is exactly what many physicists today think is the solution
to our problem: how to reconcile the appearance of an early state of
the universe of high entropy with the demanded universal validity of
the Second Law. 
As remarked above, Penrose (1979) started this suite of ideas when he
proposed the Conformal Curvature Hypothesis: that an entropy should be
attributed to the gravitational field proportional to some measure of
“purely gravitational” degrees of freedom, with a low
entropy attibuted to homogeneous and isotropic gravitational fields.
Some work in subsequent decades has been done, primarily based on
Goode and Wainwright (1985) and Newman (1993a, 1993b), to try to
generalize Penrose's proposal and make it rigorous. Almost all this
work has focused on the behavior of conformal singularities
(characterized at the end of
 section 1.3)
 which are, in a natural sense, “early” cosmological
singularities, such as the Big Bang, and on the behavior of various
measures of gravitational degrees of freedom moving to the future away
from such singularities. (There has been some work, such as Rudjord
et al. 2008, attempting to link the Conformal Curvature
Hypothesis directly to black-hole entropy.) The idea is that the
initial cosmological singularity, in accord with Penrose's Conformal
Curvature Hypothesis, had extraordinarily low entropy, thus
compensating the high entropy of the homogeneous ordinary matter
present then, making the early universe a state of low total entropy.
As the universe develops over time, and matter clumps into individual
system (stars, galaxies, clusters and super clusters of galaxies,
etc.), the entropy of ordinary matter seems to drop, but,
again, that is more than compensated for by the enormous increase in
gravitational entropy, thus saving the Generalized Second Law. 
This is all in accord with the so-called Past Hypothesis—the
need to postulate that the universe must have started in an extremely
special, low-entropy state—if one admits the existence of
generalized gravitational entropy. It has long been held by many
physicists and philosophers that the Past Hypothesis is the only way
to preserve the validity of the Second Law of thermodynamics over
cosmological scales (Albert 2000). Many philosophers and physicists
have balked at the Past Hypothesis, however, claiming it is
explanatorily vacuous or that it itself raises further difficult
questions, such as why the universe should have started in such a
“special and unlikely” state at all. (See Albert 2000,
Earman 2006, Callender 2010, and Wallace 2010 for discussion of many
of these issues, from competing perspectives.) Penrose (1979) put
forward the intriguing possibility that his Conformal Curvature
Hypothesis itself could point to an answer to all these questions: the
seemingly required “specialness” of the state of the early
universe may have a dynamical explanation in a more fundamental theory
of quantum gravity. As intriguing as that possibility may be, it by no
means has universal support. Wald (2006a), for example, gives
compelling arguments against the possibility that the low entropy of
the early state of the universe could have a dynamical origin. 
The Hawking temperature of a macroscopic black hole is unimaginably
small. For the black hole at the center of the Milky Way (Sagittarius
A*), approximately 4 million solar masses, it is
approximately 10-14 Kelvin. Even a black hole of one solar
mass would have a temperature of only about 60 billionths of a Kelvin.
Direct experimental verification of its existence therefore seems
beyond the realm of the imaginable, at least for macroscopic black
holes. (If nothing else, it would be utterly swamped just by the
ordinary cosmic microwave background radiation, itself approximately
2.7 Kelvin, a raging inferno in comparison.) 
In 1981, Unruh pointed out that a direct analogue of Hawking radiation
should occur in the most mundane and ordinary of physical systems,
flowing water (under particular conditions). The physical basis for
his idea is almost ridiculously simple: if water is flowing past a
boundary more rapidly than its speed of sound, than an effective event
horizon forms, for any disturbances in the water, which will propagate
with the speed of sound, will necessarily be “trapped”
behind the boundary. He then argued that the scattering of water
wavelets at the boundary will occur with a thermalized spectrum, in
exact accord with Hawking radiation (Unruh 1981, 2008). Since then,
analogue models for Hawking radiation in a wide variety of fluid,
solid-state, optical and quantum systems have been found. (See
Barceló et al. 2011, Robertson 2012, Jacobson 2013,
and Faccio et al. 2013 for recent reviews.) 
The remarkable fact that is of most interest to us is that, because
Unruh's arguments relied only on simple physical properties of
no-escape boundaries and the low-energy behavior of thermalized
radiation caused by scattering of fields off of such boundaries, Unruh
concluded that these so-called “dumb holes” (dumb because
silent) could serve as experimentally viable proxies for testing the
existence of Hawking radiation for black holes (Leonhardt and Philbin
2008). In particular, the validity of the analogue models is argued
for on the grounds that the essential features of Hawking radiation
are due solely to a few simple, formal kinematical conditions
satisfied by a wide range of kinds of physical systems (Visser 1998a,
2013; Unruh and Schützhold 2005; Unruh 2014). In particular, the
manifestation of radiation-like behavior formally analogous to true
Hawking radiation from a black hole has nothing to do with any
specific, dynamical features of general relativity. Therefore, the
thought goes, to detect the analogue of Hawking radiation in any of
these systems provides indirect but strong confirmational support for
the existence of actual Hawking radiation. There are, moreover, now
several claims to have experimentally detected analogue Hawking
radiation: Belgiorno et al. (2010) based on ultrashort laser
pulse filaments, i.e., intense laser pulses in a transparent
Kerr medium (those with a third-order optical nonlinearity);
Weinfurtner et al. (2011) based on obstructed supersonic
fluid flow; Steinhauer (2014) based on a “black-hole
laser” composed of phonons in an Einstein-Bose condensate; and
the list goes on. So, has Hawking radiation been experimentally
confirmed, even if only indirectly? 
Until recently, little philosophical work has been done on these
analogue black holes. Dardashti et al. (2017) argue that such
analogue models of event horizons and Hawking radiation can provide
powerful confirmatory support for the existence of Hawking radiation
around actual black holes. Indeed, they argue that these particular
kinds of analogue model and the concomitant support they purport to
provide are novel, both in the sense of being of a sort not
investigated before in the philosophical literature and in the sense
of representing an innovation in actual scientific practice. (See the
Encyclopaedia entry
 Analogy and Analogical Reasoning.)
 They base their claim on the fact that these are not only theoretical
models, but that they can be—and are—implemented as actual
experiments, and thus constitute not merely analogical reasoning, but
experimentally controlled physical simulation. If one accepts a
certain kind of universality argument (Unruh and Schützhold
2005), they claim, then it is this latter characteristic that lends
the analogue models the possibility of strong confirmatory support of
actual Hawking radiation; and to the contrary, without acceptance of
that universality argument—if the models were based merely on
standard analogical theoretical reasoning—no confirmatory
support at all would be had. 
Gryb et al. (2018) compare the kinds of universality argument
seemingly needed in this case to the more standard, familiar form of
such arguments made in the context of renormalization group methods.
They conclude that all available universality arguments made to
support taking analogue experiments to confirm the existence of
Hawking radiation are wanting in at least one of six categories that
they collectively deem necessary for such arguments to work
(robustness, physical plausibility, degree of universality, empirical
support, and integration of robustness and universality), with failure
of integration being the most serious problem. 
There is room, moreover, for yet more skepticism here. The arguments
are prima facie strong that the analogue of Hawking radiation
should manifest in a wide range of systems, as a purely kinematical
effect following directly from a few simple kinematical principles
that all those systems satisfy (Unruh 2014). Nonetheless, true
gravitational black holes are radically different from all the
proposed analogue systems, in a variety of extensive and deep ways, as
is general relativity as a physical theory from all the theories
governing those other types of systems. As the debate and dissension
discussed in
 section 6.1
 illustrates, the fundamental physics of Hawking radiation may not be
well enough understood to have confidence that some confounding
physical factor cannot be present in purely gravitational systems that
is not present in any of the analogue systems, a factor that would
block production of Hawking radiation by true black holes. In other
words, there seems prima facie little reason to have faith
that the universality condition holds, except on the basis of purely
theoretical arguments pertaining to systems we have no empirical
experience of nor access to whatsoever. 