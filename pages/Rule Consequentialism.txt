A moral theory is a form of consequentialism if and only if it
assesses acts and/or character traits, practices, and institutions
solely in terms of the goodness of the consequences. Historically,
utilitarianism has been the best-known form of consequentialism.
Utilitarianism assesses acts and/or character traits, practices, and
institutions solely in terms of overall net benefit. Overall net
benefit is often referred to as aggregate well-being or
welfare. Aggregate welfare is calculated by counting a benefit or harm
to any one individual the same as the same size benefit or harm to any
other individual, and then adding all the benefits and harms together
to reach an aggregate sum. There is considerable dispute among
consequentialists about what the best account of welfare is.
Classical utitilitarians (i.e., Jeremy Bentham, J.S. Mill, and
Henry Sidgwick) took benefit and harm to be purely a matter of
pleasure and pain. The view that welfare is a matter of pleasure minus
pain has generally been called hedonism. It has grown in
sophistication (Parfit 1984: Appendix I; Sumner 1996; Crisp 2006; de
Lazari-Radek and Singer 2014: ch. 9) but remains committed to the
thesis that how well someone’s life goes depends entirely on
his or her pleasure minus pain, albeit with pleasure and pain being
construed very broadly.
Even if pleasures and pains are construed very broadly, hedonism
encounters difficulties. The main one is that many (if not all) people
care very strongly about things other than their own pleasures and
pains. Of course these other things can be important as means to
pleasures and to the avoidance of pain. But many people care very
strongly about things over and beyond their hedonistic instrumental
value. For example, many people want to know the truth about various
matters even if this won’t increase their (or anyone else’s) pleasure.
Another example is that many people care about achieving things over
and beyond the pleasure such achievements might produce. Again, many
people care about the welfare of their family and friends in a
non-instrumental way. A rival account of these points, especially the
last, is that people care about many things other than their own
welfare.
On any plausible view of welfare, the satisfaction people can feel
when their desires are fulfilled constitutes an addition to their
welfare. Likewise, on any plausible view, frustration felt as a result
of unfulfilled desires constitutes a reduction in welfare. What is
controversial is whether the fulfilment of someone’s desire
constitutes a benefit to that person apart from any effect that the
fulfilment of the desire has on that person’s felt satisfaction or
frustration.  Hedonism answers No, claiming that only effects on felt
satisfaction or felt frustration matter.
A different theory of welfare answers Yes. This theory holds that
the fulfilment of any desire of the agent’s constitutes a benefit to
the agent, even if the agent never knows that desire has been
fulfilled and even if the agent derives no pleasure from its
fulfilment. This theory of human welfare is often referred to as the
desire-fulfillment theory of welfare. 
Clearly, the desire-fulfillment theory of welfare is broader than
hedonism, in that the desire-fulfillment theory accepts that what can
constitute a benefit is wider than merely pleasure. But there are
reasons for thinking that this broader theory is too broad. For one
thing, people can have sensible desires that are simply too
disconnected from their own lives to be relevant to their own welfare
(Williams 1973: 262; Overvold 1980, 1982; Parfit 1984: 494). I desire
that the starving in far-away countries get food. But the fulfilment
of this desire of mine does not benefit me.
For another thing, people can have desires for absurd things for
themselves. Suppose I desire to count all the blades of grass in the
lawns on this road. If I get satisfaction out of doing this, the felt
satisfaction constitutes a benefit to me. But the bare fulfilment of
my desire to count all the blades of grass in the lawns on this road
does not (Rawls 1971: 432; Parfit 1984: 500; Crisp 1997: 56).
On careful reflection, we might think that the fulfilment of
someone’s desire constitutes an addition to that person’s welfare
only if that desire has one of a certain set of contents. We
might think, for example, that the fulfilment of someone’s desire for
pleasure, friendship, knowledge, achievement, or autonomy for herself
does constitute an addition to her welfare, and that the
fulfilment of any desires she might have for others things do not
directly benefit her (though, again, the pleasure she derives from
their satisfaction does). If we think this, it seems we think there is
a list of things that constitute anyone’s welfare (Parfit 1984:
Appendix I; Brink 1989: 221–36; Griffin 1996: ch. 2; Crisp 1997: ch. 3;
Gert 1998: 92–4; Arneson 1999a).
Insofar as the goods to be promoted are parts of welfare, the
theory remains utilitarian. There is a lot to be said for
utilitarianism.  Obviously, how lives go is important. And there is
something deeply attractive (if not downright irresistible) in the
idea that morality is fundamentally impartial, i.e., the idea that, at
the most fundamental level of morality, everyone is equally important
— women and men, strong and weak, rich and poor, Blacks, Whites,
Hispanics, Asians, etc.  And utilitarianism plausibly interprets this
equal importance as dictating that in the calculation of overall
welfare a benefit or harm to any one individual counts neither more
nor less that the same size benefit or harm to any other
individual.
The nonutilitarian members of the consequentialist family are
theories that assess acts and/or character traits, practices, and
institutions solely in terms of resulting good, where good is not
restricted to welfare. “Nonutilitarian” here means
“not purely utilitarian”, rather than “completely
unutilitarian”. When writers describe themselves as
consequentialists rather than as utilitarians, they are normally
signalling that their fundamental evaluations will be in terms of not
only welfare but also some other goods.
What are these other goods? The most common answers have been
justice, fairness, and equality.
Justice, according to Plato, is “rendering to each his
due” (Republic, Bk. 1). We might suppose that what
people are due is a matter of what people are owed, either because
they deserve it or because they have a moral right to it. Suppose we
plug these ideas into consequentialism. Then we get the theory that
things should be assessed in terms of not only how much welfare
results but also the extent to which people get what they deserve and
the extent to which moral rights are respected.
For consequentialism to take this line, however, is for it to
restrict its explanatory ambitions. What a theory simply presupposes,
it does not explain. A consequentialist theory that presupposes both
that justice is constituted by such-and-such and that justice is one
of the things to be promoted does not explain why the components of
justice are important. It does not explain what desert is. It does not
explain the importance of moral rights, much less try to determine
what the contents of these moral rights are. These are matters too
important and contentious for a consequentialist theory to leave
unexplained or open. If consequentialism is going to refer to justice,
desert, and moral rights, it needs to analyze these concepts and
justify the role it gives them.
Similar things can be said about fairness. If a consequentialist
theory presupposes an account of fairness, and simply stipulates that
fairness is to be promoted, then this consequentialist theory is not
explaining fairness. But fairness (like justice, desert, and moral
rights) is a concept too important for consequentialism not to try to
explain.
One way for consequentialists to deal with justice and fairness is
to contend that justice and fairness are constituted by conformity
with a certain set of justified social practices, and that what
justifies these practices is that they generally promote overall
welfare and equality. Indeed, the contention might be that what people
are due, what people have a moral right to, what justice and fairness
require, is conformity to whatever practices promote overall welfare
and equality.
Whether equality needs to be included in the formula, however, is
very controversial. Many think that a purely utilitarian formula has
sufficiently egalitarian implications. They think that, even if the
goal is promotion of welfare, not the promotion of
welfare-plus-equality, there are some contingent but pervasive facts
about human beings that push in the direction of equal distribution of
material resources (Brandt 1979).
According to the “law of diminishing marginal utility of
material resources”, the amount of benefit a person gets out of
a certain unit of material resources is less the more units of that
material good the person already has. Suppose I go from having no way
of getting around except by foot to having a bicycle, or, though I
live in a place where one can get very cold, I go from having no warm
coat to having one. I will benefit more from getting that first
bicycle or coat than I would if I go from having nine bicycles or
coats to having ten.
There are exceptions to the law of diminishing marginal utility.
In most of these exceptions, an additional unit of material resource
pushes someone over some important threshold. For example, consider
the meal or pill or gulp of air that saves someone’s life, or the car
whose acquisition pushes the competitive collector into first
place. In such cases, the unit that puts the person over the threshold
might well be as beneficial to that person as any prior unit
was. Still, as a general rule, material resources do have diminishing
marginal utility.
To the assumption that material resources have diminishing marginal
utility, let us add the assumption that different people generally get
roughly the same benefits from the same material resources.
Again, there are exceptions. If you live in a freezing climate and I
live in a hot climate, then you would benefit much more from a warm
coat than I would.
But suppose we live in the same place, which has freezing winters,
good paths for riding bicycles, and no public transportation. And
suppose you have ten bicycles and ten coats (though you are not vying
for some bicycle- or coat-collector prize). Meanwhile, I am so poor
that I have none. Then, redistributing one of your bicycles and one of
your coats to me will probably harm you less than it will benefit me.
This sort of phenomenon pervades societies where resources are
unequally distributed. Wherever the phenomenon occurs, a fundamentally
impartial morality is under pressure to redistribute resources from
the richer to the poorer.
However, there are also contingent but pervasive facts about human
beings that pull in favor of practices that have the foreseen
consequence of material inequality. First of all, higher levels of
overall welfare can require higher levels of productivity (think of
the welfare gains resulting from improvements in agricultural
productivity). In many areas of the economy, the provision of material
rewards for greater productivity seems the most efficient acceptable
way of eliciting higher productivity. Some individuals and groups will
be more productive than others (especially if there are incentive
schemes). So the practice of providing material rewards for greater
productivity will result in material inequality.
Thus, on the one hand, the diminishing marginal utility of material
resources exerts pressure in favor of more equal distributions of
resources. On the other hand, the need to promote productivity exerts
pressure in favor of incentive schemes that have the foreseen
consequence of material inequality. Utilitarians and most other
consequentialists find themselves balancing these opposed
pressures.
Note that those pressures concern the distribution of resources.
There is a further question about how equally welfare itself should be
distributed. Many recent writers have taken utilitarianism to be
indifferent about the distribution of welfare. Imagine a choice
between an outcome where overall welfare is large but distributed
unequally and an outcome where overall welfare is smaller but
distributed equally.  Utilitarians are taken to favor outcomes with
greater overall welfare even if it is also less equally
distributed.
To illustrate this, let us take an artificially simple population,
divided into just two groups.
Many people would think Alternative 2 above better than Alternative
1, and might think that the comparison between these alternatives
shows that there is always pressure in favor of greater equality of
welfare.
As Derek Parfit (1997) in particular has argued, however, we must
not be too hasty. Consider the following choice:
Is equality of welfare so important that Alternative 3 is superior
to Alternative 1? To take an example of Parfit’s, suppose the only way
to make everyone equal with respect to sight is to make everyone
totally blind. Is such “levelling down” required by
morality? Indeed, is it in any way at all morally desirable?
If we think the answer is No, then we might think that equality of
welfare as such is not really an ideal (cf. Temkin 1993). Losses to
the better off are justified only where this benefits the worse
off. What we had thought of as pressure in favor of equality of
welfare was instead pressure in favor of levelling up. We might say
that additions to welfare matter more the worse off the person is
whose welfare is affected. This view has come to be called
prioritarianism (Parfit 1997; Arneson 1999b). It has
tremendous intuitive appeal.
For a simplistic example of how prioritarianism might work, suppose
the welfare of the worst off counts five times as much as the welfare
of the better off. Then Alternative 1 from the tables above comes out
at \((1 \times 5 \times 10,000) + (10 \times 100,000)\), which comes
to 1,050,000 total units of welfare. Again with the welfare of the
worst off counting five times as much, Alternative 2 comes out at \((8
\times 5 \times 10,000) + (9 \times 100,000)\), which comes to
1,300,000 total units of welfare. This accords with the common
reaction that Alternative 2 is morally superior to Alternative 1.
Of course in real examples there is never only one division in
society. Rather there is a scale from the worst off, to the not quite
so badly off, and so on up to the best off. Prioritarianism is
committed to variable levels of importance of the welfare of people at
different places on this scale: the worse off a person is, the greater
the importance attached to that person’s level of welfare.
This raises two serious worries about prioritarianism. The first
concerns prioritarianism’s difficulty in nonarbitrarily
determining how much more importance to give to the welfare of the
worse off. For example, should a unit of benefit to the worst off
count 10 times the same size benefit to the best off and 5 times the
same size benefit to the averagely well off? Or should the multipliers
be 20 and 10, or 4 and 2? The second worry about prioritarianism is
whether attaching greater importance to increases in welfare for some
than to the same size increases in welfare for others contradicts
fundamental impartiality (Hooker 2000: 60–2).
This is not the place to go further into debates between
prioritarianism and its critics. So the rest of this article sets
aside those debates.
Consequentialists have distinguished three components of their
theory: (1) their thesis about what makes acts morally wrong, (2)
their thesis about the procedure agents should use to make their moral
decisions, and (3) their thesis about the conditions under which moral
sanctions such as blame, guilt, and praise are appropriate.
What we might call full rule-consequentialism consists of
rule-consequentialist criteria for all three. Thus, full
rule-consequentialism claims that an act is morally wrong if and only
if it is forbidden by rules justified by their consequences. It also
claims that agents should do their moral decision-making in terms of
rules justified by their consequences. And it claims that the
conditions under which moral sanctions should be applied are
determined by rules justified by their consequences.
Full rule-consequentialists may think that there is really only one
set of rules about these three different subject matters. Or they may
think that there are different sets that in some sense correspond to
or complement one another.
Much more important than the distinction between different kinds of
full rule-consequentialism is the distinction between full
rule-consequentialism and partial rule-consequentialism.
Partial rule-consequentialism might take many forms. Let us focus on
the most common form. The most common form of partial
rule-consequentialism claims that agents should make their moral
decisions about what to do by reference to rules justified by their
consequences, but does not claim that moral wrongness is determined by
rules justified by their consequences. Partial rule-consequentialists
typically subscribe to the theory that moral wrongness is determined
directly in terms of the consequences of the act. This theory of
wrongness is called act-consequentialism.
Distinguishing between full and partial rule-consequentialism
clarifies the contrast between act-consequentialism and
rule-consequentialism. Act-consequentialism is best conceived of as
maintaining merely the following:
When confronted with that criterion of moral wrongness, many people
naturally assume that the way to decide what to do is to apply the
criterion, i.e.,
However, consequentialists nearly never defend this
act-consequentialist decision procedure as a general and typical way
of making moral decisions (Mill 1861: ch 2; Sidgwick 1907:
405–6, 413, 489–90; Moore 1903: 162–4; Smart 1956:
346; 1973: 43, 71; Bales 1971: 257–65; Hare 1981; Parfit 1984:
24–9, 31–43; Railton 1984: 140–6, 152–3; Brink
1989: 216–7, 256–62, 274–6; Pettit and Brennan 1986;
Pettit 1991, 1994, 1997: 156–61; de Lazari-Radek and Singer
2014: ch. 10). There are a number of compelling consequentialist
reasons why the act-consequentialist decision procedure would be
counter-productive.
First, very often the agent does not have detailed information
about what the consequences would be of various acts.
Second, obtaining such information would often involve greater
costs than are at stake in the decision to be made.
Third, even if the agent had the information needed to make
calculations, the agent might make mistakes in the calculations. (This
is especially likely when the agent’s natural biases intrude, or when
the calculations are complex, or when they have to be made in a
hurry.)
Fourth, there are what we might call expectation effects. Imagine a
society in which people know that others are naturally biased towards
themselves and towards their loved ones but are trying to make their
every moral decision by calculating overall good. In such a society,
each person might well fear that others will go around breaking
promises, stealing, lying, and even assaulting whenever they convinced
themselves that such acts would produce the greatest overall good. In
such a society, people would not feel they could trust one
another.
This fourth consideration is more controversial than the first
three. For example, Hodgson 1967, Hospers 1972, and Harsanyi 1982
argue that trust would break down. Singer 1972 and Lewis 1972 argue
that it would not.
Nevertheless, most philosophers accept that, for all four of the
reasons above, using an act-consequentialist decision procedure would
not maximize the good. Hence even philosophers who espouse the
act-consequentialist criterion of moral wrongness reject the
act-consequentialist moral decision procedure. In its place, they
typically advocate the following:
Since act-consequentialists about the criterion of wrongness
typically accept this decision procedure, act-consequentialists are in
fact partial rule-consequentialists. Often, what writers refer to as
indirect consequentialism is this combination of act-consequentialism
about wrongness and rule-consequentialism about the appropriate
decision procedure.
Standardly, the decision procedure that full rule-consequentialism
endorses is the one that it would be best for society to
accept. The qualification “standardly” is needed because
there are versions of rule-consequentialism that let the rules be
relativised to small groups or even individuals (D.E. Miller 2010;
Kahn 2012).  And act-consequentialism insists upon the decision
procedure it would be best for the individual to accept. So,
according to act-consequentialism, since Jack’s and Jill’s capacities
and situations may be very different, the best decision procedures for
Jack to accept may be different from the best decision procedure for
Jill to accept.  However, in practice act-consequentialists typically
ignore for the most part such differences and endorse the above
rule-consequentialist decision procedure (Hare 1981, chs. 2, 3, 8, 9,
11; Levy 2000).
When act-consequentialists endorse the above rule-consequentialist
decision procedure, they acknowledge that following this decision
procedure does not guarantee that we will do the act with the best
consequences. Sometimes, for example, our following a decision
procedure that rules out harming an innocent person will prevent us
from doing that act that would produce the best consequences.
Similarly, there will be some circumstances in which stealing,
breaking our promises, etc., would produce the best
consequences. Still, our following a decision procedure that generally
rules out such acts will in the long run and on the whole probably
produce far better consequences than our trying to run
consequentialist calculations on an act-by-act basis.
Because act-consequentialists typically agree with a
rule-consequentialist decision procedure, whether to classify some
philosopher as an act-consequentialist or as a rule-consequentialist
can be problematic.  For example, G.E. Moore (1903, 1912) is sometimes
classified as an act-consequentialist and sometimes as a
rule-consequentialist. Like so many others, including his teacher
Henry Sidgwick, Moore combined an act-consequentialist criterion of
moral wrongness with a rule-consequentialist procedure for deciding
what to do. Moore simply went further than most in stressing the
danger of departing from the rule-consequentialist decision procedure
(see Shaw 2000).
Some writers propose that the purest and most consistent form of
consequentialism is the view that absolutely everything should be
assessed by its consequences, including not only acts but also rules,
motives, the imposition of sanctions, etc. Let us follow Pettit and
Smith (2000) in referring to this view as global
consequentialism. Kagan (2000) pictures it as multi-dimensional direct
consequentialism, in that each thing is assessed directly in terms of
whether its own consequences are as good as the consequences of
alternatives.
How does this global consequentialism differ from what we have been
calling partial rule-consequentialism? What we have been calling
partial rule-consequentialism is nothing but the combination of the
act-consequentialist criterion of moral wrongness with the
rule-consequentialist decision procedure. So defined, partial
rule-consequentialism leaves open the question of when moral sanctions
are appropriate.
Some partial rule-consequentialists say that agents should be
blamed and feel guilty whenever they fail to choose an act that would
result in the best consequences. A much more reasonable position for a
partial rule-consequentialist to take is that agents should be blamed
and feel guilty whenever they choose an act that is forbidden by the
rule-consequentialist decision procedure, whether or not that
individual act fails to result in the best consequences. Finally,
partial rule-consequentialism, as we have defined it, is compatible
with the claim that whether agents should be blamed or feel guilty
depends not on the wrongness of what they did, nor on whether the
recommended procedure for making moral decisions would have led them
to choose the act they choose, but instead solely on whether this
blame or guilt will do any good. This is precisely the view of
sanctions that global consequentialism takes.
One devastating objection to global consequentialism is that
simultaneously applying a consequentialist criterion to acts, decision
procedures, and the imposition of sanctions leads to apparent
paradoxes (Crisp 1992; Streumer 2003; Lang 2004).
Suppose, on the whole and in the long run, the best decision
procedure for you to accept is one that leads you to do act x
now. But suppose also that in fact the act with the best consequences
in this situation is not x but y. So global
consequentialism tells you to use the best possible decision procedure
but also not to do the act picked out by this decision procedure. That
seems paradoxical.
Things get worse when we consider blame and guilt. Suppose you
follow the best possible decision procedure but fail to do the act
with the best consequences. Are you to be blamed? Should you feel
guilty?  Global consequentialism claims that you should be blamed if
and only if blaming you will produce the best consequences, and that
you should feel guilty if and only if this will produce the best
consequences.  Suppose that for some reason the best consequences
would result from blaming you for following the prescribed decision
procedure (and thus doing x). But surely it is paradoxical
for a moral theory to call for you to be blamed although you followed
the moral decision procedure mandated by the theory. Or suppose that
for some reason the best consequences would result from blaming you
for intentionally choosing the act with the best consequences
(y). Again, surely it is paradoxical for a moral theory to
call for you to be blamed although you intentionally chose the very
act required by the theory.
So one problem with global consequentialism is that it creates
potential gaps between what acts it claims to be required and what
decision procedures it tells agents to use, and between each of these
and blamelessness. (For explicit replies to this line of attack, see
Driver 2014: 175 and de
Lazari-Radek and Singer 2014: 315–16.)
That is not the most familiar problem with global consequentialism.
The most familiar problem with it is instead its maximising
act-consequentialist criterion of wrongness. According to this
maximising criterion, an act is wrong if and only if it fails to
result in the greatest good. This criterion judges some acts to be not
wrong which certainly seem to be wrong. It also judges some acts that
seem not wrong to be wrong.
For example, consider an act of murder that results in slightly
more good than any other act would have produced. According to the
most familiar, maximising act-consequentialist criterion of wrongness,
this act of murder is not wrong. Many other kinds of act such as
assaulting, stealing, promise breaking, and lying can be wrong even
when doing them would produce slightly more good than not doing them
would. Again, the familiar, maximising form of act-consequentialism
denies this.
Or consider someone who gives to her child, or keeps for herself,
some resource of her own instead of contributing it to help some
stranger who would have gained slightly more from that resource. Such
an action hardly seems wrong. Yet the maximising act-consequentialist
criterion judges it to be wrong. Indeed, imagine how much
self-sacrifice an averagely well-off person would have to make before
her further actions satisfied the maximising act-consequentialist
criterion of wrongness. She would have to give to the point where
further sacrifices from her in order to benefit others would harm her
more than they would benefit the others.  Thus, the maximising
act-consequentialist criterion of wrongness is often accused of being
unreasonably demanding.
The objections just directed at maximising act-consequentialism
could be side-stepped by a version of act-consequentialism that did
not require maximising the good. This sort of act-consequentialism is
now called satisficing consequentialism. See the entry on 
 consequentialism/ for more on such a
theory.
There are a number of different ways of formulating
rule-consequentialism. For example, it can be formulated in terms of
the good that actually results from rules or in terms of the
rationally expected good of the consequences of rules. It can be
formulated in terms of the consequences of compliance with rules or in
terms of the wider consequences of acceptance of rules. It can be
formulated in terms of the consequences of absolutely everyone’s
accepting the rules or in terms of the rules’ acceptance by something
less than everyone.  Rule-consequentialism is more plausible if
formulated in some ways than it is if formulated in other ways. This
is explained in the following three subsections. Questions of
formulation are also relevant in the later section on old objections
to rule-consequentialism.
As indicated, full rule-consequentialism consists in
rule-consequentialist answers to three questions. The first is, what
makes acts morally wrong? The second is, what procedure should agents
use to make their moral decisions? The third is, what are the
conditions under which moral sanctions such as blame, guilt, and
praise are appropriate?
As we have seen, the answer that full rule-consequentialists give
to the question about decision procedure is the same as other kinds of
consequentialist give to that question. So let us focus on the points
of contrast, i.e., the other two questions. These two questions
— about what makes acts wrong and about when sanctions are
appropriate — are more tightly connected than sometimes
realized.
Indeed, J.S. Mill, one of the fathers of consequentialism, affirmed
their tight connection:
Let us assume that Mill took “ought to be punished, at least
by one’s own conscience if not by others” to be roughly the same
as “blameworthy”. With this assumption in hand, we can
interpret Mill as tying wrongness tightly to blameworthiness. In a
moment, we can consider what follows if Mill is mistaken that
wrongness is tied tightly to blameworthiness. First, let us consider
what follows if Mill is correct that wrongness is tied tightly to
blameworthiness.
Consider the following argument, whose first premise comes from
Mill:
Surely, an agent cannot rightly be blamed for accepting and
following rules that the agent could not foresee would have
sub-optimal consequences. From this, we get our second premise:
From these two premises we get the conclusion:
Of course, the actual consequences of accepting a set of rules may
not be the same as the foreseeable consequences of accepting that set.
Hence, if full rule-consequentialism claims that an act is wrong if
and only if the foreseeable consequences of rules allowing
that act are sub-optimal, rule-consequentialism cannot hold that an
act is wrong if and only if the actual consequences of rules
allowing that act will be sub-optimal.
Now suppose instead the relation between wrongness and
blameworthiness is far looser than Mill suggested (cf. Sorensen 1996).
That is, suppose that our criterion of wrongness can be quite
different from our criterion of blameworthiness. In that case, we
could hold:
and
Here is how expected good of a set of rules is calculated. The
acceptance of a set of rules of course has various possible
alternative outcomes. Suppose we can identify the value or disvalue of
each possible outcome. Multiply the value of each possible outcome by
the probability of that outcome’s occurring. Take all the products of
these multiplications and add them together. The resulting number is
the expected good of that set of rules.
Note that expected good is not to be calculated by employing
whatever crazy estimates of probabilities people might assign to
possible outcomes. Rather, expected good is calculated by multiplying
the value or disvalue of possible outcomes by rational or justified
probability estimates.
There might be considerable scepticism about how often such
calculations are possible. Where such calculations are possible, they
will often be quite impressionistic and imprecise. Nevertheless, we
can reasonably hope to make at least some informed judgements
about the likely consequences of alternative possible rules.
And we could be guided by such judgements. In contrast, which rules
would actually have the very best consequences will
normally be inaccessible. Hence, the expectablist
rule-consequentialist criterion of blameworthiness is appealing.
Now return to the proposal that, while the criterion of
blameworthiness is the expectablist rule-consequentialist one, the
correct criterion of moral wrongness is the actualist
rule-consequentialist one. This is the proposal that rejects Mill’s
move of tying moral wrongness to blameworthiness. There is a very
strong objection to this proposal. What is the role and importance of
moral wrongness if it is disassociated from blameworthiness?
In order to retain an obvious role and importance for moral
wrongness, those committed to the expectablist rule-consequentialist
criterion of blameworthiness are likely to endorse:
Indeed, once we have before us the distinction between the amount
of value that actually results and the rationally expected good, the
full rule-consequentialist is likely to go for expectablist criteria
of moral wrongness, blameworthiness, and decision procedures.
What if, as far as we can tell, no one code has greater expected
value than its rivals? We will need to amend our expectablist criteria
in order to accommodate this possibility:
The argument for using closeness to conventional morality to break
ties between otherwise equally promising codes begins with the
observation that social change regularly has unexpected consequences.
And these unexpected consequences usually seem to be negative.
Furthermore, the greater the difference between a new code and the one
already conventionally accepted, the greater the scope for unexpected
consequences. So, as between two codes we judge to have equally high
expected value, we should choose the one closest to the one we already
know. (For discussion of the situation where two codes have equally
high expected value and seem equally close to conventional morality,
see Hooker 2000: 115. For a more nuanced view, see Hooker 2008: 83–4.)
An implication of this is that we should make changes to the status
quo where but only where these changes have greater expected value
than sticking with the status quo. Rule-consequentialism manifestly
has the capacity to recommend change. But it does not favor change for
the sake of change.
Rule-consequentialism most definitely does need to be formulated so
as to deal with ties in expected value. However, for the rest of this
article, I will ignore this complication.
There are other important issues of formulation that
rule-consequentialists face. One is the issue of whether
rule-consequentialism should be formulated in terms of compliance with
rules or in terms of acceptance of rules. Admittedly, the most
important aspect of accepting rules is compliance with them. And early
formulations of rule-consequentialism did indeed explicitly mention
compliance. For example, they said an act is morally wrong if and only
if it is forbidden by rules the compliance with which will maximize
the good (or the expected good). (See Austin 1832; Brandt 1959;
M. Singer 1955, 1961.)
However, acceptance of a rule can have consequences other than
compliance with the rule. As Kagan (2000: 139) writes, “once
embedded, rules can have an impact on results that is independent of
their impact on acts: it might be, say, that merely thinking about a
set of rules reassures people, and so contributes to happiness.”
(For more on what we might call these ‘beyond-compliance
consequences’ of rules, see Sidgwick 1907: 405–6, 413;
Lyons 1965: 140; Williams 1973: 119–20, 122, 129–30; Adams
1976, esp. 470; Scanlon 1998: 203–4; Kagan 1998:
227–34.)
These consequences of acceptance of rules should most definitely be
part of a cost-benefit analysis of prospective rules. Formulating
rule-consequentialism in terms of the consequences of acceptance
allows them to be part of this analysis. In fact, consideration of
assurance and incentive effects has played a large role in the
development of rule-consequentialism (Harsanyi 1977, 1982:
56–61; 1993: 116–18; Brandt 1979: 271–77; 1988:
346ff [1992: 142ff.]; 1996: 126, 144; Johnson 1991, especially chs. 3,
4, 9).
Just as we need to move from thinking about the consequences of
compliance to thinking about the wider consequences of acceptance, we
need to go further. Focusing purely on the consequences of acceptance
of rules ignores the “transition” costs of getting those
rules accepted in the first place. And yet these can certainly be
significant (Brandt 1963: section 4; 1967 [1992: 126]; 1983: 98; 1988:
346–47, 349–50 [1992: 140–143, 144–47]; 1996:
126–28, 145, 148, 152, 223).
Suppose, for example, that, once a fairly simple and relatively
undemanding code of rules Code A has been accepted, the
expected value of Code A would be n. Suppose the
more complicated and demanding alternative Code B would have
an expected value of \(n + 5\) once Code B has been
accepted. So if we just consider the expected values of acceptance of
the two alternative codes, Code B wins.
But now let us add in the relative costs of getting the two codes
accepted. Since Code A is fairly simple and relatively
undemanding, the cost of getting it accepted is −1. Since Code
B is more complicated and demanding, the cost of getting it
accepted is −7. So if our comparison of the two codes considers
the respective costs of getting them accepted, Code A’s
expected value is \(n-1\), and Code B’s is \(n+5-7\). Once we
include the respective costs of getting the codes accepted, Code
A wins.
As indicated, the costs of getting a code accepted are
“transition costs”. But of course such transitions are
always to one arrangement from another. The arrangement we are
imagining the transition being to is the acceptance of a
certain proposed code. The arrangement we are imagining the transition
being from is … well, what?
One answer is that the arrangement from which the transition is
supposed to be starting is whatever moral code the society happens to
accept already. That might seem like the natural answer. However, it
is a poor answer. The reason it is poor is that rule-consequentialism
should not let the cost/benefit analysis of a proposed code be
influenced by the costs of getting people to give up whatever rules
they may have already internalised. This is for two reasons.
Most importantly, rule-consequentialist assessment of codes needs
to avoid giving weight directly or indirectly to moral ideas that have
their source in other moral theories but not in rule-consequentialism
itself. Suppose people in a given society were brought up to believe
that women should be subservient to men. Should rule-consequentialist
evaluation of a proposed non-sexist code have to count the costs of
getting people to give up the sexist rules they have already
internalised so as to accept the new non-sexist ones? Since the sexist
rules are unjustifiable, that they were accepted should not be allowed
to infect rule-consequentialist assessment.
Another reason for rejecting the answer we are considering is that
it threatens to underwrite an unattractive relativism. Different
societies may differ considerably in their extant moral beliefs. So a
way of assessing proposed codes that considers the costs of getting
people already committed to some other code will end up having to
countenance different transition costs to get to the same code. For
example, the transition costs to a non-racist code are much more from
an already accepted racist code than from an already accepted
non-racist one. Formulating rule-consequentialism so that it endorses
the same code for 1960s Michigan as for 1960s Mississippi is
desirable.
The way to do this is to formulate the theory in terms of
acceptance by new generations of humans. So we compare the
respective “teaching costs” of alternative codes, on the
assumption that these codes will be taught to children who have not
already been educated to accept a moral code. We are to imagine the
children start off with natural (non-moral) inclinations to be very
partial towards themselves and a few others. We should also assume
that there is a cognitive cost associated with the learning of each
rule.
These are realistic assumptions, with big implications. One is that
a cost/benefit analysis of alternative codes of rules would have
reason to favor simpler codes over more complex ones. Of course there
can also be benefits from having more, or more complicated, rules. Yet
there is probably a limit on how complicated or complex a code can be
and still have greater expected value than simpler codes, once
teaching costs are included.
Another implication concerns prospective rules about making
sacrifices to help others. Since children start off focused on their
own gratifications, getting them to internalise a kind of impartiality
that constantly requires them to make large sacrifices for the sake of
others would have extremely high costs. There would also, of course,
be enormous benefits from the internalisation of such a rule —
predominately, benefits to others. Would the benefits be greater than
the costs?
At least since Sidgwick (1907: 434), many utilitarians have taken
for granted that human nature is such that the real possibilities are
(1) that human beings care passionately about some and less about each
of the rest of humanity or (2) that human beings care weakly but
impartially about everyone. In other words, what is not a realistic
possibility, according to this view of human nature, is human beings’
caring strongly and impartially about everyone in the world. If this
view is correct, then one enormous cost of successfully making people
completely impartial is that doing so would leave them with only weak
concerns.
Even if that picture of human nature is not correct, that is, even
if making people completely impartial could be achieved without draining them of enthusiasm
and passion, the cost of successfully making people care as much about
every other individual as they do about themselves would be
prohibitive. At some point on the spectrum running from complete
partiality to complete impartiality, the costs of pushing and inducing
everyone further along the spectrum outweigh the benefits.
Just as rule-consequentialists are more realistic if their
cost/benefit analyses of codes count the cost of getting those codes
internalised by new generations, they are more realistic if they
assume that the internalisation will not extend to every last
person. There will be some people who end up committed to mistaken
views about what is morally allowed. Others will never have accepted any
morality at all (psychopaths).  Rule-consequentialism needs to have
rules for dealing with such people.
These will consist mainly in rules about punishment. From a
rule-consequentialist point of view, the main point of punishment is
to deter certain kinds of act. There is also the need to get
undeterred, dangerous people off the streets. Perhaps
rule-consequentialism can admit that another point of punishment is to
appease the primitive lust for revenge on the part of victims of such
acts and their family and friends. Finally, there is the expressive
and reinforcing power of rules about punishment.
Nevertheless, some ways of formulating rule-consequentialism make
having rules about punishment difficult to explain. One such way of
formulating rule-consequentialism is:
Suppose absolutely every adult human fully accepts rules forbidding
(for example) physical attacks on the innocent, stealing, promise
breaking, and lying. Then presumably there would be little or no need
for rules about punishment. Without need for rules about punishment,
society would get little or no benefit from such rules. But there is a
cost associated with each rule included in a code. So there is a cost
associated with the inclusion of any rule about punishment. Because of
this combination of cost with no benefit, rules about punishment would
not be endorsed by the form of rule-consequentialism immediately
above.
We need a form of rule-consequentialism that includes rules for
dealing with people who are not committed to the right rules, indeed
even for people who are irredeemable. In other words,
rule-consequentialism needs to be formulated so as to conceptualise
society as containing some people insufficiently committed to the
right rules, and even some people never committed to any moral
rules. Here is a way of doing so:
Note that rule-consequentialism neither endorses nor condones the
non-acceptance of the code by those outside the overwhelming majority.
On the contrary, rule-consequentialism claims those people are morally
mistaken. Indeed, the whole point of formulating rule-consequentialism
this way is to make room for rules about how to respond negatively to
such people.
Another point to make about the above formulation is of course that
“overwhelming majority” is very imprecise. Picking a
precise percentage of society, say 90%, has an obvious element of
arbitrariness to it (why not 89% or 91%?). Nevertheless, we can argue
for a number in this range as a reasonable compromise between two
pressures. On the one hand, the percentage we pick should be close
enough to 100% to retain the idea that moral rules are for acceptance
by the whole society of human beings. On the other hand, the
percentage needs to be far enough short of 100% to leave considerable
scope for rules about punishment. It seems that 90% is in a defensible
range, given the need to balance those considerations. (For dissent
from this, see Ridge 2006; for a reply to Ridge, see Hooker and
Fletcher 2008. The matter receives further discussion in H. Smith
2010; Tobia 2013; Portmore 2015.)
We have seen that rule-consequentialism evaluates rules on the
basis of the expected value of their acceptance by the overwhelming
majority.  What rules will such an approach endorse? It will endorse
rules prohibiting physically attacking innocent people or their
property, taking the property of others, breaking one’s promises, and
lying. It will also endorse rules requiring one to pay special
attention to the needs of one’s family and friends, but more generally
to be willing to help others with their (morally permissible)
projects. Why? The crude answer is that a society where such rules are
widely internalised and thus accepted would be likely to have more
good in it than one lacking such rules.
The fact that these rules are endorsed by rule-consequentialism
makes rule-consequentialism attractive. For, intuitively, these rules
seem right. However, other moral theories endorse these rules as well.
Most obviously, a familiar kind of moral pluralism contends that these
intuitively attractive rules constitute the most basic level of
morality, i.e., that there is no deeper moral principle underlying and
unifying these rules. Call this view Rossian pluralism (in honor of
its champion W.D. Ross (1930, 1939)).
Rule-consequentialism may agree with Rossian pluralism in endorsing
rules against physically attacking the innocent, stealing, promise
breaking, and rules requiring various kinds of loyalty and more
generally doing good for others. But rule-consequentialism goes beyond
Rossian pluralism by specifying an underlying unifying principle that
provides impartial justification for such rules. Other moral theories
try to do this too. Such theories include some forms of Kantianism
(Audi 2001, 2004) and some forms of contractualism (Scanlon 1998;
Parfit 2011; Levy 2013). In any case, the first way of arguing for
rule-consequentialism is to argue that it specifies an underlying
principle that provides impartial justification for intuitively
plausible moral rules, and that no rival theory does this as well
(Urmson 1953; Brandt 1967; Hospers 1972; Hooker 2000). (Attacks on
this line of argument for rule-consequentialism include Stratton-Lake
1997; Thomas 2000; D.E. Miller 2000; Montague 2000; Arneson 2005;
Moore 2007; Hills 2010; Levy 2014.)
This first way of arguing for rule-consequentialism might be seen
as drawing on the idea that a theory is better justified to us to the
extent that it increases coherence within our beliefs (Rawls 1951,
1971: 19–21, 46–51; DePaul 1987; Ebertz 1993; Sayre-McCord
1986, 1996). [See the entry on coherentist theories of epistemic
justification.]  But the approach might also be seen as moderately
foundationalist in that it begins with a set of beliefs (in various
moral rules) to which it assigns independent credibility though not
infallibility (Audi 1996, 2004; Crisp 2000). [See the entry on foundationalist theories of epistemic
justification.]  Admittedly, coherence with our moral beliefs does
not make a moral theory true, since our moral beliefs might
of course be mistaken. Nevertheless, if a moral theory fails
significantly to cohere with our moral beliefs, this undermines the
theory’s ability to be justified to us.
The second way of arguing for rule-consequentialism is very
different. It starts from a commitment to consequentialist assessment,
and then argues that assessing acts indirectly, e.g., by
focusing on the consequences of communal acceptance of rules, will in
fact produce better consequences than assessing acts directly in terms
of their own consequences (Austin 1832; Brandt 1963, 1979; Harsanyi
1982: 58–60; 1993; Riley 2000). After all, making decisions
about what to do is the main point of moral assessment of acts. So if
a way of morally assessing acts is likely to lead to bad decisions, or
more generally lead to bad consequences, then, according to a
consequentialist point of view, so much the worse for that way of
assessing acts.
Earlier we saw that all consequentialists now accept that assessing
each act individually by its expected value is in general a terrible
procedure for making moral decisions. There is widespread
acknowledgement that agents should decide how to act by appeal to
certain rules such as “don’t physically attack others”,
“don’t steal”, “don’t break your promises”,
“pay special attention to the needs of your family and
friends”, and “be generally helpful to others”. And
these are the rules that rule-consequentialism endorses. Many
consequentialists, however, think this hardly shows that full
rule-consequentialism is the best form of consequentialism. Once a
distinction is made between, on the one hand, the best procedure for
making moral decisions about what to do and, on the other hand, the
criteria of moral rightness and wrongness, all consequentialists can
admit that we need rule-consequentialism’s rules for our decision
procedure. But consequentialists who are not rule-consequentialists
contend that such rules play no role in the criterion of moral
rightness. Hence these consequentialists reject what this article has
called full rule-consequentialism.
However, whether the objection we have just been considering to the
second way of arguing for rule-consequentialism is a good objection
depends on whether it is legitimate to distinguish between procedures
appropriate for making moral decisions and the criteria of moral
rightness or wrongness. That matter remains contentious (Hooker 2010;
de Lazari-Radek and Singer 2014: ch. 10).
Yet the second way of arguing for rule-consequentialism runs into
another and quite different objection. This objection is that the
first step in this argument for rule-consequentialism is a commitment
to consequentialist assessment. This first step itself needs
justification. Why assume that assessing things in a consequentialist
way is uniquely justified?
It might be said that consequentialist assessment is justified
because promoting the impartial good has an obvious intuitive appeal.
But that won’t do, since there are alternatives to consequentialist
assessment that also have obvious intuitive appeal. This is true, for
example, of “act on the code that no one could reasonably
reject”. In fact, no one very abstract moral idea is so clearly
superior to its rivals that it can triumph without the aid of further
justification.  What we need is a way of arguing for a moral theory
that does not start by begging the question which kind of theory is
most plausible.
A third way of arguing for rule-consequentialism is contractualist
(Harsanyi 1953, 1955, 1982, 1993; Brandt 1979, 1988, 1996; Scanlon
1982, 1998; Parfit 2011; Levy 2013). Suppose we can specify reasonable
conditions under which everyone would choose, or at least would have
sufficient reason to choose, the same code of rules. Intuitively, such
an idealized agreement would legitimate that code of rules. Now if
those rules are the ones whose internalisation would maximise the
expected good, contractualism is leading us to rule-consequentialism’s
rules.
There are different views about what would be reasonable conditions
for choosing among alternative possible moral rules. One view is that
everyone’s impartiality would have to be insured by the imposition of
a hypothetical “veil of ignorance” behind which no one
knew any specific facts about himself or herself (Harsanyi 1953,
1955). Another view is that we should imagine that people would be
choosing a moral code on the basis of (a) full empirical information
about the different effects on everyone, (b) normal concerns
(self-interested as well as altruistic), and (c) roughly equal
bargaining power (Brandt 1979; cf. Gert 1998). Parfit (2011) proposes
seeking rules that everyone has (personal or impartial) reason to
choose or will that everyone accept. If impartial reasons are always
sufficient even when opposed by personal ones, then everyone has
sufficient reason to will that everyone accept the rules whose
universal acceptance will have the best consequences impartially
considered. Similarly, Levy (2013) supposes that no one could
reasonably reject a code of rules that would impose on her burdens
that add up to less than the aggregate of burdens that every other
code would impose on others. Such arguments suggest the extensional
equivalence of contractualism and rule-consequentialism. (For
assessment of whether Parfit’s contractualist arguments for
rule-consequentialism succeed, see J. Ross 2009; Nebel 2012; Hooker
2014.)
Rule-consequentialism was not clearly formulated until Urmson 1953
and Brandt 1959. The theory attracted considerable attention until the
early 1970s. Since the early 1970s, however, most moral philosophers
have thought of rule-consequentialism as fatally impaled on one or the
other horn of the following dilemma: Either rule-consequentialism
collapses into practical equivalence with the simpler
act-consequentialism, or rule-consequentialism is incoherent.
Here is why some have thought rule-consequentialism collapses into
practical equivalence with act-consequentialism. Consider a rule that
rule-consequentialism purports to favor — e.g., “don’t
steal”. Now suppose an agent is in some situation where stealing
would produce more good than not stealing. If rule-consequentialism
selects rules on the basis of their expected good,
rule-consequentialism seems driven to admit that compliance with the
rule “don’t steal except when … or … or
…” is better than compliance with the simpler
“don’t steal”. This point generalizes. In other words, for
every situation where compliance with some rule would not produce the
greatest expected good, rule-consequentialism seems driven to favor
instead compliance with some amended rule that does not miss out on
producing the greatest expected good in the case at hand. But if
rule-consequentialism operates this way, then in practice it will end
up requiring the very same acts that act-consequentialism
requires.
If rule-consequentialism ends up requiring the very same acts that
act-consequentialism requires, then rule-consequentialism is indeed in
terrible trouble. Rule-consequentialism is the more complicated of the
two theories. This leads to the following objection. What is the point
of rule-consequentialism with its infinitely amended rules if we can
get the same practical result much more efficiently with the simpler
act-consequentialism?
Rule-consequentialists in fact have an excellent reply to the
objection that their theory collapses into practical equivalence with
act-consequentialism. This reply relies on the point that the best
kind of rule-consequentialism ranks systems of rules not in
terms of the expected good of complying with them, but in
terms of the expected good of their acceptance. Now if a rule
forbidding stealing, for example, has exception clause after exception
clause after exception clause tacked on to it, the rule with all these
exception clauses will provide too much opportunity for temptation to
convince agents that one of the exception clauses applies, when in
fact stealing would be advantageous to the agent. And this point about
temptation will also undermine other people’s confidence that their
property won’t be stolen. The same is true of most other moral rules:
incorporating too many exception clauses could undermine people’s
assurance that others will behave in certain ways (such as keeping
promises and avoiding stealing).
Furthermore, when comparing alternative rules, we must also
consider the relative costs of getting them internalised by new
generations.  Clearly, the costs of getting new generations to learn
either an enormous number of rules or hugely complicated rules would
be prohibitive. So rule-consequentialism will favor a code of rules
without too many rules, and without too much complication within the
rules.
There are also costs associated with getting new generations to
internalise rules that require one to make enormous sacrifices for
others with whom one has no particular connection. Of course,
following such demanding rules will produce many benefits, mainly to
others. But the costs associated with internalising such rules should
be weighed against the benefits of following them. At some level of
demandingness, the costs of getting such demanding rules internalised
will outweigh the benefits that following them will produce. Hence,
doing a careful cost/benefit analysis of internalising demanding rules
will come out opposing rules’ being too demanding.
The code of rules that rule-consequentialism favours (that is,
a code comprised of rules are not too numerous, too complicated, or too demanding)
can sometimes lead people to do acts that do not have the greatest
expected value. For example, following the simpler rule “Don’t
steal” will sometimes produce less good consequences than
following a more complicated rule “Don’t steal except when
… or … or … or … or
…”. Another example might be following a rule that allows
people to give some degree of priority to their own projects, even when
they could produce more good by sacrificing their own projects in order
to help others. Still, rule-consequentialism’s contention is that
bringing about widespread acceptance of a simpler and less demanding
code, even if acceptance of that code does sometimes lead people to do
acts with sub-optimal consequences, has higher expected value in the
long run than bringing about widespread acceptance of a maximally
complicated and demanding code. Since rule-consequentialism can tell
people to follow this simpler and less demanding code, even when following
it will not to maximise expected good, rule-consequentialism escapes
collapse into practical equivalence to act-consequentialism.
To the extent that rule-consequentialism circumvents collapse, this
theory is accused of incoherence. Rule-consequentialism is accused of
incoherence for maintaining that an act can be morally permissible or
even required though the act fails to maximise expected good. Behind
this accusation must be the assumption that
rule-consequentialism contains an overriding commitment to maximise
the good. It is incoherent to have this overriding commitment and then
to oppose an act required by the commitment. (For a recent
developments of this line of thought, see Arneson 2005; Card 2007;
Wall 2009.)
In order to evaluate the incoherence objection to
rule-consequentialism, we need to be clearer about the supposed
location of an overriding commitment to maximize the good. Is this
commitment supposed to be part of the rule-consequentialist agent’s
moral psychology? Or is it supposed to be part of the theory
rule-consequentialism?
Well, rule-consequentialists need not have maximizing the good as
their ultimate and overriding moral goal. Instead, they could have a
moral psychology as follows:
Their fundamental moral motivation is to do what is impartially
defensible.
They believe acting on impartially justified rules is impartially
defensible.
They also believe that rule-consequentialism is on balance the best
account of impartially justified rules.
Agents with this moral psychology — i.e., this combination of
moral motivation and beliefs — would be morally motivated to do
as rule-consequentialism prescribes. This moral psychology is
certainly possible. And, for agents who have it, there is nothing
incoherent about following rules when doing so will not maximize the
expected good.
So, even if rule-consequentialist agents need not have an
overriding commitment to maximize expected good, does their theory
contain such a commitment? No, rule-consequentialism is essentially
the conjunction of two claims: (1) that rules are to be selected
solely in terms of their consequences and (2) that these rules
determine which kinds of acts are morally wrong. This is really all
there is to the theory — in particular, there is not some third
component consisting in or entailing an overriding commitment to
maximize expected good.
Without an overriding commitment to maximize the expected good,
there is nothing incoherent in rule-consequentialism’s forbidding some
kinds of act, even when they maximize the expected good. Likewise,
there is nothing incoherent about rule-consequentialism’s requiring
other kinds of act, even when they conflict with maximizing the
expected good. The best known objection to rule-consequentialism dies
once we realize that neither the rule-consequentialist agent nor the
theory itself contains an overriding commitment to maximize the
good.
The viability of this defense of rule-consequentialism against the
incoherence objection may depend in part on what the argument for
rule-consequentialism is supposed to be. The defense seems less viable
if the argument for rule-consequentialism starts from a commitment to
consequentialist assessment. For starting with such a commitment seems
very close to starting from an overriding commitment to maximize the
expected good. The defence against the incoherence objection seems far
more secure, however, if the argument for rule-consequentialism is
that this theory does better than any other moral theory at specifying
an impartial justification for intuitively plausible moral rules. (For
more on this, see Hooker 2005, 2007.)
Another old objection to rule-consequentialism is that
rule-consequentialists must be “rule-worshipers” —
i.e., people who will stick to the rules even when doing so will
obviously be disastrous.
The answer to this objection is that rule-consequentialism endorses
a rule requiring one to prevent disaster, even if doing so requires
breaking other rules (Brandt 1992: 87–8, 150–1,
156–7). To be sure, there are many complexities about what
counts as a disaster. Think about what counts as a disaster when the
“prevent disaster” rule is in competition with a rule
against lying. Now think about what counts as a disaster when the
“prevent disaster” rule is in competition with a rule
against stealing, or even more when in competition with a rule against
physically harming the innocent.  Rule-consequentialism may need to be
clearer about such matters. But at least it cannot rightly be accused
of potentially leading to disaster.
An important confusion to avoid is to think that
rule-consequentialism’s including a “prevent disaster”
rule means that rule-consequentialism collapses into practical
equivalence with maximising act-consequentialism. Maximising
act-consequentialism holds that we should lie, or steal, or harm the
innocent whenever doing so would produce even a little more
expected good than not doing so would. A rule requiring one to prevent
disaster does not have this implication.  Rather, the “prevent
disaster” rule comes into play only when there is a very much
larger difference in the amounts of expected value at stake.
From the mid 1960s until the mid 1990s, most philosophers thought
rule-consequentialism couldn’t survive the objections discussed in the
previous section. So, during those three decades, most philosophers
didn’t bother with other objections to the theory. However, if
rule-consequentialism has convincing replies to all three of the
objections just discussed, then a good question is whether or not
there are other fatal objections to the theory.
Some other objections try to show that, given the theory’s
criterion for selecting rules, there are conditions under which it
selects intuitively unacceptable rules. For example, Tom Carson (1991)
argued that rule-consequentialism turns out to be extremely demanding
in the real world. Mulgan (2001, esp. ch. 3) agreed with Carson about
that, and went on to argue that, even if rule-consequentialism’s
implications in the actual world are fine, the theory has
counterintuitive implications in possible worlds. If Mulgan were right
about that, this would cast doubt on rule-consequentialism’s claim to
explain why certain demands are appropriate in the actual
world. Debate about such matters continues (Hooker 2003; Lawlor 2004;
Woollard 2015: 181–205). And Mulgan has become a developer of
the theory rather than a critic (Mulgan 2006, 2009, and 2015).
A related objection to rule-consequentialism is that
rule-consequentialism makes the justification of familiar rules
contingent on various empirical facts, such as what human nature is
like, and how many people there are in need or in positions to help.
The objection to rule-consequentialism is that some familiar moral
rules are necessarily, not merely contingently, justified (McNaughton
and Rawling 1998; Gaut 1999, 2002; Montague 2000; Suikkanen 2008). A
sibling of this objection is that rule-consequentialism makes the
justification of rules depend on the wrong facts (Arneson 2005;
Portmore 2009). Again, debate about whether the theory does point to the wrong facts
continues (see Woollard 2015, esp. pp. 185–86, 203–205).
The mechanics of teaching new codes throws up serious questions for
forms of rule-consequentialism that count the costs of getting rules
internalised by new generations. The reference to new generations is
meant to avoid having to count the costs of getting rules internalised
by existing generations of people who have already internalised some
other moral rules and ideas. But can we come up with a coherent
description of those who are supposed to do the teaching of these new
generations? If the teachers are imagined to have already internalised
the ideal code themselves, then how is that supposed to have happened?
If these teachers are imagined not to have already internalised the
ideal code, then there will be costs associated with the conflict
between the ideal code and whatever they have already
internalised. (This objection was formulated by John Andrews, Robert
Ehman, and Andrew Moore. Cf. Levy 2000.) A related objection is that
rule-consequentialism has not yet been formulated in a way that
enables it to deal plausibly with conflicts among rules (Eggleston
2007).
Another line of objection to rule-consequentialism has focused on
its idea that the considerations that determine moral right and wrong
must be suitable for public acknowledgement. Arneson (2005) and de
Lazari-Radek and Singer (2014) argue, as against
rule-consequentialism, that there is a potential gap between the
considerations suitable for public acknowledgement and the
considerations that really do determine moral right and wrong. Others
take rule-consequentialism’s idea that the considerations that
determine moral right and wrong must be suitable for public
acknowledgement to be not only one of the aspects that
rule-consequentialism shares with Kantian ethics but also one of
rule-consequentialism’s attractions (Hooker 2000, 2010; Hill 2005;
Parfit 2011; Cureton 2015).  