Our best current theories of fundamental physics are the Standard
Model of elementary particle physics and the theory of general
relativity. The Standard Model accounts for three of the known four
fundamental forces of nature—the strong, the weak, and the
electromagnetic force—while general relativity accounts for the
fourth—gravity. Arguments according to which our universe is
fine-tuned for life are aimed at showing that life could not have
existed for the vast majority of other forms of the laws of nature,
other values of the constants of nature, and other conditions in the
very early universe.
The following is an—incomplete—list of suggested instances
of fine-tuning for life. (For popular overviews see Leslie 1989: ch.
2, Rees 2000, Davies 2006, and Lewis & Barnes 2016; for more
technical ones see Hogan 2000, Uzan 2011, Barnes 2012, Adams 2019 and
the contributions to Sloan et al. 2020.)
It has been claimed that the laws of physics are fine-tuned for life
not only with respect to the constants that appear in them but also
with respect to their form itself. Three of the four known fundamental
forces—gravity, the strong force, and
electromagnetism—play key roles in the organisation of complex
material systems. A universe in which one of these forces is
absent—and the others are present as in our own
universe—would most likely not give rise to life, at least not
in any form that resembles life as we know it. The fundamental force
whose existence is least clearly needed for life is the weak force
(Harnik et al. 2006). A universe without any weak force but with all
the other forces of the Standard Model in place and suitably adjusted
might be habitable (Grohs et al. 2018). Further general features of
the actual laws of nature that have been claimed to be necessary for
the existence of life are the quantization principle and the Pauli
exclusion principle in quantum theory (Collins 2009: 213f.).
Considerations according to which the laws of nature, values of the
constants, and boundary conditions of the universe are fine-tuned for
life refer to life in general, not merely human life. According to
them, a universe with different laws, constants, and boundary
conditions would almost certainly not give rise to any form
of life. A common worry about such considerations is that they are
ill-founded due to lack of a widely accepted definition of
“life”. Another worry is that we may seriously
underestimate life’s propensity to appear under different laws,
constants, and boundary conditions because we are biased to assume
that all possible kinds of life will resemble life as we know it. A
joint response to both worries is that, according to the fine-tuning
considerations, universes with different laws, constants, and boundary
conditions would typically give rise to much less structure and
complexity, which would seem to make them life-hostile, irrespective
of how exactly one defines “life” (Lewis & Barnes
2016: 255–274).
Victor Stenger (2011) is extremely critical of considerations
according to which the laws, constants, and boundary conditions of our
universe are fine-tuned. According to Stenger, the form of the laws of
nature is fixed by the reasonable—very weak—requirement
that they be “point-of-view-invariant” in that, as he
claims, the laws “will be the same in any universe where no
special point of view is present” (p. 91). Luke Barnes
criticizes this claim (2012: sect. 4.1), arguing that it relies on
confusingly identifying point-of-view-invariance with non-trivial
symmetry properties that the laws in our universe happen to exhibit.
Notably, as Barnes emphasizes, neither general relativity nor the
Standard Model of elementary particle physics are without conceptually
viable, though perhaps empirically disfavoured, alternatives.
A further criticism by Stenger is that considerations according to
which the conditions in our universe are fine-tuned for life routinely
fail to consider the consequences of varying more than one parameter
at a time. In response to this criticism, Barnes (2012: sect. 4.2)
gives an overview of various studies such as Barr and Khan 2007 and
Tegmark et al. 2006 that explore the complete parameter space of
(segments of) the Standard Model and arrives at the conclusion that
the life-permitting range in multidimensional parameter space is
likely very small.
Fred Adams (2019) cautions against claims that the universe is
extremely fine-tuned for life. According to him, the range of
parameters for which the universe would have been habitable is quite
considerable. In addition, as he sees it, the universe could have been
more, rather than less, life-friendly. Notably, if the vacuum energy
density had been smaller, the primordial fluctuations (quantified by
Q) had been larger, the baryon-to-photon ratio had been larger, the
strong force had been slightly stronger, and gravity slightly weaker,
there might have been more opportunities for life to develop (Adams
2019: sect. 10.3). If Adams is right, our universe may just be
garden-variety habitable rather than maximally life-supporting.
Biological organisms are fine-tuned for life in the sense that their
ability to solve problems of survival and reproduction depends
crucially and sensitively on specific details of their behaviour and
physiology. For example, many animals rely on their visual apparatus
to spot prey, predators, or potential mates. The proper functioning of
their visual apparatus, in turn, depends sensitively on physiological
details of their eyes and brain.
Biological fine-tuning has a long tradition of being regarded as
evidence for divine design (Paley 1802), but modern biology regards it
as the product of Darwinian evolution, notably as driven by natural
and sexual selection. Relatively recently, some researchers have
claimed that some specific “fine-tuned” features of
organisms cannot possibly be the outcomes of Darwinian evolutionary
development alone and that interventions by some designer must be
invoked to account for them. For example, Michael Behe (1996) claims
that the so-called flagellum, a bacterial organ that enables
motion, is irreducibly complex in the sense that it cannot be
the outcome of consecutive small-scale individual evolutionary steps,
as they are allowed by standard, Darwinian, evolutionary theory. In a
similar vein, William Dembski (1998) argues that some evolutionary
steps hypothesized by Darwinian are so improbable that one would not
rationally expect them to occur even once in a volume the size of the
visible universe. Behe and Dembski conclude that an intelligent
designer likely intervened in the evolutionary course of events.
The overwhelming consensus in modern biology is that the challenges to
Darwinian evolutionary theory brought forward by Behe, Dembski and
others can be met. According to Kenneth Miller (1999), Behe’s
arguments fail to establish that there are no plausible small-step
evolutionary paths which have Behe’s allegedly
“irreducibly complex” features as outcomes. For example,
as Miller argues, there is in fact strong evidence for a Darwinian
evolutionary history of the flagellum and its constituents (Miller
1999: 147–148).
Many researchers believe that the fine-tuning of the universe’s
laws, constants, and boundary conditions for life calls for inferring
the existence of a divine designer (see
 Section 3)
 or a multiverse—a vast collection of universes with differing
laws, constants, and boundary conditions (see
 Section 4).
 The inference to a divine designer or a multiverse typically rests on
the idea that, in view of the required fine-tuning, life-friendly
conditions are in some sense highly improbable if there is
only one, un-designed, universe. It is controversial, however, whether
this idea can coherently be fleshed out in terms of any philosophical
account of probability.
Considerations as reviewed in
 Section 1.1
 according to which the laws, constants and boundary conditions in our
universe are fine-tuned for life are based on investigations of
physical theories and their parameter spaces. It may therefore seem
natural to expect that the relevant probabilities in the light of
which fine-tuning for life is improbable will be physical
probabilities. On closer inspection, however, it is difficult to see
how this could be the case: according to the standard view of physical
possibility, alternative physical laws and constants are physically
impossible by the definition of physical possibility (Colyvan et al.
2005: 329). Accordingly, alternative laws and constants trivially have
physical probability zero, whereas the actual laws and constants have
physical probability one. If the laws and constants that physics has
so far determined turned out to be merely effective laws and constants
fixed by some random process in the early universe which might be
governed by more fundamental physical laws, it would start to make
sense to apply the concept of physical probability to those effective
laws and constants (Juhl 2006: 270). However, the fine-tuning
considerations as outlined in
 Section 1.1
 do not seem to be based on speculations about any such process, so
they do not seem to implicitly rely on the notion of physical
probability in that sense.
Attempts to apply the notion of logical probability to
fine-tuning for life are beset with difficulties as well. Critics
argue that, from a logical point of view, arbitrary real numbers are
possible values of the constants (McGrew et al. 2001; Colyvan et al.
2005). According to them, any probability measure over the real
numbers as values of the constants that differs from the uniform
measure would be arbitrary and unmotivated. The uniform measure
itself, however, assigns zero probability to any finite interval. By
this standard, the life-permitting range, if finite, trivially has
probability zero, which would mean that life-friendly constants are
highly improbable whether or not fine-tuning in the sense of
 Section 1.1
 is required for life. This conclusion seems counterintuitive, but
Koperski (2005) argues that it is not as unacceptable for proponents
of the view that life-friendly conditions are improbable and require a
response as it may initially seem.
Motivated by the difficulties that arise in attempts to apply the
physical and logical notions of probability to fine-tuning for life,
contemporary accounts often appeal to an essentially epistemic notion
of probability (e.g., Monton 2006; Collins 2009). According to these
approaches, life-friendly conditions are improbable in that we would
not rationally expect them. An obvious problem for this view is that
life-friendly conditions are not literally unexpected for us: as a
matter of fact, we have long been aware that the conditions are right
for life in our universe, so the epistemic probability of
life-friendly conditions appears to be trivially \(1\). As Monton
(2006) highlights, to make sense of the idea that life-friendly
conditions are improbable in an epistemic sense, we must find a way of
strategically abstracting from some of our background knowledge,
notably from our knowledge that life exists, and assess the
probability of life’s existence from that perspective. (See
 Section 3.3
 for further discussion.)
Views according to which life-friendly conditions are epistemically
improbable face the challenge to provide reasons as to why we
should not expect life-friendly conditions from an epistemic
perspective which ignores that life exists. One response to this
challenge is to point out that there is no clear systematic pattern in
the actual, life-permitting, combination of values of the constants
(Donoghue 2007: sect. 8), which suggests that this combination is
disfavoured in terms of elegance and simplicity. Another response is
to appeal to the criterion of naturalness (see
 Section 5),
 which would lead one to expect values for at least two constants of
nature—the cosmological constant and the mass of the Higgs
particle—which differ radically from the actual ones. Neither
elegance and simplicity nor naturalness dictate any specific
probability distribution over the values of the constants, however,
let alone over the form of the laws itself. But proponents of the view
that fine-tuning for life is epistemically improbable can appeal to
these criteria to argue that life-friendly conditions will be ascribed
very low probability by any probability distribution that respects
these criteria.
Even if fine-tuned conditions are improbable in some substantive
sense, it might be wisest to regard them as primitive coincidences
which we have to accept without resorting to such speculative
responses as divine design or a multiverse. It is indeed
uncontroversial that being improbable does not by itself automatically
amount to requiring a theoretical response. For example, any specific
sequence of outcomes in a long series of coin tosses has low initial
probability (namely, \(2^{-N}\) if the coin is fair, which approaches
zero as the number \(N\) of tosses increases), but one would not
reasonably regard any specific sequence of outcomes as calling for
some theoretical response, e.g., a re-assessment of our initial
probability assignment. The same attitude is advocated by Gould (1983)
and Carlson and Olsson (1998) with respect to fine-tuning for life.
Leslie concedes that improbable events do not in general call for an
explanation, but he argues that the availability of reasonable
candidate explanations of fine-tuning for life—namely, the
design hypothesis and the multiverse hypothesis—suggests that we
should not “dismiss it as how things just happen to be”
(Leslie 1989: 10). Views similar to Leslie’s are defended by van
Inwagen (1993), Bostrom (2002: 23–41), and Manson and Thrush
(2003: 78–82).
Cory Juhl (2006) argues along independent lines that we should not
regard fine-tuning for life as calling for a response. According to
Juhl, forms of life are plausibly “causally ramified” in
that they “causally depend, for [their] existence, on a large
and diverse collection of logically independent facts” (2006:
271). He argues that one would expect “causally ramified”
phenomena to depend sensitively on the values of potentially relevant
parameters such as, in the case of life, the values of the constants
and boundary conditions. According to him, fine-tuning for life
therefore does not require “exotic explanations involving
super-Beings or super-universes” (2006: 273).
The sense in which fine-tuning for life fails to be
surprising according to Juhl differs from the sense in which it
is surprising according to authors such as Leslie, van
Inwagen, Bostrom, Manson and Thrush: while the latter hold that
life-friendly conditions are rationally unexpected from an epistemic
point of view which sets aside our knowledge that life exists, Juhl
holds that—given our knowledge that life exists and is
causally ramified—it is unsurprising that life depends
sensitively, for its existence, on the constants and boundary
conditions.
Biological fine-tuning for survival and reproduction, as marvellous as
it often appears, is regarded as unmysterious by biologists because
evolution as driven by natural and sexual selection can generate it
(see
 Section 1.3).
 One may hope that, similarly, future developments in fundamental
physics will reveal principles or mechanisms which explain the
life-friendly conditions in our universe.
There are two different types of scenarios of how future developments
in physics could realize this hope: first, physicists may hit upon a
so-called theory of everything according to which, as
envisaged by Albert Einstein, 
nature is so constituted that it is possible logically to lay down
such strongly determined laws that within these laws only rationally
completely determined constants occur (not constants, therefore, whose
numerical values could be changed without destroying the theory).
(Einstein 1949: 63) 
Einstein’s idea is that, ultimately, the laws and constants of
physics will turn out to be dictated completely by fundamental general
principles. This would make considerations about alternative laws and
constants obsolete, and thereby undermine any perspective according to
which these are fine-tuned for life.
Unfortunately, developments in the last few decades have not been kind
to hopes of the sort expressed by Einstein. In the eyes of many
physicists, string theory is still the most promising
candidate “theory of everything” in that it potentially
offers a unified account of all known forces of nature, including
gravity. (See Susskind 2005 for a popular introduction, Rickles 2014
for a philosopher’s historical account and Dawid 2013 for a
recent, favourable, methodological appraisal.) But according to our
present understanding of string theory, the theory has an enormous
number of lowest energy states, or vacua, which would
manifest themselves at the empirical level in terms of radically
different effective physical laws and different values of the
constants. These would be the laws and constants that we have
empirical access to, and so string theory would not come close to
uniquely determining the laws and constants in the manner envisaged by
Einstein.
A second type of scenario according to which future developments in
physics may eliminate at least some fine-tuning for life would be a
dynamical account of the generation of life-friendly
conditions, in analogy to the Darwinian “dynamical”
evolutionary account of biological fine-tuning for survival and
reproduction. Inflationary cosmology (Guth 1981, 2000) is a paradigm
candidate example of such an account in that it dynamically explains
why the total cosmic energy density \(\Omega\) in the early universe
is extremely close to the so-called critical value \(\Omega_c\) (see
 Section 1.1)—or,
 equivalently, why the overall spatial curvature of the universe is
close to zero. According to inflationary cosmology, the very early
universe undergoes a period of exponential or near-exponential
expansion (“inflation”) which effectively flattens out
space and results in near-zero post-inflation curvature, leading to a
total energy density \(\Omega\) extremely close to the critical
density \(\Omega_c\). Further claimed achievements of inflationary
cosmology include its ability to account for the observed near-perfect
isotropy of the universe and the absence of magnetic monopoles. The
strongest empirical support for inflationary cosmology, however, is
now widely believed to come from of its apparently correct predictions
of the shape of the cosmic microwave background fluctuations (PLANCK
collaboration 2014).
Inflationary cosmology’s attractions notwithstanding, its
suggested achievements are not universally recognized. (See Steinhardt
& Turok [2008] for harsh criticism by two eminent cosmologists and
Earman & J. Mosterín [1999] and McCoy [2015] for critical
appraisals by philosophers.) However, even if its dynamical accounts
of the flatness, isotropy, and absence of magnetic monopoles in the
early universe are correct, there is little reason to accept that
similar accounts will be forthcoming for many other constants,
boundary conditions, or even laws of nature that seem fine-tuned for
life: whereas, notably, the critical energy density \(\Omega_c\) has
independently specifiable dynamical properties that characterize it as
a systematically distinguished value of the energy density \(\Omega\),
the actual values of most other constants and parameters that
characterize boundary conditions are not similarly distinguished and
do not form any clear systematic pattern (Donoghue 2007: sect. 8).
This makes it difficult to imagine that future physical theories will
indeed reveal dynamical mechanisms which inevitably lead to these
values (Lewis & Barnes 2016: 181f.).
A classic response to the observation that the conditions in our
universe seem fine-tuned for life is to infer the existence of a
cosmic designer who created life-friendly conditions. If one
identifies this designer with some supernatural agent or God, the
inference from fine-tuning for life to the existence of a designer
becomes a version of the teleological argument. Indeed, many regard
the argument from fine-tuning for a designer as the strongest version
of the teleological argument that contemporary science affords.
Expositions of the argument from fine-tuning for design are typically
couched in terms of probabilities (e.g., Holder 2002; Craig 2003;
Swinburne 2004; Collins 2009); see also the review Manson 2009. An
elementary Bayesian formulation considers the rational impact of the
observation \(R\)—that the constants (and laws and boundary
conditions) are right for life—on our degree of belief
concerning the design hypothesis \(D\)—that there is a cosmic
designer. According to standard Bayesian conditioning, our posterior
degree of belief \(P^+(D)\) after taking into account \(R\)
is given by our prior conditional degree of belief \(P(D\mid R)\).
Analogously, our posterior \(P^+(\neg D)\) that there is no cosmic
designer is given by our prior conditional degree of belief \(P(\neg
D\mid R)\). By Bayes’ theorem, the ratio between the two
posteriors is 

\[\begin{equation}
\label{simpledes}
\frac{P^+(D)}{P^+(\neg D)} = \frac{P(D\mid R)}{P(\neg D\mid R)} = \frac{P(R\mid D)}{P(R\mid \neg D)} \frac{P(D)}{P(\neg D)}\,. 
\end{equation}
\]

 Proponents of the argument from fine-tuning
for design argue that, in view of the required fine-tuning,
life-friendly conditions are highly improbable if there is no divine
designer; see Barnes 2020 for a careful case for this claim. Thus, the
conditional probability \(P(R\mid \neg D)\) should be set close to
zero. In contrast, it is highly likely according to them that the
constants are right for life if there is indeed a designer. Thus the
conditional probability \(P(R\mid \neg D)\) should be given a value
not far from \(1\). If a sufficiently powerful divine being
exists—the idea goes—it is only to be expected that she/he
will be interested in creating, or at least enabling, intelligent
life, which means that we can expect the constants to be right for
life on that assumption. This motivates the likelihood inequality 
which expresses that life-friendly conditions confirm the designer
hypothesis and which likelihoodists such as Sober (2003) regard as at
the core of the argument from fine-tuning for design.
Bayesians focus not only on likelihoods but also on priors and
posteriors, and in their eyes the crucial significance of the
inequality \(\eqref{likelihoods}\) is that it leads to a ratio
\(P^+(D)/P^+(\neg D)\) of posteriors that is much larger than the
ratio \(P(D)/P(\neg D)\) of the priors. Whether belief in a designer
is rational depends ultimately on the priors as well, but unless those
values dramatically favour \(\neg D\) over \(D\) in that \(P(\neg
D)\gg P(D)\), the posteriors will favour design in that \(P^+(D)>
P^+(\neg D)\). Bayesian proponents of the argument from fine-tuning
for design conclude that our degree of belief in the existence of some
divine designer should be greater than \(1/2\) in view of the fact
that there is life, given the required fine-tuning.
We could not possibly have existed in conditions that are incompatible
with the existence of observers. The famous weak anthropic
principle (WAP) (Carter 1974) suggests that this apparently
trivial point may have important consequences:
[W]e must be prepared to take account of the fact that our location in
the universe is necessarily privileged to the extent of being
compatible with our existence as observers. (Carter 1974: 293,
emphasis due to Carter)
Our methods of empirical observation are unavoidably biased
towards detecting conditions which are compatible with the existence
of observers. For example, even if life-hostile places vastly
outnumber life-friendly places in our universe, we should not be
surprised to find ourselves in one of the relatively few places that
are life-friendly and seek an explanation for this finding, simply
because—in virtue of being living organisms—we could not
possibly have found ourselves in a life-hostile place. Biases that
result from the fact that what we observe must be compatible with the
existence of observers are referred to as observation selection
effects. The observation selection effects emphasized by the weak
anthropic principle with respect to location in the universe are
emphasized by what Carter dubs the strong anthropic principle
(SAP) with respect to the universe as a whole:
[T]he Universe (and hence the fundamental parameters on which it
depends) must be such as to admit within it the creation of observers
within it at some stage. (Carter 1974: 294)
Carter’s formulation of the SAP has led some authors, most
influentially Barrow and Tipler (1986), to misinterpret it along
teleological lines and as thereby categorically different from the
WAP. But, as Carter himself highlights (1983: 352), see also Leslie
(1989: 135–145), the SAP is meant to highlight exactly the same
type of bias as the WAP and is literally stronger than the WAP only
when conjoined with a version of the multiverse hypothesis.
The so-called anthropic objection against the argument from
fine-tuning for design argues that that argument breaks down once our
biasedness due to the observation selection effects emphasized by the
weak and strong anthropic principles is taken into account. Elliott
Sober (2003, 2009) advocates this objection. According to him, the
argument from fine-tuning for design requires not the likelihood
inequality \(\eqref{likelihoods}\) but the much more problematic 
where “OSE” stands for “observation
selection effect”. Sober himself spells out OSE as
“We exist, and if we exist, the constants must be right”
(2003: 44). According to this interpretation, \(\eqref{ose}\) is
patently false: our existence as living organisms entails that the
constants are right for life, which means that the terms on both sides
of \(\eqref{ose}\) are trivially \(1\) and hence equal, so
\(\eqref{ose}\) does not hold on Sober’s analysis.
Critics of the anthropic objection argue that Sober’s reasoning
delivers highly implausible results when transferred to examples where
rational inferences are less controversial. Most famous is
Leslie’s firing squad (Leslie 1989: 13f.), in which a prisoner
expects to be executed by a firing squad but, to his own surprise,
finds himself alive after all the marksmen have fired and wonders
whether they intended to miss. The firing squad scenario involves an
observation selection effect because the prisoner cannot contemplate
his post-execution situation unless he somehow survives the execution.
His observations, in other words, are “biased” towards
finding himself alive (see Juhl [2007] and Kotzen [2012] for further
useful examples). Sober’s analysis, applied to the firing squad
scenario, suggests that it would not be rational for the prisoner to
suspect that the marksmen intended to miss (unless independent
evidence suggests so) because that would mean overlooking the
observation selection effect that he faces. But, as Leslie, Weisberg
(2005) and Kotzen (2012) argue, this recommendation seems very
implausible.
According to Weisberg, Sober’s analysis fails due to its
incorrect identification of the observation selection effect
OSE with “We exist, and if we exist, the constants must
be right”. Weisberg argues that the weaker, purely conditional,
statement “If we exist, the constants must be right”
(Weisberg 2005: 819, Weisberg’s wording differs) suffices to
capture the observation selection effect. But if we interpret
“OSE” as this statement, there is no reason to
suppose that the inequality \(\eqref{ose}\) fails and the argument
from fine-tuning for design appears vindicated inasmuch as the
anthropic objection is concerned. (See Sober 2009 for Sober’s
response.)
To resolve the difficulty of accommodating observation selection
effects in likelihood arguments, Kotzen (2012) suggests that bias due
to such effects be taken into account as evidence rather than
background information. Notably, instead of \(\eqref{ose}\) Kotzen
proposes to consider 
where \(I\) contains information about the observation process,
including observation selection effects (Kotzen 2012: 835). According
to this analysis, the argument from fine-tuning for design can be
saved from the anthropic objection for a variety of ways to spell out
the information \(I\) about the observation process and anthropic
bias.
Views according to which life-friendly conditions are improbable in an
epistemic sense due to the required fine-tuning are challenged to come
to terms with the fact that, as a matter of fact, we have long known
that our universe is life-friendly, which means that life-friendly
conditions are not literally unexpected for us. As a
consequence of this fact, the Bayesian version of the argument from
fine-tuning for a designer as outlined in
 Section 3.1
 must adopt some solution to Bayesianism’s notorious problem
of old evidence (Glymour 1980) because \(R\)—that the
constants are right for life—is inevitably old evidence
for us.
An obvious choice, endorsed by Monton (2006), who is critical of the
argument from fine-tuning for design, and Collins (2009), who supports
it, is the so-called counterfactual or ur-probability
solution to the problem of old evidence, as defended by Howson (1991).
The main advantage of this solution as applied to the argument from
fine-tuning for design is that it allows to essentially preserve the
argument, including \(\eqref{simpledes}\) and \(\eqref{likelihoods}\),
with the sole refinement that one must consistently construe all prior
probabilities \(P(\cdot)\), conditional and unconditional, as
“ur-probabilities”, i.e., rational credences of some
counterfactual epistemic agent who is unaware that the constants are
right for life. Somewhat bizarrely, as Monton points out (2006: 416),
such an agent would have to be at least temporarily unaware of her/his
existence (or at least her/his existence as a form of life) because
otherwise she/he could not possibly be unaware that the conditions are
right for life. Tentative suggestions concerning the background
knowledge that can reasonably be ascribed to such an agent are
developed by Monton (2006: sect. 4) and Collins (2009: sect. 4.3).
An advantage of approaching the argument from fine-tuning for design
using the ur-probability solution is that it offers proponents of the
argument a clear-cut rejection of the anthropic objection: as in
Kotzen’s (2012) approach, the fact that we exist is treated not
as background knowledge but as evidence taken into account by Bayesian
conditioning. The appropriate comparison between likelihoods to
consider is thus not Sober’s \(\eqref{ose}\)—at least not
under Sober’s own interpretation of “OSE”
as including “We exist”—but rather
\(\eqref{likelihoods}\) or \(\eqref{ose_kotzen}\), both of which evade
the anthropic objection.
The likelihood inequality \(\eqref{likelihoods}\) on which the
fine-tuning argument for design rests is based on the assumption that,
reasonably, \(P(R\mid \neg D)\) is very small because life-friendly
conditions are improbable if there is no designer. This assumption can
be challenged, as already discussed in
 2.1.
 But the likelihood inequality \(\eqref{likelihoods}\) also rests on
the assumption that \(P(R\mid D)\) is comparatively large, i.e., on
the view that, if there is indeed a designer, life-friendly conditions
are more to be expected than if there is no designer. This assumption
can be challenged as well.
Reasonable assignments of \(P(R\mid D)\) depend on how exactly the
designer hypothesis \(D\) is spelled out. According to Swinburne, the
most promising candidate designer is “the God of traditional
theism” whom he characterizes as “a being essentially
eternal, omnipotent (in the sense that He can do anything logically
possible), omniscient, perfectly free, and perfectly good”
(2003: 107). Swinburne argues that we can be at least moderately
confident that the God of traditional theism, if he exists,
“will bring about an orderly, spatially extended, world in which
humans have a location” (2003: 113; note that Swinburne operates
with a generalized, non-biological concept of “humans”).
Hence, according to Swinburne, life-friendly conditions, conditional
on the existence of the God of traditional theism, do not have very
low probability, i.e. \(P(R\mid D)\), plausibly, is not many orders of
magnitude smaller than \(1\). According to Rota (2016: 119f.), even if
we assign to \(P(R\mid D)\) a value as low as \(1\) in a billion, this
suffices for the fine-tuning argument for a divine designer to be
strong, simply because, in view of the fine-tuning considerations,
life-friendly conditions in the absence of a designer are so utterly
unexpected. A similar point is made by Hawthorne and Isaacs
(2018).
Criticisms of the view that life-friendly constants are to be expected
if there is a designer have a long tradition and go back to John Venn
(1866) and John Maynard Keynes (1921). More recently, Sober has voiced
general reservations about our abilities to competently judge what a
divine designer, if real, would do:
Our judgements about what counts as a sign of intelligent design must
be based on empirical information about what designers often do and
what they rarely do. As of now, these judgements are based on our
knowledge of human intelligence. The more our hypotheses of
intelligent designers depart from the human case, the more in the dark
we are as to what the ground rules are for inferring intelligent
design. (Sober 2003: 38)
In a similar spirit, Narveson complains that we are in no position to
predict how a cosmic designer would behave because “[b]odiless
minded super-creators are a category that is way, way out of
control” (Narveson 2003: 99). According to Sober and Narveson it
is particularly problematic for theists to confidently assume that
God, if she/he exists, would create life-friendly conditions and, at
the same time, react to the problem of evil by highlighting our
inability to understand “the mysterious ways of the Deity”
(Narveson 2003: 99). Manson (2020) provides an updated defense of the
position of Sober and Narveson against the arguments of Rota (2016) as
well as Hawthorne and Isaacs (2018).
One can construct versions of the designer hypothesis \(D\) that are
tailored to fulfil the likelihood inequality \(\eqref{likelihoods}\)
by defining the designer as a being with both the intention
and ability to create life-friendly conditions. However, one may
question whether such tailored versions of the designer hypothesis
have sufficient independent motivation and plausibility to deserve
serious consideration in the first place. To use Bayesian terms, one
may hesitate to ascribe them non-negligible prior probabilities
\(P(D)\).
Motivating a non-negligible prior \(P(D)\) for design is especially
challenging in the framework of the ur-probability solution to the
problem of old evidence because it constrains the background evidence
to facts that do not entail the existence of life. Collins argues that
if we focus only on a limited class of constants \(C\), the background
evidence that we can use to motivate the prior \(P(D)\) is allowed to
“includ[e] the initial conditions of the universe, the laws of
physics, and the values of all the other constants except C”.
But appeals to the sacred texts of religions cannot be used to
motivate the ascription of a non-negligible ur-prior \(P(D)\) because
they presuppose, and thus entail, the existence of life. Notably, as
pointed out by Monton, “[i]n formulating an urprobability for
the existence of God, one cannot take into account Biblical accounts
about Jesus” (2006: 418). According to Monton (2006: 419),
proponents of the argument from fine-tuning for design may, however,
try to motivate a non-negligible ur-prior \(P(D)\) by resorting to
arguments for the existence of God that are either a priori, e.g.,
ontological argument, or appeal only to very general empirical facts
that do not entail that the conditions are right for life, e.g., the
cosmological argument. According to Swinburne (2004: ch. 5), the
hypothesis of traditional theism is a simple one and, as such,
warrants the ascription of a non-negligible prior.
The argument from fine-tuning for design as reviewed in
 Section 3.1
 treats the fact that life requires fine-tuned conditions as
background knowledge and assesses the evidential significance of the
observation that life-friendly conditions obtain against that
background. An alternative argument from fine-tuning for design,
explored by John Roberts (2012) and independently investigated and
endorsed by Roger White (2011) in a reply to Weisberg (2010), treats
our knowledge that the conditions are right for life as background
information and assesses the rational impact of physicists’
insight that life requires fine-tuned conditions against this
background. An advantage of this alternative is that it fits better
with our actual epistemic situation: that the conditions are right for
life is something we have known for a long time; our actual new
evidence is that the laws of physics—as White (2011) and
Weisberg (2012) put it—are stringent rather than
lax in the constraints that they impose on the constants and
boundary conditions if there is to be life.
The central likelihood inequality around which White’s version
of the argument revolves is 
where “\(D\)” is, again, the designer hypothesis,
“\(S\)” is the proposition that the laws are stringent
(i.e., that life requires delicate fine-tuning of the constants) and
“\(O\)” is our background knowledge that life exists
(White 2011: 678). (See Roberts [2012: 296] for an assumption that
plays an analogous role as \(\eqref{alternative}\).) The inequality
\(\eqref{alternative}\) expresses the statement that stringent laws
confirm the designer hypothesis, given our background knowledge that
life exists. Does it plausibly hold for reasonable probability
assignments? White argues that it does and supports this claim by
giving a rigorous derivation of \(\eqref{alternative}\) from
assumptions that he regards as plausible. Crucial among them is the
inequality 
which White motivates by arguing that “the fact that the laws
put stringent conditions on life does not by itself provide any
evidence against design” (White 2011: 678). Put
differently, according to White, absent information that life exists,
information that the laws are stringent does at least not speak
against the existence of a designer.
Weisberg (2012) criticizes \(\eqref{alternative1}\)—and takes
his criticism to undermine \(\eqref{alternative}\)—arguing that
it is implausible by the design theorist’s own standards. The
design theorist holds a combination of views according to which, on
the one hand, life is more probable if there is a designer than if
there is no designer and life is less probable if the laws are
stringent rather than lax. If one adds to this combination of views
the assumption that none of the possible life-friendly conditions has
higher probability than the others, both if there is a designer and if
there is no designer, it dictates that—bracketing knowledge that
life exists—stringent laws speak against the existence
of a designer, i.e., it dictates \(P(D\mid S) < P(D\mid \neg S)\),
contrary to \(\eqref{alternative1}\). Absent any evidence that life
exists, evidence that the laws are stringent speaks against the
existence of life in that stringent laws make life unexpected.
A possible response for the design theorist, anticipated by Weisberg
(2012: 713), would be to support \(\eqref{alternative1}\) by arguing
that the designer would plausibly first choose either
stringent or lax laws, sidestepping her intention to enable the
existence of life at that stage or actively preferring stringent laws,
and only then choosing life-friendly constants. A problem
with this response, similar to the difficulties discussed in
 Section 3.4,
 is that we have little experience with cosmic designers and,
therefore, difficulties to predict the hypothesized designer’s
preferences and likely actions.
According to the multiverse hypothesis, there are multiple universes,
some of them radically different from our own. Many of those who
believe that fine-tuning for life requires some theoretical response
regard it as the main alternative beside the designer hypothesis. The
idea that underlies it is that, if there is a sufficiently diverse
multiverse in which the conditions differ between universes, it is
only to be expected that there is at least one where they are right
for life. As the strong anthropic principle highlights (see
 Section 3.2),
 the universe in which we, as observers, find ourselves must be one
where the conditions are compatible with the existence of observers.
This suggests that, on the assumption that there is a sufficiently
diverse multiverse, it is neither surprising that there is at least
one universe that is hospitable to life nor—since we could not
have found ourselves in a life-hostile universe—that we find
ourselves in a life-friendly one. Many physicists (e.g., Susskind
[2005], Greene [2011], Tegmark [2014]) and philosophers (e.g., Leslie
[1989], Smart [1989], Parfit [1998], Bradley [2009]) regard this line
of thought as suggesting the inference to a multiverse as a rational
response to the finding that the conditions are right for life in our
universe despite the required fine-tuning.
The argument from fine-tuning for the multiverse as just sketched is
sometimes characterized as an inference to the multiverse as the best
explanation of fine-tuning for life—an explanation which, in
view of its appeal to anthropic reasoning, is sometimes characterized
as “anthropic” (e.g., Leslie 1986, 1989: ch. 6; McMullin
1993: 376f., sect. 7; Bostrom 2002). It is controversial, however,
whether this characterization is adequate. A paradigmatic anthropic
“explanation”, characterized as such by Carter in the
seminal paper (1974) that introduces the anthropic principles, is
astrophysicist Robert Dicke’s (1961) account of coincidences
between large numbers in cosmology. A prominent example of such a
coincidence is that the relative strength of electromagnetism and
gravity as acting on an electron/proton pair is of roughly the same
order of magnitude (namely, \(10^{40}\)) as the age of the universe,
measured in natural units of atomic physics. Impressed by this and
other coincidences, Dirac (1938) stipulated that they might hold
universally and as a matter of physical principle. He conjectured that
the strength of gravity may decrease as the age of the universe
increases, which would indeed make it possible for the coincidence to
hold across all cosmic times.
Dicke (1961), criticizing Dirac, argues that standard cosmology with
time-independent gravity suffices to account for the coincidence,
provided that we take into account the fact that our existence is tied
to the presence of mainline stars like then sun and of various
chemical elements produced in supernovae. As Dicke shows, this
requirement dictates that we could only have found ourselves in that
cosmic period in which the coincidence holds. Accordingly, contrary to
Dirac, there is no reason to assume that gravity varies with time to
make the coincidence unsurprising. Carter (1974) and Leslie (1986,
1989: ch. 6) describe Dicke’s account as an “anthropic
explanation” of the coincidence that impressed Dirac, and Leslie
discusses it continuously with the argument from fine-tuning for the
multiverse. (Earman [1987: 309], however, disputes that Dicke’s
account is adequately characterized as an “explanation”.)
But whereas Dicke’s account of the coincidence uses life’s
existence as background knowledge to show that standard cosmology
suffices to make the coincidence expectable, the standard argument
from fine-tuning for the multiverse, as reviewed in what follows,
treats life’s existence as requiring a theoretical response
(rather than as background knowledge) and advocates the multiverse
hypothesis as the best such response. Friederich (2019b; 2021: ch. 6)
outlines how to set up the fine-tuning argument for the multiverse so
that it uses anthropic reasoning analogously to the Dicke/Carter
accounts of large number coincidences.
More often than as an inference to the best explanation the argument
from fine-tuning for the multiverse is formulated using probabilities,
in analogy to the argument from fine-tuning for design (see
 Section 3.1).
 In a simple version of the argument, reasonable probability
assignments are compared for a single-universe hypothesis \(U\) (where
the universe has uniform laws and constants) and a rival multiverse
hypothesis \(M\) according to which there are many universes with
conditions that differ between universes. (For the purposes of the
discussion about fine-tuning for life, hypotheses according to which
there is only a single universe with constants that vary across
space-time qualify as versions of the multiverse hypothesis. They seem
to be disfavoured by the available evidence, however, see Uzan 2003
for a review.)
As in Bradley 2009, we consider as the fine-tuning evidence the
proposition \(R\) that there is (at least) one universe with the right
constants for life. Using Bayesian conditioning and Bayes’
theorem one obtains for the ratio of the posteriors 
If the multiverse according to \(M\) is sufficiently vast and varied,
life unavoidably appears somewhere in it, so the conditional prior
\(P(R\mid M)\) must be \(1\) (or very close to \(1\)). If we assume
that, on the assumption that there is only a single universe, it is
improbable that it has the right conditions for life (see
 Section 2.1
 for discussion), the conditional prior \(P(R\mid U)\) must be much
smaller than \(1\). This gives \(P(R\mid M) \gg P(R\mid U)\), which
entails \(P(R\mid M)/P(R\mid U)\gg1\), which in turn entails a ratio
of posteriors that is much larger than the ratio of the priors:
\(\frac{P(M\mid R)}{P(U\mid R)}\gg\frac{P(M)}{P(U)}\). Unless we have
prior reasons to dramatically prefer a single universe over the
multiverse, i.e., unless \(P(U)\gg P(M)\), the ratio of the posteriors
\(\frac{P^+(M)}{P^+(U)}\) will be larger than \(1\).
Just as the argument from fine-tuning for design, the argument from
fine-tuning for the multiverse must come to terms with the problem
that the existence of life is old evidence for us. If one applies
Howson’s ur-probability solution to it, one must consistently
interpret all the probabilities in equation \(\eqref{simplemult}\) as
assigned from the perspective of a counterfactual epistemic agent who
is unaware of her/his own existence. At least prima facie, it is
unclear what background knowledge can be assumed for an agent in that
curious condition (see
 Section 3.3
 for considerations). Juhl (2007) speculates that motivating a
non-negligible prior \(P(M)\) is impossible without implicitly relying
on evidence which entails that the conditions are right for life. If
this is correct, it means that running the fine-tuning argument for
the multiverse as in equation \(\eqref{simplemult}\) based on an
empirically well motivated non-negligible prior \(P(M)\) would
inevitably involve fallacious double-counting
(“double-dipping”, as Juhl calls it (2007: 554)) of the
fine-tuning evidence \(R\).
The inverse gambler’s fallacy, identified by Ian Hacking (1987),
consists in inferring from an event with a remarkable outcome that
there have likely been many more events of the same type in the past,
most with less remarkable outcomes. For example, the inverse
gambler’s fallacy is committed by someone who enters a casino
and, upon witnessing a remarkable outcome at the nearest
table—say, a five-fold six in a quintuple die
toss—concludes that the toss is most likely part of a large
sequence of tosses. Critics of the argument from fine-tuning for the
multiverse accuse it of committing the inverse gambler’s
fallacy. According to them, the argument commits this fallacy by, as
White puts it, 
supposing that the existence of many other universes makes it more
likely that this one—the only one that we have
observed—will be life-permitting. (White 2000: 263) 
Versions of this criticism are endorsed by Draper et al. (2007) and
Landsman (2016). Hacking (1987) regards only those versions of the
argument from fine-tuning for the multiverse as guilty of the inverse
gambler’s fallacy that infer the existence of multiple universes
in a temporal sequence.
Adherents of the inverse gambler’s fallacy charge against the
argument from fine-tuning for the multiverse object against focusing
on the impact of the proposition \(R\)—that the conditions are
right for life in some universe. According to them, we should
instead consider the impact of the more specific proposition \(H\):
that the conditions are right for life here, in this
universe. If we replace \(R\) by \(H\), they argue, it becomes clear
that the argument breaks down because the existence of other universe
does not raise the probability that this universe here is
life-friendly.
Many philosophers defend the argument from fine-tuning for the
multiverse against this objection (McGrath 1988; Leslie 1988; Bostrom
2002; Manson & Thrush 2003; Juhl 2005; Bradley 2009; Epstein
2017). In an early response to Hacking, McGrath (1988) argues that the
analogy between the argument from fine-tuning for the multiverse and a
person who randomly enters a casino and witnesses a remarkable outcome
is misleading: while the person entering a casino could have found any
arbitrary outcome, we could not have found ourselves in a universe
with conditions that are not right for life. The appropriate analogy
to consider, according to McGrath, involves someone who is allowed to
enter the casino only if and when some specific remarkable outcome
occurs and who, upon being called in and finding that this outcome has
occurred, infers the existence of other trials in the past. In
that scenario, the inference to multiple trials (in the past)
is indeed rational, and so, according to McGrath, is the inference
from fine-tuned conditions to multiple universes.
The adequacy of McGrath’s casino analogy is contested as well.
Whereas in McGrath’s analogy the epistemic agent waits outside
the casino until the remarkable outcome occurs and she/he is called
in, “it is not as though we were disembodied spirits waiting for
a big bang to produce some universe which could accommodate us”,
as White puts it (2000: 268). Epstein (2017: 653) retorts that
“it is also not as though we were disembodied spirits keenly
observing [the universe] ɑ—our designated
potential home—and hoping that it, in particular, would be able
to accommodate us.” Epstein’s diagnosis is that the
inverse gambler’s fallacy charge rests on the Requirement of
Total Evidence in Bayesianism, which, according to him, is to be
rejected in cases like these. Draper (2020) as well as Barrett and
Sober (2020) defend the Requirement of Total Evidence and, in doing
so, attack Epstein’s criticism of the inverse gambler’s
fallacy charge. Bradley (2009) offers further casino analogies beyond
those considered by Hacking, McGrath and White which, according to
him, speak in favour of rejecting the inverse gambler’s fallacy
charge, but White’s diagnosis continues to find support, e.g.,
by Landsman (2016). Friederich (2019a; 2021: ch. 4) suggests that the
question of whether we can rationally infer a multiverse from
fine-tuning for life is so different from questions encountered in
more familiar contexts such as the casino scenarios that whether or
not the inference from fine-tuning to a multiverse commits the inverse
gambler’s fallacy may not have a determinate answer by the
standards of accepted rationality criteria.
As just outlined, it is controversial whether it is rational to infer
the existence of multiple universes from our universe’s
fine-tuning for life. However, if we had strong independent
evidence for other universes with life-hostile conditions, attempts to
account for why our own universe is life-friendly would most likely
seem futile. Thus independent evidence for some multiverse scenario
could have a strong impact on what we regard as a rational response to
fine-tuning for life. Proponents of the argument from fine-tuning for
the multiverse could moreover welcome such evidence as potentially
helping to motivate a non-negligible prior \(P(M)\) for the
multiverse.
Many physicists nowadays believe that a specific version of the
multiverse hypothesis is indeed suggested by contemporary developments
in fundamental physics, notably by the combination of inflationary
cosmology and string theory, both of which have been introduced in
 Section 2.3.
 According to many advocates of inflationary cosmology, the process of
inflation results in causally isolated space-time regions, so-called
“island universes”. This process is in general
“eternal” in that the formation of island universes never
ends. As a result, it leads to the production of a vast (and,
according to most models, infinite) “multiverse” of island
universes (Guth 2000).
As remarked in
 Section 2.3,
 string theory has an enormous number of lowest energy states (vacua),
which would manifest themselves at the level of observations and
experiments in terms of different higher level physical laws and
values of the constants. When combined with the idea of island
universes as suggested by inflationary cosmology one obtains a
cosmological picture in which there are infinitely many island
universes where all the different string theory
vacua—corresponding to different higher-level physical laws and
constants in these laws—are actually realized in the different
island universes. This so-called landscape multiverse
qualifies as a concrete multiverse scenario in the sense of the
argument from fine-tuning for the multiverse. A necessary condition is
of course that the collection of island universes that are part of the
landscape multiverse includes, as is widely believed to be the case,
at least one universe with the same effective (higher-level) laws and
constants as our own.
Unfortunately, concrete multiverse scenarios such as the landscape
multiverse are extremely difficult to test, precisely because they
entail that different universes exhibit very different conditions. The
broad consensus in the literature on multiverse cosmology is that, in
order for a multiverse scenario to qualify as empirically confirmed,
it must entail that those conditions that we find in our own universe
are typical among those found by observers across the
multiverse. Widely used formulations of typicality are
Vilenkin’s principle of mediocrity (Vilenkin 1995) and
Bostrom’s self-sampling assumption (Bostrom 2002).
Typicality principles can be regarded as refinements of the anthropic
principles (Bostrom 2002) in the form of indifference principles
of self-locating belief (Elga 2004): inasmuch as we are ignorant
about who and where among observers we are, they recommend to reason
as if we were equally likely to be any of the observers who we
might possibly be, given our empirical evidence.
Typicality principles have the benefit of making multiverse theories
at least in principle testable (Aguirre 2007; Barnes 2017). They are
controversial, however, because it is contested whether typicality is
always a reasonable assumption (Hartle & Srednicki 2007; Smolin
2007) and because it is difficult to specify with respect to which
reference class of observers typicality should be assumed. In
practice, observer proxies are chosen such as share of baryon matter
clustered in large galaxies (Martel et al. 1998) or entropy gradient
(Bousso et al. 2007). These difficulties are exacerbated in
cosmological scenarios such as the landscape multiverse in which
reference classes of observers that one might reasonably choose are
all infinite. The problem of regularizing those infinities corresponds
to the so-called measure problem of cosmology, according to
some cosmologists “the greatest crisis in physics today”
(Tegmark 2014: 314). (See Schellekens [2013: sect. VI.B] for an
introduction to the measure problem aimed at physicists, Smeenk [2014]
for a philosopher’s sceptical assessment of its solvability, and
Dorr & Arntzenius [2017] for a more optimistic perspective.)
Friederich (2021: ch. 8) argues that the freedom to choose, for
example, an observer proxy and a cosmic measure makes typicality-based
predictions from multiverse theories untrustworthy and susceptible to
confirmation bias.
The persisting difficulties with testing multiverse theories are a
prime reason why the multiverse idea itself continues to be viewed
very critically by many leading physicists (e.g., Ellis 2011).
According to many contemporary physicists, the most deeply problematic
instances of fine-tuning do not concern fine-tuning for life but
violations of naturalness—a principle of theory choice
in particle physics and cosmology that can be characterized as a
no fine-tuning criterion.
The idea that underlies naturalness is that the phenomena
described by some physical theory should not depend sensitively on
specific details of a more fundamental (currently unknown) theory to
which it is an effective low-energy approximation. In what follows,
the motivation, significance, and implementation of this idea in the
framework of quantum field theory are explained. For a more detailed
introduction aimed at physicists see Giudice (2008), for one aimed at
philosophers of physics see Williams (2015).
Modern physics regards our currently best theories of particle physics
collected in the Standard Model as effective field theories.
Effective field theories are low-energy effective approximations to
hypothesized more fundamental physical theories whose details are
currently unknown. An effective field theory has an in-built limit to
its range of applicability, determined by some energy scale
\(\Lambda\). When applied to phenomena associated with energies higher
than \(\Lambda\) the effective field theory will not deliver correct
results. At this point, the more fundamental theory must be considered
to which the effective field theory supposedly is a low-energy
approximation. For the theories collected in the Standard Model, it is
known that they cannot possibly be empirically adequate beyond
energies around the Planck scale \(\Lambda_{\textrm{Planck}}\approx
10^{19} \,\textrm{GeV}\), where—presently unknown—quantum
gravitational effects become relevant. However, the Standard Model may
well be empirically inadequate already at energy scales significantly
below the Planck scale. For example, if there is some presently
unknown particle with mass \(M\) smaller than the Planck scale
\(\Lambda_{\textrm{Planck}}\) but beyond the range of current
accelerator technology which interacts with particles described by the
Standard Model, the cut-off scale \(\Lambda\) where the Standard Model
becomes inadequate may well be \(M\) rather than
\(\Lambda_{\textrm{Planck}}\).
In an effective field theory, any physical quantity \(g_{\Fphys}\) can
be represented as the sum of a so-called bare quantity \(g_0\) and a
contribution \(\Delta g\) from vacuum fluctuations corresponding to
energies up to the cut-off \(\Lambda\): 
The bare quantity \(g_0\) can be regarded as a black box that sums up
effects associated with energies beyond the cut-off scale \(\Lambda\)
where unknown effects must be taken into account. Viewing a theory as
an effective field theory means viewing it as a self-contained
description of phenomena up to the cut-off scale \(\Lambda\). This
perspective suggests that one may only consider an effective theory as
natural if the physical quantity \(g_{\Fphys}\) can be of its
actual order of magnitude without any need for a delicate cancellation
between \(g_0\) and \(\Delta g\) to many orders of magnitude. Since
the bare quantity \(g_0\) sums up information about physics beyond the
cut-off scale \(\Lambda\), such a delicate cancellation between
\(g_0\) and \(\Delta g\) would mean that the order of magnitude of the
physical quantity \(g_{\Fphys}\) would be different if phenomena
associated with energies beyond the cut-off scale \(\Lambda\) were
slightly different.
One can characterize violations of naturalness as instances of
fine-tuning in that, where naturalness is violated,
low-energy phenomena depend sensitively on the details of some unknown
fundamental theory concerning phenomena at very high energies.
Physicists have developed ways of quantifying fine-tuning in this
sense (Barbieri & Guidice 1988), critically discussed by Grinbaum
(2012). It is controversial whether, to the degree that violations of
naturalness can be seen as instances of fine-tuning, they should be
regarded as problematic. Wetterich (1984) suggests that any
fine-tuning of bare parameters is unproblematic because those
parameters depend on the chosen regularization scheme and have no
independent physical meaning. As highlighted by Rosaler and Harlander
(2019), Wetterich’s perspective depends on an understanding of
quantum field theories as defined by entire trajectories
\(g^i(\Lambda)\) in parameter space.
An alternative criterion of naturalness—sometimes dubbed
absolute naturalness (see Wells [2015] for an empirical
motivation)—is that a theory is natural if and only if it can be
formulated using dimensionless numbers that are all of order \(1\).
More permissive is ’t Hooft’s technical
naturalness criterion (’t Hooft 1980), according to which a
theory is natural if it can be formulated in terms of numbers that are
either of order \(1\) or very small but such that, if they were
exactly zero, the theory would have an additional symmetry. The
motivation for this prima facie arbitrary criterion is that it
elegantly reproduces verdicts based on the above formulation of
naturalness according to which low-energy phenomena should not depend
sensitively on the details of some more fundamental theory with
respect to high energies.
A prime example of a violation of naturalness occurs in quantum field
theories with a spin \(0\) scalar particle such as the Higgs particle.
In this case, the dependence of the squared physical mass on the
cut-off \(\Lambda\) is quadratic: 
The physical mass of the Higgs particle is empirically known to be
\(m_{\FH,\Fphys}\approx125\,\textrm{GeV}\). The dominant contribution
to \(\Delta m^2\), specified as \(h_t\Lambda^2\) in equation
\(\eqref{Higgs}\), is due to the interaction between the Higgs
particle and the heaviest fermion, the top quark, where \(h_t\) is
some parameter that measures the strength of that interaction. Given
the empirically known properties of the top quark, the factor
\(\frac{h_t}{16\pi^2}\) is of order \(10^{-2}\). Due to its quadratic
dependence on the cut-off scale \(\Lambda\) the term
\(\frac{h_t}{16\pi^2}\Lambda^2\) is very large if the cut-off scale is
large. If the Standard Model is valid up to the Planck scale
\(\Lambda_{\textrm{Planck}}\approx10^{19}\,\textrm{GeV}\), the squared
bare mass \(m_{\FH,0}^2\) and the effect of the vacuum fluctuations
would have to cancel each other out to about 34 orders of magnitude in
order to result in a physical Higgs mass of \(125\,\textrm{GeV}\).
There is no known physical reason why the effects collected in the
bare mass \(m_{\FH}\) should be in such a delicately balance with the
effects from the vacuum fluctuations collected in \(\Delta m^2\). The
fact that two fundamental scales—the Planck scale and the Higgs
mass—are so widely separated from each other is referred to as
the hierarchy problem. As a consequence of this problem, the
violation of naturalness due to the Higgs mass is so severe.
Various solutions to the naturalness problem for the Higgs mass have
been proposed in the form of theoretical alternatives to the Standard
Model. In supersymmetry (see Martin [1998] for an introduction),
contributions to \(\Delta m_{\FH}^{2}\) from supersymmetric partner
particles can compensate the contribution from heavy fermions such as
the top quark and thereby eliminate the fine-tuning problem. However,
supersymmetric theories with this feature appear to be disfavoured by
more recent experimental results, notably from the Large Hadron
Collider (Draper et al. 2012). Other suggested solutions to the
naturalness problem for the Higgs particle include so-called
Technicolour models (Hill & Simmons 2003), in which the
Higgs particle is replaced by additional fermionic particles, models
with large extra dimensions, where the hierarchy between the Higgs
mass and the Planck scale is drastically diminished (Arkani-Hamed et
al. 1998), and models with so-called warped extra dimensions
(Randall & Sundrum 1999).
An even more severe violation of naturalness is created by the
cosmological constant \(\rho_V\), which specifies the overall vacuum
energy density. Here the contribution due to vacuum fluctuations is
proportional to the fourth power of the cut-off scale \(\Lambda\):
The physical value \(\rho_V\) of the cosmological constant is
empirically found to be of order \(\rho_V\sim10^{-3}\,\textrm{eV}\).
The constant \(c\), which depends on parameters that characterize the
top quark and the Higgs particle, is empirically known to be roughly
of order \(1\). If we take the cut-off to be of the order of the
Planck scale \(\Lambda\sim10^{19}\,\textrm{GeV}\), the bare term
\(\rho_0\), must cancel the contribution \(c\Lambda^4\) to more than
120 orders of magnitude. Even if we assume a cut-off as low as
\(\Lambda\sim1\,\textrm{TeV}\), i.e., already within reach of current
accelerator technology, we find that a cancellation between \(\rho_0\)
and \(c\Lambda^4\) to about 50 digits remains necessary. Contrary to
the case of the Higgs mass, there are few ideas of how future physical
theories might be able to avoid this problem.
As explained in
 Section 5.1,
 violations of naturalness can be seen as instances of fine-tuning,
but not in the sense of fine-tuning for life. A connection between
naturalness and fine-tuning for life can be constructed, however,
along the following lines:
One can interpret equations \(\eqref{Higgs}\) and
\(\eqref{cosmo_constant}\) as suggesting that the actual physical
values of the Higgs mass and the cosmological constant are much
smaller than the values that one would expect for them in the
framework of the Standard Model. Notably, if the Higgs mass were of
order of the cut-off \(\Lambda\), e.g., the Planck scale, and if the
cosmological constant were of order \(\Lambda^4\), the bare parameters
would not need to be fixed to many digits in order for the physical
parameters to have their respective orders of magnitude, which means
that the physical values would be natural. Thus, assuming naturalness
and the validity of our currently best physical theories up to the
Planck scale, one would expect values for the Higgs mass and the
cosmological constants of the same order of magnitude as their vacuum
contributions, i.e., values much larger than the actual ones.
With respect to the problem of specifying probability distributions
over possible values of physical parameters discussed in
 Section 2.1
 naturalness may be taken to suggest that all reasonable such
distributions have most of their probabilistic weight close to the
natural values. As explained, for the Higgs mass and the
cosmological constant the natural values are much larger than the
observed ones. Advocates of the view that fine-tuning for life
requires a response because life-friendly constants are improbable
therefore put particular emphasis on those instances of fine-tuning
for life that are associated with violations of naturalness, notably
the cosmological constant (e.g., Susskind 2005: ch. 2; Donoghue 2007;
Collins 2009: sect. 2.3.3; Tegmark 2014: 140f.).