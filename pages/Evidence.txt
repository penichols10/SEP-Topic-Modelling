 Evidence, whatever else it is, is the kind of thing which can make a
difference to what one is justified in believing or (what is
often, but not always, taken to be the same thing) what it is
reasonable for one to believe. Some philosophers hold that
what one is justified in believing is entirely determined by one's
evidence. This view—which sometimes travels under the banner of
‘Evidentialism’—can be formulated as a supervenience
thesis, according to which normative facts about what one is justified
in believing supervene on facts about one's evidence (See especially
Conee and Feldman 2004). Thus, according to the Evidentialist, any two
individuals who possessed exactly the same evidence would be exactly
alike with respect to what they are justified in believing about any
given question.
 Given Evidentialism, various traditional debates within the theory of
knowledge are naturally cast as debates about the status of various
underdetermination theses. Thus, the skeptic about our knowledge
of the external world maintains that one's evidence (understood,
perhaps, as the totality of one's present experiences) does not favor
one's ordinary, commonsense views about one's surroundings over
various skeptical alternatives (e.g., the hypothesis that one is
hallucinating in an undetectable way). Similarly, one longstanding
controversy that divides realists and antirealists in the philosophy
of science can be understood as a debate about whether the kind of
evidence which is available to scientists is ever sufficient to
justify belief in theories that quantify over entities that are in
principle unobservable, such as electrons or quarks.
 Inasmuch as evidence is the sort of thing which confers
justification, the concept of evidence is closely related to other
fundamental normative concepts such as the concept of a
reason. Indeed, it is natural to think that ‘reason to
believe’ and ‘evidence’ are more or less synonymous,
being distinguished chiefly by the fact that the former functions
grammatically as a count noun while the latter functions as a mass
 term.[4] 
 To the extent that what one is justified in believing depends upon
one's evidence, what is relevant is the bearing of one's
total evidence. Even if evidence E is sufficient to
justify believing hypothesis H when considered in isolation,
it does not follow that one who possesses evidence E is
justified in believing H on its basis. For one might possess
some additional evidence E′, such that one is not
justified in believing H given E and
E′. In these circumstances, evidence E′
defeats the justification for believing H that would
be afforded by E in its absence. Thus, even if I am initially
justified in believing that your name is Fritz on the basis
of your testimony to that effect, the subsequent acquisition of
evidence which suggests that you are a pathological liar tends to
render this same belief unjustified. A given piece of evidence is
defeasible evidence just in case it is in principle
susceptible to being undermined by further evidence in this way;
evidence which is not susceptible to such undermining would be
indefeasible evidence. It is controversial whether any
evidence is indefeasible in this
 sense.[5]
 Following Pollock (1986), we can distinguish between
undercutting and rebutting defeaters. Intuitively,
where E is evidence for H, an undercutting defeater
is evidence which undermines the evidential connection between
E and H. Thus, evidence which suggests that you are
a pathological liar constitutes an undercutting defeater for your
testimony: although your testimony would ordinarily afford excellent
reason for me to believe that your name is Fritz, evidence
that you are a pathological liar tends to sever the evidential
connection between your testimony and that to which you testify. In
contrast, a rebutting defeater is evidence which prevents E
from justifying belief in H by supporting not-H in a
more direct way. Thus, credible testimony from another source that
your name is not Fritz but rather Leopold constitutes a rebutting
defeater for your original testimony. It is something of an open
question how deeply the distinction between ‘undermining’
and ‘rebutting’ defeaters cuts.
 Significantly, defeating evidence can itself be defeated by yet
further evidence: at a still later point in time, I might acquire
evidence E″ which suggests that you are not a
pathological liar after all, the evidence to that effect having been
an artifice of your sworn enemy. In these circumstances, my initial
justification for believing that your name is Fritz afforded by the
original evidence E is restored. In principle, there is no
limit to the complexity of the relations of defeat that might obtain
between the members of a given body of evidence. Such complexity is
one source of our fallibility in responding to evidence in the
appropriate way. 
 In order to be justified in believing some proposition then, it is
not enough that that proposition be well-supported by some proper
subset of one's total evidence; rather, what is relevant is how
well-supported the proposition is by one's total evidence. In
insisting that facts about what one is justified in believing
supervene on facts about one's evidence, the Evidentialist should be
understood as holding that it is one's total evidence that is
relevant. Of course, this leaves open questions about what relation
one must bear to a piece of evidence E in order for
E to count as part of one's total evidence, as well as the
related question of what sorts of things are eligible for inclusion
among one's total
 evidence.[6]
 Given the thesis that evidence is that which justifies belief, one's
intuitions about the evidence that is available to an individual in a
hypothetical scenario will shape one's views about what the individual
would be justified in believing in that scenario. Of course, one can
also theorize in the opposite direction as well: to the extent that
one has independent intuitions about what an individual would be
justified in believing in a given scenario, such intuitions will shape
one's views about what evidence must be available to an individual so
situated—and therefore, one's views about the more general
theoretical issue about what evidence is, or what sorts of things can
and cannot qualify as evidence. Thus, if one is firmly convinced that
an individual in circumstances C might be justified in
believing that p is the case, it follows immediately that
being in circumstances of kind C is consistent with having
evidence sufficient to justify the belief that p. As we will
see below (Section 2), reasoning of this general form has often
encouraged a picture according to which one's total evidence is
exhausted by one's present experiences.
 Here is an example of the way in which intuitions about justification
can drive one's account of evidence, given a commitment to the
Evidentialist thesis that changes in what an individual is justified
in believing always reflect changes in her total evidence. It is
sometimes suggested that how confident a scientist is justified in
being that a given hypothesis is true depends, not only on the
character of relevant data to which she has been exposed, but also on
the space of alternative hypotheses of which she is aware. According
to this line of thought, how strongly a given collection of data
supports a hypothesis is not wholly determined by the content of the
data and the hypothesis. (Nor is it wholly determined by their content
together with the scientist's background theory of how the world
works.) Rather, it also depends upon whether there are other plausible
competing hypotheses in the field. It is because of this that the mere
articulation of a plausible alternative hypothesis can dramatically
reduce how likely the original hypothesis is on the available
 data.[7] 
 Consider an historical example that is often thought to illustrate
this phenomenon. Many organisms manifest special characteristics that
enable them to flourish in their typical environments. According to
the Design Hypothesis, this is due to the fact that such
organisms were designed by an Intelligent Creator (i.e., God). The
Design Hypothesis is a potential explanation of the relevant facts: if
true, it would account for the facts in question. How much support do
the relevant facts lend to the Design Hypothesis? Plausibly, the
introduction of the Darwinian Hypothesis as a competitor in the
nineteenth century significantly diminished the support enjoyed by the
Design Hypothesis. That is, even if there were no reason to
prefer the Darwinian Hypothesis to the Design Hypothesis, the
mere fact that the Design Hypothesis was no longer the only potential
explanation in the field tends to erode (to some extent at least) how
much credence the Design Hypothesis merits on the basis of the
relevant considerations.
 Assume for the sake of illustration that what one is justified in
believing does in fact depend upon the space of alternative hypotheses
of which one is aware: as new hypotheses are introduced, one's
justification for believing already proposed hypotheses changes. Given
the Evidentialist thesis that differences in justification are always
underwritten by differences in evidence, it follows that a complete
specification of one's evidence at any given time will make reference
to the set of hypotheses which one is aware of at that time. This is
an example of the way in which intuitive judgments about what
individuals are justified in believing in certain circumstances, when
coupled with a commitment to Evidentialism, can drive one's theory of
evidence (i.e., make a difference to which items one classifies as
‘evidence’ in one's theorizing).
 The justifying or rationalizing role of evidence is also central to
other prominent epistemological views, including views which are
strictly speaking incompatible with Evidentialism as formulated
above. Consider, for example, Bayesianism. (See the entry on
 Bayesian epistemology.)
 The Bayesian holds that what it is reasonable for one to believe
depends both on the evidence to which one is exposed as well as on
one's prior probability distribution. According to the Bayesian then,
two individuals who share exactly the same total evidence might differ
in what it is reasonable for them to believe about some question in
virtue of having started with different prior probability
distributions. Still, inasmuch as Bayesians often focus upon rational
belief change, or on what is involved in rationally revising
one's beliefs over time, the justificatory role of evidence retains a
certain pride of place within the Bayesian scheme. For Bayesians
typically maintain that that which distinguishes those changes in
one's beliefs that are reasonable from those that are not is that the
former, unlike the latter, involve responding to newly-acquired
evidence in an appropriate
 way.[8]
 Thus, for the Bayesian no less than for the Evidentialist, it is
evidence which justifies that which stands in need of justification. 
 Notably, even views which tend to marginalize the role of a subject's
evidence in determining facts about what he or she is justified in
believing typically do not take facts about the subject's evidence to
be wholly irrelevant. Consider, for example, reliabilist
theories of epistemic justification (Goldman 1979, 1986). In its
purest and most straightforward form, reliabilism holds that the
status of a token belief as justified or unjustified depends upon
whether or not the psychological process which gives rise to the
belief is a reliable one, i.e., one that is truth-conducive. When
formulated in this way, the concept of evidence plays no role in the
reliabilist account of justification: in particular, the status of a
given belief as justified or unjustified depends upon whether the
relevant belief-forming process is in fact reliable, and not on
any evidence which the believer might possess which bears on the
question of its reliability (or even, for that matter, on any evidence
which the believer might possess which bears more directly on the
truth of the belief itself). Thus, someone who was in fact a reliable
clairvoyant would be justified in holding beliefs that she forms on
the basis of clairvoyance, even if her total evidence strongly
suggested both that (i) she does not possess the faculty of
clairvoyance, and that (ii) the relevant beliefs are false (BonJour
1985, Chapter 3). However, in response to such examples, reliabilists
typically seek to accommodate the intuition that such a subject is not
justified in maintaining her reliably-arrived-at beliefs in the face
of her evidence, and they seek to modify the simple reliabilist
account to allow for this (See, e.g., Goldman 1986: 109–112). The felt
need to modify the original, more straightforward account is, perhaps,
a testament to the resilience of the idea that one's evidence can make
a difference to what one is justified in believing—even if
other factors are also taken to be relevant.
 It is characteristic of rational thinkers to respect their evidence.
Insofar as one is rational, one is disposed to respond appropriately
to one's evidence: at any given time, one's views accurately reflect
the character of one's evidence at that time, and one's views manifest
a characteristic sensitivity or responsiveness to changes in one's
evidence through time. Of course, rationality is no guarantee of
correctness. Indeed, in a given case one might be led astray by
following one's evidence, as when one's evidence is
misleading.  But being mistaken is not the same as being
unreasonable. To the extent that one respects one's evidence, one is
not unreasonable even when one is
 wrong.[9] 
 The foregoing remarks, although bordering on the platitudinous,
naturally suggest a substantive model of the norms which govern our
practice of belief attribution. According to the model in question, in
attributing beliefs to you, I should, all else being equal, attribute
to you the belief that p just in case it would be reasonable
for you to believe that p given your total evidence. This is
the core idea behind one popular version of the Principle of
Charity. According to this line of thought, I am justified in drawing
inferences about what you believe on the basis of my knowledge of your
epistemic situation. Thus, if I know that your evidence strongly
suggests that it will rain today, then (all else being equal)
I should attribute to you the belief that it will rain today. On the
other hand, if I know that your evidence strongly suggests that it
will not rain today, then I should likewise attribute to you
a belief to that effect. Although on a given occasion a thinker who is
generally reasonable might fail to believe in accordance with her
evidence, such cases are exceptional. In the absence of any reason to
think that a given case is exceptional in this way, one is licensed to
draw inferences about the contents of another's beliefs on the basis
of information about the character of her evidence. The default
assumption is that a person's beliefs are those that it is appropriate
for her to hold given the evidence to which she has been exposed.
 Above, we noted that in a given case one might be led astray by
following one's evidence: even if p is true, one's evidence
might misleadingly suggest that p is not true. When one's
evidence is misleading, one typically arrives at a false belief by
believing in accordance with it. We ordinarily assume that such cases
are exceptional. Are there possible worlds in which such cases are the
norm? Consider a careful and judicious thinker who consistently and
scrupulously attends to his evidence in arriving at his beliefs. In
our world, these habits lead to cognitive prosperity—the
individual holds a relatively large number of true beliefs and
relatively few false beliefs. (Or at least, he fares significantly
better with respect to truth and falsity than those who fail to attend
to their evidence and instead form their beliefs in a hasty or
haphazard manner.)  Consider next how the same individual fares in a
world that is subject to the machinations of a Cartesian evil demon, a
being bent on deceiving the world's inhabitants as to its true
character. Although the true character of the world in question
differs radically from our own, it is, from the point of view of its
inhabitants, utterly indistinguishable, for the Demon takes care to
ensure that the courses of experiences that the inhabitants undergo
are qualitatively identical to the courses of experiences that they
undergo in our non-delusory world. In the world run by the Cartesian
Demon, our thinker is no less judicious and no less scrupulous in
attending to (what he blamelessly takes to be) relevant considerations
than he is in our world. Because of his unfortunate circumstances,
however, his beliefs embody a radically false picture of his
environment. Granted that the thinker's beliefs about his environment
are false, are they any less justified than in our world? Is the
thinker himself any less rational? Many philosophers maintain that the
thinker's beliefs are equally well-justified and that the thinker
himself is equally rational in the two worlds (See e.g., Cohen 1984
and Pryor 2001). Apparently, there is strong intuitive resistance to
the idea that a thinker whose underlying dispositions and habits of
thought remain unchanged might become less rational simply in
virtue of being located in less fortuitous circumstances. As
Williamson (2000) has forcefully emphasized, however, embracing the
judgement that the thinker is equally rational in ‘the good
case’ and ‘the bad case’ tends to push one
inexorably towards a conception of evidence according to which one's
evidence is exhausted by one's subjective, non-factive mental
states. For if rationality is a matter of responding correctly to
one's evidence, then the judgement that the thinker is equally
rational in the good case and the bad case would seem to require that
the thinker has the same evidence for his beliefs in both cases. But
ex hypothesi, the only thing common to the good case and the
bad case that is a plausible candidate for being the thinker's
evidence are his non-factive mental states. Thus, the judgement that
the thinker is equally rational in both cases, when conjoined with the
view that rationality is a matter of responding to one's evidence in
the appropriate way, seems to force the conclusion that the thinker's
evidence is limited to his non-factive mental states even in the
good case. In this way, the requirement that the thinker has the
same evidence in the good case and the bad case seems to encourage
what Williamson calls ‘the Phenomenal Conception of
 Evidence’.[10]
 Consider also how the aforementioned Principle of Charity encourages
such a picture of evidence when it is applied to the world run by the
Demon. In attributing beliefs to an individual in the bad case, one
attributes exactly those beliefs that one would attribute if the same
individual were in the good case. For if the Demon's illusions are
truly undetectable, failure to detect them hardly seems to constitute
a failure of rationality. In attributing commonsense beliefs
to the individual in the bad case, one proceeds according to the
Principle of Charity: after all, it seems that these are exactly the
beliefs that even a perfectly rational (though not infallible) being
would have in the circumstances. But if the commonsense beliefs are no
less reasonable when held in the bad case, then the individual's
evidence for those beliefs must be just as strong in the bad case as
in the good case. Indeed, it is natural to describe the bad case as a
world in which the thinker's evidence is systematically
misleading. The trick to being a good Evil Demon (one
might think) is to be effective at planting misleading
evidence. Intuitively, the Demon misleads his victims by
exploiting their rationality, inasmuch as he trades on the
sensitivity of their beliefs to misleading evidence. (Indeed, those
who dogmatically cling to favored theories in the face of apparently
disconfirming evidence would seem to be relatively less vulnerable to
being manipulated by the Demon.) But the Demon misleads by providing
his victims with misleading experiences. Hence the temptation to
simply identify one's evidence with one's experiences: once again, the
phenomenal conception of evidence looms. 
 As Williamson emphasizes, the insistence that one's evidence is
identical in the good case and the bad case effectively rules out many
otherwise-attractive accounts of evidence:
 In view of its apparent consequences for the theory of evidence, the
idea that one's evidence is the same in the good case and the bad case
warrants further scrutiny. Again, it is uncontroversial that there is
a robust distinction between nonculpable error on the one hand and
irrationality or unreasonableness on the other. Nonculpable error does
not in general make for irrationality, even when such error is
relatively widespread and pervasive. Still, it's worth asking just how
much weight the distinction in question can bear. Is any amount of
nonculpable error about the environment in which one is embedded
compatible with perfect rationality? Or rather, at some point, does a
sufficient amount of error about one's environment tend to compromise
one's ability to form rational beliefs about that environment?
 Here is one line of thought for the conclusion that a sufficient
level of nonculpable error about one's environment does tend
to compromise one's ability to arrive at rational beliefs about that
environment. It's plausible to suppose that much if not all of the
value that we place on believing rationally depends on a connection
between believing rationally and believing what is true (although the
precise nature of this connection is no doubt a particularly vexing
topic). One might worry that a view according to which even ideal,
perfect rationality can coexist harmoniously with a more or less
completely mistaken view of one's situation threatens to attenuate the
connection between believing rationally and believing truly too far,
and to render obscure why the former would be valuable relative to the
latter. To put the same point in terms of evidence: plausibly, much if
not all of the value of respecting one's evidence consists in the
putative link between doing so and believing the truth. Given this,
one might worry that a view according to which perfectly following
one's evidence is compatible with a more or less completely mistaken
view of one's situation threatens to render obscure why following
one's evidence would be a good thing to do relative to the goal of
having true rather than false beliefs.
 This line of thought is not decisive, however. In general, the value
of x might consist in its serving as a means to y,
even if there are conditions in which relying on x utterly
fails to bring about (or even frustrates the achievement of)
y. Thus, it might be that the value of a given drug consists
in its being the miracle cure for some disease, even though in certain
conditions the drug would have the effect of aggravating the disease.
Similarly, it might be that we value following our evidence as a means
to believing what is true, even though we recognize that there are
circumstances such that, were we unfortunate enough to be in them,
doing so would hinder or frustrate that goal.
 A different tack is pursued by Williamson, who argues at length that
we should not accept the idea that one has the same evidence in the
good case and the bad case. Central to his argument is the contention
that, even if one were to adopt the phenomenal conception of evidence,
this would not allow one to vindicate the underlying intuitions that
seemed to make its adoption attractive in the first place; hence, the
phenomenal conception of evidence is ultimately not well-motivated. As
we have seen, it is the desire to preserve the intuition that a
sufficiently scrupulous thinker in the bad case can be reasonable in
his beliefs (indeed, no less reasonable than a similarly scrupulous
thinker in the good case) which seems to rule out any conception of
evidence according to which one's evidence might consist of (say) true
propositions or facts about the external world. For a thinker in the
bad case is not in a position to recognize facts about the external
world; he is, however, in a position to recognize facts about his own
experiences. The view that one's evidence is limited to one's
experiences thus seems to be motivated by the idea that one's
evidence, no matter what else is true of it, must be the sort of thing
that one is always in a position to correctly take into account, at
least in principle. But (it is claimed) one's experiences are the
things that one is always in a position to correctly take into
account. Williamson argues that this last thought is a mistake: in
fact, one is not always in a position to correctly take into account
one's experiences, even in principle. Indeed, Williamson argues that
there is no non-trivial condition which is such that one is
always in a position to know that it obtains.  Thus, the thought that
evidence might be such that one is always in a position to know what
one's evidence is is a chimerical one. To insist that in order for x
to be among one's evidence, x must be such that one is always in a
position to know whether one's evidence includes x is thus to impose a
misguided and unrealizable desideratum on the theory of
evidence. In short: ‘Whatever evidence is, one is not
always in a position to know what one has of it’ (2000: 178,
emphasis added).
 Having
rejected the phenomenal conception, Williamson proposes that we take a
subject's evidence to consist of all and only those propositions that
the subject
 knows.[11]
 Williamson elaborates this simple and straightforward idea with great
sophistication; here we focus exclusively on the way in which the
resulting theory interacts with the theme that rational thinkers
respect their evidence. Of course, one immediate consequence of the
view that a subject's evidence consists of his knowledge is that a
thinker in the good case and a thinker in the bad case will
differ—indeed, differ significantly—in the evidence which
they possess.  When a thinker in the good case comes to know that
there is blood on the knife in virtue of having a visual
experience as of blood on the knife, the relevant proposition becomes
part of his total evidence. In contrast, when a thinker in the bad
case is caused by the Demon to undergo the same experience (or at
least, an experience that is qualitatively indistinguishable) and
arrives at the same belief, the relevant proposition is not part of
his total evidence, for the relevant proposition is not true and hence
not known. Inasmuch as the scrupulous thinker in the good case will
know far more than the scrupulous thinker in the bad case, the former
will have far more evidence for his beliefs than the latter. Given
that the two thinkers have the same beliefs, it seems that the thinker
in the good case will be significantly more reasonable in holding
those
 beliefs.[12] 
 How much of a cost is this? We should distinguish between two
different intuitions one might have about a thinker in the bad
case. The first intuition is that a thinker in the bad case has
exactly the same evidence as a thinker in the good
case. Perhaps abandoning this intuition is not much of a cost (if it
is any cost at all). A different intuition is the following: when a
thinker in the bad case takes his experiences at face value and forms
beliefs about the external world in the usual manner, those beliefs
are not simply unreasonable, in the way that they would be if, for
example, the thinker adopted those same beliefs on a whim, or in the
absence of any reason to do so at all.  Abandoning this intuition
would seem to be a much heavier price to pay.  However, it is
contentious whether this intuition can be preserved on a view
according to which one's evidence consists of one's knowledge.
Consider, once again, a thinker in the bad case who is caused by the
Demon to have a visual experience as of being in the presence of a
bloodied knife. Possessing no reason to doubt that the experience is
veridical, the thinker forms the belief that there is blood on the
knife in the usual manner. Intuitively, this belief is at the
very least better justified than it would be in the absence of the
relevant visual experience. On the supposition that one's evidence
consists of those propositions that one knows, we can ask: what known
proposition or propositions justify this belief, to the extent that it
is justified?  The proposition that there is blood on the knife is
false and therefore not known. Perhaps the thinker's evidence for his
belief that there is blood on the knife is the true proposition that
(i) it appears that there is blood on the knife or the true
proposition that (ii) my experience is as of there being blood on
the
 knife.[13]
 However, some philosophers maintain that in typical cases of
perception, one does not form beliefs about how things appear to one,
or about how one's perceptual experience presents things as being:
rather, in response to one's experiences, one simply forms beliefs
about the external world
 itself.[14]
 If this is correct, then, given that knowledge requires belief,
propositions like (i) and (ii) are not known because they are not
believed. Hence, if this model is correct, then, on the view that
one's evidence consists only of known propositions, the thinker's
belief that there is blood on the knife seems to lack any
 justification.[15]
 According to the phenomenal conception of evidence, only
one's experiences can serve as evidence. According to Williamson's
conception of evidence as knowledge, one's experiences are excluded
from counting as evidence—at best, one's evidence includes
whatever propositions about one's experiences that one knows. Even if
one abandons the phenomenal conception of evidence, however, one might
hold on to the idea that one's evidence includes one's
experiences, inasmuch as one's experiences can and often do make some
difference to what one is justified in believing, regardless of
whether one forms beliefs about those experiences themselves. A view
of evidence that is more liberal than either Williamson's or the
phenomenal conception might thus take one's evidence to include both
one's experiences and one's knowledge, on the grounds that the beliefs
of a rational thinker will exhibit direct sensitivity both to what he
knows and to the experiences that he undergoes. The question of
whether one's experiences—as opposed to one's beliefs about
one's experiences, or one's knowledge of one's
experiences—can play a direct role in justifying beliefs
about the external world is a much contested one in the philosophy of
perception; it will not be pursued further here.
An issue that has recently come to the fore concerns the distinction
between first-order evidence and higher-order
evidence (Christensen 2010, Feldman 2005, Kelly 2005, 2010,
Lasonen-Aarnio 2014). Intuitively, first-order evidence E is
evidence that bears directly on some target proposition or hypothesis
H. Higher-order evidence is evidence about the character
of E itself, or about subjects' capacities and dispositions for
responding rationally to E. Suppose that a trained
meteorologist carefully surveys the available meteorological data and
concludes that it will rain tomorrow. Here, the
meteorological data (E) is first-order evidence that bears on
the hypothesis (H), that it will rain tomorrow.  Now
consider the fact that the meteorologist arrived at the view that
it will rain tomorrow on the basis of E. This fact is
higher-order evidence, inasmuch as it is evidence about the content
and import of the original meteorological data E. In
particular, given that the meteorologist is generally competent when
it comes to assessing the relevant kind of evidence, the fact that she
arrived at the view that H on the basis of E is evidence for
the epistemic propositions that E supports H. Moreover, at
least in many contexts, the fact that the meteorologist arrived at the
view that H on the basis of E will count as evidence,
not only for the epistemic proposition that E supports H, but
also for the hypothesis itself, i.e., it will rain
tomorrow. This seems especially clear in cases in which a third
party lacks access to the original meteorological evidence E
(or is incompetent to assess that evidence) but does know that the
meteorologist arrived at the verdict that it will rain
tomorrow on its basis. In these circumstances, it makes sense for
the third party to increase his credence in rain tomorrow, once he
learns what the meteorologist has concluded. In effect, in these
circumstances, one treats the fact that the meteorologist arrived at
the belief that it will rain tomorrow as a kind of proxy for
the meteorological evidence to which one lacks access, or which one is
incompetent to assess (Kelly 2005). Here evidence of evidence
(for H) is itself evidence for H (Feldman 2005). The
general lesson is that higher-order evidence sometimes serves as
evidence that should make a difference not only to what one believes
about the first-order evidence, but also to one's beliefs about the
world itself.
Other cases, however, are less clear-cut. For example, suppose that a
second trained meteorologist evaluates the available meteorological
data E and arrives at her own view about the possibility that
it will rain tomorrow. She then learns that the first meteorologist
arrived at the view that it will rain tomorrow on the basis of
evidence E. Should the second meteorologist count her
colleague's opinion as additional evidence for the hypothesis
that it will rain tomorrow? Or would doing so be in effect to engage
in a kind of illegitimate double counting of the original evidence
(Kelly 2005, Matheson 2009)? More generally, in what circumstances,
exactly, is evidence of evidence (for some proposition) evidence for
that proposition (Fitelson 2012, Feldman 2014)? Questions about the
nature and bearing of higher-order evidence are topics of active
research.
 If E is evidence for some hypothesis H, then
E makes it more likely that H is true: in such
circumstances, E confirms H. On the other
hand, if E is evidence against H, then E
makes it less likely that H is true: E
disconfirms H. Verification is the limiting
case of confirmation: a piece of evidence verifies a hypothesis in
this sense just in case it conclusively establishes that hypothesis as
true. At the other end of the spectrum, falsification is the
limiting case of disconfirmation: a piece of evidence falsifies a
hypothesis just in case it conclusively establishes that the
hypothesis is false. It is at least somewhat controversial whether
full-fledged verification or falsification in this sense ever
 occurs.[16]
 Plausibly, there are some propositions whose truth or falsity we
grasp in an utterly direct, unmediated way. Consider, for example,
simple arithmetical truths such as the proposition that
2+2=4. Traditionally, such truths have been held to be
‘self-evident’; allegedly, they need only to be understood
in order to be known. If the truth value of every proposition were
transparent in this way, perhaps we would have little or no need for
evidence. In contrast, a central function of evidence is to make
evident that which would not be so in its absence.
 In general, we rely on evidence in cases in which our access to truth
would otherwise be problematic. One's recognition that the earth
is roughly spherical in shape seems to depend on one's evidence
in a way that one's recognition that 2+2=4 does not. Of course, it can
be a contested matter whether one's access to truth in some domain is
problematic—and thus, whether one is dependent upon evidence for
grasping truths about that domain. Common sense holds that we often
have unproblematic access to facts about our immediate physical
environment via sense perception; perhaps in part for this
reason, common sense regards it as at the very least odd, if not
simply wrong, to say that one who finds himself face-to-face with what
is clearly a pig thereby has strong evidence that the animal
is a pig. (Although it would no doubt also be odd to assert that one
lacks evidence that the animal is a pig in such
circumstances.) In contrast, much traditional epistemology holds that
one's access to such truths is always deeply problematic; what is
unproblematic, rather, is one's recognition that one's experiences
represent the world as being a certain way. Hence, much traditional
epistemology construes the relationship between one's experiences and
one's beliefs about the physical world on the model of the
relationship between evidence and hypothesis. On this model, the
fallibility of sense perception is assimilated to the fallibility of
non-deductive inference. (The above quotation from Austin is, of
course, a protest against the model in question.)
 As a general matter, evidence seems to play a mediating
role vis-a-vis our efforts to arrive at an accurate picture of the
world: we seek to believe what is true by means of holding beliefs that
are well-supported by the evidence, and we seek to avoid believing what
is false by means of not holding beliefs that are not well-supported by
the evidence. The picture is well summarized by Blanshard:
 Indeed, it is plausible to suppose that both the capacity of evidence
to justify belief (Section 1) and the fact that rational thinkers
respect their evidence (Section 2) depends upon the connection between
evidence and truth.
 Why should attending to evidence constitute a
promising way of pursuing an accurate view of the world? This question
is more readily answered on some conceptions of evidence than on
others. Thus, consider a theory according to which evidence consists of
facts. Given that no true proposition is inconsistent with any
fact, one has an immediate rationale for not believing any proposition
that is inconsistent with one's evidence, for only propositions that
are consistent with one's evidence are even candidates for being true.
The same holds for Williamson's conception of evidence as knowledge:
inasmuch as any known proposition is true, inconsistency with one's
evidence entails inconsistency with some truth. Put the other way
around: if evidence consists of facts or known propositions, then no
body of evidence rules out any truth. Notice that the same is not true
for conceptions of evidence according to which one's evidence consists
of one's beliefs, or one's experiences, or propositions of which one is
psychologically certain: that a proposition is inconsistent with one of
my beliefs, or with the content of one of my experiences, or with a
proposition of which I am psychologically certain, does not guarantee
that it is false.
 Perhaps the root notion of evidence is that of something which serves
as a reliable sign, symptom, or mark of that which it is evidence
of. In Ian Hacking's phrase, this is ‘the evidence of
one thing that points beyond itself’ (1975: 37). Thus, smoke is
evidence of fire, Koplik spots evidence of measles, a certain
distinctive and off-putting smell evidence of rotten egg. Here, the
paradigm would seem to be that of straightforward correlation: the
reason why smoke counts as evidence of fire, but not of impending
rain, is that smoke is a reliable indicator or symptom of the former
but not of the latter. Taken at face value, the idea of evidence as
reliable indicator tends to encourage an inclusive picture of what
sorts of things are eligible to count as evidence, according to which
either mental or non-mental objects, events and states of affairs can
qualify as such. For such entities would seem to be perfectly capable
of standing in the relevant relation to other objects, events and
states of affairs.
 Consider the claim that
 On what is perhaps its most natural reading, the truth of this claim
was an empirical discovery of medical science. At a certain point in
time, it was discovered that Koplik spots are a reliable indication of
measles—something which was true, presumably, long before
the discovery in question. Here, the evidence relation is understood
as a relation that either obtains or fails to obtain independently of
what anyone knows or believes about its
 obtaining.[17]
 To the extent that one is concerned to arrive at an accurate picture
of the world, knowledge of instances of this relation—roughly,
knowledge of what bits of the world tend to accompany what other bits
of the world—would seem to be exactly the sort of thing which
one is seeking.  When the evidence relation is construed in this way,
investigating it is of a piece with investigating the world
itself.
 Similarly 
 
 would seem to have the same empirical status as (1), differing
chiefly in that it is much more widely known. 
 When evidence is understood in this way, it is no mystery why
attending to evidence is a good strategy for one who is concerned to
arrive at an accurate picture of the world: given that Koplik spots
are in fact a reliable indicator of measles, it obviously behooves
those who are concerned to have true beliefs about which individuals
are suffering from measles to pay attention to facts about which
individuals have Koplik spots. Similarly, given that smoke is in fact
a reliable indicator of fire, those who are concerned to have true
beliefs about the presence or absence of fire do well to pay attention
to the presence or absence of smoke. Thus, when we understand
‘E is evidence for H’ as more or less
synonymous with ‘E is a reliable indicator of
H’, the connection between evidence and truth seems
easily secured and relatively straightforward. 
 Of course, although the presence of Koplik spots is in fact a
reliable guide to the presence of measles, one who is ignorant of this
fact is not in a position to conclude that a given patient has
measles, even if he or she is aware that the patient has Koplik
spots. Someone who knows that Koplik spots are evidence of measles is
in a position to diagnose patients in a way that someone who is
ignorant of that fact is not. In general, the extent to which one is
in a position to gain new information on the basis of particular
pieces of evidence typically depends upon one's background
knowledge. This fact is a commonplace among philosophers of science
and has also been emphasized by philosophically-sophisticated
 historians.[18]
 Suppose that one knows that a particular patient has Koplik spots but
is ignorant of the connection between Koplik spots and
measles. Moreover, suppose that one's ignorance is not itself the
result of any prior irrationality or unreasonableness on one's part:
rather, one has simply never had the opportunity to learn about the
connection between Koplik spots and measles. In these circumstances,
does one have evidence that the patient has measles? Taken in one
sense, this question should be answered affirmatively: one does have
evidence that the patient has measles, although one is not in a
position to recognize that one does.  However, the idea that one has
evidence in such circumstances seems to sit somewhat awkwardly with
the themes that evidence tends to justify belief, and that rational
thinkers are sensitive to their evidence. For consider the moment when
one first learns that the patient has Koplik spots. Given one's
ignorance of the connection between Koplik spots and measles, one is
not in any way unreasonable if one fails to become more confident that
the patient has measles. Indeed, given one's ignorance, it seems that
one would be unreasonable if one did become more confident
that the patient has measles upon learning that she has Koplik spots,
and that a belief that the patient has measles held on this basis
would be unjustified.
 This suggests that the notion of evidence in play in statements such
as ‘evidence tends to justify belief’ and ‘rational
thinkers respect their evidence’ cannot simply be identified
with evidence in the sense of reliable indicator.  Let's call evidence
in the former sense normative evidence, and evidence in the
latter sense indicator evidence.  Although the normative
notion of evidence cannot simply be identified with the indicator
notion, we would expect the two to be closely linked, inasmuch as
one's possession of normative evidence frequently depends upon one's
awareness that one thing is indicator evidence of something else.
 Reflection on the role that considerations of background theory play
in determining how it is reasonable for one to respond to new
information have convinced some that the normative notion of evidence
is better understood in terms of a three place-relation rather than a
two-place relation. According to this view, judgements of the form
‘E is evidence for H’—when this is
understood as more or less synonymous with ‘E tends to
make it more reasonable to believe H’— are
typically elliptical for judgements of the form ‘E is
evidence for H relative to background theory
T’. Thus, given that your background theory includes
the claim that Koplik spots are a reliable indication of measles, the
fact that a particular patient has Koplik spots constitutes normative
evidence for you (gives you a reason to believe that) the patient has
measles. On the other hand, given that my background theory does
not include the claim that Koplik spots are a reliable
indication of measles, the fact that the same patient has Koplik spots
does not constitute normative evidence for me (give me a
reason to believe that) the patient has measles.
 The view that the status of E as normative evidence for
H can depend upon considerations of background theory
immediately raises questions about the status of the background theory
itself. Given that one's background theory consists of some set of
propositions, which set is it? Is it the set of propositions that one
knows? Or rather, the set of propositions that one believes? Or
perhaps, the set of propositions that one justifiably
believes? It seems that E might count as evidence for
H relative to the set of propositions that one believes but
not relative to the set of propositions that one knows (or vice
versa)—which of these, if either, determines whether E
is evidence for H, in the sense that one's possession of
E tends to justify one in believing that H is true?
The issues that arise here are subtle and delicate; Christensen (1997)
is a careful and illuminating discussion.
A note about confirmation theory.  Although philosophy had
in some sense long been concerned with questions about when evidence
makes a theory more likely to be true, the investigation of this
relationship reached new levels of systematicity and rigor during the
positivist era. The positivists thought of philosophy as ‘the
logic of science’; they thus took it to be a central task of
philosophy to furnish detailed analyses and explications of
fundamental scientific concepts such as explanation and
confirmation.
 Hempel (1945) and Carnap (1950) each distinguished two different
‘concepts’ of confirmation: the
‘classificatory’ or ‘qualitative’ concept on
the one hand and the ‘quantitative’ concept on the
other. Roughly, the classificatory concept is employed in the making
of yes-or-no judgements about whether a given piece of evidence does
or does not support a given hypothesis.  Thus, it is the
classificatory concept which is in play when one is concerned with
judgements of the following form: ‘Hypothesis H is
confirmed by evidence E’. On the other hand, the
quantitative concept is employed in making numerical judgements about
how much support a hypothesis derives from a given piece of
evidence (e.g,. ‘Hypothesis H is confirmed by evidence
E to degree
 R’).[19]
 Formal theories attempting to explicate each of these notions were
developed. Hempel (1945) took the lead in attempting to explicate the
qualitative concept while Carnap (1950, 1952) concentrated on the
quantitative concept. During this period, the philosophical study of
the relationship between evidence and theory took on, perhaps for the
first time, the characteristics of something like normal science, and
became a discipline replete with technical problems, puzzles, and
paradoxes, the anticipated solutions to which were viewed as items on
the agenda for future
 research.[20]
 Here lie the origins of present-day confirmation theory, as
represented by Bayesianism in its protean forms (See, e.g., Jeffrey
1965, 1992, 2004, Horwich 1983, Howson and Urbach 1993) and its rivals
(e.g., Glymour's (1980) ‘bootstrapping’ model of
confirmation).
 Although Carnap's own vision for confirmation theory was ultimately
 abandoned,[21]
 the quantitative approach that he championed proved influential to
the subsequent development of the subject. In particular, the emphasis
on attempting to understand confirmation in quantitative terms paved
the way for the increased use of mathematics—and
specifically, the probability calculus—in the philosophical
study of evidence. The idea that the probability calculus provides the
key to understanding the relation of confirmation is central to
Bayesianism, the dominant view within contemporary confirmation
theory. An examination of Bayesianism will not be undertaken
 here.[22]
 Instead, we will simply take note of the explication of the concept
of evidence which the Bayesian offers. At the outset of the present
section, we noted that evidence confirms a theory just in case that
evidence makes the theory more likely to be true; evidence disconfirms
a theory just in case the evidence renders the theory less likely to
be true. The Bayesian takes these platitudes at face value and offers
the following probabilistic explication of what it is for E
to be evidence for H: 
 That is, E is evidence for H just in case the
conditional probability of H on E is greater than
the unconditional probability of H. Thus, the fact that
the suspect's blood is on the knife is evidence for the
hypothesis that the suspect committed the murder if and only
if the probability that the suspect committed the murder is greater
given that his blood is on the knife than it would be otherwise.
 Similarly 
 That is, E is evidence against H just in case the
conditional probability of H on E is less than the
unconditional probability of H. Thus, the fact that the
suspect's fingerprints are not on the knife is evidence against
the hypothesis that the suspect committed the murder if and
only if the probability that the suspect committed the murder is lower
given the absence of his fingerprints on the knife than it would be
otherwise. Within this probabilistic model, verification (in the sense
of conclusive confirmation) would involve bestowing probability 1 on
an hypothesis while falsification would involve bestowing probability
0 on it.
 This straightforward probabilistic model of evidence and confirmation
is an attractive and natural one. Indeed, hints of it are found in
Anglo-American
 law.[24]
 The model is not without its critics, however. Achinstein (1983)
contends that something can increase the probability of a claim
without providing evidence for that claim. For example, the
information that seven-time Olympic swimming champion Mark Spitz went
swimming increases the probability that Mark Spitz has just drowned;
nevertheless, according to Achinstein, the former hardly constitutes
evidence that the latter is
 true.[25]
 According to a line of thought in Goodman (1955), the notion of
confirmation that is crucial for science is not to be understood in
terms of straightforward increase-in-probability. Thus, consider the
generalization that All of the change in my pocket consists of
nickels.  Examining one of the coins in my pocket and finding
that it is a nickel undoubtedly increases the probability that the
generalization is true, inasmuch as it now has one less potential
falsifier. But Goodman contends the relevant observation does not
confirm the generalization, inasmuch as the observation should not
make one more confident that any of the other, as-yet-unexamined coins
in my pocket is itself a nickel.  (According to Goodman, although
‘accidental generalizations’ such as ‘All the coins
in my pockets are nickels’ can have their probabilities raised
by observations, they cannot be confirmed by them, in the way that
‘law-like generalizations’ can be.)
 In general, the idea
that the probability calculus provides the key to understanding the
concept of evidence has found greater favor among philosophers of
science than among traditional epistemologists. In the next and final
section, we turn to a cluster of themes that have also been much
emphasized by philosophers of science, themes which came to the fore as
a result of philosophical reflection upon the role that evidence plays
within scientific practice itself.
 It is natural to suppose that the concept of evidence is intimately
related to the cognitive desideratum of objectivity.
According to this line of thought, individuals and institutions are
objective to the extent that they allow their views about what is the
case or what ought to be done to be guided by the evidence, as opposed
to (say) the typically distorting influences of ideological dogma,
prejudice in favor of one's kin, or texts whose claim to authority is
exhausted by their being venerated by tradition. To the extent that
individuals and institutions are objective in this sense, we should
expect their views to increasingly converge over time: as shared
evidence accumulates, consensus tends to emerge with respect to
formerly disputed questions. Objective inquiry is evidence-driven
inquiry, which makes for intersubjective agreement among
 inquirers.[26]
 Thus, it is widely thought that the reason why the natural sciences
exhibit a degree of consensus that is conspicuously absent from many
others fields is that the former are evidence-driven—and therefore,
objective—in ways that the latter are not. 
 According to this picture, a central function of evidence is to serve
as a neutral arbiter among rival theories and their
adherents. Whatever disagreements might exist at the level of theory,
if those who disagree are objective, then the persistence of their
disagreement is an inherently fragile matter, for it is always hostage
to the emergence of evidence which decisively resolves the dispute in
one direction or the other. Our ability to arrive at consensus in such
circumstances is thus constrained only by our resourcefulness and
ingenuity in generating such evidence (e.g., by designing and
executing crucial experiments) and by the generosity of the world in
offering it up.
 The slogan ‘the priority of evidence to theory’ has
sometimes been employed in an attempt to capture this general
theme. However, this slogan has itself been used in a number of
importantly different ways that it is worth pausing to
distinguish. 
 First, the claim that ‘evidence is prior to theory’ might
suggest a simple model of scientific method that is often associated
(whether fairly or unfairly) with Francis Bacon. According to the
model in question, the collection of evidence is temporally
prior to the formulation of any theory or theories of the
relevant domain. That is, the first stage in any properly-conducted
scientific inquiry consists in gathering and classifying a large
amount of data; crucially, this process is in no way guided or aided
by considerations of theory. Only after a large amount of data has
been gathered and classified does the scientist first attempt to
formulate some theory or theories of the domain in question. On this
model, then, evidence is prior to theory within the context of
discovery. This model—which Hempel (1966) dubbed ‘the
narrow inductivist account of scientific inquiry’—is now
universally rejected by philosophers. For it is now appreciated that,
at any given time, which theories are accepted—or more weakly,
which theories are taken to be plausible hypotheses—typically
plays a crucial role in guiding the subsequent search for evidence
which bears on those theories. Thus, a crucial experiment might be
performed to decide between two rival theories T1
and T2; once performed, the outcome of that
experiment constitutes an expansion in the total evidence which is
subsequently available to the relevant scientific community. If,
however, the two leading contenders had been theories
T1 and T3, a different crucial
experiment would have been performed, which would have (typically)
resulted in a different expansion in the total evidence. The point
that the formulation of hypotheses is often temporally prior to the
collection of evidence which bears on their truth (and that this
priority is no accident) is perhaps most immediately apparent on
Popper's falsificationist model of science (1959), according to which
exemplary scientific practice consists in repeatedly attempting to
falsify whichever theory is presently most favored by the relevant
scientific community. But it is no less true on other, less radical
models of science, which (unlike Popper's) allow a role for confirming
evidence as well as disconfirming evidence. As Hempel puts the
point,
 A second, quite different, sense of ‘priority’ in which
evidence has sometimes been held to be prior to theory is that of
semantic priority. According to this view, the meanings of
hypotheses that involve ‘theoretical terms’ (e.g.,
‘electron’) depend upon the connections between such
hypotheses and that which would count as evidence for their
truth—typically on such accounts, the observations that would
confirm them. The view that (observational) evidence is semantically
prior to theories was central to the logical positivist conception of
science. On this picture, meaning flows upward from the level of
observation; a given theory is imbued with whatever meaning it has in
virtue of standing in certain relations to the observational level,
which constitutes the original locus of meaning. The picture was
gradually abandoned, however, in the face of repeated failures to
carry through the kind of theoretical reductions that the picture
seemed to demand, as well as appreciation of the point, forcefully
emphasized by
 Putnam[28]
 and others, that the meanings of
theoretical terms do not seem to change as our views about what counts
as confirming evidence for hypotheses in which they occur evolve.
 A third and final sense of ‘priority’ in which evidence
has often been thought to be prior to theory is that of
epistemic priority. On this view, it is not that the task of
evidence gathering either is or ought to be performed earlier in time
than the task of formulating theories; nor is semantic priority at
issue. Rather, the thought is that theories depend for their
justification on standing in certain relations to evidence (understood
here, once again, as that which is observed), but that observations do
not themselves depend upon theories for their own justification. That
is: (observational) evidence is prior to theory within the context
of justification. This, perhaps, is the interpretation of
‘evidence is prior to theory’ on which the slogan enjoys
its greatest plausibility. For it seems that our justification for
believing any presently-accepted theory of natural or social science
typically does depend upon suitable observations having been made, but
that, on the other hand, one can be justified in taking oneself to
have observed that such-and-such is the case even if there is
at present no available theory as to why such-and-such is the
case (and indeed, even if such-and-such's being the case is unexpected
or unlikely given the theories that one presently accepts). To the
extent that such a justificational asymmetry exists, there would seem
to be some truth to the idea that evidence is epistemically prior to
 theory.[29]
 And this sort of priority might seem to be exactly what is required
if evidence is to play the role of neutral arbiter among those who
come to the table with different theoretical commitments.
 The idea of evidence as a kind of ultimate court of appeal, uniquely
qualified to generate agreement among those who hold rival theories,
is a highly plausible one. Nevertheless, complications with this
simple picture—some more serious than
others—abound. Above, we took note of the widely-held view
according to which the bearing of a given piece of evidence on a given
hypothesis depends on considerations of background theory. Thus, two
individuals who hold different background theories might disagree
about how strongly a particular piece of evidence confirms a given
theory, or indeed, about whether the evidence confirms the theory at
all. Of course, if the question of who has the superior background
theory is itself susceptible to rational adjudication, then this
possibility need pose no deep threat to objectivity. Often enough,
this will be the case. Suppose, for example, that you treat the fact
that 
as evidence that
 while I do not, simply because you know that Koplik spots are
typically an effect of measles while I am ignorant of this fact. Here,
that you treat (i) as evidence for (ii) while I do not is attributable
to the straightforward superiority of your background theory to mine:
you possess a crucial piece of medical knowledge that I
lack. Presumably, if I were to acquire the relevant bit of medical
knowledge, then I too would treat (i) as confirming evidence for
(ii). The possibility that those who are relatively ill-informed might
differ from those who are better informed in the way that they respond
to evidence does not itself cast doubt on the capacity of evidence to
play the role of neutral arbiter among theories; clearly, what is
needed in such cases is for the worse informed party to become privy
to those facts of which they are presently ignorant. 
 However, a recurrent motif in twentieth century philosophy of science
is that the bearing of evidence on theory is mediated by factors that
might vary between individuals in ways that do not admit of such
rational adjudication. Imagine two eminent scientists, both of whom
are thoroughly acquainted with all of the available evidence
which bears on some theory. One believes the theory, the other
believes some different, incompatible theory instead.  Must one of the
two scientists be making a mistake about what their shared evidence
supports? Perhaps the most natural answer to this question, at least
at first blush, is ‘Yes’. A fair amount of twentieth
century philosophy of science shied away from this natural answer,
however, at least in part because of a growing awareness of just how
often eminent, fully-informed and seemingly rational
scientists have disagreed in the history of
 science.[30]
 Thus, Thomas Kuhn (1962) would maintain that both scientists might be
perfectly rational in holding incompatible theories, inasmuch as
rationality is relative to a paradigm, and the two scientists might be
operating within different paradigms. Similarly, Carnap (1952) would
maintain that both scientists might be rational because rationality is
relative to an inductive method or confirmation function, and the two
scientists might be employing different inductive methods or
confirmation functions. As we have noted, for the contemporary
Bayesian, how it is reasonable to respond to a given body of evidence
depends upon one's prior probability distribution: the two scientists
might thus both be rational in virtue of possessing different prior
probability distributions. There are, of course, important differences
among these accounts of rationality. But notice that they each possess
the same structure: what it is reasonable for one to believe depends
not only on one's total evidence but also on some further feature
F (one's prior probability distribution, paradigm, inductive
method.).  Because this further feature F can vary between
different individuals, even quite different responses to a given body
of evidence might be equally reasonable. On such views, the bearing of
a given body of evidence on a given theory becomes a highly
relativized matter. For this reason, the capacity of evidence to
generate agreement among even impeccably rational individuals is in
principle subject to significant limitations. 
 Why is relativity to a prior probability
distribution (paradigm, inductive method) any more threatening to the
idea of evidence as neutral arbiter than the previously mentioned
relativity to background theory? The difference is this: when I fail to
treat Koplik spots as evidence of measles, there is a clear and
straightforward sense in which my background theory is inferior to
yours. By contrast, proponents of the views presently under
consideration typically insist that a relatively wide range of prior
probability distributions (confirmation functions, inductive methods,
paradigms) might be equally good. For this reason, such views
are often criticized on the grounds that they turn the relationship
between evidence and theory into an overly subjective affair.
 Proponents of such views are not wholly without resources for
attempting to alleviate this concern. In particular, many subjective
Bayesians place great weight on a phenomenon known as the
‘swamping’ or ‘washing out’ of the
priors. Here, the idea is that even individuals who begin with quite
different prior probabilities will tend to converge in their views
given subsequent exposure to a sufficiently extensive body of common
evidence. However, the significance of the relevant convergence
results is highly
 controversial.[31] 
 According to the accounts briefly canvassed here, two individuals of
impeccable rationality might radically disagree about the bearing of a
particular piece of evidence E on a given hypothesis
H. Even in such cases, the disputants are not wholly without
common ground. For they at least agree about the characterization of
the evidence E: their disagreement concerns rather the
probative force of E with respect to H. An even more
radical challenge to the capacity of evidence to serve as neutral
arbiter among rival theories concerns the alleged theory-ladenness
of observation.  According to proponents of the doctrine of
theory-ladenness, in cases of fundamental theoretical dispute, there
will typically be no theoretically-neutral characterization of the
evidence available.  Rather, adherents of rival theories will
irremediably differ as to the appropriate description of the
 data itself.[32] 
 The doctrine of theory-ladenness is perhaps best appreciated when it
is viewed against the background of the positivist tradition to which
it was in large part a reaction. For the positivists, evidence was
both epistemically and semantically prior to theory. Moreover, central
to the positivist conception of science as a paradigm of rationality
and objectivity is the idea that its disputes admitted of rational
adjudication by appeal to evidence that could be appreciated by both
sides. For these reasons, the positivists often insisted that the
fundamental units of evidential significance—observation
statements or ‘protocol sentences’—should employ
only vocabulary that is within the idiolect of any minimally competent
speaker of the relevant language.  Ideally then, observation
statements should be comprehensible to and verifiable by individuals
who possess no specialized knowledge or terminological
sophistication. Thus, Carnap recommended ‘blue’ and
‘hard’ as exemplary predicates for observation
 sentences.[33]
 As against this, Hanson insisted that
 But what holds for the physicist and the layman holds also for two
physicists with sufficiently different theoretical
commitments. Thus
 The extent to which observation is theory-laden remains a contested
 matter.[34]
 To the extent that evidence which bears on a theory does not admit of
a neutral characterization among those with sufficiently different
theoretical commitments, this threatens to limit the ability of such
evidence to successfully discharge the role of neutral arbiter in such
cases. Still, whatever concessions to the proponents of
theory-ladenness might be in order, the significance of the doctrine
that they defend should not be overstated. For in any case, it seems
undeniable that theories sometimes are discredited by the emergence of
evidence which is taken to undermine them—even in the eyes
of their former
 proponents.[35]
 Let's assume then that evidence sometimes does successfully discharge
the function of neutral arbiter among theories and is that which
secures intersubjective agreement among inquirers. What must evidence
be like, in order for it to play this role? That is, given that
evidence sometimes underwrites intersubjective agreement, what
constraints does this place on answers to the question: what sorts of
things are eligible to count as evidence?
 Above, we noted that the traditional epistemological demand that
one's evidence consist of that to which one has immediate and
unproblematic access—and indeed, that one's evidence must be
such that one can appreciate it (at least it in principle) even when
one is in the most dire of epistemic predicaments—has often
encouraged a phenomenal conception of evidence, according to which
one's evidence is limited to one's experiences or sense data. On this
picture, evidence consists of essentially private mental states,
accessible only to the relevant subject. This picture of evidence
stands in no small measure of tension with the idea that a central
function of evidence is to serve as a neutral arbiter among competing
views. For it is natural to think that the ability of evidence to play
this latter role depends crucially on its having an
essentially public character, i.e., that it is the sort of
thing which can be grasped and appreciated by multiple
individuals. Here, the most natural contenders would seem to be
physical objects and the states of affairs and events in which they
participate, since it is such entities that are characteristically
accessible to multiple observers. (I ask what evidence there is for
your diagnosis that the patient suffers from measles; in response, you
might simply point to or demonstrate the lesions on
her skin.) On the other hand, to the extent that one's evidence
consists of essentially private states there would seem to be no
possibility of sharing one's evidence with others. But it is precisely
the possibility of sharing relevant evidence which is naturally
thought to secure the objectivity of science. Indeed, it has often
been held that inasmuch as the objectivity of science is underwritten
by the fact that science is evidence driven, it is the public
character of scientific evidence which is crucial. On this view, it is
a central methodological norm of science to eschew as inadmissible
(e.g.) any alleged episodes of incommunicable insight in considering
whether to accept or reject a claim. The theme of the essentially
public character of scientific evidence has been prominent from the
earliest days of modern science—it was championed, for example,
by Robert Boyle, founder of the Royal Society, who insisted that the
‘witnessing’ of experiments was to be a collective
 act[36]—and
 has remained so up until the present day. Unsurprisingly, the theme
was taken up by philosophers of science with gusto. Thus, Hempel
required that ‘all statements of empirical science be capable of
test by reference to evidence which is public, i.e., evidence which
can be secured by different observers and does not depend essentially
on the observer’ (1952: 22). The idea was echoed by other
leading positivists (see, e.g., Feigl 1953) as well as by Popper
(1959). More recently, in the course of reviewing ‘some of the
things objectivity, and specifically scientific objectivity, has been
thought to involve’, Peter Railton singles out the idea that
 In short, it is not simply that the conception of evidence gleaned
from exemplary scientific practice differs from the conception of
evidence that traditional epistemology seemed to demand (as would be
the case if, for example, one was somewhat more inclusive than the
other). Rather, what seemed to be one of the characteristic features
of scientific evidence, viz. its potential publicity, is the exact
contrary of one of the characteristic features of evidence as
construed by much traditional epistemology, viz. its privacy. Such a
tension was bound to generate a certain amount of dissonance. One
historically noteworthy manifestation of this dissonance was the
protracted debate within the positivist tradition over the nature of
‘protocol sentences’. Here, Carnap's odyssey is perhaps
the most illuminating. Early in his philosophical career, Carnap,
under the influence of Russell and Ernst Mach (and through them, the
tradition of classical empiricism) took sense data as the ultimate
evidence for all of our empirical knowledge.  (Indeed, his early work
Der Logischer Aufbau der Welt (1928) is largely devoted to
the Russellian project of ‘constructing’ the external
world out of sense data.) During this period, Carnap maintained that
the protocol sentences that serve as the ‘confirmation
basis’ for scientific theories refer to sense data or (to be
more precise) ‘the sensing of sense data’. However, under
the influence of both Popper and Otto Neurath, Carnap ultimately
abandoned this view of protocol sentences in favor of one on which
such sentences refer not to private mental events but rather to public
objects and states of affairs. As recounted in his philosophical
autobiography, the primary motivation for this change of heart was the
growing conviction that the conception of evidence as private mental
states rendered it inadequate to ground the intersubjectivity and
objectivity of
 science.[37]
 In this area as in others, the evolution of Carnap's own views was
crucial to the evolution of the views of the Vienna Circle as a whole.
The episode was well-recounted years later by Ayer: 
 Inasmuch as it is the distinctive function of protocol sentences to
report one's evidence, a view about what sorts of contents a protocol
sentence might have is in effect a view about what sorts of things can
count as evidence. To abandon a view according to which the subject
matter of any protocol sentence concerns the ‘private
incommunicable experiences’ of some particular individual is in
effect to abandon a version of the phenomenal conception of
evidence. To adopt in its place a view according to which the subject
matter of any protocol sentence concerns ‘public physical
events’ is thus a particularly radical shift, inasmuch as this
latter view seems to entail that only public physical events
can count as evidence—and in particular, that experiences,
formerly taken to exhaust the category of evidence, are ineligible to
count as
 such.[38] 
 Underneath the positivists' changing views about the nature of
protocol sentences, however, lay a fundamental assumption that they
never questioned. The assumption in question is the following: that
there is some general restriction on the subject matter of a sentence
if that sentence is a potential statement of one's evidence. This
assumption was explicitly rejected by Austin: 
 According to the alternative picture sketched by Austin, there are,
indeed, circumstances in which one's having knowledge that things
are a certain way depends upon one's having
‘phenomenal’ evidence provided by the fact that is how
things look, seem, or appear.  Traditional
epistemology errs, however, in thinking that this is the general
case. Indeed, there are circumstances in which the fact that things
are a certain way constitutes one's evidence for judgements about how
they look, seem or
 appear.[39] 
 Inasmuch as he denies that there are any general restrictions on the
subject matter of an evidence statement, Austin's way of thinking
about evidence is considerably more liberal or inclusive than that of
much of the tradition. In this respect, his account of evidence
resembles Williamson's (2000) later theory. As we have seen,
Williamson holds that one's evidence consists of everything that one
knows. In particular, one's evidence is not limited to one's knowledge
of one's experiences, nor is it limited to one's observational
knowledge—one's evidence also includes any theoretical
knowledge that one might possess (p. 190). 
 On such liberalized views, although one's evidence is not limited to
one's introspectively arrived at knowledge of one's experiences, it
includes everything that one knows about one's experiences on
the basis of introspection. In this respect, such views are
incompatible not only with the phenomenal conception of evidence but
also with views that would rule out the objects of introspection as
evidence on the grounds that the objects of introspection lack the
objectivity and publicity that is characteristic of genuine
evidence. However, it is dubious that any view on which evidence plays
a role in justifying belief can consistently observe a constraint
which would preclude the objects of introspection from counting as
genuine evidence. Goldman (1997) argues that any such constraint is
inconsistent with the introspectionist methodology employed in various
areas of contemporary cognitive science and that this undercuts
‘the traditional view … that scientific evidence can be
produced only by intersubjective methods that can be used by different
investigators and will produce agreement’ (p. 95).
 Reflection on examples drawn from more homely contexts also casts
doubt on the idea that all genuine evidence is in principle accessible
to multiple individuals. When one has a headache, one is typically
justified in believing that one has a headache. While others might
have evidence that one has a headache—evidence afforded,
perhaps, by one's testimony, or by one's non-linguistic
behavior—it is implausible that whatever evidence others
possess is identical with that which justifies one's own belief that
one has a headache. Indeed, it seems dubious that others could have
one's evidence, given that others cannot literally share one's
headache. 
 Here then we see another context in which theoretical demands are
placed on the concept of evidence that seem to pull in different
directions. On the one hand, it is thought central to the concept of
evidence that evidence is by its very nature the kind of thing that
can generate rational convergence of opinion in virtue of being shared
by multiple individuals. This encourages the idea that any genuine
piece of evidence can in principle be grasped by multiple individuals;
anything which cannot be so grasped is either not genuine evidence or
is at best a degenerate species thereof. On the other hand, evidence
is taken to be that which justifies belief. And it seems that many of
the beliefs which individuals hold about their own mental lives on the
basis of introspection are justified by factors with respect to which
they enjoy privileged access. Notably, the positivists' embrace of the
idea that protocol sentences refer exclusively to publicly-observable
physical objects and events was accompanied by an embrace of
behaviorism in
 psychology.[40]
 It is characteristic of behaviorism to denigrate the idea that the
deliverances of introspection can constitute genuine evidence; on this
combination of views then, the thesis that all evidence consists of
that which can be shared by multiple observers is upheld. For those who
reject behaviorism, however, the idea that at least some evidence does
not meet this condition is a more difficult one to resist.