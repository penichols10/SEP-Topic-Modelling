A teacher announces that there will be a surprise test next week. A
student objects that this is impossible: “The class meets on
Monday, Wednesday, and Friday. If the test is given on Friday, then on
Thursday I would be able to predict that the test is on Friday. It
would not be a surprise. Can the test be given on Wednesday? No,
because on Tuesday I would know that the test will not be on Friday
(thanks to the previous reasoning) and know that the test was not on
Monday (thanks to memory). Therefore, on Tuesday I could foresee that
the test will be on Wednesday. A test on Wednesday would not be a
surprise. Could the surprise test be on Monday? On Sunday, the
previous two eliminations would be available to me. Consequently, I
would know that the test must be on Monday. So a Monday test would
also fail to be a surprise. Therefore, it is impossible for there to
be a surprise test.”
Can the teacher fulfill her announcement? We have an embarrassment of
riches. On the one hand, we have the student’s elimination
argument. (For a recent formalization, see Holliday 2017.) On the
other hand, common sense says that surprise tests are possible even
when we have had advance warning that one will occur at some point.
Either of the answers would be decisive were it not for the
credentials of the rival answer. Thus we have a paradox. But a paradox
of what kind? ‘Surprise test’ is being defined in terms of
what can be known. Specifically, a test is a surprise if and only if
the student cannot know beforehand which day the test will
occur. Therefore the riddle of the surprise test qualifies as an
epistemic paradox.
Paradoxes are more than edifying surprises. Professor
Statistics announces she will give random quizzes: “Class meets
every day of the week. Each day I will open by rolling a die. When the
roll yields a six, I will immediately give a quiz.” Today,
Monday, a six came up. So you are taking a quiz. The last question of
her quiz is: “Which of the subsequent days is most likely to be
the day of the next random test?” Most people answer that each
of the subsequent days has the same probability of being the next
quiz. But the correct answer is: Tomorrow (Tuesday). 
Uncontroversial facts about probability reveal the mistake and
establish the correct answer. For the next test to be on Wednesday,
there would have to be a conjunction of two events: no test
on Tuesday (a 5/6 chance of that) and a test on Wednesday (a
1/6 chance). The probability for each subsequent day becomes less and
less. (It would be astounding if the next quiz day were a hundred days
from now!) The question is not whether a six will be rolled on any
given day, but when the next six will be rolled. Which day is
the next one depends partly on what happens meanwhile, as well as
depending partly on the roll of the die on that day. 
This riddle is instructive and will be referenced throughout this
entry. But the existence of quick, decisive solution shows that only a
mild revision of our prior beliefs was needed. In contrast, when our
deep beliefs conflict, proposed amendments reverberate unpredictably.
“Problems worthy of attack prove their worth by fighting
back” (Hein 1966).
The solution to a complex epistemic paradox relies on
solutions (or partial solutions) to more fundamental epistemic
paradoxes. The surprise test paradox, which will be disassembled in
stages throughout this essay, conveniently illustrates this nesting of
paradox within paradox. Inside the surprise test is the lottery
paradox; inside the lottery paradox is the preface paradox; inside the
preface paradox is Moore’s paradox (all of which will discussed
below). In addition to this depth-wise connection, there are lateral
connections to other epistemic paradoxes such as the knower paradox
and the problem of foreknowledge.
There are also ties to issues that are not clearly paradoxes –
or to issues whose status as paradoxes is at least contested. Some
philosophers find only irony in self-defeating predictions,
only cognitive illusion in the Monty Hall problem, only an
embarrassment in the “knowability paradox”
(discussed below). Calling a problem a paradox tends to quarantine it
from the rest of our inquiries. Those who wish to rely on the
surprising result will therefore deny that there is any paradox. 
The surprise test paradox has yet more oblique connections to some
paradoxes that are not epistemic, such as the liar paradox and
Pseudo-Scotus’ paradoxes of validity. They will be discussed in
passing, chiefly to set boundaries. 
We can look forward to future philosophers drawing edifying historical
connections. The backward elimination argument underlying the surprise
test paradox can be discerned in German folktales dating back to 1756
(Sorensen 2003a, 267). Perhaps, medieval scholars explored these
slippery slopes. But let me turn to commentary to which we presently
have access.
In the twentieth century, the first published reaction to the surprise
text paradox was to endorse the student’s elimination argument.
D. J. O’Connor (1948) regarded the teacher’s
announcement as self-defeating. If the teacher had not announced that
there would be a surprise test, the teacher would have been able to
give the surprise test. The pedagogical moral of the paradox would
then be that if you want to give a surprise test do not announce your
intention to your students!
More precisely, O’Connor compared the teacher’s
announcement to sentences such as ‘I remember nothing at
all’ and ‘I am not speaking now’. Although these
sentences are consistent, they “could not conceivably be true in
any circumstances” (O’Connor 1948, 358). L. Jonathan Cohen
(1950) agreed and classified the announcement as a pragmatic paradox.
He defined a pragmatic paradox to be a statement that is falsified by
its own utterance. The teacher overlooked how the manner in which a
statement is disseminated can doom it to falsehood. 
Cohen’s classification is too monolithic. True, the
teacher’s announcement does compromise one aspect of the
surprise: Students now know that there will be a test. But this
compromise is not itself enough to make the announcement
self-falsifying. The existence of a surprise test has been
revealed but perhaps that allows surviving uncertainty as to
which day the test will occur. The announcement of a
forthcoming surprise aims at changing uninformed ignorance into
action-guiding awareness of ignorance. A student who misses the
announcement does not realize that there is a test. If no one passes
on the intelligence about the surprise test, the student with simple
ignorance will be less prepared than classmates who know they do not
know the day of the test. 
Announcements are made to serve different goals simultaneously.
Competition between accuracy and helpfulness makes it possible for an
announcement to be self-fulfilling by being self-defeating. Consider a
weatherman who warns ‘The midnight tsunami will cause fatalities
along the shore’. Because of the warning, spectacle-seekers make
a special trip to witness the wave. Some drown. The weatherman’s
announcement succeeds as a prediction by backfiring as a warning.
Instead of viewing self-defeating predictions as showing how the
teacher is refuted, some philosophers construe self-defeating
predictions as showing how the student is refuted. The
student’s elimination argument embodies hypothetical predictions
about which day the teacher will give a test. Isn’t the student
overlooking the teacher’s ability and desire to thwart those
expectations? Some game theorists suggest that the teacher could
defeat this strategy by choosing the test date at random. 
Students can be kept uncertain if the teacher is willing to be
faithfully random. She will need to prepare a quiz each day. She will
need to brace for the possibility that she will give too many quizzes
or too few or have an unrepresentative distribution of quizzes. 
If the instructor finds these costs onerous, then she may be tempted
by an alternative: at the beginning of the week, randomly select a
single day. Keep the identity of that day secret. Since the student
will only know that the quiz is on some day or other, pupils will not
be able to predict the day of the quiz. 
Unfortunately, this plan is risky. If, through the chance process, the
last day happens to be selected, then abiding by the outcome means
giving an unsurprising test. For as in the original scenario, the
student has knowledge of the teacher’s announcement and
awareness of past testless days. So the teacher must exclude random
selection of the last day. The student is astute. He will replicate
this reasoning that excludes a test on the last day. Can the teacher
abide by the random selection of the next to last day? Now the
reasoning becomes all too familiar. 
Another critique of the student’s replication of the
teacher’s reasoning adapts a thought experiment from Michael
Scriven (1964). To refute predictive determinism (the thesis that all
events are foreseeable), Scriven conjures an agent
“Predictor” who has all the data, laws, and calculating
capacity needed to predict the choices of others. Scriven goes on to
imagine, “Avoider”, whose dominant motivation is to avoid
prediction. Therefore, Predictor must conceal his prediction. The
catch is that Avoider has access to the same data, laws, and
calculating capacity as Predictor. Thus Avoider can duplicate
Predictor’s reasoning. Consequently, the optimal predictor
cannot predict Avoider. Let the teacher be Avoider and the student be
Predictor. Avoider must win. Therefore, it is possible to give a
surprise test.
Scriven’s original argument assumes that Predictor and Avoider
can simultaneously have all the needed data, laws, and calculating
capacity. David Lewis and Jane Richardson object:
According to Lewis and Richardson, Scriven equivocates on ‘Both
Predictor and Avoider have enough time to finish their
calculations’. Reading the sentence one way yields a truth:
against any given avoider, Predictor can finish and against any given
predictor, Avoider can finish. However, the compatibility premise
requires the false reading in which Predictor and Avoider can finish
against each other. 
Idealizing the teacher and student along the lines of Avoider and
Predictor would fail to defeat the student’s elimination
argument. We would have merely formulated a riddle that falsely
presupposes that the two types of agent are co-possible. It would be
like asking ‘If Bill is smarter than anyone else and Hillary is
smarter than anyone else, which of the two is the
smartest?’.
Predictive determinism states that everything is foreseeable.
Metaphysical determinism states that there is only one way the future
could be given the way the past is. Simon Laplace used metaphysical
determinism as a premise for predictive determinism. He reasoned that
since every event has a cause, a complete description of any stage of
history combined with the laws of nature implies what happens at any
other stage of the universe. Scriven was only challenging predictive
determinism in his thought experiment. The next approach challenges
metaphysical determinism.
Prior knowledge of an action seems incompatible with it being a free
action. If I know that you will finish reading this article tomorrow,
then you will finish tomorrow (because knowledge implies truth). But
that means you will finish the article even if you resolve not to.
After all, given that you will finish, nothing can stop you from
finishing. So if I know that you will finish reading this article
tomorrow, you are not free to do otherwise.
Maybe all of your reading is compulsory. If God exists, then He knows
everything. So the threat to freedom becomes total for the theist. The
problem of divine foreknowledge insinuates that theism precludes
morality.
In response to the apparent conflict between freedom and
foreknowledge, medieval philosophers denied that future contingent
propositions have a truth-value. They took themselves to be extending
a solution Aristotle discusses in De Interpretatione to the
problem of logical fatalism. According to this truth-value gap
approach, ‘You will finish this article tomorrow’ is not
true now. The prediction will become true tomorrow.
A morally serious theist can agree with the Rubaiyat of Omar
Khayyam:
God’s omniscience only requires that He knows every true
proposition. God will know ‘You will finish this article
tomorrow’ as soon it becomes true – but not before. 
The teacher has freewill. Therefore, predictions about what he will do
are not true (prior to the examination). Accordingly, Paul Weiss
(1952) concludes that the student’s argument falsely assumes he
knows that the announcement is true. The student can know that the
announcement is true after it becomes true – but not
before.
W. V. Quine (1953) agrees with Weiss’ conclusion that the
teacher’s announcement of a surprise test fails to give the
student knowledge that there will be a surprise test. Yet Quine
abominates Weiss’ reasoning. Weiss breeches the law of bivalence
(which states that every proposition has a truth-value, true or
false). Quine believes that the riddle of the surprise test should not
be answered by surrendering classical logic.
W. V. Quine insists that the student’s elimination argument is
only a reductio ad absurdum of the supposition that the
student knows that the announcement is true (rather than a
reductio of the announcement itself). He accepts this
epistemic reductio but rejects the metaphysical
reductio. Given the student’s ignorance of the
announcement, Quine concludes that a test on any day would be
unforeseen. 
Common sense suggests that the students are informed by the
announcement. The teacher is assuming that the announcement will
enlighten the students. She seems right to assume that the
announcement of this intention produces the same sort of knowledge as
her other declarations of intentions (about which topics will be
selected for lecture, the grading scale, and so on).
There are skeptical premises that could yield Quine’s conclusion
that the students do not know the announcement is true. If no one can
know anything about the future, as alleged by David Hume’s
problem of induction, then the student cannot know that the
teacher’s announcement is true. (See the entry on
 the problem of induction.)
 But denying all knowledge of the future in order to deny the
student’s knowledge is disproportionate. A fly swatter should be
used to kill a fly, not a nuclear winter of ignorance. 
In later writings, Quine evinces general reservations about the
concept of knowledge. One of his pet objections is that
‘know’ is vague. If knowledge entails absolute certainty,
then too little will count as known. Quine infers that we must equate
knowledge with firmly held true belief. Asking just how firm the
belief must be is akin to asking just how big something has to be to
count as being big. There is no answer to the question because
‘big’ lacks the sort of boundary enjoyed by precise
words.
Quine is alluding to Rudolf Carnap’s (1950) generalization that
scientists replace qualitative terms (tall) with comparatives
(taller than) and then replace the comparatives with
quantitative terms (being n millimeters in height). 
It is true that some borderline cases of a qualitative term are not
borderline cases for the corresponding comparative. But the reverse
holds as well. A tall man who stoops may stand less high than another
tall man who is not as lengthy but better postured. Both men are
clearly tall. It is unclear that ‘The lengthier man is
taller’. Qualitative terms can be applied when a vague quota is
satisfied without the need to sort out the details. Only comparative
terms are bedeviled by tie-breaking issues.
Science is about what is the case rather than what ought to be case.
This seems to imply that science does not tell us what we ought to
believe. The traditional way to fill the normative gap is to delegate
issues of justification to epistemologists. However, Quine is
uncomfortable with delegating such authority to philosophers. He
prefers the thesis that psychology is enough to handle the issues
traditionally addressed by epistemologists (or at least the issues
still worth addressing in an Age of Science). This “naturalistic
epistemology” seems to imply that ‘know’ and
‘justified’ are antiquated terms – as empty as
‘phlogiston’ or ‘soul’.
Those willing to abandon the concept of knowledge can dissolve the
surprise test paradox. But to epistemologists, this is like using a
suicide bomb to kill a fly. 
Our suicide bomber may protest that the flies have been undercounted.
Epistemic eliminativism dissolves all epistemic paradoxes.
According to the eliminativist, epistemic paradoxes are symptoms of a
problem with the very concept of knowledge.
Notice that the eliminativist is more radical than the skeptic. The
skeptic thinks the concept of knowledge is fine. We just fall short of
being knowers. The skeptic treats ‘No man is a knower’
like ‘No man is an immortal’. There is nothing wrong with
the concept of immortality. Biology just winds up guaranteeing that
every man falls short of being immortal. 
Unlike the believer in ‘No man is an immortal’, the
skeptic has trouble asserting ‘There is no knowledge’. For
assertion expresses the belief that one knows. That is why Sextus
Empiricus (Outlines of Pyrrhonism, I., 3, 226) condemns the
assertion ‘There is no knowledge’ as dogmatic
skepticism. Sextus prefers agnosticism about knowledge rather than
skepticism (considered as “atheism” about knowledge). Yet
it just as inconsistent to assert ‘No one can know whether
anything is known’. For that conveys the belief that one knows
that no one can know whether anything is known.
Agnostics overestimate how easy it is to identify what cannot be
known. To know, one need only find a single proof. To know that there
is no way to know, one must prove the negative generalization that
there is no proof. After all, inability to imagine a proof is commonly
due to a failure of ingenuity rather than the non-existence of a
proof. In addition to being a more general proposition, a proof of
unknowability requires epistemological premises about what constitutes
proof. Consequently, meta-proof (proof about proofs) is even more
demanding than proof.
The agnostic might be tempted to avoid presumptuousness by converting
to meta-agnosticism. But this “retreats” in the wrong
direction. Meta-meta-proof is, in turn, even more demanding than
meta-proof. Meta-meta-proof requires both the epistemological premises
about what constitutes proof that meta-proof needs and, in addition,
meta-meta-proof needs epistemological premises about what constitutes
meta-proof.
The eliminativist has even more severe difficulties in stating his
position than the skeptic. Some eliminativists dismiss the threat of
self-defeat by drawing an analogy. Those who denied the existence of
souls were accused of undermining a necessary condition for asserting
anything. However, the soul theorist’s account of what is needed
gives no reason to deny that a healthy brain suffices for mental
states.
If the eliminativist thinks that assertion only imposes the aim of
expressing a truth, then he can consistently assert that
‘know’ is a defective term. However, an epistemologist can
revive the charge of self-defeat by showing that assertion does indeed
require the speaker to attribute knowledge to himself. This
knowledge-based account of assertion has recently been supported by
work on our next paradox. 
Lotteries pose a problem for the theory that we can assert whatever we
think is true. Given that there are a million tickets and only one
winner, the probability of ‘This ticket is a losing
ticket’ is very high. If our aim were merely to utter truths, we
should be willing to assert the proposition. Yet we are reluctant.
What is missing? Speakers will assert the proposition after seeing the
result of the lottery drawing or hearing about the winning ticket from
a newscaster or remembering what the winning ticket was. This suggests
that asserters represent themselves as knowing. This in turn suggests
that there is a rule, or norm, governing the practice of making
assertions that requires us to assert only what we know. This
knowledge norm explains why the hearer can appropriately ask
“How do you know?” (Williamson 2000, 249–255).
Perception, testimony, and memory are reliable processes that furnish
answers to this challenge. 
Do these processes furnish certainty? When pressed, we admit
there is a small chance that we misperceived the drawing or that the
newscaster misread the winning number or that we are misremembering.
While in this conciliatory mood, we are apt to relinquish our claim to
know. The skeptic generalizes from this surrender (Hawthorne 2004).
For any contingent proposition, there is a lottery statement that is
more probable and which is unknown. A known proposition cannot be less
probable than an unknown proposition. So no contingent proposition is
known.
This skeptical paradox was noticed by Gilbert Harman (1968, 166). But
his views about the role of causation in inferential knowledge seemed
to solve the problem (DeRose 2017, chapter 5). The baby paradox was
dismissed as stillborn. Since the new arrival did not get the
customary baptism of attention, epistemologists did not notice that
the demise of the causal theory of knowledge meant new life for
Harman’s lottery paradox. 
The probability skeptic’s mild suggestions about how we might be
mistaken contrast with the extraordinary possibilities conjured by
René Descartes’ skeptic. The Cartesian skeptic tries to
undermine vast swaths of knowledge with a single untestable
counter-explanation of the evidence (such as the hypothesis that you
are dreaming or the hypothesis that an evil demon is deceiving you).
These comprehensive alternatives are designed to evade any empirical
refutation. The probabilistic skeptic, in contrast, points to a
plethora of pedestrian counter-explanations. Each is easy to test:
maybe you transposed the digits of a phone number, maybe the ticket
agent thought you wanted to fly to Moscow, Russia rather than Moscow,
Idaho, etc. You can check for errors, but any check itself has a small
chance of being wrong. So there is always something to check, given
that the issues cannot be ignored on grounds of improbability. 
You can check any of these possible errors but you cannot
check them all. You cannot discount these pedestrian
possibilities as science fiction. These are exactly the sorts of
possibilities we check when plans go awry. For instance, you think you
know that you have an appointment to meet a prospective employer for
lunch at noon. When she fails to show at the expected time, you begin
a forced march backwards through your premises: Is your watch slow?
Are you remembering the right restaurant? Could there be another
restaurant in the city with the same name? Is she just detained?
Could she have just forgotten? Could there have been a
miscommunication? 
Probabilistic skepticism dates back to Arcesilaus who took over the
Academy two generations after Plato’s death. This moderate kind
of skepticism, recounted by Cicero (Academica 2.74, 1.46)
from his days as a student at the Academy, allows for justified
belief. Many scientists are attracted to probabilism and dismiss the
epistemologist’s preoccupation with knowledge as
old-fashioned.
Despite the early start of the qualitative theory of probability, the
quantitative theory did not develop until Blaise Pascal’s study
of gambling in the seventeenth century (Hacking 1975). Only in the
eighteenth century did it penetrate the insurance industry (even
though insurers realized that a fortune could be made by accurately
calculating risk). Only in the nineteenth century did probability make
a mark in physics. And only in the twentieth century do probabilists
make important advances over Arcesilaus.
Most of these philosophical advances are reactions to the use of
probability by scientists. In the twentieth century, editors of
science journals began to demand that the author’s hypothesis
should be accepted only when it was sufficiently probable – as
measured by statistical tests. The threshold for acceptance was
acknowledged to be somewhat arbitrary. And it was also conceded that
the acceptance rule might vary with one’s purposes. For
instance, we demand a higher probability when the cost of accepting a
false hypothesis is high.
In 1961 Henry Kyburg pointed out that this policy conflicted with a
principle of agglomeration: If you rationally believe \(p\) and
rationally believe \(q\) then you rationally believe both
\(p\) and \(q\). Little pictures of the same scene should
sum to a bigger picture of the same scene. If rational belief can be
based on an acceptance rule that only requires a high probability,
there will be rational belief in a contradiction! To see why, suppose
the acceptance rule permits belief in any proposition that has a
probability of at least .99. Given a lottery with 100 tickets and
exactly one winner, the probability of ‘Ticket \(n\) is a
loser’ licenses belief. Symbolize propositions about ticket
\(n\) being a loser as \(p_n\). Symbolize
‘I rationally believe’ as \(B\). Belief in a
contradiction follows:
Since belief in an obvious contradiction is a paradigm example of
irrationality, Kyburg poses a dilemma: either reject agglomeration or
reject rules that license belief for a probability of less than one.
(Martin Smith (2016, 186–196) warns that even a probability of
one leads to joint inconsistency for a lottery that has infinitely
many tickets.) Kyburg rejects agglomeration. He promotes toleration of
joint inconsistency (having beliefs that cannot all be true
together) to avoid belief in contradictions. Reason forbids us from
believing a proposition that is necessarily false but permits us to
have a set of beliefs that necessarily contains a falsehood. Henry
Kyburg’s choice was soon supported by the discovery of a
companion paradox.
In D. C. Makinson’s (1965) preface paradox, an author rationally
believes each of the assertions in his book. But since the author
regards himself as fallible, he rationally believes the conjunction of
all his assertions is false. If the agglomeration principle holds,
\((Bp \amp Bq) \rightarrow B(p \amp q)\), then it follows that it
would be rational for the author to believe the conjunction of all
assertions in his book and also that it would be rational for the
author to disbelieve the same thing! 
The preface paradox does not rely on a probabilistic acceptance rule.
The preface belief is generated in a qualitative fashion. The author
is merely reflecting on his humbling resemblance to other authors who
are fallible, his own past failing that he subsequently discovered,
his imperfection in fact checking, and so on.
At this juncture many philosophers join Kyburg in rejecting
agglomeration and conclude that it can be rational to have jointly
inconsistent beliefs. Kyburg’s solution to the preface paradox
raises a methodological question about the nature of paradox. How can
paradoxes change our minds if joint inconsistency is permitted? 
A paradox is commonly defined as a set of propositions that are
individually plausible but jointly inconsistent. Paradoxes pressure us
to revise beliefs in a highly structured way. For instance, much
epistemology orbits a riddle posed by the regress of justification,
namely, which of the following is false?
Foundationalists reject (1). They take some propositions to be
self-evident. Coherentists reject (2). They tolerate some forms of
circular reasoning. For instance, Nelson Goodman (1965) has
characterized the method of reflective equilibrium as
virtuously circular. Charles Peirce (1933–35, 5.250)
rejected (3), an approach later refined by Peter Klein (2007) and
championed at book-length by Scott F. Aikin (2011). Infinitists
believe that infinitely long chains of justification are no more
impossible than infinitely long chains of causation. Finally, the
epistemological anarchist rejects (4). As Paul Feyerabend refrains in
Against Method, “Anything goes” (1988, vii, 5,
14, 19, 159).
Very elegant! But if joint inconsistency is rationally tolerable, why
do these philosophers bother to offer solutions? Why is it not
rational to believe each of (1)–(4), despite their joint
inconsistency?
Kyburg might answer that there is a scale effect. Although the dull
pressure of joint inconsistency is tolerable when diffusely
distributed over a large set of propositions, the pain of
contradiction becomes unbearable as the set gets smaller (Knight
2002). And indeed, paradoxes are always represented as a
small set of propositions. 
If you know that your beliefs are jointly inconsistent, then you
should reject R. M. Sainsbury’s definition of a paradox as
“an apparently unacceptable conclusion derived by apparently
acceptable reasoning from apparently acceptable premises” (1995,
1). Take the negation of any of your beliefs as a conclusion and your
remaining beliefs as the premises. You should judge this jumble
argument as valid, and as having premises that you accept, and yet as
having a conclusion you reject (Sorensen 2003b, 104–110). If the
conclusion of this argument counts as a paradox, then the negation of
any of your beliefs counts as a paradox.
The resemblance between the preface paradox and the surprise test
paradox becomes more visible through an intermediate case. The preface
of Siddhartha Mukherjee’s The Emperor of All Maladies: A
Biography of Cancer warns: “In cases where there was no
prior public knowledge, or when interviewees requested privacy, I have
used a false name, and deliberately confounded identities to make it
difficult to track.” Those who refuse consent to be lied to are
free to close Doctor Mukherjee’s chronicle. But nearly all
readers think the physician’s trade-off between lies and new
information is acceptable. They rationally anticipate being rationally
misled. Nevertheless, these readers learn much about the history of
cancer. Similarly, students who are warned that they will receive a
surprise test rationally expect to be rationally misled about the day
of the test. The prospect of being misled does not lead them to drop
the course. 
The preface paradox pressures Kyburg to extend his tolerance of joint
inconsistency to the acceptance of contradictions (Sorensen 2001,
156–158). Consider a logic student who is required to pick one
hundred truths from a mixed list of tautologies and contradictions.
Although the modest student believes each of his answers, \(A_1, A_2,
\ldots, A_{100}\), he also believes that at least of one these answers
is false. This ensures he believes a contradiction. If any of his
answers is false, then the student believes a contradiction (because
the only falsehoods on the question list are contradictions).  If all
of his test answers are true, then the student believes the following
contradiction: \({\sim}(A_1 \amp A_2 \amp \ldots \amp A_{100})\).
After all, a conjunction of tautologies is itself a tautology and the
negation of any tautology is a contradiction.
If paradoxes were always sets of propositions or arguments or
conclusions, then they would always be meaningful. But some paradoxes
are semantically flawed (Sorensen 2003b, 352) and some have answers
that are backed by a pseudo-argument employing a defective
“lemma” that lacks a truth-value. Kurt Grelling’s
paradox, for instance, opens with a distinction between autological
and heterological words. An autological word describes itself, e.g.,
‘polysyllabic’ is polysllabic, ‘English’ is
English, ‘noun’ is a noun, etc. A heterological word does
not describe itself, e.g., ‘monosyllabic’ is not
monosyllabic, ‘Chinese’ is not Chinese, ‘verb’
is not a verb, etc. Now for the riddle: Is ‘heterological’
heterological or autological? If ‘heterological’ is
heterological, then since it describes itself, it is autological. But
if ‘heterological’ is autological, then since it is a word
that does not describe itself, it is heterological. The common
solution to this puzzle is that ‘heterological’, as
defined by Grelling, is not a genuine predicate (Thomson 1962). In
other words, “Is ‘heterological’
heterological?” is without meaning. There can be no predicate
that applies to all and only those predicates it does not apply to for
the same reason that there can be no barber who shaves all and only
those people who do not shave themselves. 
The eliminativist, who thinks that ‘know’ or
‘justified’ is meaningless, will diagnose the epistemic
paradoxes as questions that only appear to be well-formed.
For instance, the eliminativist about justification would not accept
proposition (4) in the regress paradox: ‘Some beliefs are
justified’. His point is not that no beliefs meet the high
standards for justification, as an anarchist might deny that any
ostensible authorities meet the high standards for legitimacy.
Instead, the eliminativist unromantically diagnoses
‘justified’ as a pathological term. Just as the astronomer
ignores ‘Are there a zillion stars?’ on the grounds that
‘zillion’ is not a genuine numeral, the eliminativist
ignores ‘Are some beliefs justified?’ on the grounds that
‘justified’ is not a genuine adjective.
In the twentieth century, suspicions about conceptual pathology were
strongest for the liar paradox: Is ‘This sentence is
false’ true? Philosophers who thought that there was something
deeply defective with the surprise test paradox assimilated it to the
liar paradox. Let us review the assimilation process.
In the surprise test paradox, the student’s premises are
self-defeating. Any reason the student has for predicting a test date
or a non-test date is available to the teacher. Thus the teacher can
simulate the student’s forecast and know what the student
expects. 
The student’s overall conclusion, that the test is impossible,
is also self-defeating. If the student believes his conclusion then he
will not expect the test. So if he receives a test, it will be a
surprise. The event will be all the more unexpected because the
student has deluded himself into thinking the test is impossible. 
Just as someone’s awareness of a prediction can affect the
likelihood of it being true, awareness of that sensitivity to his
awareness can also affect its truth. If each cycle of awareness is
self-defeating, then there is no stable resting place for a
conclusion.
Suppose a psychologist offers you a red box and a blue box (Skyrms
1982). The psychologist can predict which box you will choose with 90%
accuracy. He has put one dollar in the box he predicts you will choose
and ten dollars in the other box. Should you choose the red box or the
blue box? You cannot decide. For any choice becomes a reason to
reverse your decision. 
Epistemic paradoxes affect decision theory because rational choices
are based on beliefs and desires. If the agent cannot form a
rational belief, it is difficult to interpret his behavior as a
choice. The purpose of attributing beliefs and desires is to
set up practical syllogisms that make sense of actions as means to
ends. Subtracting rationality from the agent makes framework useless.
Given this commitment to charitable interpretation, there is no
possibility of your rationally choosing an option that you believe to
be inferior. So if you choose, you cannot really believe you were
operating as an anti-expert, that is, someone whose opinions on a
topic are reliably wrong  (Egan and Elga 2005).
The medieval philosopher John Buridan (Sophismata, Sophism
13) gave a starkly minimal example of such instability:
If you believe (B) it is false. If you do not believe (B) it is true.
You are an anti-expert about (B); your opinion is reliably wrong. An
outsider who monitors your opinion can reckon whether (B) is true. But
you are not able to exploit your anti-expertise.
On the bright side, you are able to exploit the anti-expertise of
others. Four out of five anti-experts recommend against reading any
further.
David Kaplan and Richard Montague (1960) think the announcement by the
teacher in our surprise exam example is equivalent to the
self-referential
Kaplan and Montague note that the number of alternative test dates can
be increased indefinitely. Shockingly, they claim the number of
alternatives can be reduced to zero! The announcement is then
equivalent to 
If (K-0) is true then it known to be false. Whatever is known to be
false, is false. Since no proposition can be both true and false, we
have proven that (K-0) is false. Given that proof produces knowledge,
(K-0) is known to be false. But wait! That is exactly what (K-0) says
– so (K-0) must be true.
The (K-0) argument stinks of the liar paradox. Subsequent commentators
sloppily switch the negation sign in the formal presentations of the
reasoning from \(K{\sim}p\) to \({\sim}Kp\) (that is, from
‘It is known that not-\(p\)’, to ‘It is not the
case that it is known that \(p\)’). Ironically, this garbled
transmission results in a cleaner variation of the knower:
Is (K) true? On the one hand, if (K) is true, then what it says is
true, so no one knows it. On the other hand, that very reasoning seems
to be a proof of (K). Proving a proposition is sufficient for
knowledge of it, so someone must know (K). But then (K) is false!
Since no one can know a proposition that is false, (K) is not
known.
The skeptic could hope to solve (K-0) by denying that anything is
known. This remedy does not cure (K). If nothing is known then (K) is
true. Can the skeptic instead challenge the premise that proving a
proposition is sufficient for knowing it? This solution would be
particularly embarrassing to the skeptic. The skeptic presents himself
as a stickler for proof. If it turns out that even proof will not sway
him, he bears a damning resemblance to the dogmatist he so frequently
chides.
But the skeptic should not lose his nerve. Proof does not always yield
knowledge. Consider a student who correctly guesses that a step in his
proof is valid. The student does not know the conclusion but did prove
the theorem. His instructor might have trouble getting the student to
understand why his answer constitutes a valid proof. The intransigence
may stem from the prover’s intelligence rather than his
stupidity. L. E. J. Brouwer is best known in mathematics for his
brilliant fixed point theorem. But Brouwer regarded his proof as
dubious. He had philosophical doubts about the Axiom of Choice and Law
of Excluded Middle. Brouwer persuaded a minority of mathematicians and
philosophers, known as intuitionists, to emulate his inability to be
educated by non-constructive proofs. 
The logical myth that “You cannot prove a universal
negative” is itself a universal negative. So it implies its own
unprovability. This implication of unprovability is correct but only
because the principle is false. For instance, exhaustive inspection
proves the universal negative ‘No adverbs appear in this
sentence’. A reductio ad absurdum proves the universal
negative ‘There is no largest prime number’. 
Trivially, false propositions cannot be proved true. Are
there any true propositions that cannot be proved true?
Yes, there are infinitely many. Kurt Gödel’s incompleteness
theorem demonstrated that any system that is strong enough to express
arithmetic is also strong enough to express a formal counterpart of
the self-referential proposition in the surprise test example
‘This statement cannot be proved in this system’. If the
system cannot prove its “Gödel sentence”, then this
sentence is true. If the system can prove its Gödel sentence, the
system is inconsistent. So either the system is incomplete or
inconsistent. (See the entry on
 Kurt Gödel.)
Of course, this result concerns provability relative to a system. One
system can prove another system’s Gödel sentence. Kurt
Gödel (1983, 271) thought that proof was not needed for knowledge
that arithmetic is consistent. 
J. R. Lucas (1964) claims that this reveals human beings are not
machines. A computer is a concrete instantiation of a formal system.
Hence, its “knowledge” is restricted to what it can prove.
By Gödel’s theorem, the computer will be either
inconsistent or incomplete. However, a human being with a full command
of arithmetic can be consistent (even if he is actually
inconsistent due to inattention or wishful thinking).
Critics of Lucas defend the parity between people and computers. They
think we have our own Gödel sentences (Lewis 1999,
166–173). In this egalitarian spirit, G. C. Nerlich (1961)
models the student’s beliefs in the surprise test example as a
logical system. The teacher’s announcement is then a Gödel
sentence about the student: There will be a test next week but you
will not be able to prove which day it will occur on the basis of this
announcement and memory of what has happened on previous exam days.
When the number of exam days equals zero the announcement is
equivalent to sentence K.
Several commentators on the surprise test paradox object that
interpreting surprise as unprovability changes the topic. Instead of
posing the surprise test paradox, it poses a variation of the liar
paradox. Other concepts can be blended with the liar. For instance,
mixing in alethic notions generates the possible liar:  Is
‘This statement is possibly false’ true? (Post 1970)
(If  it is false, then it is false that it is possibly false.
What cannot possibly be false is necessarily true. But if it is
necessarily true, then it cannot be possibly false.) Since the
semantic concept of validity involves the notion of possibility, one
can also derive validity liars such as Pseudo-Scotus’ paradox:
‘Squares are squares, therefore, this argument is invalid’
(Read 1979). Suppose Pseudo-Scotus’ argument is valid. Since the
premise is necessarily true, the conclusion would be necessarily true.
But the conclusion contradicts the supposition that argument is valid.
Therefore, by reductio, the argument is necessarily invalid. Wait! The
argument can be invalid only if it is possible for the premise to be
true and the conclusion to be false. But we have already proved that
the conclusion of ‘Squares are squares, therefore, this argument
is invalid’ is necessarily true. There is no consistent judgment
of the argument’s validity. A similar predicament follows from
‘The test is on Friday but this prediction cannot be soundly
deduced from this announcement’.
One can mock up a complicated liar paradox that resembles the surprise
test paradox. But this complex variant of the liar is not an
epistemic paradox. For the paradoxes turn on the semantic
concept of truth rather than an epistemic concept. 
Frederic Fitch (1963) reports that in 1945 he first learned of this
proof of unknowable truths from a referee report on a manuscript he
never published. Thanks to Joe Salerno’s (2009) archival
research, we now know that referee was Alonzo Church. 
Assume there is a true sentence of the form ‘p but p is not
known’. Although this sentence is consistent, modest principles
of epistemic logic imply that sentences of this form are
unknowable.
Since all the assumptions are discharged, the conclusion is a
necessary truth. So it is a necessary truth that \(p \amp{\sim}Kp\) is
not known. In other words, \(p \amp{\sim}Kp\) is unknowable. 
The cautious draw a conditional moral: If there are actual unknown
truths, there are unknowable truths. After all, some philosophers will
reject the antecedent because they believe there is an omniscient
being. 
But secular idealists and logical positivists concede that there are
some actual unknown truths. How can they continue to believe that all
truths are knowable?  Astonishingly, these eminent philosophers
seem refuted by a pinch of epistemic logic. Also injured are those who
limit their claims of universal knowability to a limited domain. For
instance, Immanuel Kant (A223/B272) asserts that all empirical
propositions are knowable. This pocket of optimism would be enough to
ignite the contradiction (Stephenson 2015). 
Timothy Williamson doubts that this casualty list is enough for the
result to qualify as a paradox:
An apparent counterexample can be set aside as anomaly if it conflicts
with a highly confirmed law of nature. But if the counterexample only
conflicts with a speculative generalization, the theory should be
rejected. 
Those who believe that the Church-Fitch result is a genuine paradox
can respond to Williamson with paradoxes that accord with common sense
(and science –and religious orthodoxy). For instance, common
sense heartily agrees with the conclusion that something exists. But
it is surprising that this can be proved without empirical premises.
Since the quantifiers of standard logic (first order predicate logic
with identity) have existential import, the logician can deduce that
something exists from the principle that everything is identical to
itself. Most philosophers balk at this simple proof because they feel
that the existence of something cannot be proved by sheer logic.
Likewise, many philosophers balk at the proof of unknowables because
they feel that such a profound result cannot be obtained from such
limited means. 
Church’s referee report was composed in 1945. The timing and
structure of his argument for unknowables suggests that Church may
have been by inspired G. E. Moore’s (1942, 543) sentence:
Moore’s problem is to explain what is odd about declarative
utterances such as (M). This explanation needs to encompass both
readings of (M): ‘\(p \amp B{\sim}p\)’
and ‘\(p \amp{\sim}Bp\)’. (This scope ambiguity
is exploited by a popular joke: René Descartes sits in a bar, having a
drink. The bartender asks him if he would care for another. “I
think not,” he says, and disappears.)
The common explanation of Moore’s absurdity is that the speaker
has managed to contradict himself without uttering a contradiction. So
the sentence is odd because it is a counterexample to the
generalization that anyone who contradicts himself utters a
contradiction.
There is no problem with third person counterparts of (M). Anyone else
can say about Moore, with no paradox, ‘G. E. Moore went to the
pictures last Tuesday but he does not believe it’. (M) can also
be embedded unparadoxically in conditionals: ‘If I went to the
pictures last Tuesday but I do not believe it, then I am suffering
from a worrisome lapse of memory ’. The past tense is fine:
‘I went to the picture shows last Tuesday but I did not believe
it’. The future tense, ‘I went to the picture shows last
Tuesday but I will not believe it’, is a bit more of a stretch
(Bovens 1995). We tend to picture our future selves as better
informed. Later selves are, as it were, experts to whom earlier selves
should defer. When an earlier self foresees that his later self
believes \(p\), then the prediction is a reason to believe
\(p\). Bas van Fraassen (1984, 244) dubs this “the
principle of reflection”: I ought to believe a proposition given
that I will believe it at some future time.
Robert Binkley (1968) anticipates van Fraassen by applying the
reflection principle to the surprise test paradox. The student can
foresee that he will not believe the announcement if no test is given
by Thursday. The conjunction of the history of testless days
and the announcement will imply the Moorean sentence:
Since the less evident member of the conjunction is the announcement,
the student will choose not to believe the announcement. At the
beginning of the week, the student foresees that his future self may
not believe the announcement. So the student on Sunday will not
believe the announcement when it is first uttered.
Binkley illuminates this reasoning with doxastic logic. The inference
rules for this logic of belief can be understood as idealizing the
student into an ideal reasoner. In general terms, an ideal reasoner is
someone who infers what he ought and refrains from inferring any more
than he ought. Since there is no constraint on his premises, we may
disagree with the ideal reasoner. But if we agree with the ideal
reasoner’s premises, we appear bound to agree with his
conclusion. Binkley specifies some requirements to give teeth to the
student’s status as an ideal reasoner: the student is perfectly
consistent, believes all the logical consequences of his beliefs, and
does not forget. Binkley further assumes that the ideal reasoner is
aware that he is an ideal reasoner. According to Binkley, this ensures
that if the ideal reasoner believes p, then he believes that he will
believe p thereafter.
Binkley’s account of the student’s hypothetical epistemic
state on Thursday is compelling. But his argument for spreading the
incredulity from the future to the past is open to three
challenges.
The first objection is that it delivers the wrong result. The student
\(is\) informed by the teacher’s announcement, so Binkley
ought not to use a model in which the announcement is as absurd as the
conjunction ‘I went to the pictures last Tuesday but I do not
believe it’. 
Second, the future mental state envisaged by Binkley is only
hypothetical: \(If\) no test is given by Thursday, the student
will find the announcement incredible. At the beginning of the week,
the student does not know (or believe) that the teacher will wait that
long. A principle that tells me to defer to the opinions of my future
self does not imply that I should defer to the opinions of my
hypothetical future self. For my hypothetical future self is
responding to propositions that need not be actually true.
Third, the principle of reflection may need more qualifications than
Binkley anticipates. Binkley realizes that an ordinary agent foresees
that he will forget details. That is why we write reminders for our
own benefit. An ordinary agent foresees periods of impaired judgment.
That is why we limit how much money we bring to the bar. 
Binkley stipulates that the students do not forget. He needs to add
that the students know that they will not forget. For the mere threat
of a memory lapse sometimes suffices to undermine knowledge. Consider
Professor Anesthesiology’s scheme for surprise tests: “A
surprise test will be given either Wednesday or Friday with the help
of an amnesia drug. If the test occurs on Wednesday, then the drug
will be administered five minutes after Wednesday’s class. The
drug will instantly erase memory of the test and the students will
fill in the gap by confabulation.” You have just completed
Wednesday’s class and so temporarily know that the test will be
on Friday. Ten minutes after the class, you lose this knowledge. No
drug was administered and there is nothing wrong with your memory. You
are correctly remembering that no test was given on Wednesday.
However, you do not know your memory is accurate because you also know
that if the test was given Wednesday then you would have a
pseudo-memory indistinguishable from your present memory. Despite not
gaining any new evidence, you change your mind about the test
occurring on Wednesday and lose your knowledge that the test is on
Friday. (The change of belief is not crucial; you would still lack
foreknowledge of the test even if you dogmatically persisted
in believing that the test will be on Friday.) 
If the students know that they will not forget and know there will be
no undermining by outside evidence, then we may be inclined to agree
with Binkley’s summary that his idealized student never loses
the knowledge he accumulates. As we shall see, however, this overlooks
other ways in which rational agents may lose knowledge.
A blindspot is a consistent but inaccessible proposition. Blindspots
are relative to the means of reaching the proposition, the person
making the attempt, and time at which he tries. Although I cannot
know the blindspot ‘There is intelligent
extra-terrestrial life but no one knows it’, I can
suspect it. Although \(I\) cannot rationally believe
‘Polar bears have black skin but I do not believe it’
you can. This means there can be disagreement between ideal
reasoners (even under strong idealizations such as Binkley’s).
The anthropologist Gontran de Poncins begins his chapter on the arctic
missionary, Father Henry, with a prediction:
Gontran de Poncins’ subsequent testimony might lead the reader
to believe someone can indeed be content to live in an ice-house. The
same testimony might lead another reader to doubt that Poncins is
telling the truth. But no reader ought to believe ‘Someone can
be content to live in an ice house and I doubt it’.
If Gontran believes a proposition that is a blindspot to his reader,
then he cannot furnish good grounds for his reader to share his
belief. This holds even if they are ideal reasoners. So one
implication of blindspots is that there can be disagreement among
ideal reasoners because they differ in their blindspots. 
This is relevant to the surprise test paradox. The students are the
surprisees. Since the announcement entails that the date of the
surprise test a blindspot for them, non-surprisees cannot persuade
them. 
The same point holds for intra-personal disagreement over time.
Evidence that persuaded me on Sunday that ‘This security code is
390524085 but on Friday I will not believe it’ should no longer
persuade me on Friday (given my belief that the day is Friday). For
that proposition is a blindspot to my Friday self.
Although each blindspot is inaccessible, a disjunction of blindspots
is normally not a blindspot. I can rationally believe that
‘Either the number of stars is even and I do not believe it, or
the number of stars is odd and I do not believe it’. The
author’s preface statement that there is some mistake in his
book is equivalent to a very long disjunction of blindspots. The
author is saying he either falsely believes his first statement or
falsely believes his second statement or … or falsely believes
his last statement. 
The teacher’s announcement that there will be a surprise test is
equivalent to a disjunction of future mistakes: ‘Either there
will be a test on Monday and the student will not believe it
beforehand or there will be a test Wednesday and the student will not
believe it beforehand or the test is on Friday and the student will
not believe it beforehand.’
The points made so far suggest a solution to the surprise test paradox
(Sorensen 1988, 328–343). As Binkley (1968) asserts, the test
would be a surprise even if the teacher waited until the last day. Yet
it can still be true that the teacher’s announcement is
informative. At the beginning of the week, the students are justified
in believing the teacher’s announcement that there will be a
surprise test.  This announcement is equivalent to:
Consider the student’s predicament on Thursday (given that the
test has not been on Monday or Wednesday). If he knows that no test
has been given, he cannot also know that (A) is true. Because that
would imply
Although (iii) is consistent and might be knowable by others, (iii)
cannot be known by the student before Friday. (iii) is a blindspot for
the students but not for, say, the teacher’s colleagues. Hence,
the teacher can give a surprise test on Friday because that would
force the students to lose their knowledge of the original
announcement (A). Knowledge can be lost without forgetting anything.
This solution makes who you are relevant to what you can know. In
addition to compromising the impersonality of knowledge, there will be
compromise on its temporal neutrality. 
Since the surprise test paradox can also be formulated in terms of
rational belief, there will be parallel adjustments for what we ought
to believe. We are criticized for failures to believe the logical
consequences of what we believe and criticized for believing
propositions that conflict with each other. Anyone who meets these
ideals of completeness and consistency will be unable to believe a
range of consistent propositions that are accessible to other complete
and consistent thinkers. In particular, they will not be able to
believe propositions attributing specific errors to them, and
propositions that entail these off-limit propositions.
Some people wear T-shirts with Question Authority! written on
them. Questioning authority is generally regarded as a matter of
individual discretion. The surprise test paradox shows that it is
sometimes mandatory. The student is rationally required to doubt the
teacher’s announcement even though the teacher has not given any
evidence of being unreliable. Indeed, the student can foresee that
their change of mind opens a new opportunity for surprise.
There can be disagreement amongst ideal reasoners who agree on the
same impersonal data. Consider the colleagues of the teachers. They
are not amongst those that the teacher targets for surprise. Since
‘surprise’ here means ‘surprise to the
students’, the teacher’s colleagues can consistently infer
that the test will be on the last day from the premise that it has not
been given on any previous day. 
The above anomalies (losing knowledge without forgetting, disagreement
amongst equally well-informed ideal reasoners, rationally changing
your mind without the acquisition of counter-evidence) would be more
tolerable if reinforced by separate lines of reasoning. The most
fertile source of this collateral support is in puzzles about updating
beliefs.
The natural strategy is to focus on the knower when he is stationary.
However, just as it is easier for an Eskimo to observe an arctic fox
when it moves, we often get a better understanding of the knower
dynamically, when he is in the process of gaining or losing knowledge.
When on trial for impiety, Socrates traced his inquisitiveness to the
Oracle at Delphi (Apology 21d in Cooper 1997). Prior to
beginning his mission of inquiry, Chaerephon asked the Oracle:
“Who is the wisest of men?” The Oracle answered “No
one is wiser than Socrates.” This astounded Socrates because he
believed he knew nothing. Whereas a less pious philosopher might have
questioned the reliability of the Delphic Oracle, Socrates followed
the general practice of treating the Oracle as infallible. The only
cogitation appropriate to an infallible answer is interpretation.
Accordingly, Socrates resolved his puzzlement by inferring that his
wisdom lay in recognizing his own ignorance. While others may know
nothing, Socrates knows that he knows nothing. 
Socrates continues to be praised for his insight. But his
“discovery” is a contradiction. If Socrates knows that he
knows nothing, then he knows something (the proposition that he knows
nothing) and yet does not know anything (because knowledge implies
truth).
Socrates could regain consistency by downgrading his meta-knowledge to
the status of a belief. If he believes he knows nothing, then he
naturally wishes to remedy his ignorance by asking about everything.
This rationale is accepted throughout the early dialogues. But when we
reach the Meno, one of his interlocutors has an epiphany.
After Meno receives the standard treatment from Socrates about the
nature of virtue, Meno discerns a conflict between Socratic ignorance
and Socratic inquiry (Meno 80d, in Cooper 1997). How would
Socrates recognize the correct answer even if Meno gave it? 
The general structure of Meno’s paradox is a dilemma: If you
know the answer to the question you are asking, then nothing can be
learned by asking. If you do not know the answer, then you cannot
recognize a correct answer even if it is given to you. Therefore, one
cannot learn anything by asking questions.
The natural solution to Meno’s paradox is to characterize the
inquirer as only partially ignorant. He knows enough to recognize a
correct answer but not enough to answer on his own. For instance,
spelling dictionaries are useless to six year old children because
they seldom know more than the first letter of the word in question.
Ten year old children have enough partial knowledge of the
word’s spelling to narrow the field of candidates. Spelling
dictionaries are also useless to those with full knowledge of spelling
and those with total ignorance of spelling. But most of us have an
intermediate amount of knowledge. 
It is natural to analyze partial knowledge as knowledge of
conditionals. The ten year old child knows the spoken version of
‘If the spelling dictionary spells the month after January as
F-e-b-r-u-a-r-y, then that spelling is correct’. Consulting the
spelling dictionary gives him knowledge of the antecedent of the
conditional.
Much of our learning from conditionals runs as smoothly as this
example suggests. Knowledge of the conditional is conditional
knowledge (that is, conditional upon learning the antecedent and
applying the inference rule modus ponens: If P then Q, P, therefore
Q). But the next section is devoted to some known conditionals that
are repudiated when we learn their antecedents.
Saul Kripke’s ruminations on the surprise test paradox led him
to a paradox about dogmatism. He lectured on both paradoxes at
Cambridge University to the Moral Sciences Club in 1972. (A descendent
of this lecture now appears as Kripke 2011). Gilbert Harman
transmitted Kripke’s new paradox as follows: 
Dogmatists accept this reasoning. For them, knowledge closes inquiry.
Any “evidence” that conflicts with what is known can be
dismissed as misleading evidence. Forewarned is forearmed. 
This conservativeness crosses the line from confidence to
intransigence. To illustrate the excessive inflexibility, here is a
chain argument for the dogmatic conclusion that my reliable colleague
Doug has given me a misleading report (corrected from Sorensen
1988b):
By hypothesis, I am justified in believing (C\(_1)\). Premise
(C\(_2)\) is a certainty because it is analytically true. The
argument from (C\(_1)\) and (C\(_2)\) to (C\(_3)\)
is valid. Therefore, my degree of confidence in (C\(_3)\) must
equal my degree of confidence in (C\(_1)\). Since we are also
assuming that I gain sufficient justification for (C\(_4)\), it
seems to follow that I am justified in believing (C\(_5)\) by
modus ponens. Similar arguments will lead me to dismiss further
evidence such as a phone call from the towing service and my failure
to see my car when I confidently stride over to the parking lot.
Gilbert Harman diagnoses the paradox as follows:
In effect, Harman denies the hardiness of knowledge. The hardiness
principle states that one knows only if there is no evidence such that
if one knew about the evidence one would not be justified in believing
one’s conclusion. New knowledge cannot undermine old knowledge.
Harman disagrees.
Most epistemologists have accepted Harman’s vague solution. They
have just tried to make it precise. Some import details philosophy of
language (Sorensen 1988b). Earl Conee (2004) argues epistemologists
has sufficient indigenous resources. All we need is evidentialism, the
doctrine that you are justified in believing p exactly when supported
by the totality of your evidence. However, Mike Vesey rejects
Harman’s solution as irrational. One is never entitled to
discard evidence, even after it has been identified as misleading. And
indeed, intelligence analysts during World War II scoured German
propaganda for clues about bombing accuracy, shortages, and rivalry
between branches of the German military. In a close study of our
actual responses to identified misleading evidence, Maria
Lasonen-Aarnio (2014) suggests that we are sometimes justified in
ignoring misleading evidence and sometimes not. Since Harman has not
provided criteria for warranted ignoring, his solution is incomplete.
Harman’s belief that new knowledge can undermine old knowledge
may be relevant to the surprise test paradox. Perhaps the students
lose knowledge of the test announcement even though they do not forget
the announcement or do anything else incompatible with their
credentials as ideal reasoners. A student on Thursday is better
informed about the outcomes of test days than he was on Sunday. He
knows the test was not on Monday and not on Wednesday. But he can only
predict that the test is on Friday if he continues to know the
announcement. Perhaps the extra knowledge of the testless days
undermines knowledge of the announcement.
We cannot coherently predict that any specific new epistemic paradox
awaits discovery. To see why, consider the prediction Jon Wynne-Tyson
attributes to Leonardo Da Vinci: “I have learned from an early
age to abjure the use of meat, and the time will come when men such as
I will look upon the murder of animals as they now look upon the
murder of men.” (1985, 65) By predicting this progress, Leonardo
inadvertently reveals he already believes that the murder of
animals is the same as the murder of men. If you believe that a
proposition is true but will be first believed at a later time, then
you already believe it – and so are inconsistent. (The actual
truth is irrelevant.) 
Specific regress can be anticipated. During the Korean war, vague
charges that the United States military was conducting biological
warfare set the stage for precise confessions by two captured American
pilots in 1953. Other captured pilots expected to be
“brainwashed” into corroborating the sensational
confessions. The asymmetry between predicting progress and predicting
regress is based on a magnetic asymmetry between truth and falsehood.
Truth attracts belief. Falsehood repels. More precisely, perceived
truth creates belief while perceived falsehood creates disbelief. When
I try to predict my first acquisition of a specific truth, I pre-empt
myself. When I try to predict my first acquisition of a specific
falsehood, there is no pre-emption. 
There would be no problem with predicting progress if Leonardo thinks
the moral progress lies in the moral preferability of the vegetarian
belief rather than the truth of the matter. One might admire
vegetarianism without accepting the correctness of vegetarianism. But
Leonardo is endorsing the correctness of the belief. This sentence
embodies a Moorean absurdity. It is like saying ‘Leonardo took
twenty five years to complete The Virgin on the Rocks but I
will first believe so tomorrow’. (This absurdity will prompt
some to object that I have uncharitably interpreted Leonardo; he must
have intended to make an exception for himself and only be referring
to men of his kind.)
I cannot specifically anticipate the first acquisition of the true
belief that \(p\). For that prediction would show that I already
have the true belief that \(p\). The truth cannot wait. The
impatience of the truth imposes a limit on the prediction of
discoveries. 