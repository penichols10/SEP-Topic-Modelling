First developed in Sadi Carnot’s Reflections on the Motive
Power of Fire 1824, the science of classical thermodynamics is
intimately associated with the industrial revolution. Most of the
results responsible for the science originated from the practice of
engineers trying to improve steam engines. Originating in France and
England in the late eighteenth and early nineteenth centuries, the
science quickly spread throughout Europe. By the mid-nineteenth
century, Rudolf Clausius in Germany and William Thomson (later Lord
Kelvin) in England had developed the theory in great detail. Once
developed, its scope grew from steam engines and the like to arguably
all macroscopic processes.
Thermodynamics is a “phenomenal” science. That means that
its variables range over macroscopic parameters such as temperature,
pressure and volume. These are properties that hold at equilibrium,
i.e., when the values of the macroscopic variables remain
approximately stable. Whether the microphysics underlying these
variables are motive atoms in the void or an imponderable fluid is
largely irrelevant to this science. The developers of the theory both
prided themselves on this fact and at the same time worried about it.
Clausius, for instance, was one of the first to speculate that heat
consisted solely of the motion of particles (without an ether), for it
made the equivalence of heat with mechanical work less surprising.
However, as was common, he kept his “ontological” beliefs
separate from his official statement of the principles of
thermodynamics because he didn’t wish to (in his words)
“taint” the latter with the speculative character of the
 former.[1]
A treatment of thermodynamics naturally begins with the statements it
takes to be laws of nature. These laws are founded upon observations
of relationships between particular macroscopic parameters and they
are justified by the fact they are empirically adequate. No further
justification of these laws is to be found—at this
stage—from the details of microphysics. Rather, stable,
counterfactual-supporting generalizations about macroscopic features
are enshrined as law. The typical textbook treatment of thermodynamics
describes some basic concepts, states the laws in a more or less rough
way and then proceeds to derive the concepts of temperature and
entropy and the various thermodynamic equations of state. It is worth
remarking, however, that in the last fifty years the subject has been
presented with a degree of mathematical rigor not previously achieved.
Originating from the early axiomatization by Carathéodory in
1909, the development of “rational thermodynamics” has
clarified the concepts and logic of classical thermodynamics to a
degree not generally appreciated. There now exist many quite
different, mathematically exact approaches to thermodynamics, each
starting with different primitive kinds and/or observational
regularities as axioms. (For a popular presentation of a recent
axiomatization, see Lieb and Yngvason 2000.)
In the traditional approach classical thermodynamics has two laws, the
First and Second
 Laws.[2]
 The First Law expresses the conservation of energy and is founded
upon the impossibility of creating a machine that can create energy.
The law uses the concept of the internal energy of a system, \(U\),
which is a function of the system’s macroscopic variables, e.g.,
temperature, volume. For thermally isolated (adiabatic)
systems—think of systems such as coffee in a thermos—the
law states that this function, \(U\), is such that the work \(W\)
delivered to a system’s surroundings is compensated by a loss of
internal energy, i.e., \(dW = -dU\). When James Joule and others
showed that mechanical work and heat were interconvertible,
consistency with the principle of energy conservation demanded that
heat, \(Q\), considered as a different form of energy, be taken into
account. For non-isolated systems we extend the law as \(dQ = dU +
dW\), where \(dQ\) is the differential of the amount of heat added to
the system (in a reversible manner).
The conservation of energy tells us nothing about temporally
asymmetric behavior. It doesn’t follow from the First Law that
interacting systems quickly tend to approach equilibrium, and once
achieved, never leave this state. It is perfectly consistent with the
First Law that systems in equilibrium leave equilibrium. In
particular, no limitations are placed on transforming energy from one
form into another, so the Law permits the possibility of machines that
remove heat from their environment and turn it into work (a so-called
perpetual mobile of the second kind). To rule out such machines, and
more generally, to capture the amazingly general temporally asymmetric
behavior we find, another law is needed. Although Carnot was the first
to state it, the formulations of Kelvin and Clausius are standard:
Kelvin’s Second Law: There is no thermodynamic process whose
sole effect is to transform heat extracted from a source at uniform
temperature completely into work.
Clausius’ Second Law: There is no thermodynamic process whose
sole effect is to extract a quantity of heat from a colder reservoir
and deliver it to a hotter reservoir.
Kelvin’s version is essentially the same as the version arrived
at by both Carnot and Planck, whereas Clausius’ version differs
from these in a few
 ways.[3]
Clausius’ version transparently rules out anti-thermodynamic
behavior such as a hot iron bar extracting heat from a neighboring
cold iron bar. The cool bar cannot give up a quantity of heat to the
warmer bar (without something else happening). Kelvin’s
statement is perhaps less obvious. It originates in an observation
about steam engines, namely, that heat energy is a “poor”
grade of energy. Consider a gas-filled cylinder with a frictionless
piston holding the gas down at one end. If we put a flame under the
cylinder, the gas will expand and the piston can perform work, e.g.,
it might move a ball. However, we can never convert the heat energy
straight into work without some other effect occurring. In this case,
the gas occupies a larger volume.
In 1854, Clausius introduced the notion of the “equivalence
value” of a transformation, a concept that is the ancestor of
the modern day concept of entropy. Later in 1865 Clausius coined the
term “entropy” for a similar concept (the word derives
from the Greek word for transformation). The entropy of a state \(A\),
\(S(A)\) is defined as the integral \(S(A) = \int^{A}_{O} dQ/T\) over
a reversible transformation, where \(O\) is some arbitrary fixed
state. For \(A\) to have an entropy, the transformation from \(O\) to
\(A\) must be quasi-static, i.e., a succession of equilibrium states.
Continuity considerations then imply that the initial and final states
\(O\) and \(A\) must also be equilibrium states. Put in the language
of entropy, the Second Law states that in a transformation from
equilibrium state \(A\) to equilibrium state \(B\), the inequality
\(S(B) - S(A)\) is greater than or equal to the \(\int^{A}_{B} dQ/T\).
Loosely put, for realistic systems, this implies that in the
spontaneous evolution of a thermally closed system the entropy can
never decrease and that it attains its maximum value at equilibrium.
We are invited to think of the Second Law as driving the system to its
new, higher entropy equilibrium state.
With the Second Law thermodynamics is able to characterize an
extraordinary range of phenomena under one simple law. Remarkably,
whether they are gases filling their available volumes, iron bars in
contact coming to the same temperature, vinegar and oil separating, or
milk mixing in your coffee, they all have an observable property in
common: their entropy increases. Coupled with the First Law, the
Second Law is remarkably powerful. It appears that all classical
thermodynamical behavior can be derived from these two simple
statements (O. Penrose 1970).
The above sketch represents the conventional way of describing
thermodynamics and its Second Law. Let me mention a few questions that
it raises.
First, what is the precise location of the time-asymmetry? Almost all
commentators claim that it lay in the Second Law. If Uffink (2001) and
Brown and Uffink (2001) are correct, however, then this
“static” Second Law does not encode any time asymmetry at
all. It is, after all, simply a relation between a few variables at
equilibrium. While that may be right, there is no question that
thermodynamics, if not its Second Law, makes time-asymmetric
claims. The spontaneous movement from non-equilibrium to equilibrium
happens and is assumed throughout the field. The only question is
whether it must be regarded as a separate assumption (perhaps
demanding its own name) or can somehow be derived from existing
principles. It’s also worth remarking that many other principles
of thermodynamics are time-asymmetric, e.g., the classical heat
equation.
Second, what is the scope of the Second Law? There are two issues
here. First, does it apply to the universe as a whole, so that we can
say the universe’s entropy is increasing, or does it only apply
to select sub-systems of the universe? (See Uffink 2001 for an
interesting historical discussion of this topic.) Many philosophers
and physicists have balked at the idea that the universe itself has an
entropy. As one might expect, those in the grip of an operationalist
philosophy are especially prone to deny that the universe as a whole
has an entropy. Second, what sub-systems of the universe does it
govern? Are the principles of thermodynamics responsible for
generalizations about black holes? The field of black hole
thermodynamics assumes it is (see the section on black hole
thermodynamics in the entry on
 singularities and black holes,
 for discussion and references), although not all are convinced
(Dougherty & Callender forthcoming). What about the
micro-realm?
Third, how are these laws framed in a relativistic universe? They were
developed in the nineteenth century with a classical spacetime
background in mind. How do we write the theory in a modern
formulation? Surprisingly, the issue is as much conceptual as
technical. The correct (special) relativistic transformation rules for
thermodynamic quantities are controversial. Do Lorentz boosted gases
appear hotter or colder in the new inertial frame? Albert Einstein
himself answered the question about the gas differently throughout his
life! With all the current activity of physicists being focused on the
thermodynamics of black holes in general relativity and quantum
gravity, it is amusing to note that special relativistic
thermodynamics is still a field with many open questions, both
physically and philosophically (see Earman 1981 and Liu 1994).
Fourth, another important question concerns the reduction of
thermodynamic concepts such as entropy to their mechanical, or
statistical mechanical, basis. As even a cursory glance at statistical
mechanics reveals, there are many candidates for the statistical
mechanical entropy, each the center of a different program in the
foundations of the field. Surprisingly, there is no consensus as to
which entropy is best suited to be the reduction basis of the
thermodynamic entropy (see, for example, Sklar 1993; Callender 1999;
Lavis 2005; Frigg 2008; Robertson forthcoming). Consequently, there is
little agreement about what grounds the Second Law in statistical
mechanics.
Despite the worthiness of all of these issues, this article focuses on
two distinct problems associated with the direction of time.
The first “problem of the direction of time” is: what
accounts for the time asymmetry of thermodynamics? Thermodynamics is
not a fundamental physical science. Hence it must inherit its massive
time asymmetry from the microworld. But where? In virtue of what,
fundamentally, is thermodynamics time asymmetric? The puzzle is
usually said to arise due to fundamental physics being time symmetric,
or more precisely, time reversal invariant. (A theory is time reversal
invariant, loosely speaking, if its laws don’t care about the
direction of time.) No asymmetry in, no asymmetry out; therefore there
is a puzzle over where the asymmetry enters. However, even if
fundamental physics is time asymmetric one can and should still demand
an answer to the question of what accounts for thermodynamics time
asymmetry. The answer could be non-trivial because the time asymmetry
of fundamental physics may have nothing to do with the time asymmetry
of thermodynamics. This situation actually appears to be the case, as
weak interactions between quarks and leptons can violate time symmetry
yet these violations don’t appear to be responsible for
thermodynamic behavior.
Historically the problem arose in a wonderful series of debates and
arguments between the great physicist Ludwig Boltzmann and some of his
contemporaries, notably, Johann Loschmidt, Ernst Zermelo and Edward
Culverwell. Boltzmann was one of the founders and most influential
developers of the field of statistical mechanics, as well as (later in
life) a philosopher. While seeking a mechanical underpinning of the
Second Law, he discovered a particularly ingenious explanation for why
systems tend toward equilibrium.
Ignoring historical details (Brush 1976, Frigg & Werndl 2011,
Sklar 1993, Uffink 2006), here is the core idea loosely reconstructed
from Boltzmann’s later writings. Consider an isolated gas of
\(N\) particles in a box, where \(N\) is large enough to make the
system macroscopic \((N \approx 10^{23}+)\). For the sake of
familiarity we will work with classical mechanics. We can characterize
the gas by the coordinates and momenta \(x_{in}, p_{in}\) of each of
its particles and represent the whole system by a point \(X = (q,p)\)
in a \(6N\)-dimensional phase space known as \(\Gamma\), where \(q =
(q_1 \ldots q_{3N})\) and \(p = (p_1 \ldots p_{3N})\).
Boltzmann’s great insight was to see that the thermodynamic
entropy arguably “reduced” to the volume in \(\Gamma\)
picked out by the macroscopic parameters of the system. The key
ingredient is partitioning \(\Gamma\) into compartments, such that all
of the microstates \(X\) in a compartment are macroscopically (and
thus thermodynamically) indistinguishable. To each macrostate \(M\),
there corresponds a volume of \(\Gamma\), \(\lvert\Gamma_M\rvert\),
whose size will depend on the macrostate in question. For
combinatorial reasons, almost all of \(\Gamma\) corresponds to a state
of thermal equilibrium. There are simply many more ways to be
distributed with uniform temperature and pressure than ways to be
distributed with nonuniform temperature and pressure. There is a vast
numerical imbalance in \(\Gamma\) between the states in thermal
equilibrium and the states in thermal nonequilibrium.
We now introduce Boltzmann’s famous formula (up to an additive constant) for what we might call the “Boltzmann entropy” \(S_B\): \[ S_B (M(X)) = k \log \lvert\Gamma_M\rvert \] where \(\lvert\Gamma_M\rvert\) is the volume in \(\Gamma\) associated with the macrostate \(M\), \(X\) is the microstate of the system, and \(k\) is Boltzmann’s constant. \(S_B\) provides a relative measure of the amount of \(\Gamma\) corresponding to each \(M\).
Given the mentioned asymmetry in \(\Gamma\), almost all microstates
realizing non-equilibrium macrostates are such that their
entropy value is overwhelmingly likely to increase with time. When the
constraints are released on systems initially confined to small
sections of \(\Gamma\), typical systems will evolve into
larger compartments. Since the new equilibrium distribution occupies
almost all of the newly available phase space, nearly all of
the microstates originating in the smaller volume will tend toward
equilibrium. Except for those incredibly rare microstates conspiring
to stay in small compartments, microstates will evolve in such a way
as to have \(S_B\) increase. Substantial questions can be raised about
the details of this approach. What justifies, for instance, the
standard probability measure on \(\Gamma\)? Nonetheless, the
Boltzmannian explanation seems to offer a plausible and powerful
framework for understanding why the entropy of systems tends
to increase with time. (For further explanation and discussion see
Bricmont 1995; Frigg 2008, 2009; Goldstein 2001; Hemmo & Shenker
2012; Klein 1973; Lavis 2005; Lebowitz 1993; Uffink 2006.)
Trouble looms over this explanation of time asymmetry (see Brown,
Myrvold, & Uffink 2009). Before Boltzmann explained entropy
increase as described above, he proposed a now notorious
“proof” known as the “\(H\)-theorem” to the
effect that entropy must always increase. Loschmidt 1876/1877 and
Zermelo 1896 launched objections to the \(H\)-theorem. If we take as
premises classical mechanical dynamics, they pointed out, it’s
impossible to get any function of the classical state to monotonically
increase. Loschmidt focused on the time reversal invariance of the
classical dynamics and Zermelo on its recurrence property (roughly,
that a bounded system, left to itself, will eventually return
arbitrarily close to its initial state, for any given initial state).
They were right: time reversal means that for every entropy-increasing
solution to the classical equations there is a mirror
entropy-decreasing solution; and recurrence means that every solution
will at some point have its entropy decrease if we wait long enough.
Some time asymmetric ingredient that had not been properly announced
had been smuggled into the theorem.
The reader can find this story in many textbooks and in many
references cited above. An objection in their spirit (specifically,
Loschmidt’s) can also be advanced against Boltzmann’s
later view sketched above. Loosely put, because the classical
equations of motion are time reversal invariant, nothing in the
original explanation necessarily referred to the direction of time
(see Hurley 1986). Although we just stated the Boltzmannian account of
entropy increase in terms of entropy increasing into the future, the
explanation can be turned around and made for the past
temporal direction as well. Given a gas in a box that is in a
nonequilibrium state, the vast majority of microstates that are
antecedents of the dynamical evolution leading to the present
macrostate correspond to a macrostate with higher entropy
than the present one. Therefore, not only is it highly likely that
typical microstates corresponding to a nonequilibrium state will
evolve to  higher entropy states, but it is also highly likely
that they evolved from higher entropy states.
Concisely put, the problem is that given a nonequilibrium state at
time \(t_2\), it is overwhelmingly likely that
but that due to the reversibility of the dynamics it is also
overwhelmingly likely that
where \(t_1 \lt t_2 \lt t_3\). However, transitions described by
 (2)
 do not seem to occur; or phrased more carefully, not both
 (1)
 and
 (2)
 occur. However we choose to use the terms “earlier” and
“later”, clearly entropy doesn’t increase in both
temporal directions. For ease of exposition let us dub
 (2)
 the culprit.
The traditional problem is not merely that nomologically possible
(anti-thermodynamic) behavior does not occur when it could. That is
not straightforwardly a problem: all sorts of nomologically
allowed processes do not occur. Rather, the problem is that
statistical mechanics seems to make a prediction that is falsified,
and that is a problem according to anyone’s theory of
confirmation.
Many solutions to this problem have been proposed. Generally speaking,
there are two ways to solve the problem: eliminate transitions of type
 (2)
 either with special boundary conditions or with laws of nature. The
former method works if we assume that earlier states of the
universe are of comparatively low-entropy and that
(relatively) later states are not also low-entropy states.
There are no high-to-low-entropy processes simply because earlier
entropy was very low. Alternatively, the latter method works if we can
somehow restrict the domain of physically possible worlds to those
admitting only low-to-high transitions. The laws of nature are the
straightjacket on what we deem physically possible. Since we need to
eliminate transitions of type
 (2)
 while keeping those of type
 (1)
 (or vice versa), a necessary condition of the laws doing this job is
that they be time reversal noninvariant. Our choice of strategy boils
down to either assuming temporally asymmetric boundary conditions or
of adding (or changing to or restricting to) time reversal
noninvariant laws of nature that make entropy increase likely. Many
approaches to this problem have thought to avoid this dilemma, but a
little analysis of any proposed “third way” arguably
proves this to be false.
Motivations for restrictions of type
 (2)
 transitions originate in both philosophy and in particular physical
theories. The rest of this section describes some of the wide range of
views found on the issue.
Without proclaiming the laws of nature time asymmetric, there is no
way to eliminate as impossible transitions
 (2)
 in favor of
 (1).
 Nevertheless, appealing to temporally asymmetric boundary conditions
allows us to describe a world wherein
 (1)
 but not
 (2)
 occur. A cosmological hypothesis claiming that in the very distant
past entropy was much lower will work. Boltzmann, as well as many of
this century’s greatest scientists, e.g., Einstein, Richard
Feynman, and Erwin Schroedinger, saw that this hypothesis is necessary
given our (mostly) time asymmetric laws. (Boltzmann, however,
explained this low-entropy condition by treating the observable
universe as a natural statistical fluctuation away from equilibrium in
a vastly larger universe.) Earlier states do not have higher entropy
than present states because we make the cosmological posit that the
universe began in an extremely tiny section of its available phase
space. Albert (2000) calls this the “Past Hypothesis” and
argues that it solves both this problem of the direction of time and
also the one to be discussed below. Note that classical mechanics is
also compatible with a “Future Hypothesis”: the claim that
entropy is very low in the distant future. The restriction to
“distant” is needed, for if the near future were of
low-entropy, we would not expect the thermodynamic behavior that we
see—see Cocke 1967, Price 1996, and Schulman 1997 for discussion
of two-time boundary conditions.
The Past Hypothesis offers an elegant solution to the problem of the
direction of time. However, there are some concerns.
First, some find it incredible that (e.g.) gases everywhere for all
time should expand through their available volumes due to special
initial conditions. The common cause of these events is viewed as
itself monstrously unlikely. Expressing this feeling, R. Penrose
(1989) estimates that the probability, given the standard measure on
phase space, of the universe starting in the requisite state is
astronomically small. In response, one may hold that the Past
Hypothesis is lawlike. If so, then the probability for this state, if
such exists, is one! Even if one doesn’t go down this path, one
may have other problems with claiming that the initial condition of
the universe needs further explanation. See Callender 2004a,b for such
a view and Price 1996, 2004 for the contrary position.
Second, another persistent line of criticism might be labeled the
“subsystem” worry. It’s consistent with the Past
Hypothesis, after all, that none of the subsystems on Earth ever
display thermodynamically asymmetric behavior. How exactly does the
global entropy increase of the universe imply local
entropy increase among the subsystems (which, after all, is what
causes us to posit the Second Law in the first place)? See Winsberg
2004 for this objection and Callender 2011a, Frisch 2010, and North
2011 for discussion.
Third, what exactly does the Past Hypothesis say in the context of our
best and most recent physics? While not denying that temporally
asymmetric boundary conditions are needed to solve the problem, Earman
(2006) is very critical of the Past Hypothesis, concluding that it
isn’t even coherent enough to be false. The main problem Earman
sees is that we cannot state the Past Hypothesis in the language of
general relativity. Callender (2010, 2011b) and Wallace (2010) discuss
the related question of stating the Past Hypothesis when
self-gravitation is included. One may also consider the question in
the context of quantum theory (see Wallace 2013).
If we place an isolated concentrated homogeneous gas in the middle of
a large empty volume, we would expect the particles to spread out in
an expanding sphere about the center of the gas, much as waves of
radiation spread out from concentrated charge sources. It is therefore
tempting to think that there is a relationship between the
thermodynamic and electromagnetic arrows of time. In a debate in 1909,
Albert Einstein and Walther Ritz apparently disagreed about the nature
of this relationship, although the exact points of dispute remain a
bit unclear. The common story told is that Ritz took the position that
the asymmetry of radiation had to be judged lawlike and that the
thermodynamic asymmetry could be derived from this law.
Einstein’s position is instead that “irreversibility is
exclusively based on reasons of probability” (Ritz and Einstein
1909, English translation from Zeh 1989: 13). It is unclear whether
Einstein meant probability plus the right boundary conditions, or
simply probability alone. In any case, Ritz is said to believe that
the radiation arrow causes the thermodynamic one, whereas Einstein is
said to hold something closer to the opposite position. The real story
is far more complicated, as Ritz had a particle-based ontology in mind
as well as many additional considerations (see Frisch and Pietsch 2016
for subtleties of the actual historical debate).
If this common tale is correct—and there is reason to
think it isn’t the full story—then it seems that Einstein
must be closer to being correct than Ritz. Ritz’ position
appears implausible if only because it implies gases composed of
neutral particles will not tend to spread out. That aside,
Einstein’s position is attractive if we concentrate on the wave
asymmetry mentioned above. Using Popper 1956’s famous mechanical
wave example as an analogy, throwing a rock into a pond so that waves
on the surface spread out into the future requires every bit the
conspiracy that is needed for waves to converge on a point in order to
eject a rock from the bottom. However, here it does seem clear that
one process is favored thermodynamically and the other disfavored once
we have a thermodynamic arrow in hand. Given a solution to the
thermodynamic arrow, impulses directed toward the center of a pond
such as to eject a rock are unlikely, whereas a rock triggering
spherical waves diverging from the point of impact are likely. Here
the radiation arrow seems plausibly connected to and perhaps even
derivable from the thermodynamic arrow. The main interesting
difference is that Popper’s time-reversed pond seems
approximately attainable whereas anti-thermodynamic processes seem
more absolutely forbidden (or at least dramatically harder to engine,
requiring a so-called Maxwell Demon).
If the wave asymmetry were the only electromagnetic arrow, then the
above sketch would plausibly capture the core connection between the
thermodynamic and electromagnetic arrows of time. We would have reason
to think that whatever causes the thermodynamic arrow also is
responsible for the electromagnetic arrow. That may ultimately be
correct. However, it’s too early to conclude that, for
electromagnetism is chock full of arrows of time besides the wave
asymmetry.
Maxwell’s equations are well-known to include both “advanced” and “retarded” solutions. The retarded solution \[ \phi_{\text{ret}}(r,t) = \int dr' \rho\frac{(r', t- \frac{\lvert r'-r\rvert}{c})}{\lvert r'-r\rvert} \] gives the field amplitude \(\phi_{\text{ret}}\) at \(r,t\) by finding the source density \(r\) at \(r'\) at earlier times. The advanced solution \[ \phi_{\text{adv}}(r,t) = \int dr' \rho\frac{(r', t+ \frac{\lvert r'-r\rvert}{c})}{\lvert r'-r\rvert} \] gives the field amplitude in terms of the source density at \(r'\) at later times. Physicists routinely discard the advanced solutions for reasons of “causality”. It is not so clear thermodynamic considerations are behind this rejection of solutions, an asymmetry made all the harder to see given the freedom electromagnetism has to rewrite retarded fields in terms of advanced fields and outgoing sourceless radiation (and vice versa). Electromagnetism is also said to be allow emissions and not absorptions. Accelerating charges are also damped and not anti-damped by the field. With so many arrows besides the wave asymmetry—emission/absorption, in/out, retarded/advanced, damped/anti-damped—it’s premature to say that the thermodynamic arrow is the one arrow to rule them all. Most agree that the wave asymmetry is ultimately “thermodynamic” but after that matters are contested.
For further discussion of these controversial points, see the
articles/chapters by Allori 2015; Arntzenius 1994; Atkinson 2006;
Earman 2011; Frisch 2000, 2006; Frisch and Pietsch 2016; North 2003;
Price 1996, 2006; Rohrlich 2006; and Zeh 1989.
Cosmology presents us with a number of apparently temporally
asymmetric mechanisms. The most obvious one is the inexorable
expansion of the universe. The spatial scale factor \(a(t)\), which we
might conceive roughly as the radius of the universe (it gives the
distance between co-moving observers), is increasing. The universe
seems to be uniformly expanding relative to our local frame. Since
this temporal asymmetry occupies a rather unique status it is natural
to wonder whether it might be the “master” arrow.
The cosmologist Thomas Gold 1962 proposed just this. Believing that
entropy values covary with the size of the universe, Gold asserts that
at the maximum radius the thermodynamic arrow will “flip”
due to the re-contraction. However, as Richard Tolman 1934 has shown
in some detail, a universe filled with non-relativistic particles will
not suffer entropy increase due to expansion, nor will an expanding
universe uniformly filled with blackbody radiation increase its
entropy either. Interestingly, Tolman demonstrated that more realistic
universes containing both matter and radiation will change
their entropy contents. Coupled with expansion, various processes will
contribute to entropy increase, e.g., energy will flow from the
“hot” radiation to the “cool” matter. So long
as the relaxation time of these processes is larger than the expansion
time scale, they should generate entropy. We thus have a purely
cosmological method of entropy generation.
Others (e.g., Davies 1994) have thought inflation provides a kind of
entropy-increasing behavior—again, given the sort of matter
content we have in our universe. The inflationary model is an
alternative of sorts to the standard big bang model, although by now
it is so well entrenched in the cosmology community that it really
deserves the tag “standard”. In this scenario, the
universe is very early in a quantum state called a “false
vacuum”, a state with a very high energy density and negative
pressure. Gravity acts like Einstein’s cosmological constant, so
that it is repulsive rather than attractive. Under this force the
universe enters a period of exponential inflation, with geometry
resembling de Sitter space. When this period ends any initial
inhomogeneities will have been smoothed to insignificance. At this
point ordinary stellar evolution begins. Loosely associating
gravitational homogeneity with low-entropy and inhomogeneity with
higher entropy, inflation is arguably a source of a low entropy
“initial” condition.
There are other proposed sources of cosmological entropy generation,
but these should suffice to give the reader a flavor of the idea. We
shall not be concerned with evaluating these scenarios in any detail.
Rather, our concern is about how these proposals explain time’s
arrow. In particular, how do they square with our earlier claim that
the issue boils down to either assuming temporally asymmetric boundary
conditions or of adding time reversal non-invariant laws of
nature?
The answer is not always clear, owing in part to the fact that the
separation between laws of nature and boundary conditions is
especially slippery in the science of cosmology. Advocates of the
cosmological explanation of time’s arrow typically see
themselves as explaining the origin of the needed low-entropy
cosmological condition. Some explicitly state that special initial
conditions are needed for the thermodynamic arrow, but differ with the
conventional “statistical” school in deducing the origin
of these initial conditions. Earlier low-entropy conditions are not
viewed as the boundary conditions of the spacetime. They came about,
according to the cosmological schools, about a second or more after
the big bang. But when the universe is the size of a small particle, a
second or more is enough time for some kind of cosmological mechanism
to bring about our low-entropy “initial” condition. What
cosmologists (primarily) differ about is the precise nature of this
mechanism. Once the mechanism creates the “initial”
low-entropy we have the same sort of explanation of the thermodynamic
asymmetry as discussed in the previous section. Because the proposed
mechanisms are supposed to make the special initial conditions
inevitable or at least highly probable, this maneuver seems like the
alleged “third way” mentioned above.
The central question about this type of explanation, as far as
we’re concerned, is this: Is the existence of the low
“initial” state a consequence of the laws of nature alone
or the laws plus boundary conditions? In other words, first, does the
proposed mechanism produce low-entropy states given any
initial condition, and second, is it a consequence of the
laws alone or a consequence of the laws plus initial
conditions? We want to know whether our question has merely been
shifted back a step, whether the explanation is a disguised appeal to
special initial conditions. Though we cannot here answer the question
in general, we can say that the two mechanisms mentioned are not
lawlike in nature. Expansion fails on two counts. There are boundary
conditions in expanding universes that do not lead to an entropy
gradient, i.e., conditions without the right matter-radiation content,
and there are boundary conditions that do not lead to expansion in
which entropy nonetheless increases, e.g., matter-filled Friedmann
models that do not expand. Inflation fails at least on the second
count. Despite advertising, arbitrary initial conditions will not give
rise to an inflationary period. Furthermore, it’s not clear that
inflationary periods will give rise to thermodynamic asymmetries
(Price 1996: ch. 2). The cosmological scenarios do not seem to make
the thermodynamic asymmetries a result of nomic necessity. The
cosmological hypotheses may be true, and in some sense, they may even
explain the low-entropy initial state. But they do not appear to
provide an explanation of the thermodynamic asymmetry that makes it
nomologically necessary or even likely.
Another way to see the point is to consider the question of whether
the thermodynamic arrow would “flip” if (say) the universe
started to contract. Gold, as we said above, asserts that at the
maximum radius the thermodynamic arrow must “flip” due to
the re-contraction. Not positing a thermodynamic flip while
maintaining that entropy values covary with the radius of the universe
is clearly inconsistent—it is what Price (1996) calls the
fallacy of a “temporal double standard”. Gold does not
commit this fallacy, and so he claims that the entropy must decrease
if ever the universe started to re-contract. However, as Albert
writes,
there are plainly locations in the phase space of the world from which
… the world’s radius will inexorably head up and the
world’s entropy will inexorably head down. (2000: 90)
Since that is the case, it doesn’t follow from law that the
thermodynamic arrow will flip during re-contraction; therefore,
without changing the fundamental laws, the Gold mechanism cannot
explain the thermodynamic arrow in the sense we want.
From these considerations we can understand the basic dilemma
that runs throughout Price (1995, 1996): either we explain the earlier
low-entropy condition Gold-style or it is inexplicable by
time-symmetric physics. Because there is no net asymmetry in a Gold
universe, we might paraphrase Price’s conclusion in a more
disturbing manner as the claim that the (local) thermodynamic arrow is
explicable just in case (globally) there isn’t one. However,
notice that this remark leaves open the idea that the laws governing
expansion or inflation are not time reversal invariant. (For more on
Price’s basic dilemma, see Callender 1998 and Price 1995.)
Finally, it’s important to remember that this dilemma and the
need for a Past Hypothesis are dependent upon a particular physical
set-up. Can we explain the thermodynamic arrow without invoking a Past
Hypothesis? Inspired by the idea of eternal spontaneous inflation,
Carroll and Chen (2004, Other Internet Resources) describe a model in
which new baby universes (or “pocket universes”) are
repeatedly born from existing universes. Each birth increases the
overall entropy of the multiverse although within each baby universe
we have our familiar thermodynamic asymmetry. The crucial assumption
in this model – one also found in the gravitational theory of
Barbour, Koslowski, and Mercati (2014) – is that entropy is
unbound. It can be arbitrarily high. With this assumption and in these
models, one can do without a past Hypothesis. For discussion, see
Goldstein, Tumulka, & Zanghi 2016 and Lazarovici and Reichert
2020.
Quantum cosmology, it is often said, is the theory of the
universe’s initial conditions. Presumably this entails that its
posits are to be regarded as lawlike. Because theories are typically
understood as containing a set of laws, quantum cosmologists
apparently assume that the distinction between laws and initial
conditions is fluid. Particular initial conditions will be said to
obtain as a matter of law. Hawking writes, for example,
we shall not have a complete model of the universe until we can say
more about the boundary conditions than that they must be whatever
would produce what we observe, (1987: 163).
Combining such aspirations with the observation that thermodynamics
requires special boundary conditions leads quite naturally to the
thought that “the second law becomes a selection principle for
the boundary conditions of the universe [for quantum cosmology]”
(Laflamme 1994: 358). In other words, if one is to have a theory of
initial conditions, it would certainly be desirable to deduce initial
conditions that will lead to the thermodynamic arrow. This is
precisely what many quantum cosmologists have sought. (This should be
contrasted with the arrows of time discussed in semiclassical quantum
gravity, for example, the idea that quantum scattering processes in
systems with black holes violate the CPT theorem.) Since quantum
cosmology is currently very speculative, it might be premature to
start worrying about what it says about time’s arrow.
Nevertheless, there has been a substantial amount of debate on this
issue (see Haliwell et al. 1994).
Penrose and Percival (1962) propose a general causal principle to
handle our problem. The principle states that the effects of
interactions happen after those interactions but not
before. Similar to Reichenbach’s principle of the
common cause, they suggest what they dub the Law of Conditional
Independence, namely, that “If A and B are two disjoint
4-regions, and C is any 4-region which divides the union of the pasts
of A and B into two parts, one containing A and the other containing
B, then A and B are conditionally independent given c. That is,
Pr(a&b/c) = Pr(a/c) × Pr(b/c), for all a,b.” (Penrose
and Percival 1962, p. 611). Here c is an event that is a common cause
that screens off the correlation between events in A and B.
In terms of statistical mechanics, this law would have the effect of
making the phase space density associated with a system at a time
determined by earlier events but not later events. This would more or
less directly preclude the temporal "parity of reasoning" motivated
transitions assumed in the problem of the direction of time,
transition of type (2). To achieve this, the Law of Conditional
Independence must be time asymmetric, which it is, and it must be a
kind of fundamental principle that restricts the lawlike correlation
otherwise allowed. After all, if we assume that the laws of nature are
time reversal invariant, then there is no asymmetry between pre- and
post-interaction correlations.
Price 1996 (chapter 5) and Sklar 1993 hold that this nomic restriction
is unwarranted or unexplanatory. There is the sense that the causal
asymmetry should come out of more basic physics, not be baked into
this physics. Horwich 1987 is an example of someone trying to derive
what he calls the fork asymmetry, which is similar to the Law of
Conditional Independence, from more basic assumptions. 
A recent contribution that has some affinities with the Penrose and
Percival move can be found in Myrvold 2020. 
Some philosophers have sought an answer to the problem of time’s
arrow by claiming that time itself is directed. They do not
mean time is asymmetric in the sense intended by advocates of the
tensed theory of time. Their proposals are firmly rooted in the idea
that time and space are properly represented on a four-dimensional
manifold. The main idea is that the asymmetries in time indicate
something about the nature of time itself. Christensen (1993) argues
that this is the most economical response to our problem since it
posits nothing besides time as the common cause of the asymmetries,
and we already believe in time. A proposal similar to
Christensen’s is Weingard’s “time-ordering
field” (1977). Weingard’s speculative thesis is that
spacetime is temporally oriented by a “time potential”, a
timelike vector field that at every spacetime point directs a vector
into its future light cone. In other words, supposing our spacetime is
temporally orientable, Weingard wants to actually orient it. The main
virtue of this is that it provides a time sense everywhere, even in
spacetimes containing closed timelike curves (so long as they’re
temporally orientable). As he shows, any explication of the
“earlier than” relation in terms of some other physical
relation will have trouble providing a consistent description of time
direction in such spacetimes. Another virtue of the idea is that it is
in principle capable of explaining all the temporal
asymmetries. If coupled to the various asymmetries in time, it would
be the “master arrow” responsible for the arrows of
interest. As Sklar (1985) notes, Weingard’s proposal makes the
past-future asymmetry very much like the up-down asymmetry. As the
up-down asymmetry was reduced to the existence of a gravitational
potential—and not an asymmetry of space itself—so the
past-future asymmetry would reduce to the time potential—and not
an asymmetry of time itself. Of course, if one thinks of the
gravitational metric field as part of spacetime, there is a sense in
which the reduction of the up-down asymmetry really was a reduction to
a spacetime asymmetry. And if the metric field is conceived as part of
spacetime—which is itself a huge source of contention in
philosophy of physics—it is natural to think of Weingard’s
time-ordering field as also part of spacetime. Thus his proposal
shares a lot in common with Christensen’s suggestion.
This sort of proposal has been criticized by Sklar on methodological
grounds. Sklar claims that scientists would not accept such an
explanation (1985: 111–2). One might point out, however, that
many scientists did believe in analogues of the time-ordering field as
possible causes of the CP
 violations.[4]
 The time-ordering field, if it exists, would be an unseen (except
through its effects) common cause of strikingly ubiquitous phenomena.
Scientists routinely accept such explanations. To find a problem with
the time-ordering field we need not invoke methodological scruples;
instead we can simply ask whether it does the job asked of it. Is
there a mechanism that will couple the time-ordering field to
thermodynamic phenomena? Weingard says the time potential field needs
to be suitably coupled (1977: 130) to the non-accidental asymmetric
processes, but neither he nor Christensen elaborate on how this is to
be accomplished. Until this is addressed satisfactorily, this
speculative idea must be considered interesting yet embryonic. For
more recent work in this vein, see Maudlin 2002.
When explaining time’s arrow many philosophers and physicists
have focused their attention upon the unimpeachable fact that real
systems are open systems that are subjected to interactions of various
sorts. Thermodynamic systems cannot be truly isolated. To take the
most obvious example, we can not shield a system from the influence of
gravity. At best, we can move systems to locations feeling less and
less gravitational force, but we can never completely decouple a
system from the gravitational field. Not only do we ignore the weak
gravitational force when doing classical thermodynamics, but we also
ignore less exotic matters, such as the walls in the standard gas in a
box scenario. We can do this because the time it takes for a gas to
reach equilibrium with itself is vastly shorter than the time it takes
the gas plus walls system to reach equilibrium. For this reason we
typically discount the effects of the box walls on the gas.
In this approximation many have thought there lies a possible solution
to the problem of the direction of time. Indeed, many have thought
herein lies a solution that does not change the laws of
classical mechanics and does not allow for the nomological
possibility of anti-thermodynamic behavior. In other words, advocates
of this view seem to believe it embodies a third way. Blatt 1959;
Reichenbach 1956; Redhead and Ridderbos 1998, and to some extent,
Horwich 1987 are a few works charmed by this idea.
The idea is to take advantage of what a random perturbation of the
representative phase point would do to the evolution of a system.
Given our Boltzmannian setup, there is a tremendous asymmetry in phase
space between the volumes of points leading to equilibrium and of
points leading away from equilibrium. If the representative point of a
system were knocked about randomly, then due to this asymmetry, it
would be very probable that the system at any given time be on a
trajectory leading toward equilibrium. Thus, if it could be argued
that the earlier treatment of the statistical mechanics of ideal
systems ignored a random perturber in the environment of the system,
then one would seem to have a solution to our problems. Even if the
perturbation were weak it would still have the desired effect. The
weak “random” previously ignored knocking of the
environment is is claimed to be the cause of the approach to
equilibrium. Prima facie, this answer to the problem escapes
the appeal to special initial conditions and the appeal to new
laws.
But only prima facie. A number of criticisms have been
leveled against this maneuver. One that seems on the mark is the
observation that if classical mechanics is to be a universal theory,
then the environment must be governed by the laws of classical
mechanics as well. The environment is not some mechanism outside the
governance of physical law, after all, and when we treat it too, the
“deus ex machina”—the random
perturber—disappears. If we treat the gas-plus-the-container
walls as a classical system, it is still governed by time-reversible
laws that will cause the same problem as we met with the gas alone. At
this point one sometimes sees the response that this combined system
of gas plus walls has a neglected environment too, and so on, and so
on, until we get to the entire universe. It is then questioned whether
we have a right to expect laws to apply universally (Reichenbach 1956:
81ff). Or the point is made that we cannot write down the Hamiltonian
for all the interactions a real system suffers, and so there will
always be something “outside” what is governed by the
time-reversible Hamiltonian. Both of these points rely, one suspects,
on an underlying instrumentalism about the laws of nature. Our problem
only arises if we assume or pretend that the world literally is the
way the theory says; dropping this assumption naturally
“solves” the problem. Rather than further address these
responses, let us turn to the claim that this maneuver need not modify
the laws of classical mechanics.
If one does not make the radical proclamation that physical law does
not govern the environment, then it is easy to see that whatever law
describes the perturber’s behavior, it cannot be the laws of
classical mechanics \(if\) the environment is to do the job required
of it. A time-reversal noninvariant law, in contrast to the time
symmetric laws of classical mechanics, must govern the external
perturber. Otherwise we can in principle subject the whole system,
environment plus system of interest, to a Loschmidt reversal. The
system’s velocities will reverse, as will the velocities of the
millions of tiny perturbers. “Miraculously”, as if there
were a conspiracy between the reversed system and the millions of
“anti-perturbers”, the whole system will return to a time
reverse of its original state. What is more, this reversal will be
just as likely as the original process if the laws are time reversal
invariant. A minimal criterion of adequacy, therefore, is that the
random perturbers be time reversal noninvariant. But the laws
of classical mechanics are time reversal invariant. Consequently, if
this “solution” is to succeed, it must exercise new laws
and modify or supplement classical mechanics. (Since the perturbations
need to be genuinely random and not merely unpredictable, and since
classical mechanics is deterministic, the same sort of argument could
be run with indeterminism instead of irreversibility. See Price 2002
for a diagnosis of why people have made this mistake, and also for an
argument objecting to interventionism for offering a
“redundant” physical mechanism responsible for entropy
 increase.)[5]
To the best of our knowledge our world is fundamentally quantum
mechanical, not classical mechanical. Does this change the situation?
“Maybe” is perhaps the best answer. Not surprisingly,
answers to the question are affected by one’s interpretation of
quantum mechanics. Quantum mechanics suffers from the notorious
measurement problem, a problem which demands one or another
interpretation of the quantum formalism. These interpretations fall
broadly into two types, depending on their view of the unitary
evolution of the quantum state (e.g., evolution according to the
Schroedinger equation): they either say that there is something more
than the quantum state, or that the unitary evolution is not entirely
correct. The former are called “no-collapse”
interpretations while the latter are dubbed “collapse”
interpretations. This is not the place to go into the details of these
interpretations, but we can still sketch the outlines of the picture
painted by quantum mechanics (for more see Albert 1992).
Modulo some philosophical concerns about the meaning of time reversal
(Albert 2000; Earman 2002), the equation governing the unitary
evolution of the quantum state is time reversal invariant. For
interpretations that add something to quantum mechanics, this
typically means that the resulting theory is time reversal invariant
too (since it would be odd or even inconsistent to have one part of
the theory invariant and the other part not). Since the resulting
theory is time reversal invariant, it is possible to generate the
problem of the direction of time just as we did with classical
mechanics. While many details are altered in the change from classical
to no-collapse quantum mechanics, the logical geography seems to
remain the same.
Collapse interpretations are more interesting with respect to our
topic. Collapses interrupt or outright replace the unitary evolution
of the quantum state. To date, they have always done so in a time
reversal noninvariant manner. The resulting theory,
therefore, is not time reversal invariant. This fact offers a
potential escape from our problem: the transitions of type
 (2)
 in our above statement of the problem may not be lawful. And this has
led many thinkers throughout the century to believe that collapses
somehow explain the thermodynamic time asymmetry.
Mostly these postulated methods fail to provide what we want. We think
gases relax to equilibrium even when they’re not measured by
Bohrian observers or Wignerian conscious beings. This complaint is,
admittedly, not independent of more general complaints about the
adequacy of these interpretations. But perhaps because of these
controversial features they have not been pushed very far in
explaining thermodynamics.
More satisfactory collapse theories exist, however. One, due to
Ghirardi, Rimini, and Weber, commonly known as GRW, can describe
collapses in a closed system—no dubious appeal to observers
outside the quantum system is required. Albert (1992, 2000) has
extensively investigated the impact GRW would have on statistical
mechanics and thermodynamics. GRW would ground a temporally asymmetric
probabilistic tendency for systems to evolve toward equilibrium.
Anti-thermodynamic behavior is not impossible according to this
theory. Instead it is tremendously unlikely. The innovation of the
theory lies in the fact that although entropy is overwhelmingly likely
to increase toward the future, it is not also overwhelmingly likely to
increase toward the past (because there are no dynamic backwards
transition probabilities provided by the theory). So the theory does
not suffer from a problem of the direction of time as stated
above.
This does not mean, however, that it removes the need for something
like the Past Hypothesis. GRW is capable of explaining why, given a
present nonequilibrium state, later states should have higher entropy;
and it can do this without also implying that earlier states have
higher entropy too. But it does not explain how the universe ever got
into a nonequilibrium state in the first place. As indicated before,
some are not sure what would explain this fact, if anything,
or whether it’s something we should even aspire to explain. The
principal virtue GRW would bring to the situation, Albert thinks, is
that it would solve or bypass various troubles involving the nature of
probabilities in statistical mechanics.
The same type of benefit, plus arguably others, come from a recent
proposal by Chen (forthcoming). Chen suggests that we adopt a position
known as density matrix realism to help understand time’s arrow.
Instead of regarding the wavefunction as the basic ontology of quantum
theory, we take the quantum state to be represented by an impure
density matrix. When we express the Past Hypothesis in terms of a
density matrix, a number of virtues appear, including greater harmony
between the probabilities of statistical mechanics and quantum
mechanics. It may be that interpretations of quantum mechanics that
are not like GRW can possess some of the same benefits that GRW
brings.
More detailed discussion of the impact quantum mechanics has on our
problem can be found in Albert 2000, North 2002, Price 2002 and Chen
forthcoming. But if our superficial review is correct, we can say that
quantum mechanics will not obviate our need for a Past Hypothesis
though it may well solve at least one problem related to the direction
of time.
Finally, let’s return to a point made in passing about the
status of the Past Hypothesis. Without some new physics that
eliminates or explains the Past Hypothesis, or some satisfactory
“third way”, it seems we are left with a bald posit of
special initial conditions. One can question whether there really is
anything unsatisfactory about this (Sklar 1993; Callender 2004b). But
perhaps we were wrong in the first place to think of the Past
Hypothesis as a contingent boundary condition. The question “why
these special initial conditions?” would be answered with
“it’s physically impossible for them to be
otherwise”, which is always a conversation stopper. Indeed,
Feynman (1965: 116) speaks this way when explaining the statistical
version of the second law.
Absent a particular understanding of laws of nature, there is perhaps
not much to say about the issue. But given particular conceptions of
lawhood, it is clear that various judgments about this issue follow
naturally—as we will see momentarily. However, let’s
acknowledge that this may be to get matters backwards. It might be
said that we first ought to find out whether the boundary
conditions are lawlike, and then devise a theory of
law appropriate to the answer. To decide whether or not the boundary
conditions are lawlike based merely on current philosophical theories
of law is to prejudge the issue. Perhaps this objection is really
evidence of the feeling that settling the issue based on one’s
conception of lawhood seems a bit unsatisfying. It is hard to deny
this. Even so, it is illuminating to have a brief look at the
relationships between some conceptions of lawhood and the topic of
special initial conditions. For discussion and references on laws of
nature, please refer to the entry on that topic.
For instance, if one agrees with John Stuart Mill that from the laws
one should be able to deduce everything and one considers the
thermodynamic part of that “everything”, then the special
initial condition will be needed for such a deduction. The modern heir
of this conception of lawhood, the one associated with Frank Ramsey
and David Lewis (see Loewer 1996), sees laws as the axioms of the
simplest, most powerful, consistent deductive system possible. It is
likely that the specification of a special initial condition would
emerge as an axiom in such a system, for such a constraint may well
make the laws much more powerful than they otherwise would be.
We should not expect the naïve regularity view of laws to follow
suit, however. On this sort of account, roughly, if \(B\)s always
follow \(A\)s, then it is a law of nature that \(A\) causes \(B\). To
avoid finding laws everywhere, however, this account needs to assume
that \(A\)s and \(B\)s are instantiated plenty of times. But the
initial conditions occur only once.
For more robust realist conceptions of law, it’s difficult to
predict whether the special initial conditions will emerge as lawlike.
Necessitarian accounts like Pargetter’s (1984) maintain that it
is a law that \(P\) in our world iff \(P\) obtains at every possible
world joined to ours by a nomic accessibility relation. Without more
specific information about the nature of the accessibility relations
and the worlds to which we’re related, one can only guess
whether all of the worlds relative to ours have the same special
initial conditions. Nevertheless some realist theories offer
apparently prohibitive criteria, so they are able to make negative
judgments. For instance, “universalist” theories
associated with David Armstrong say that laws are relations between
universals. Yet a constraint on initial conditions isn’t in any
natural way put in this form; hence it would seem the universalist
theory would not consider this constraint lawlike.
Philosophical opinion is certainly divided. The problem is that a
lawlike boundary condition lacks many of the features we ordinarily
attribute to laws, e.g., multiple instances, governing temporal
evolution, etc., yet different accounts of laws focus on different
subsets of these features. When we turn to the issue at hand, what we
find is the disagreement we expect.
Life is filled with temporal asymmetries. This directedness is one of
the most general features of the world we inhabit. We can break this
general tendency down into a few more specific temporal arrows.
The above list is not meant to be exhaustive or especially clean.
Temporal asymmetries are everywhere. We age and die. Punchlines are at
the ends of jokes. Propensities and dispositions and reproductive
fitness are all future-directed. We prefer rags-to-riches stories to
riches-to-rags stories. Obviously there are connections amongst many
of these arrows. Some authors have explicitly or implicitly proposed
various “dependency charts” that are supposed to explain
which of the above arrows depend on which for their existence. Horwich
(1987) argues for an explanatory relationship wherein the
counterfactual arrow depends on the causal arrow, which depends on the
arrow of explanation, which depends on the epistemological arrow.
Lewis (1979), by contrast, thinks an alleged over-determination of
traces grounds the asymmetry of counterfactuals and that this in turn
grounds the rest. Suhler and Callender (2011) ground the psychological
arrow on the causal and knowledge asymmetries. The chart one judges
most appropriate will depend, to a large degree, upon one’s
general philosophical stance on many large topics.
Which dependency chart is the correct one is not our concern here.
Rather, the second “problem of the direction of time”
asks: do any (all?) of these arrows ultimately hold in virtue of the
thermodynamic arrow of time (or what grounds it)?
Sklar (1985) provides useful examples to have in mind. Consider the
up-down asymmetry. It plausibly reduces to the local gravitational
gradient. Astronauts on the moon think down is the direction toward
the center of the moon, not wherever it was when they left Earth. By
contrast, there is (probably) merely a correlation between the
left-right asymmetry (say, in snail shells) and parity violations in
high-energy particle physics. The second problem asks whether any of
the above temporal asymmetries are to the thermodynamic arrow as the
up-down asymmetry is to the local gravitational gradient. Of course,
we don’t expect anything quite so straightforward. Sklar
describes an experiment where iron dust inserted in the ear sacs of
fish cause the fish to swim upside down when a magnet is held over the
tank, presumably altering their sense of up and down. But as Jos
Uffink remarked to me, going inside a refrigerator doesn’t cause
us to remember the future. The connections, if any, are bound to be
subtle.
Inspired by Boltzmann’s attempts in this regard, many
philosophers have sought such reductions, either partial or total.
Grünbaum (1973) and Smart (1967) develop entropic accounts of the
knowledge asymmetry. Lewis (1979) suspects the asymmetry of traces is
linked to the thermodynamic arrow but provides no specifics. Dowe
(1992), like a few others, ties the direction of causation to the
entropy gradient. And some have also tied the psychological arrow to
this gradient (for a discussion see Kroes 1985). Perhaps the most
ambitious attempts at grounding many arrows all at once can be found
in Reichenbach 1956, Horwich 1987, and Albert 2000, 2015. Each of
these books offers possible thermodynamic explanations for the causal
and epistemic arrows, as well as many subsidiary arrows.
A straightforward reduction of these arrows to entropy is probably not
in the cards (Earman 1974; Horwich 1987). Consider the epistemic arrow
of time. The traditional entropic account claimed that because we know
there are many more entropy-increasing rather than entropy-decreasing
systems in the world (or our part of it), we can infer when we see a
low-entropy system that it was preceded and caused by an interaction
with something outside the system. To take the canonical example,
imagine you are walking on the beach and come across a footprint in
the sand. You can infer that earlier someone walked by (in contrast to
it arising as a random fluctuation). In other words, you infer, due to
its high order, that it was caused by something previously also of
high (or higher) order, i.e, someone walking.
However, the entropic account faces some very severe challenges.
First, do footprints on beaches have well-defined thermodynamic
entropies? To describe the example we switched from low-entropy to
high order, but the association between entropy and our ordinary
concept of order is tenuous at best and usually completely misleading.
(To appreciate this, just consider what happens to your salad dressing
after it is left undisturbed. Order increases when the oil and vinegar
separate, yet entropy has increased.) To describe the range of systems
about which we have knowledge, the account needs something broader
than the thermodynamic entropy. But what? Reichenbach is forced to
move to a notion of quasi-entropy, losing the reduction in the
process. Second, the entropic account doesn’t license the
inference to a human being walking on the beach. All it tells you is
that the grains of sand in the footprint interacted with its
environment previously, which barely scratches the surface of our
ability to tell detailed stories about what happened in the past.
Third, even if we entertain a broader understanding of entropy, it
still doesn’t always work. Consider Earman’s (1974)
example of a bomb destroying a city. From the destruction we may infer
that a bomb went off; yet the bombed city does not have lower entropy
than its surroundings or even any type of intuitively higher order
than its surroundings.
Presumably for these reasons, contemporary theories abandon the
attempt to ground the arrows of time on thermodynamic entropy.
Instead, they turn to statistical mechanics, that which grounds the
thermodynamic arrow. This more general basis is regarded as more
fertile ground for the other arrows. In effect, the thermodynamic
arrow is just regarded as another non-basic arrow like those four
mention above. Horwich (1987) traces the arrows back to initial
micro-chaos. Albert (2000, 2015) and Loewer (2012) instead trace them
to a package dubbed the Mentaculus (after the Coen
brothers’ film, A Serious Man, 2009). Let’s
briefly consider how Albert and Loewer propose to derive the
thermodynamic arrow, the epistemic arrow, and the causal arrow all
from the Mentaculus.
In the Coen brothers’ film, the character Arthur Gopnik, a
mathematician, spends his days on a couch filling a notebook with a
probability map of the universe, the Mentaculus. It is an apt name for
what statistical mechanics provides us according to Albert and Loewer.
In effect, it provides us with a probability map for every macroscopic
generalization because it provides probabilities over all the
microstates realizing these macrostates. The package is composed of
the following elements: the past hypothesis (that the entropy of
initial macrostate \(M(0)\) is extremely low), a uniform probability
distribution over the microstates that realize \(M(0)\), the present
macrostate \(M(t)\), and the dynamical laws of the microlevel.
This package, they say, implies the thermodynamic arrow. We
“derive” it from basic physics by making a case at time
\(t\) that
Boltzmann, Gibbs and many others make the case, although it’s
worth bearing in mind that they do so rigorously only in ideal cases
and much remains controversial (see above). Still, it strikes many as
physically plausible. One could say a lot more, but let’s grant
this. Then notice that the first problem of the direction of time is
blocked by the Past Hypothesis. One conditionalizes on the uniform
distribution given \(M(0)\) and \(M(t)\), not merely \(M(t)\). The
constraint at one end of the universe makes the claim that earlier
entropy was higher unlikely. If correct, we have an honest-to-goodness
reduction of a special science law “the second law of
thermodynamics” from the bottom.
But this package also implies more. Turn to the causal arrow. As a
very rough first approximation, causation can be analyzed
probabilistically. Cause \(C\) causes effect \(E\) just in case \(C\)
is prior to \(E\) and the probability of \(E\) given \(C\) and
background \(B\) is greater than the probability of \(E\) given \(B\)
alone. Of course, there are major problems with this account (see the
entry on Probabilistic Causation). Yet the core intuition appears to
come from the package, as one gets the temporal priority of causes
from the Past Hypothesis and the probabilities from statistical
mechanics. Together, they are claimed to explain why we can manipulate
causes to produce effects but not vice versa. Turn to the epistemic
arrow. Reflect on the nature of records. When you weigh yourself on a
scale, one produces a record of one’s weight. This record is
based on an inference comparing the states of the scale at two
different times. I’m (say) 180 lbs if the scale was in its
functioning ready state at 0 lbs before I stepped on it. The idea,
very loosely (see Albert 2000, 2015 and Loewer 2012 for the details)
is that the Past Hypothesis effectively is the world’s Ready
State. This highly constrained state is what causes there to be
macroscopic traces of the past in the present but not macroscopic
traces of the future in the present.
Naturally, this ambitious program met with vigorous criticism. The
idea that statistical mechanics implies (probabilistically) the truth
or falsity of virtually every counterfactual-supporting generalization
in all of science and everyday life strikes many as going too far. See
Callender and Cohen 2010, Earman 2006, Frisch 2010, Leeds 2003, North
2011, Westlake 2014, Winsberg 2004 and some essays in Wilson 2014.
Long ago Boltzmann (e.g., 1895) suggested that the temporal
asymmetries discussed above are explained by the direction of
increasing entropy. A lot of progress has been made developing this
tantalizing thesis. Nevertheless, just as work on the first problem of
the origins of the thermodynamic arrow remains active, so too does
research on the second.