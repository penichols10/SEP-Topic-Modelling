{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abd85efc-4069-485b-b63d-e534cac9818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346e9a90-eb66-42e0-b47d-20a8bb46c273",
   "metadata": {},
   "source": [
    "# Read in the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2079c8bf-5cef-49ac-8799-6998a530af79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_docs():\n",
    "    doc_dir = 'pages//'\n",
    "    doc_locations = []\n",
    "    for file in os.listdir(doc_dir):\n",
    "        if file.split('.')[-1] == 'txt':\n",
    "            doc_locations.append(doc_dir+file)\n",
    "    return doc_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3df6bab1-7a89-41b7-9bea-56bd400b1ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(doc_locations):\n",
    "    raw_docs = []\n",
    "    for loc in doc_locations:\n",
    "        with open(loc, encoding='utf8') as f:\n",
    "            raw_docs.append('\\n'.join(f.readlines()))\n",
    "    print(f'There are {len(raw_docs)} documents')\n",
    "    return raw_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e302668-43c4-4ba3-ae51-4bc132f4c94b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1730 documents\n"
     ]
    }
   ],
   "source": [
    "doc_locations = find_docs()\n",
    "raw_docs = load_docs(doc_locations)\n",
    "os.chdir('next_word')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f75abd-f301-4bcd-95b2-5162c369c488",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tokenization and Cleaning\n",
    "\n",
    "Here the documents are tokenized into words and cleaned. Cleaning consists of:\n",
    "* Removing LateX\n",
    "* Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4367644f-9b7f-48e9-9990-003282558627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1730/1730 [04:18<00:00,  6.70it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_docs = [word_tokenize(doc) for doc in tqdm(raw_docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ac15898-9083-4d3b-b788-7e6c069ea18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Even', 'recognizing', 'some', 'early', 'modern', 'writings', 'on', 'the', 'emotions', 'for', 'what', 'they', 'are', 'is', 'no', 'easy', 'task', '.', 'In', 'part']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_docs[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5dddfa54-91ee-41d9-bbc3-22bbb5faaa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a token and a list of characters\n",
    "# Returns true if any of those characters is in the token\n",
    "# Otherwise returns false\n",
    "def clean_token(tok, bad_chars):\n",
    "    for char in bad_chars:\n",
    "        if char in tok:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Takes in a list of tokenized documents\n",
    "# Returns a list of lowercase tokens with LaTeX and punctuation removed\n",
    "def clean_docs(tokenized_docs):\n",
    "    flat_docs = []\n",
    "    bad_chars = string.punctuation\n",
    "    bad_chars = bad_chars.replace('-', '')\n",
    "    for doc in tqdm(tokenized_docs):\n",
    "        for token in doc:\n",
    "            token = token.lower()\n",
    "            if clean_token(token, bad_chars):\n",
    "                flat_docs.append(token)\n",
    "    return flat_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e4c0781-bc1c-4f41-82fe-25e4f899d1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1730/1730 [00:40<00:00, 42.44it/s]\n"
     ]
    }
   ],
   "source": [
    "clean_docs = clean_docs(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ed2007-e324-4b7b-b6df-ff68824bca37",
   "metadata": {},
   "source": [
    "# Vectorize Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a4508-3d33-4b85-a385-4dc17b4cb1ff",
   "metadata": {},
   "source": [
    "## Create Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9989977-b485-4bb5-bf90-53db30e0cab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has 206356 words.\n",
      "Total text length is 19667516 tokens.\n"
     ]
    }
   ],
   "source": [
    "id2word = Dictionary([clean_docs])\n",
    "vectorized_docs = id2word.doc2idx(clean_docs)\n",
    "vocab_size = len(id2word.keys())\n",
    "print(f'Vocabulary has {vocab_size} words.')\n",
    "print(f'Total text length is {len(vectorized_docs)} tokens.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efe51f1-34a4-467f-a115-0c9fc4e06e4d",
   "metadata": {},
   "source": [
    "## Split into Inputs and Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4cb6eec6-f372-499f-8d1c-869910665f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 5\n",
    "\n",
    "def make_inputs_targets(vectorized_docs, context_size):\n",
    "    targets = []\n",
    "    inputs = []\n",
    "    for i in range(len(vectorized_docs) - context_size):\n",
    "        inputs.append(vectorized_docs[i:i+context_size])\n",
    "        targets.append(vectorized_docs[i+context_size])\n",
    "    return (np.asarray(inputs), np.asarray(targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5738fa68-399e-41b0-9219-4beae5f2c65d",
   "metadata": {},
   "source": [
    "# GloVe Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08af3a1e-a067-4335-b41a-2a6d49b5350a",
   "metadata": {},
   "source": [
    "## Embedding Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1938dfa-03ea-4be4-a6b0-6e314ad6a492",
   "metadata": {},
   "source": [
    "### Load the Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9dd2b704-12b7-4809-9899-725ebd04e3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52310it [00:08, 7151.38it/s]C:\\Users\\Patrick\\AppData\\Local\\Temp/ipykernel_4228/4136429660.py:8: DeprecationWarning: string or file could not be read to its end due to unmatched data; this will raise a ValueError in the future.\n",
      "  vec = np.fromstring(vec, sep=' ')\n",
      "2196017it [09:11, 3985.14it/s]\n"
     ]
    }
   ],
   "source": [
    "#path_to_glove = 'glove.6B.100d.txt'\n",
    "path_to_glove = 'glove.840B.300d.txt'\n",
    "\n",
    "embed_index = {}\n",
    "with open(path_to_glove, encoding='utf8') as f:\n",
    "    for line in tqdm(f):\n",
    "        word, vec = line.split(maxsplit=1)\n",
    "        vec = np.fromstring(vec, sep=' ')\n",
    "        embed_index[word] = vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c7a36c-893c-4850-8543-7298b9f59e5c",
   "metadata": {},
   "source": [
    "### Create the Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f6c30f4a-0f1d-40fc-9e67-b6f2fc7e0136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 206356/206356 [00:22<00:00, 8995.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found vectors for 84714 words.\n",
      "Count not find vectors for 121642 words.\n",
      "In total found vectors for 0.410523561224292% of the words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "hits = 0\n",
    "misses = 0\n",
    "missing = []\n",
    "\n",
    "vocab_words = list(id2word.items())\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for i, tok in tqdm(vocab_words):\n",
    "    embedding_vector = embed_index.get(tok)\n",
    "    if embedding_vector is not None and embedding_vector.shape[0] == 300:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        missing.append(tok)\n",
    "        misses += 1\n",
    "\n",
    "print(f'Found vectors for {hits} words.')\n",
    "print(f'Count not find vectors for {misses} words.')\n",
    "print(f'In total found vectors for {hits/vocab_size}% of the words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7a83b8c0-de05-4e37-aa59-43385cd7efbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "𝕍m\n"
     ]
    }
   ],
   "source": [
    "print(missing[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a44f88-0d02-4084-9b8c-bc3574aada27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
